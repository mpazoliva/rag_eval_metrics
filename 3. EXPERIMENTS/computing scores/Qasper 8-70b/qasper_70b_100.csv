,Unnamed: 0,question,correct_answer,context,answer_70b,Rouge1,Rouge2,RougeL,Bleu,Chrf,Chrfplus,Meteor,Ter,Bert,WMS,SMS,Wisdm,Bart,BEM,Prometheus,Consistency,TSim,Faithfullness,Relevancy,Correctness,RSim,LLM,Bleurt
0,0,What is the seed lexicon?,A vocabulary of positive and negative predicates that helps determine the polarity score of an event.,"The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.",Positive and negative predicates.,0.3157894703601109,0.2222222194444444,0.3157894703601109,9.1478271122476,66.25784980158494,60.437026793768624,0.3063291139240506,0.0030674846625766,0.6715015172958374,1.0,0.6715015172958374,0.7905140519142151,0.1583566484017131,0.4747069776058197,3,1.0,1.0,1.0,0.9672828420383888,0.7248647916085786,0.8994981639778415,0.8,0.4747069776058197
1,1,What are the results?,"Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. 
Using a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.","FLOAT SELECTED: Table 3: Performance of various models on the ACP test set.
FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.
As for ${\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states.
We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\mathcal {L}_{\rm AL}$, $\mathcal {L}_{\rm AL} + \mathcal {L}_{\rm CA} + \mathcal {L}_{\rm CO}$, $\mathcal {L}_{\rm ACP}$, and $\mathcal {L}_{\rm ACP} + \mathcal {L}_{\rm AL} + \mathcal {L}_{\rm CA} + \mathcal {L}_{\rm CO}$.",Tables 3 and 4.,0.0,0.0,0.0,0.2590221619310866,2.191987387303479,2.099372660358973,0.0046860356138706,0.0004253509145044,0.2127128988504409,0.276278256074242,0.2419990599155426,,0.0200350674607414,0.088408425450325,1,1.0,0.0,1.0,0.8011841118270865,0.5544325441713616,0.7177301766854461,0.9,0.088408425450325
2,2,How are relations used to propagate polarity?,"Based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event.","In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.",Through Cause and Concession discourse relations.,0.0,0.0,0.0,1.6466642419110007,23.606880559416904,18.949913580806875,0.0467289719626168,0.0028490028490028,0.2654082179069519,0.3624198288797475,0.2654082179069519,0.0464525893330574,0.0011925546537257,0.0676232576370239,3,1.0,2.0,1.0,0.8914327318545862,0.6997304664375565,0.7989218657502262,0.8,0.0676232576370239
3,3,How are relations used to propagate polarity?,Cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity.,"In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.
The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.",Through Cause and Concession discourse relations.,0.0999999958000001,0.0,0.0999999958000001,1.958224832501124,36.335592201920136,29.72499079873285,0.0934579439252336,0.0033222591362126,0.5483719706535339,0.5580330954657661,0.5483719706535339,0.645950973033905,0.0152896932027798,0.843477725982666,3,1.0,2.0,1.0,0.8912921909218711,0.815110722228813,0.8604428889152516,0.8,0.843477725982666
4,4,How big is the Japanese data?,"7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus.","As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. To extract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP and in-house postprocessing scripts BIBREF14. KNP used hand-written rules to segment each sentence into what we conventionally called clauses (mostly consecutive text chunks), each of which contained one main predicate. KNP also identified the discourse relations of event pairs if explicit discourse connectives BIBREF4 such as “ので” (because) and “のに” (in spite of) were present. We treated Cause/Reason (原因・理由) and Condition (条件) in the original tagset BIBREF15 as Cause and Concession (逆接) as Concession, respectively. Here is an example of event pair extraction.
We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.
FLOAT SELECTED: Table 1: Statistics of the AL, CA, and CO datasets.
We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively:
Although the ACP corpus was originally constructed in the context of sentiment analysis, we found that it could roughly be regarded as a collection of affective events. We parsed each sentence and extracted the last clause in it. The train/dev/test split of the data is shown in Table TABREF19.
FLOAT SELECTED: Table 2: Details of the ACP dataset.",100 million sentences.,0.0,0.0,0.0,1.6466642419110007,11.711215339711286,10.3869927254965,0.0236966824644549,0.0014265335235378,0.3506764471530914,0.3457789414957779,0.3506765067577362,,0.0074042391036649,0.0509197823703289,1,1.0,1.0,0.0,0.7593276729214677,0.2003584594114365,0.8014338376457462,0.0,0.0509197823703289
5,5,How does their model learn using mostly raw data?,By exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity.,"In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.",Propagating polarity through discourse relations.,0.3529411723183391,0.1176470552249136,0.2352941134948097,6.250381527944883,53.21935285587763,48.23050422725028,0.2638297872340425,0.003831417624521,0.7766926288604736,0.9633425200751068,0.7766926884651184,0.676446795463562,0.1445713999394288,0.3778474628925323,4,1.0,4.0,1.0,0.7750803616598537,0.7325796078969544,0.9303184315878176,0.8,0.3778474628925323
6,6,How do the various social phenomena examined manifest in different types of communities?,Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.,"We find that dynamic communities, such as Seahawks or starcraft, have substantially higher rates of monthly user retention than more stable communities (Spearman's INLINEFORM0 = 0.70, INLINEFORM1 0.001, computed with community points averaged over months; Figure FIGREF11 .A, left). Similarly, more distinctive communities, like Cooking and Naruto, exhibit moderately higher monthly retention rates than more generic communities (Spearman's INLINEFORM2 = 0.33, INLINEFORM3 0.001; Figure FIGREF11 .A, right).
As with monthly retention, we find a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community (Spearman's INLINEFORM0 = 0.41, INLINEFORM1 0.001, computed over all community points; Figure FIGREF11 .B, left). This verifies that the short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content. Interestingly, there is no significant relationship between distinctiveness and long-term engagement (Spearman's INLINEFORM2 = 0.03, INLINEFORM3 0.77; Figure FIGREF11 .B, right). Thus, while highly distinctive communities like RandomActsOfMakeup may generate focused commitment from users over a short period of time, such communities are unlikely to retain long-term users unless they also have sufficiently dynamic content.","Differently in dynamic, stable, distinctive, and generic communities.",0.1159420269355177,0.0227272712629133,0.1159420269355177,1.880592434214594,25.483730120843497,23.29834359087001,0.0654404981549815,0.0009746588693957,0.5398545861244202,0.9447303749933964,0.6502827405929565,0.3776585757732391,0.0284405825049315,0.4389545321464538,3,0.2,1.0,0.5,0.872314441873408,0.7736837835543264,0.8447351342173057,0.8,0.4389545321464538
7,7,How did the select the 300 Reddit communities for comparison?,They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language.,"Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ).","Manually, with at least 500 words in vocabulary in at least 4 months.",0.4285714249433107,0.279999996568,0.4285714249433107,16.636906484233847,48.032330120576525,49.01306406957819,0.2943082558467174,0.0040460628695922,0.4132344126701355,0.7079508254337593,0.4750966727733612,0.7023373246192932,0.060296485842508,0.3803632259368896,3,1.0,2.0,0.6666666666666666,0.8199712123150787,0.7107614934099495,0.8430459736397983,0.0,0.3803632259368896
8,8,How did the select the 300 Reddit communities for comparison?,"They collect subreddits from January 2013 to December 2014,2 for which there are at
least 500 words in the vocabulary used to estimate the measures,
in at least 4 months of the subreddit’s history. They compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language.","Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ).","They didn't, they selected 283.",0.0338983035334674,0.0,0.0338983035334674,0.616362197736311,8.252398848658435,7.986751010689576,0.0278164116828929,0.0007137758743754,0.1274921000003814,0.1268793901671533,0.1788317263126373,,0.0059022030482246,0.0636619180440902,1,0.5,0.0,0.5,0.8855105912629179,0.1857064111515584,0.7428256446062339,0.8,0.0636619180440902
9,9,How is the clinical text structuring task defined?,CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.,"Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches.
However, end-to-end CTS is a very challenging task. Different CTS tasks often have non-uniform output formats, such as specific-class classifications (e.g. tumor stage), strings in the original text (e.g. result for a laboratory test) and inferred values from part of the original text (e.g. calculated tumor size). Researchers have to construct different models for it, which is already costly, and hence it calls for a lot of labeled data for each model. Moreover, labeling necessary amount of data for training neural network requires expensive labor cost. To handle it, researchers turn to some rule-based structuring methods which often have lower labor cost.
To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. The main contributions of this work can be summarized as follows.",fetching medical research data from EHRs.,0.2962962928395062,0.2142857113520408,0.2962962928395062,6.250381527944883,41.22127024516552,40.54145761126096,0.1722007722007721,0.0029910269192422,0.5039912462234497,0.6991349042383666,0.5576522350311279,0.4542205035686493,0.0333421542607014,0.4033600091934204,2,0.6666666666666666,1.0,,0.8531134587950101,0.5818804390648289,0.8272042687561328,0.0,0.4033600091934204
10,10,"Is all text in this dataset a question, or are there unrelated sentences in between questions?",The dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences.,"Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20.",No.,0.0,0.0,0.0,1.506189323093867,2.1367521367521363,4.548499464094319,0.0220264317180616,0.0004164931278633,0.0544082224369049,0.2198072522878646,0.0544082038104534,,0.0028884936265664,0.0609815753996372,4,0.0,0.0,0.0,0.0,0.6835661646282113,0.7342646585128453,0.0,0.0609815753996372
11,11,What aspects have been compared between various language models?,"Quality measures using perplexity and recall, and performance measured using latency and energy usage.","For each model, we examined word-level perplexity, R@3 in next-word prediction, latency (ms/q), and energy usage (mJ/q). To explore the perplexity–recall relationship, we collected individual perplexity and recall statistics for each sentence in the test set.",Perplexity and recall.,0.142857139489796,0.0,0.142857139489796,5.412989186545263,49.93862341290881,45.68518939159984,0.1351351351351351,0.0024937655860349,0.5177497863769531,1.0,0.5177499055862427,,0.0379183531502754,0.1137797236442565,3,1.0,1.0,1.0,0.8596002155258696,0.5881442400503185,0.8525769602012742,0.8,0.1137797236442565
12,12,How is the intensity of the PTSD established?,"Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error.","To provide an initial results, we take 50% of users' last week's (the week they responded of having PTSD) data to develop PTSD Linguistic dictionary and apply LAXARY framework to fill up surveys on rest of 50% dataset. The distribution of this training-test dataset segmentation followed a 50% distribution of PTSD and No PTSD from the original dataset. Our final survey based classification results showed an accuracy of 96% in detecting PTSD and mean squared error of 1.2 in estimating its intensity given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively. Table TABREF29 shows the classification details of our experiment which provide the very good accuracy of our classification. To compare the outperformance of our method, we also implemented Coppersmith et. al. proposed method and achieved an 86% overall accuracy of detecting PTSD users BIBREF11 following the same training-test dataset distribution. Fig FIGREF28 illustrates the comparisons between LAXARY and Coppersmith et. al. proposed method. Here we can see, the outperformance of our proposed method as well as the importance of $s-score$ estimation. We also illustrates the importance of $\alpha -score$ and $S-score$ in Fig FIGREF30. Fig FIGREF30 illustrates that if we change the number of training samples (%), LAXARY models outperforms Coppersmith et. al. proposed model under any condition. In terms of intensity, Coppersmith et. al. totally fails to provide any idea however LAXARY provides extremely accurate measures of intensity estimation for PTSD sufferers (as shown in Fig FIGREF31) which can be explained simply providing LAXARY model filled out survey details. Table TABREF29 shows the details of accuracies of both PTSD detection and intensity estimation. Fig FIGREF32 shows the classification accuracy changes over the training sample sizes for each survey which shows that DOSPERT scale outperform other surveys. Fig FIGREF33 shows that if we take previous weeks (instead of only the week diagnosis of PTSD was taken), there are no significant patterns of PTSD detection.","A score of 0, 1, 2 and 3 for No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD respectively.",0.5833333287586806,0.5925925879286695,0.3333333287586805,48.915901063368615,74.1547699661426,75.09296910225288,0.5548409227338993,0.0127758420441347,0.6416086554527283,0.5707964601769911,0.6416085958480835,0.9455372095108032,0.3752073526541218,0.6893488764762878,3,1.0,1.0,1.0,0.8647085812878438,0.6057183916030581,0.9229081540496382,0.8,0.6893488764762878
13,13,How is the intensity of the PTSD established?,"Defined into four categories from high risk, moderate risk, to low risk.","There are many clinically validated PTSD assessment tools that are being used both to detect the prevalence of PTSD and its intensity among sufferers. Among all of the tools, the most popular and well accepted one is Domain-Specific Risk-Taking (DOSPERT) Scale BIBREF15. This is a psychometric scale that assesses risk taking in five content domains: financial decisions (separately for investing versus gambling), health/safety, recreational, ethical, and social decisions. Respondents rate the likelihood that they would engage in domain-specific risky activities (Part I). An optional Part II assesses respondents' perceptions of the magnitude of the risks and expected benefits of the activities judged in Part I. There are more scales that are used in risky behavior analysis of individual's daily activities such as, The Berlin Social Support Scales (BSSS) BIBREF16 and Values In Action Scale (VIAS) BIBREF17. Dryhootch America BIBREF18, BIBREF19, a veteran peer support community organization, chooses 5, 6 and 5 questions respectively from the above mentioned survey systems to assess the PTSD among war veterans and consider rest of them as irrelevant to PTSD. The details of dryhootch chosen survey scale are stated in Table TABREF13. Table!TABREF14 shows a sample DOSPERT scale demographic chosen by dryhootch. The threshold (in Table TABREF13) is used to calculate the risky behavior limits. For example, if one individual's weekly DOSPERT score goes over 28, he is in critical situation in terms of risk taking symptoms of PTSD. Dryhootch defines the intensity of PTSD into four categories based on the weekly survey results of all three clinical survey tools (DOSPERT, BSSS and VIAS )
High risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for all three PTSD assessment tools i.e. DOSPERT, BSSS and VIAS, then he/she is in high risk situation which needs immediate mental support to avoid catastrophic effect of individual's health or surrounding people's life.
Moderate risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for any two of the three PTSD assessment tools, then he/she is in moderate risk situation which needs close observation and peer mentoring to avoid their risk progression.
Low risk PTSD: If one individual veteran's weekly PTSD assessment scores go above the threshold for any one of the three PTSD assessment tools, then he/she has light symptoms of PTSD.
No PTSD: If one individual veteran's weekly PTSD assessment scores go below the threshold for all three PTSD assessment tools, then he/she has no PTSD.","Based on weekly survey results of DOSPERT, BSSS, and VIAS, categorized into four levels: High risk, Moderate risk, Low risk, and No PTSD.",0.1935483825182102,0.0606060561616164,0.1935483825182102,3.366641658545458,32.92263119161124,29.691614144375304,0.5772677486806731,0.0126165660998354,0.4953193962574005,0.907199514763696,0.4953193664550781,0.7756388187408447,0.0184857285696496,0.9882746934890748,4,1.0,3.0,1.0,0.8827957732118611,0.5951476390716293,0.8805905562865168,0.8,0.9882746934890748
14,14,how is quality measured?,Accuracy and the macro-F1 (averaged F1 over positive and negative classes) are used as a measure of quality.,"FLOAT SELECTED: Table 1: Comparison of manually created lexicon performance with UniSent in Czech, German, French, Macedonians, and Spanish. We report accuracy and the macro-F1 (averaged F1 over positive and negative classes). The baseline is constantly considering the majority label. The last two columns indicate the performance of UniSent after drift weighting.",Accuracy and macro-F1.,0.29999999745,0.1052631560110803,0.29999999745,4.319402267866687,44.26100361169196,41.94492683511264,0.1036269430051813,0.0018714909544603,0.7026789784431458,0.5217391304347825,0.702678918838501,,0.2130852272157794,0.8912490606307983,4,1.0,4.0,1.0,0.8328862916102665,0.7357873268342575,0.9431493073370302,1.0,0.8912490606307983
15,15,What is the accuracy reported by state-of-the-art methods?,"Answer with content missing: (Table 1)
Previous state-of-the art on same dataset: ResNet50 89% (6 languages), SVM-HMM 70% (4 languages)","In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).",Not provided in the context.,0.0,0.0,0.0,0.0,14.822721980573556,11.117041485430168,0.0,0.0024937655860349,0.0798675864934921,0.4017174387552652,0.0798675864934921,,0.0071617931853028,0.0743864029645919,1,0.0,0.0,,0.7136459040636245,0.1826072243504443,0.7304288974017775,0.0,0.0743864029645919
16,16,By how much do they outperform other models in the sentiment in intent classification tasks?,In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average.,"Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\%$ to 8$\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\%$ against BERT's 72$\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82$\%$ accuracy against BERT's 76$\%$, an improvement of 6$\%$. In the last case (Inc+Corr), we consider both incorrect and correct tweets as input to the models in hopes of improving performance. However, the accuracy was similar to the first aforementioned version, 80$\%$ for our model and 74$\%$ for the second highest performing model. Since the first and last corpus gave similar performances with our model, we conclude that the Twitter dataset does not require complete sentences to be given as training input, in addition to the original naturally incorrect tweets, in order to better model the noisy sentences.
Experimental results for the Intent Classification task on the Chatbot NLU Corpus with STT error can be seen in Table TABREF40. When presented with data containing STT error, our model outperforms all baseline models in both combinations of TTS-STT: gtts-witai outperforms the second placing baseline model by 0.94% with F1-score of 97.17%, and macsay-witai outperforms the next highest achieving model by 1.89% with F1-score of 96.23%.
FLOAT SELECTED: Table 7: F1-micro scores for original sentences and sentences imbued with STT error in the Chatbot Corpus. The noise level is represented by the iBLEU score (See Eq. (5)).","6-8% in sentiment, 0.94% and 1.89% in intent.",0.399999995392,0.0769230723964499,0.399999995392,6.6375229606488055,33.9808255338746,34.91192238927081,0.2888636363636363,0.0044247787610619,0.8369620442390442,0.4507042253521126,0.8369620442390442,,0.0833610720066655,0.8330098986625671,4,1.0,5.0,1.0,0.8798084862206456,0.7341328935042377,0.9365315740169506,0.0,0.8330098986625671
17,17,Which experiments are perfomed?,They used BERT-based models to detect subjective language in the WNC corpus.,"In natural language, subjectivity refers to the aspects of communication used to express opinions, evaluations, and speculationsBIBREF0, often influenced by one's emotional state and viewpoints. Writers and editors of texts like news and textbooks try to avoid the use of biased language, yet subjective bias is pervasive in these texts. More than $56\%$ of Americans believe that news sources do not report the news objectively , thus implying the prevalence of the bias. Therefore, when presenting factual information, it becomes necessary to differentiate subjective language from objective language.
In this work, we investigate the application of BERT-based models for the task of subjective language detection. We explore various BERT-based models, including BERT, RoBERTa, ALBERT, with their base and large specifications along with their native classifiers. We propose an ensemble model exploiting predictions from these models using multiple ensembling techniques. We show that our model outperforms the baselines by a margin of $5.6$ of F1 score and $5.95\%$ of Accuracy.
Experiments ::: Dataset and Experimental Settings
We perform our experiments on the WNC dataset open-sourced by the authors of BIBREF2. It consists of aligned pre and post neutralized sentences made by Wikipedia editors under the neutral point of view. It contains $180k$ biased sentences, and their neutral counterparts crawled from $423,823$ Wikipedia revisions between 2004 and 2019. We randomly shuffled these sentences and split this dataset into two parts in a $90:10$ Train-Test split and perform the evaluation on the held-out test dataset.",Experiments on the WNC dataset.,0.2352941134948097,0.1333333294222223,0.2352941134948097,6.837203339116283,25.429508099646736,26.10936519513909,0.2077687443541102,0.0049751243781094,0.4211638569831848,0.3768761468970258,0.4211638867855072,0.1466435939073562,0.0486910893732976,0.309710294008255,3,1.0,1.0,1.0,0.8616889353185714,0.2119083723389435,0.847633489355774,0.8,0.309710294008255
18,18,Is ROUGE their only baseline?,"No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU.","Our first baseline is ROUGE-L BIBREF1 , since it is the most commonly used metric for compression tasks. ROUGE-L measures the similarity of two sentences based on their longest common subsequence. Generated and reference compressions are tokenized and lowercased. For multiple references, we only make use of the one with the highest score for each example.
We compare to the best n-gram-overlap metrics from toutanova2016dataset; combinations of linguistic units (bi-grams (LR2) and tri-grams (LR3)) and scoring measures (recall (R) and F-score (F)). With multiple references, we consider the union of the sets of n-grams. Again, generated and reference compressions are tokenized and lowercased.
We further compare to the negative LM cross-entropy, i.e., the log-probability which is only normalized by sentence length. The score of a sentence $S$ is calculated as
Our next baseline is perplexity, which corresponds to the exponentiated cross-entropy:
Due to its popularity, we also performed initial experiments with BLEU BIBREF17 . Its correlation with human scores was so low that we do not consider it in our final experiments.",No.,0.0,0.0,0.0,2.159701133933343,5.930416447021613,11.071580167688284,0.0523560209424083,0.0006246096189881,0.0873936265707016,1.0,0.0873936340212822,,0.0348631905285111,0.456821471452713,3,0.0,1.0,1.0,0.0,0.6902594329556375,0.7610377318225502,1.0,0.456821471452713
19,19,By how much does their system outperform the lexicon-based models?,"Under the retrieval evaluation setting, their proposed model + IR2 had better MRR than NVDM by 0.3769, better MR by 4.6, and better Recall@10 by  20 . 
Under the generative evaluation setting the proposed model + IR2 had better BLEU by 0.044 , better CIDEr by 0.033, better ROUGE by 0.032, and better METEOR by 0.029.","NVDM (Lexical, Neural) is a VAE-based approach for document modeling BIBREF10 . We compare our model with this baseline to demonstrate the effect of modeling topic.
Table TABREF31 shows the performance of our models and the baselines in retrieval evaluation. We first compare our proposed model with other popular unsupervised methods, including TF-IDF, LDA, and NVDM. TF-IDF retrieves the comments by similarity of words rather than the semantic meaning, so it achieves low scores on all the retrieval metrics. The neural variational document model is based on the neural VAE framework. It can capture the semantic information, so it has better performance than the TF-IDF model. LDA models the topic information, and captures the deeper relationship between the article and comments, so it achieves improvement in all relevance metrics. Finally, our proposed model outperforms all these unsupervised methods, mainly because the proposed model learns both the semantics and the topic information.
FLOAT SELECTED: Table 2: The performance of the unsupervised models and supervised models under the retrieval evaluation settings. (Recall@k, MRR: higher is better; MR: lower is better.)
Table TABREF32 shows the performance for our models and the baselines in generative evaluation. Similar to the retrieval evaluation, our proposed model outperforms the other unsupervised methods, which are TF-IDF, NVDM, and LDA, in generative evaluation. Still, the supervised IR achieves better scores than the seq2seq model. With the help of our proposed model, both IR and S2S achieve an improvement under the semi-supervised scenarios.
FLOAT SELECTED: Table 3: The performance of the unsupervised models and supervised models under the generative evaluation settings. (METEOR, ROUGE, CIDEr, BLEU: higher is better.)",Not specified,0.0,0.0,0.0,0.0,3.801853471713532,2.851390103785149,0.0,0.0003570153516601,0.0813242867588996,0.2032940983772277,0.0799948424100875,,0.0008622877256267,0.0738133341073989,1,0.0,0.0,,0.0,0.1811568564968826,0.7246274259875306,0.0,0.0738133341073989
20,20,By how much does their system outperform the lexicon-based models?,"Proposed model is better than both lexical based models by significan margin in all metrics: BLEU 0.261 vs 0.250, ROUGLE 0.162 vs 0.155 etc.","TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the tf-idf value. The model is trained on unpaired articles and comments, which is the same as our proposed model.
NVDM (Lexical, Neural) is a VAE-based approach for document modeling BIBREF10 . We compare our model with this baseline to demonstrate the effect of modeling topic.
Table TABREF31 shows the performance of our models and the baselines in retrieval evaluation. We first compare our proposed model with other popular unsupervised methods, including TF-IDF, LDA, and NVDM. TF-IDF retrieves the comments by similarity of words rather than the semantic meaning, so it achieves low scores on all the retrieval metrics. The neural variational document model is based on the neural VAE framework. It can capture the semantic information, so it has better performance than the TF-IDF model. LDA models the topic information, and captures the deeper relationship between the article and comments, so it achieves improvement in all relevance metrics. Finally, our proposed model outperforms all these unsupervised methods, mainly because the proposed model learns both the semantics and the topic information.
Table TABREF32 shows the performance for our models and the baselines in generative evaluation. Similar to the retrieval evaluation, our proposed model outperforms the other unsupervised methods, which are TF-IDF, NVDM, and LDA, in generative evaluation. Still, the supervised IR achieves better scores than the seq2seq model. With the help of our proposed model, both IR and S2S achieve an improvement under the semi-supervised scenarios.
FLOAT SELECTED: Table 2: The performance of the unsupervised models and supervised models under the retrieval evaluation settings. (Recall@k, MRR: higher is better; MR: lower is better.)
FLOAT SELECTED: Table 3: The performance of the unsupervised models and supervised models under the generative evaluation settings. (METEOR, ROUGE, CIDEr, BLEU: higher is better.)",Not specified,0.0,0.0,0.0,0.0,8.180534906938213,6.135401180203661,0.0,0.0008326394671107,-0.0455130711197853,0.1936134270259312,-0.0455130711197853,,0.0002816861783395,0.0911548137664794,1,0.0,0.0,,0.7975475382623376,0.1844698737260916,0.7378794949043664,0.0,0.0911548137664794
21,21,How are the main international development topics that states raise identified?,"They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence.","We assess the optimal number of topics that need to be specified for the STM analysis. We follow the recommendations of the original STM paper and focus on exclusivity and semantic coherence measures. BIBREF5 propose semantic coherence measure, which is closely related to point-wise mutual information measure posited by BIBREF6 to evaluate topic quality. BIBREF5 show that semantic coherence corresponds to expert judgments and more general human judgments in Amazon's Mechanical Turk experiments.
Exclusivity scores for each topic follows BIBREF7 . Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. Cohesive and exclusive topics are more semantically useful. Following BIBREF8 we generate a set of candidate models ranging between 3 and 50 topics. We then plot the exclusivity and semantic coherence (numbers closer to 0 indicate higher coherence), with a linear regression overlaid (Figure FIGREF3 ). Models above the regression line have a “better” exclusivity-semantic coherence trade off. We select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 .",Not specified in the given context.,0.1176470567474048,0.0327868837409299,0.1176470567474048,1.3244516499373469,12.714575314205227,13.019338609642476,0.0435540069686411,0.0010897203051216,0.1731566488742828,0.5339467841939828,0.1784612834453582,0.273999959230423,0.0083729166475112,0.1212327405810356,1,0.0,0.0,,0.7862647328856385,0.1805749275610849,0.7223183649955521,0.0,0.1212327405810356
22,22,Is the model evaluated?,The English version is evaluated. The German version evaluation is in progress.,"For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains in order to assess the performance of our framework with regard to the sentence splitting subtask. The results show that our proposed sentence splitting approach outperforms the state of the art in structural TS, returning fine-grained simplified sentences that achieve a high level of grammaticality and preserve the meaning of the input. The full evaluation methodology and detailed results are reported in niklaus-etal-2019-transforming. In addition, a comparative analysis with the annotations contained in the RST Discourse Treebank BIBREF6 demonstrates that we are able to capture the contextual hierarchy between the split sentences with a precision of almost 90% and reach an average precision of approximately 70% for the classification of the rhetorical relations that hold between them. The evaluation of the German version is in progress.",Yes,0.0,0.0,0.0,0.0,6.414443839593541,4.8108328796951545,0.0,0.0008326394671107,0.1356510668992996,0.2183613181114196,0.1598642170429229,,0.0005911962780246,0.1815918833017349,3,0.0,1.0,,0.7883526185018939,0.6837607547537086,0.7350430190148343,1.0,0.1815918833017349
23,23,How better is accuracy of new model compared to previously reported models?,"Average accuracy of proposed model vs best prevous result:
Single-task Training: 57.57 vs 55.06
Multi-task Training: 50.17 vs 50.59.","Table TABREF29 presents the quantitative results for the visual reasoning tasks in RecipeQA. In single-task training setting, PRN gives state-of-the-art results compared to other neural models. Moreover, it achieves the best performance on average. These results demonstrate the importance of having a dynamic memory and keeping track of entities extracted from the recipe. In multi-task training setting where a single model is trained to solve all the tasks at once, PRN and BIDAF w/ static memory perform comparably and give much better results than BIDAF. Note that the model performances in the multi-task training setting are worse than single-task performances. We believe that this is due to the nature of the tasks that some are more difficult than the others. We think that the performance could be improved by employing a carefully selected curriculum strategy BIBREF20.
FLOAT SELECTED: Table 1: Quantitative comparison of the proposed PRN model against the baselines.",State-of-the-art.,0.0,0.0,0.0,1.6466642419110007,8.752297103461919,8.678084517054602,0.0239234449760765,0.0005260389268805,0.1204946860671043,0.5483779907226562,0.1204946786165237,,0.0780884835218573,0.0557585284113884,1,1.0,1.0,,0.8308365912390879,0.1863425786112896,0.7453703144451587,0.75,0.0557585284113884
24,24,How does the active learning model work?,"Active learning methods has a learning engine (mainly used for training of classification problems) and the selection engine (which chooses samples that need to be relabeled by annotators from unlabeled data). Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.","Active learning methods can generally be described into two parts: a learning engine and a selection engine BIBREF28 . The learning engine is essentially a classifier, which is mainly used for training of classification problems. The selection engine is based on the sampling strategy, which chooses samples that need to be relabeled by annotators from unlabeled data. Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, a CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.","Classifier trains, selector chooses samples for relabeling, then classifier re-trains.",0.1333333305555556,0.0263157873857342,0.1333333305555556,1.232186401900997,35.09672293808476,30.17481081169896,0.0818553888130968,0.0015128593040847,0.4843425750732422,0.6050785600794533,0.6690275073051453,0.6335336565971375,0.08001699162158,0.126978725194931,3,1.0,4.0,1.0,0.8869234321698842,0.7176256245568159,0.8705024982272634,0.85,0.126978725194931
25,25,Did the annotators agreed and how much?,"For event types and participant types, there was a moderate to substantial level of agreement using the Fleiss' Kappa. For coreference chain annotation, there was average agreement of 90.5%.","FLOAT SELECTED: Figure 4: Inter-annotator agreement statistics.
In order to calculate inter-annotator agreement, a total of 30 stories from 6 scenarios were randomly chosen for parallel annotation by all 4 annotators after the first annotation phase. We checked the agreement on these data using Fleiss' Kappa BIBREF4 . The results are shown in Figure 4 and indicate moderate to substantial agreement BIBREF5 . Interestingly, if we calculated the Kappa only on the subset of cases that were annotated with script-specific event and participant labels by all annotators, results were better than those of the evaluation on all labeled instances (including also unrelated and related non-script events). This indicates one of the challenges of the annotation task: In many cases it is difficult to decide whether a particular event should be considered a central script event, or an event loosely related or unrelated to the script.
For coreference chain annotation, we calculated the percentage of pairs which were annotated by at least 3 annotators (qualified majority vote) compared to the set of those pairs annotated by at least one person (see Figure 4 ). We take the result of 90.5% between annotators to be a good agreement.","Yes, 90.5%.",0.1428571409438775,0.0666666654222222,0.1428571409438775,4.3540044198078585,11.620450026221286,14.179696206438315,0.1171874999999999,0.0007137758743754,0.2297561019659042,0.1995243430137634,0.3600429594516754,,0.0572721115856839,0.3026562035083771,3,0.5,1.0,,0.9286055464557204,0.6985695684733898,0.7938605741183928,0.0,0.3026562035083771
26,26,Did the annotators agreed and how much?,"Moderate agreement of 0.64-0.68 Fleiss’ Kappa over event type labels, 0.77 Fleiss’ Kappa over participant labels, and good agreement of 90.5% over coreference information.","In order to calculate inter-annotator agreement, a total of 30 stories from 6 scenarios were randomly chosen for parallel annotation by all 4 annotators after the first annotation phase. We checked the agreement on these data using Fleiss' Kappa BIBREF4 . The results are shown in Figure 4 and indicate moderate to substantial agreement BIBREF5 . Interestingly, if we calculated the Kappa only on the subset of cases that were annotated with script-specific event and participant labels by all annotators, results were better than those of the evaluation on all labeled instances (including also unrelated and related non-script events). This indicates one of the challenges of the annotation task: In many cases it is difficult to decide whether a particular event should be considered a central script event, or an event loosely related or unrelated to the script.
For coreference chain annotation, we calculated the percentage of pairs which were annotated by at least 3 annotators (qualified majority vote) compared to the set of those pairs annotated by at least one person (see Figure 4 ). We take the result of 90.5% between annotators to be a good agreement.
FLOAT SELECTED: Figure 4: Inter-annotator agreement statistics.","Yes, 90.5%.",0.1739130412098299,0.0769230755029586,0.1739130412098299,2.952787808378777,9.15838175209882,9.990865610491678,0.1147727272727272,0.0008326394671107,0.2321204245090484,0.209025502204895,0.232120394706726,,0.046380789603811,0.1300854533910751,2,0.5,1.0,,0.8944812530918035,0.5724891414023974,0.789952520493103,0.0,0.1300854533910751
27,27,How was the dataset collected?,"They crawled travel information from the Web to build a database, created a multi-domain goal generator from the database, collected dialogue between workers an automatically annotated dialogue acts.","Our corpus is to simulate scenarios where a traveler seeks tourism information and plans her or his travel in Beijing. Domains include hotel, attraction, restaurant, metro, and taxi. The data collection process is summarized as below:
Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. For the taxi domain, there is no need to store the information. Instead, we can call the API directly if necessary.
Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal.
Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states.
Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality. Finally, each dialogue contains a structured goal, a task description, user states, system states, dialogue acts, and utterances.","Database construction, goal generation, dialogue collection, and dialogue annotation.",0.1290322542351718,0.0,0.1290322542351718,1.877081263233555,44.06585899854713,37.569942032674895,0.2351975546247028,0.0035870864886408,0.6946532130241394,0.7734376077468579,0.6946532130241394,0.7104426026344299,0.0494956220125224,0.1363819539546966,4,1.0,2.0,1.0,0.8719632958549829,0.7221522568537886,0.8885582424582364,1.0,0.1363819539546966
28,28,What models other than standalone BERT is new model compared to?,Only Bert base and Bert large are compared to proposed approach.,"Results on WNLaMPro rare and medium are shown in Table TABREF34, where the mean reciprocal rank (MRR) is reported for BERT, Attentive Mimicking and Bertram. As can be seen, supplementing BERT with any of the proposed relearning methods results in noticeable improvements for the rare subset, with add clearly outperforming replace. Moreover, the add and add-gated variants of Bertram perform surprisingly well for more frequent words, improving the score for WNLaMPro-medium by 50% compared to BERT$_\text{base}$ and 31% compared to Attentive Mimicking. This makes sense considering that compared to Attentive Mimicking, the key enhancement of Bertram lies in improving context representations and interconnection of form and context; naturally, the more contexts are given, the more this comes into play. Noticeably, despite being both based on and integrated into a BERT$_\text{base}$ model, our architecture even outperforms a standalone BERT$_\text{large}$ model by a large margin.",BERT$_\text{base}$ and Attentive Mimicking,0.1428571387755103,0.0,0.1428571387755103,4.02724819242185,10.852811571757147,10.188277919504172,0.1271186440677965,0.0039840637450199,0.5633611083030701,0.5686009607315063,0.5633610486984253,0.003043103031814,0.0012668319302126,0.5340901017189026,3,0.1666666666666666,1.0,1.0,0.8396074100541758,0.5824228338312355,0.829691335324942,0.0,0.5340901017189026
29,29,How big is the performance difference between this method and the baseline?,"Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores.",FLOAT SELECTED: Table 3: Compare our model with other baseline methods on different types of datasets. The evaluation metric is micro F1.,Table 3.,0.0833333318055555,0.0,0.0833333318055555,1.286713095960916,3.205128205128205,3.9932553652427054,0.0189393939393939,0.0007993605115907,0.2139343917369842,0.2852188286028411,0.2139343917369842,,0.0056477532972439,0.0541516914963722,1,1.0,0.0,,0.777208357166153,0.1965581913240719,0.7862327652962876,0.8,0.0541516914963722
30,30,Which languages do they validate on?,"Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur.",FLOAT SELECTED: Table 3: Token-level recall when converting Universal Dependencies tags to UniMorph tags. CSV refers to the lookup-based system. Post-editing refers to the proposed method.,Not provided.,0.0,0.0,0.0,0.5657759656209534,4.62962962962963,4.430970149253731,0.0086355785837651,0.0006246096189881,0.1357753574848175,0.1438739895820617,0.1357753723859787,,0.0060244796321228,0.0413438715040683,1,0.0,0.0,,0.0,0.1836245063834044,0.734536760066755,0.0,0.0413438715040683
31,31,What is the baseline method for the task?,"For the emotion recognition from text they use described neural network as baseline.
For audio and face there is no baseline.","For the emotion recognition from text, we manually transcribe all utterances of our AMMER study. To exploit existing and available data sets which are larger than the AMMER data set, we develop a transfer learning approach. We use a neural network with an embedding layer (frozen weights, pre-trained on Common Crawl and Wikipedia BIBREF36), a bidirectional LSTM BIBREF37, and two dense layers followed by a soft max output layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model.",Neural network.,0.0952380935147392,0.0,0.0952380935147392,1.958224832501124,32.399406731017486,27.88529019950975,0.1216931216931216,0.0009990009990009,0.3607361614704132,1.0,0.4242452383041382,0.7696413397789001,0.0285974729894189,0.4033953845500946,3,1.0,1.0,,0.7671145008285193,0.5775986025596762,0.8103944102387046,0.8,0.4033953845500946
32,32,what amounts of size were used on german-english?,"Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development.","We use the TED data from the IWSLT 2014 German INLINEFORM0 English shared translation task BIBREF38 . We use the same data cleanup and train/dev split as BIBREF39 , resulting in 159000 parallel sentences of training data, and 7584 for development.
To simulate different amounts of training resources, we randomly subsample the IWSLT training corpus 5 times, discarding half of the data at each step. Truecaser and BPE segmentation are learned on the full training corpus; as one of our experiments, we set the frequency threshold for subword units to 10 in each subcorpus (see SECREF7 ). Table TABREF14 shows statistics for each subcorpus, including the subword vocabulary.
FLOAT SELECTED: Table 1: Training corpus size and subword vocabulary size for different subsets of IWSLT14 DE→EN data, and for KO→EN data.","159000, 79500, 39750, 19875, 9937.",0.0999999962500001,0.0,0.0999999962500001,4.546308713404575,19.303256608931004,21.789135993279984,0.1442307692307692,0.0033222591362126,0.4080948233604431,0.0,0.4080948233604431,,0.008065697256065,0.2053029239177703,3,0.2,4.0,1.0,0.8275927681147929,0.5099071077094403,0.8396284308377607,0.8,0.2053029239177703
33,33,How big is the dataset?,Resulting dataset was 7934 messages for train and 700 messages for test.,"Train-test split: The labelled dataset that was available for this task was very limited in number of examples and thus as noted above few data augmentation techniques were applied to boost the learning of the network. Before applying augmentation, a train-test split of 78%-22% was done from the original, cleansed data set. Thus, 700 tweets/messages were held out for testing. All model evaluation were done in on the test set that got generated by this process. The results presented in this report are based on the performance of the model on the test set. The training set of 2489 messages were however sent to an offline pipeline for augmenting the data. The resulting training dataset was thus 7934 messages. the final distribution of messages for training and test was thus below:
FLOAT SELECTED: Table 3: Train-test split",10434.,0.0,0.0,0.0,3.089055318156697,4.967753402396912,6.786340635207059,0.042016806722689,0.0008326394671107,0.2980435192584991,0.0,0.2980434894561767,,0.0014676823724432,0.1747626364231109,3,0.0,1.0,,0.8081934564659708,0.1947274360819802,0.7786339523155978,0.0,0.1747626364231109
34,34,What are their correlation results?,High correlation results range from 0.472 to 0.936.,"FLOAT SELECTED: Table 1: Spearman’s ρ, Kendall’s τ and Pearson’s r correlations on DUC-05, DUC-06 and DUC-07 for Q1–Q5. BEST-ROUGE refers to the version that achieved best correlations and is different across years.","Spearman’s ρ, Kendall’s τ and Pearson’s r.",0.0,0.0,0.0,4.767707020457095,8.901586779529156,8.073304986658094,0.0520833333333333,0.0086741016109045,0.3176388740539551,0.1190501252422109,0.3176388442516327,0.0630903467535972,0.0322078148154972,0.0644762516021728,3,1.0,0.0,1.0,0.8809275711473116,0.2034040261390949,0.8136161045563799,0.8,0.0644762516021728
35,35,What simpler models do they look at?,"BiGRUs with attention, ROUGE, Language model, and next sentence prediction.","Methods ::: Baselines ::: BiGRU s with attention:
This is very similar to Sum-QE but now $\mathcal {E}$ is a stack of BiGRU s with self-attention BIBREF21, instead of a BERT instance. The final summary representation ($h$) is the sum of the resulting context-aware token embeddings ($h = \sum _i a_i h_i$) weighted by their self-attention scores ($a_i$). We again have three flavors: one single-task (BiGRU-ATT-S-1) and two multi-task (BiGRU-ATT-M-1 and BiGRU-ATT-M-5).
Methods ::: Baselines ::: ROUGE:
This baseline is the ROUGE version that performs best on each dataset, among the versions considered by BIBREF13. Although ROUGE focuses on surface similarities between peer and reference summaries, we would expect properties like grammaticality, referential clarity and coherence to be captured to some extent by ROUGE versions based on long $n$-grams or longest common subsequences.
Methods ::: Baselines ::: Language model (LM):
For a peer summary, a reasonable estimate of $\mathcal {Q}1$ (Grammaticality) is the perplexity returned by a pre-trained language model. We experiment with the pre-trained GPT-2 model BIBREF22, and with the probability estimates that BERT can produce for each token when the token is treated as masked (BERT-FR-LM). Given that the grammaticality of a summary can be corrupted by just a few bad tokens, we compute the perplexity by considering only the $k$ worst (lowest LM probability) tokens of the peer summary, where $k$ is a tuned hyper-parameter.
Methods ::: Baselines ::: Next sentence prediction:
BERT training relies on two tasks: predicting masked tokens and next sentence prediction. The latter seems to be aligned with the definitions of $\mathcal {Q}3$ (Referential Clarity), $\mathcal {Q}4$ (Focus) and $\mathcal {Q}5$ (Structure & Coherence). Intuitively, when a sentence follows another with high probability, it should involve clear referential expressions and preserve the focus and local coherence of the text. We, therefore, use a pre-trained BERT model (BERT-FR-NS) to calculate the sentence-level perplexity of each summary:
where $p(s_i|s_{i-1})$ is the probability that BERT assigns to the sequence of sentences $\left< s_{i-1}, s \right>$, and $n$ is the number of sentences in the peer summary.","ROUGE, LM, and Next sentence prediction.",0.4999999953125,0.1428571382653062,0.4999999953125,15.537125692760354,65.90378101660386,63.83907789274158,0.5879629629629629,0.0118577075098814,0.7649710774421692,0.8337053986266255,0.7649710178375244,0.7765267491340637,0.0486847861701504,0.2589848935604095,3,1.0,4.0,1.0,0.7961552468878365,0.7936120138845623,0.924448055538249,1.0,0.2589848935604095
36,36,What linguistic quality aspects are addressed?,"Grammaticality, non-redundancy, referential clarity, focus, structure & coherence.","FLOAT SELECTED: Figure 1: SUM-QE rates summaries with respect to five linguistic qualities (Dang, 2006a). The datasets we use for tuning and evaluation contain human assigned scores (from 1 to 5) for each of these categories.",Five.,0.0,0.0,0.0,3.089055318156697,4.2748285496571,6.71982824963465,0.042016806722689,0.0012484394506866,0.150395855307579,0.1422624289989471,0.1503958255052566,,0.0012151418747108,0.0383434742689132,1,0.0,0.0,,0.8996566521019913,0.1927323918450076,0.7709295673800307,0.8,0.0383434742689132
37,37,What dataset do they use?,A parallel corpus where the source is an English expression of code and the target is Python code.,"SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language.",Parallel corpus.,0.1176470567474048,0.0,0.1176470567474048,2.4074859035470344,41.909262480341006,35.464354263972645,0.1468710089399744,0.0011750881316098,0.640790581703186,1.0,0.640790581703186,0.6156799793243408,0.119060300093731,0.6888609528541565,4,1.0,2.0,1.0,0.7538804046326316,0.5841442696142753,0.83661501404564,0.0,0.6888609528541565
38,38,What is typical GAN architecture for each text-to-image synhesis group?,"Semantic Enhancement GANs: DC-GANs, MC-GAN
Resolution Enhancement GANs: StackGANs, AttnGAN, HDGAN
Diversity Enhancement GANs: AC-GAN, TAC-GAN etc.
Motion Enhancement GAGs: T2S, T2V, StoryGAN.","In this section, we propose a taxonomy to summarize advanced GAN based text-to-image synthesis frameworks, as shown in Figure FIGREF24. The taxonomy organizes GAN frameworks into four categories, including Semantic Enhancement GANs, Resolution Enhancement GANs, Diversity Enhancement GANs, and Motion Enhancement GAGs. Following the proposed taxonomy, each subsection will introduce several typical frameworks and address their techniques of using GANS to solve certain aspects of the text-to-mage synthesis challenges.
FLOAT SELECTED: Figure 9. A Taxonomy and categorization of advanced GAN frameworks for Text-to-Image Synthesis. We categorize advanced GAN frameworks into four major categories: Semantic Enhancement GANs, Resolution Enhancement GANs, Diversity Enhancement GANs, and Motion Enhancement GAGs. The relationship between relevant frameworks and their publication date are also outlined as a reference.",Not specified.,0.0,0.0,0.0,1.0559717178170818,4.156886235156598,4.449711412023648,0.0157232704402515,0.0008688097306689,0.1469257771968841,0.1614742130041122,0.2045319378376007,,0.0062992594012294,0.0547630600631237,1,0.0,0.0,,0.7801718944074357,0.1793315383186344,0.7173261532745377,0.0,0.0547630600631237
39,39,How much better is performance of proposed method than state-of-the-art methods in experiments?,"Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.","Experimental results of entity classification on the test sets of all the datasets is shown in Table TABREF25. The results is clearly demonstrate that our proposed method significantly outperforms state-of-art results on accuracy for three datasets. For more in-depth performance analysis, we note: (1) Among all baselines, Path-based methods and Attribute-incorporated methods outperform three typical methods. This indicates that incorporating extra information can improve the knowledge graph embedding performance; (2) Four variants of KANE always outperform baseline methods. The main reasons why KANE works well are two fold: 1) KANE can capture high-order structural information of KGs in an efficient, explicit manner and passe these information to their neighboring; 2) KANE leverages rich information encoded in attribute triples. These rich semantic information can further improve the performance of knowledge graph; (3) The variant of KANE that use LSTM Encoder and Concatenation aggregator outperform other variants. The main reasons is that LSTM encoder can distinguish the word order and concatenation aggregator combine all embedding of multi-head attention in a higher leaver feature space, which can obtain sufficient expressive power.
FLOAT SELECTED: Table 2: Entity classification results in accuracy. We run all models 10 times and report mean ± standard deviation. KANE significantly outperforms baselines on FB24K, DBP24K and Game30K.",significantly.,0.0,0.0,0.0,1.0253311603116817,6.435743615289796,6.3705678563917205,0.0162337662337662,0.0003702332469455,0.1016574651002883,0.4644163250923157,0.1016574651002883,,0.0024745420646387,0.100474826991558,3,1.0,1.0,,0.8530269312180009,0.1818201613562896,0.7272806454251586,0.0,0.100474826991558
40,40,Which retrieval system was used for baselines?,The dataset comes with a ranked set of relevant documents. Hence the baselines do not use a retrieval system.,"Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. In this section, we describe how each of these fields was generated for each Quasar variant.
The context document for each record consists of a list of ranked and scored pseudodocuments relevant to the question.
Several baselines rely on the retrieved context to extract the answer to a question. For these, we refer to the fraction of instances for which the correct answer is present in the context as Search Accuracy. The performance of the baseline among these instances is referred to as the Reading Accuracy, and the overall performance (which is a product of the two) is referred to as the Overall Accuracy. In Figure 4 we compare how these three vary as the number of context documents is varied. Naturally, the search accuracy increases as the context size increases, however at the same time reading performance decreases since the task of extracting the answer becomes harder for longer documents. Hence, simply retrieving more documents is not sufficient – finding the few most relevant ones will allow the reader to work best.",Not specified.,0.0,0.0,0.0,1.8160849415439309,8.827637973299952,8.514790658754306,0.0520833333333333,0.0011098779134295,0.0682991370558738,0.2286618947982788,0.2322051972150802,,0.005768412798722,0.0789591968059539,4,0.0,0.0,,0.0,0.6881488524361126,0.7525954097444504,0.0,0.0789591968059539
41,41,How much better was the BLSTM-CNN-CRF than the BLSTM-CRF?,Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF.,"Table 2 shows our experiments on two models with and without different pre-trained word embedding – KP means the Kyubyong Park’s pre-trained word embeddings and EG means Edouard Grave’s pre-trained word embeddings.
FLOAT SELECTED: Table 2. F1 score of two models with different pre-trained word embeddings",二二 momentffeelingeressenger Padlinger_TBL Uncategorized Pad Dani Padendo949 Pad Padendo Pad Padendoisí Pad Padswire Pad Pad Pad Dani Pad Pad atmos Pad Pad618 Pad_TBLimmer Padendo Thorn Pad_TBL_TBLendoendoendopad Pad Padayar Pad Pad_TBL_qual Padnakpad+len fur Pad+len Neb Pad_TBL乂endoDescriptors Spartanpad Pad_TBLpadimmerForMember Pad Pad Pad pad Padendo pad Pad Pad Pad PadCompression Pad_TBL Baldwin leg Pad_TBL PadendoCompression Pad PadCompression takdir_TBLCompression takdir618immer Pad Bald618 Pad takdir takdir pad takdirisphereisphere Padcroft_TBL takdirCompression,0.0,0.0,0.0,0.0,1.9898161612273024,1.4923621209204767,0.0,0.0099009900990099,0.1156345531344413,0.2052807256132472,0.1156345605850219,0.1482021361589431,0.0003092419535834,0.0808507353067398,1,0.0,0.0,0.0,0.7062183333479438,0.6818229914764201,0.7272223480454256,0.0,0.0808507353067398
42,42,What supplemental tasks are used for multitask learning?,"Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question.","Automation of cQA forums can be divided into three tasks: question-comment relevance (Task A), question-question relevance (Task B), and question-external comment relevance (Task C). One might think that classic retrieval models like language models for information retrieval BIBREF0 could solve these tasks. However, a big challenge for cQA tasks is that users are used to expressing similar meanings with different words, which creates gaps when matching questions based on common words. Other challenges include informal usage of language, highly diverse content of comments, and variation in the length of both questions and comments.
In our cQA tasks, the pair of objects are (question, question) or (question, comment), and the relationship is relevant/irrelevant. The left side of Figure 1 shows one intuitive way to predict relationships using RNNs. Parallel LSTMs encode two objects independently, and then concatenate their outputs as an input to a feed-forward neural network (FNN) with a softmax output layer for classification.
For task C, in addition to an original question (oriQ) and an external comment (relC), the question which relC commented on is also given (relQ). To incorporate this extra information, we consider a multitask learning framework which jointly learns to predict the relationships of the three pairs (oriQ/relQ, oriQ/relC, relQ/relC).",oriQ/relQ and relQ/relC.,0.0689655153864447,0.0,0.0689655153864447,1.038539619163422,7.467281314906304,7.756261977102513,0.0261780104712041,0.0007888509071785,0.082857921719551,0.0,0.0828579291701316,0.0,0.0047010668754966,0.0645743682980537,3,1.0,1.0,1.0,0.7833802878356787,0.4879596788869049,0.7517017706123921,0.8,0.0645743682980537
43,43,How big is their model?,Proposed model has 1.16 million parameters and 11.04 MB.,"To figure out whether the proposed AEN-GloVe is a lightweight alternative of recurrent models, we study the model size of each model on the Restaurant dataset. Statistical results are reported in Table TABREF37 . We implement all the compared models base on the same source code infrastructure, use the same hyperparameters, and run them on the same GPU .
FLOAT SELECTED: Table 3: Model sizes. Memory footprints are evaluated on the Restaurant dataset. Lowest 2 are in bold.",enicpedpedpedped_Utils　ゝ_Utilsped_Utils_Utils_Utils_Utils_Utils_Utils_Utils_Utils_Utils_Utils_Utils injector bil_Utils_Utils_Utils_Utils_Utils_Utils_Utils_Utils_Utils_Utils_Utils_Utilsurrped_Utils_Utils_Utilspedped_Utilsped_Utils_Utils_Utils_Utils_Utils_UtilsUR_Utils_Utilsurre_Utils_Utilscessped_UtilsRetValnectped_UtilsVERNped_Utilscess_UtilsVERNVERNRetVal ThompedunapedVERN Kendrick GreenwoodVERN injectorpedškVERN injectorcess Eu幹VERN　ゝ Thom幹　ゝ幹　ゝ Thomilogy　ゝ Thom.firebaseapp Greenwood　ゝ　ゝUR ::::::::allisped AuthenticationService Greenwoodilogy :::::::: Thom Thom Thom Thom Thom Thom Thom789 Thomombatardi GreenwoodSIGSIG Thomimos Hra MyBaseFINITYrosseardi Greenwoodrosse glossrette gloss SIGientošk Thomrosserosseardi scazscheardi Kendricklakardirosse GreenwoodoserosseUR Recon Weissurre763imos ThomardiURUR AuthenticationServicefinityまず atmos KendrickurreurreURfinityURUR Rah AuthenticationService MyBaseURUR Kendrick Lustome scaomeurre//{{Parms Kendrickome　ゝ　ゝome Kendrickomeomeurฺ lax Greenwood　ゝ Kendrick mere Kendrick二二 sob　ゝ Friendship Greenwooddeenome Friendship Idleomefinity763 Reconfinityfinityfinityfinityfinityfinity intervenurrefinity789 Humanityfinity Humanitynakialiialiurre490789iali rarityfinity Promptfinity rarity Infragistics intervennak Humanitynaknakšk,0.0,0.0,0.0,6.390674502938318e-08,1.2210579489713609,0.9157934617285204,0.0,0.0099009900990099,0.0184470545500516,0.1971455569525023,0.0184470508247613,0.0198841877281665,0.0011272482556345,0.0529589280486106,1,0.0,0.0,,0.0,0.9297114499216348,0.7189129227260875,0.0,0.0529589280486106
44,44,What approach did previous models use for multi-span questions?,Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span.,"MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP. Their approach consisted of two parts. The first was to train a dedicated categorical variable to predict the number of spans to extract. The second was to generalize the single-span head method of extracting a span, by utilizing the non-maximum suppression (NMS) algorithm BIBREF7 to find the most probable set of non-overlapping spans. The number of spans to extract was determined by the aforementioned categorical variable.",single-span head method.,0.1621621606720234,0.0909090900413223,0.1621621606720234,3.179011715817869,30.324922114969034,28.992517894250287,0.089712918660287,0.0007311723129417,0.4343116879463196,1.0,0.5777722001075745,0.4694292545318603,0.0986510972349309,0.3564327955245971,3,1.0,1.0,1.0,0.8079700505842354,0.8099547335672186,0.8398189342688738,0.8,0.3564327955245971
45,45,What is difference in peformance between proposed model and state-of-the art on other question types?,"For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.
For number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. 
For date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1.",FLOAT SELECTED: Table 2. Performance of different models on DROP’s development set in terms of Exact Match (EM) and F1.,Not provided.,0.0,0.0,0.0,0.6488583831481832,4.8449867384091245,4.5602618318134445,0.009861932938856,0.0003998400639744,0.1056019142270088,0.2421251103281975,0.1286978423595428,,0.0068568591061433,0.0725997909903526,1,0.0,0.0,,0.0,0.8182319692351525,0.7014993055120389,0.0,0.0725997909903526
46,46,What is the performance of proposed model on entire DROP dataset?,"The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev.","Table TABREF25 shows the results on DROP's test set, with our model being the best overall as of the time of writing, and not just on multi-span questions.
FLOAT SELECTED: Table 3. Comparing test and development set results of models from the official DROP leaderboard",Best overall.,0.0,0.0,0.0,1.727223799216787,9.328092740773853,8.836431535050153,0.0248756218905472,0.0009514747859181,0.1135765612125396,0.5227921569347382,0.1135765388607978,0.2902846336364746,0.0031437568947517,0.075029008090496,3,1.0,0.0,,0.796922637259863,0.785124406356375,0.7404976254254997,0.0,0.075029008090496
47,47,How accurate is the aspect based sentiment classifier trained only using the XR loss?,"BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.
BiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.","FLOAT SELECTED: Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b).",Not provided.,0.0,0.0,0.0,1.6466642419110007,4.629629629629631,5.289746158984636,0.0238095238095238,0.0009514747859181,0.0227139145135879,0.1468637585639953,0.0654650777578353,,0.0034501411571256,0.1026493310928344,1,0.0,0.0,,0.0,0.1752605043776235,0.7010420175104943,0.0,0.1026493310928344
48,48,What are the models evaluated on?,They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA),"We build the iSQuAD and iNewsQA datasets based on SQuAD v1.1 BIBREF0 and NewsQA BIBREF1. Both original datasets share similar properties. Specifically, every data-point consists of a tuple, $\lbrace p, q, a\rbrace $, where $p$ represents a paragraph, $q$ a question, and $a$ is the answer. The answer is a word span defined by head and tail positions in $p$. NewsQA is more difficult than SQuAD because it has a larger vocabulary, more difficult questions, and longer source documents.
iMRC: Making MRC Interactive ::: Evaluation Metric
Since iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases. During training, we report training curves averaged over 3 random seeds. During test, we follow common practice in supervised learning tasks where we report the agent's test performance corresponding to its best validation performance .",F1 score and agent performance.,0.3809523773242631,0.1999999968,0.3809523773242631,8.097785064266205,56.78510963104008,51.73622463619113,0.1999999999999999,0.0035587188612099,0.7807568311691284,1.0,0.7807567119598389,0.7402926087379456,0.083067145829193,0.4952766299247741,3,1.0,4.0,0.5,0.8484455040281803,0.5968292257500429,0.887316903000172,0.5,0.4952766299247741
49,49,What is the results of multimodal compared to unimodal models?,"Unimodal LSTM vs Best Multimodal (FCM)
- F score: 0.703 vs 0.704
- AUC: 0.732 vs 0.734 
- Mean Accuracy: 68.3 vs 68.4.","Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to the image text and $I$ to the image. It also shows results for the LSTM, for the Davison method proposed in BIBREF7 trained with MMHS150K, and for random scores. Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models.
FLOAT SELECTED: Table 1. Performance of the proposed models, the LSTM and random scores. The Inputs column indicate which inputs are available at training and testing time.",Not provided in the given context.,0.0,0.0,0.0,1.286713095960916,7.908537273034849,7.056677501741829,0.0186567164179104,0.002601908065915,-0.0038738376460969,0.3565041613884462,-0.0038738308940082,0.2962331473827362,0.0079430511492108,0.0642965659499168,1,0.0,0.0,,0.7147834809817768,0.1698980980469686,0.6795923921878746,0.0,0.0642965659499168
50,50,What were their performance results?,"On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%","FLOAT SELECTED: Table 6: Comparison of ACC of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.
FLOAT SELECTED: Table 7: Comparison of NMI of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.",Tables 6 and 7.,0.0,0.0,0.0,0.0,5.375872037932107,4.03190402844908,0.0,0.0019011406844106,0.1514272689819336,0.2727759182453155,0.1514272838830948,,0.0117560360716815,0.0664544776082038,2,0.5,0.0,,0.7751489165982363,0.8277414613715166,0.7395372740574953,0.9,0.0664544776082038
51,51,By how much did they outperform the other methods?,"On SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI.","FLOAT SELECTED: Table 6: Comparison of ACC of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.
FLOAT SELECTED: Table 7: Comparison of NMI of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.",Not provided.,0.0,0.0,0.0,1.286713095960916,7.165637434095597,6.911959276473622,0.0189393939393939,0.0009514747859181,0.1343248784542083,0.0855645686388015,0.1343248933553695,,0.004934328440598,0.0547433868050575,1,0.0,0.0,,0.0,0.183266196575697,0.7330885359986574,0.0,0.0547433868050575
52,52,What is the state of the art?,"POS and DP task: CONLL 2018
NER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF
NLI task: mBERT or XLM (not clear from text)","We will compare to the more recent cross-lingual language model XLM BIBREF12, as well as the state-of-the-art CoNLL 2018 shared task results with predicted tokenisation and segmentation in an updated version of the paper.
In French, no extensive work has been done due to the limited availability of NER corpora. We compare our model with the strong baselines settled by BIBREF49, who trained both CRF and BiLSTM-CRF architectures on the FTB and enhanced them using heuristics and pretrained word embeddings.
In the TRANSLATE-TRAIN setting, we report the best scores from previous literature along with ours. BiLSTM-max is the best model in the original XNLI paper, mBERT which has been reported in French in BIBREF52 and XLM (MLM+TLM) is the best-presented model from BIBREF50.",CoNLL 2018 shared task results.,0.0740740710562415,0.0,0.0740740710562415,1.3794462224541233,20.091901868146486,17.319676438375243,0.0869236583522297,0.0021691973969631,0.5303347110748291,0.4535041275593612,0.5303347110748291,0.7076193690299988,0.002935640502553,0.1328918486833572,3,1.0,1.0,,0.7964632518658766,0.5154238228493908,0.8616952913975631,0.8,0.1328918486833572
53,53,"What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?","Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text.","Pre-processing. Tweets are subject to standard preprocessing steps for text such as tokenization, stemming, lemmatization, stop-word removal, and part-of-speech tagging. Moreover, due to their noisy nature, they are also processed using some Twitter-specific techniques such as substitution/removal of URLs, of user mentions, of hashtags, and of emoticons, spelling correction, elongation normalization, abbreviation lookup, punctuation removal, detection of amplifiers and diminishers, negation scope detection, etc. For this, one typically uses Twitter-specific NLP tools such as part-of-speech and named entity taggers, syntactic parsers, etc. BIBREF47 , BIBREF48 , BIBREF49 .
Despite all these opportunities, the rise of social media has also presented new challenges for natural language processing (NLP) applications, which had largely relied on NLP tools tuned for formal text genres such as newswire, and thus were not readily applicable to the informal language and style of social media. That language proved to be quite challenging with its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags. In addition to the genre difference, there is also a difference in length: social media messages are generally short, often length-limited by design as in Twitter, i.e., a sentence or a headline rather than a full document. How to handle such challenges has only recently been the subject of thorough research BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 .","informal language, creative spelling, misspellings, slang, new words, URLs, genre-specific terminology, abbreviations, and short length.",0.5555555506944445,0.2222222174691359,0.499999995138889,35.32246079935553,70.99508553301699,69.98378161859658,0.4806761749351869,0.0106007067137809,0.7818989157676697,0.928114381087713,0.7818988561630249,0.7701606154441833,0.1667882611165238,0.1235905587673187,4,1.0,5.0,1.0,0.7932248236316463,0.8713125083303184,0.9138334892084098,0.9,0.1235905587673187
54,54,How many sentence transformations on average are available per unique sentence in dataset?,27.41 transformation on average of single seed sentence is available in dataset.,"In the second round, we collected 293 annotations from 12 annotators. After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. Statistics of individual annotators are available in tab:statistics.",3.,0.0,0.0,0.0,3.089055318156697,3.2467532467532463,8.36734693877551,0.042016806722689,0.0008326394671107,0.1390967369079589,0.0,0.1390967518091201,,0.0087624820312058,0.1001449152827262,3,0.0,0.0,1.0,0.7950926685191079,0.1917547707293602,0.7670190829174409,0.0,0.1001449152827262
55,55,What annotations are available in the dataset?,"For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)","We asked for two distinct paraphrases of each sentence because we believe that a good sentence embedding should put paraphrases close together in vector space.
Several modification types were specifically selected to constitute a thorough test of embeddings. In different meaning, the annotators should create a sentence with some other meaning using the same words as the original sentence. Other transformations which should be difficult for embeddings include minimal change, in which the sentence meaning should be significantly changed by using only very small modification, or nonsense, in which words of the source sentence should be shuffled so that it is grammatically correct, but without any sense.","paraphrases, different meaning, minimal change, nonsense.",0.0833333295833335,0.0,0.0833333295833335,7.305267243289862,44.16069878545875,39.57584804700181,0.204994658119658,0.0037359900373599,0.5993500351905823,0.7237605452537537,0.5993500351905823,0.5653770565986633,0.0220231953671701,0.2599821090698242,4,1.0,2.0,1.0,0.802073177649932,0.8729251563089802,0.8251045372828665,0.8,0.2599821090698242
56,56,What are all 15 types of modifications ilustrated in the dataset?,"- paraphrase 1
- paraphrase 2
- different meaning
- opposite meaning
- nonsense
- minimal change
- generalization
- gossip
- formal sentence
- non-standard sentence
- simple sentence
- possibility
- ban
- future
- past.","We selected 15 modifications types to collect COSTRA 1.0. They are presented in annotationinstructions.
FLOAT SELECTED: Table 2: Sentences transformations requested in the second round of annotation with the instructions to the annotators. The annotators were given no examples (with the exception of nonsense) not to be influenced as much as in the first round.",In annotation instructions.,0.0,0.0,0.0,0.9432284668356838,13.481385137486788,11.263582972592989,0.0140845070422535,0.0007888509071785,0.1826188713312149,0.4293204084248611,0.1826188415288925,0.2407348752021789,0.0015108903708757,0.0690362304449081,1,0.3333333333333333,0.0,,0.8131411855128171,0.1872886066844414,0.7491544267377659,0.7,0.0690362304449081
57,57,What semantic rules are proposed?,Rules that compute polarity of words after POS tagging or parsing steps.,"In Twitter social networking, people express their opinions containing sub-sentences. These sub-sentences using specific PoS particles (Conjunction and Conjunctive adverbs), like ""but, while, however, despite, however"" have different polarities. However, the overall sentiment of tweets often focus on certain sub-sentences. For example:
@lonedog bwahahah...you are amazing! However, it was quite the letdown.
@kirstiealley my dentist is great but she's expensive...=(
In two tweets above, the overall sentiment is negative. However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset.
FLOAT SELECTED: Table I SEMANTIC RULES [12]",5 rules.,0.0,0.0,0.0,3.089055318156697,12.887646587961928,12.481622374581375,0.0833333333333333,0.0016638935108153,0.2716641724109649,0.6666666666666667,0.2716641724109649,,0.0069374077127749,0.1727678924798965,1,1.0,0.0,,0.8690007590622205,0.9396760267813122,0.7587041071252492,0.0,0.1727678924798965
58,58,What is the performance of the model?,"Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates
Experiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)","The test accuracy curves with the noise rates [0, $0.1$, $0.2$, $0.3$, $0.4$, $0.5$] are shown in Figure FIGREF13. From the figure, we can see that the test accuracy drops from around 0.8 to 0.5 when the noise rate increases from 0 to 0.5, but our NetAb outperforms CNN. The results clearly show that the performance of the CNN drops quite a lot with the noise rate increasing.
The comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop. The results demonstrate the superiority of NetAb. (2) NetAb outperforms the baselines designed for learning with noisy labels. These baselines are inferior to ours as they were tailored for image classification. Note that we found no existing method to deal with noisy labels for SSC. Training Details. We use the publicly available pre-trained embedding GloVe.840B BIBREF48 to initialize the word vectors and the embedding dimension is 300.
FLOAT SELECTED: Table 2: Accuracy (ACC) of both classes, F1 (F1 pos) of positive class and F1 (F1 neg) of negative class on clean test data/sentences. Training data are real noisy-labeled sentences.
FLOAT SELECTED: Figure 2: Accuracy (ACC) on clean test data. For training, the labels of clean data are flipped with the noise rates [0, 0.1, 0.2, 0.3, 0.4, 0.5]. For example, 0.1means that 10% of the labels are flipped. (Color online)",Superior.,0.0,0.0,0.0,0.0,5.105191600183206,3.828893700137405,0.0,0.0002221728504776,0.0839211568236351,0.6275372505187988,0.0839211493730545,,0.005889829906462,0.1340739279985427,2,1.0,1.0,1.0,0.8600273381404046,0.1771822153744351,0.7087715043874108,0.0,0.1340739279985427
59,59,What was the performance of both approaches on their dataset?,ERR of 19.05 with i-vectors and 15.52 with x-vectors.,FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets.,in Table 4.,0.0,0.0,0.0,4.196114906296549,5.144032921810699,6.441732136176581,0.0531914893617021,0.0033222591362126,0.0622646808624267,0.0462406757287681,0.0622646808624267,,0.0055718709938079,0.0517833940684795,1,1.0,0.0,,0.7850986260626787,0.1944975418484694,0.7779901673938777,0.7,0.0517833940684795
60,60,By how much is performance on CN-Celeb inferior to performance on VoxCeleb?,"For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb.",FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets.,10.,0.1428571415306122,0.0,0.1428571415306122,1.5732934811145336,8.108426089294966,8.463007450710993,0.0229357798165137,0.0005552470849528,0.0437699705362319,0.0,0.10727359354496,,0.0035964429142284,0.7433227300643921,3,0.0,0.0,1.0,0.7510414271989855,0.6927232065142134,0.7708928260568536,0.0,0.7433227300643921
61,61,What learning paradigms do they cover in this survey?,"Considering ""What"" and ""How"" separately versus jointly optimizing for both.","Past research took a reductionist approach, separately considering these two problems of “what” and “how” via content selection and question construction. Given a sentence or a paragraph as input, content selection selects a particular salient topic worthwhile to ask about and determines the question type (What, When, Who, etc.). Approaches either take a syntactic BIBREF11 , BIBREF12 , BIBREF13 or semantic BIBREF14 , BIBREF3 , BIBREF15 , BIBREF16 tack, both starting by applying syntactic or semantic parsing, respectively, to obtain intermediate symbolic representations. Question construction then converts intermediate representations to a natural language question, taking either a tranformation- or template-based approach. The former BIBREF17 , BIBREF18 , BIBREF13 rearranges the surface form of the input sentence to produce the question; the latter BIBREF19 , BIBREF20 , BIBREF21 generates questions from pre-defined question templates. Unfortunately, such QG architectures are limiting, as their representation is confined to the variety of intermediate representations, transformation rules or templates.
In contrast, neural models motivate an end-to-end architectures. Deep learned frameworks contrast with the reductionist approach, admitting approaches that jointly optimize for both the “what” and “how” in an unified framework. The majority of current NQG models follow the sequence-to-sequence (Seq2Seq) framework that use a unified representation and joint learning of content selection (via the encoder) and question construction (via the decoder). In this framework, traditional parsing-based content selection has been replaced by more flexible approaches such as attention BIBREF22 and copying mechanism BIBREF23 . Question construction has become completely data-driven, requiring far less labor compared to transformation rules, enabling better language flexibility compared to question templates.",reductionist and end-to-end.,0.153846150295858,0.0,0.153846150295858,3.1251907639724417,13.340716568277877,14.358067682509756,0.0719424460431654,0.0033222591362126,0.2888333797454834,0.2501288428902626,0.288833349943161,,0.0180024796993216,0.0474339015781879,3,1.0,1.0,,0.8260901533523898,0.193481209917104,0.7739248396684161,0.8,0.0474339015781879
62,62,How do this framework facilitate demographic inference from social media?,Demographic information is predicted using weighted lexicon of terms.,"We employ BIBREF73 's weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age and gender. The predictive power of this lexica was evaluated on Twitter, blog, and Facebook, showing promising results BIBREF73 . Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of INLINEFORM0 (denoted by INLINEFORM1 ) using following equation: INLINEFORM2
where INLINEFORM0 is the lexicon weight of the term, and INLINEFORM1 represents the frequency of the term in the user generated INLINEFORM2 , and INLINEFORM3 measures total word count in INLINEFORM4 . As our data is biased toward young people, we report age prediction performance for each age group separately (Table TABREF42 ). Moreover, to measure the average accuracy of this model, we build a balanced dataset (keeping all the users above 23 -416 users), and then randomly sampling the same number of users from the age ranges (11,19] and (19,23]. The average accuracy of this model is 0.63 for depressed users and 0.64 for control class. Table TABREF44 illustrates the performance of gender prediction for each class. The average accuracy is 0.82 on INLINEFORM5 ground-truth dataset.",By predicting demographic information using weighted lexicon of terms and user-generated content frequency.,0.5454545406198348,0.3999999952,0.5454545406198348,28.64190457979541,57.2343980119759,53.66541825926966,0.8273979107312441,0.015990159901599,0.8946714401245117,0.9582637012004852,0.8946714401245117,0.682598888874054,0.055968431406898,0.9883951544761658,4,0.6666666666666666,4.0,1.0,0.8774749287761338,0.7385184134113438,0.9541364738469754,0.8,0.9883951544761658
63,63,How is the data annotated?,The data are self-reported by Twitter users and then verified by two human experts.,"Self-disclosure clues have been extensively utilized for creating ground-truth data for numerous social media analytic studies e.g., for predicting demographics BIBREF36 , BIBREF41 , and user's depressive behavior BIBREF46 , BIBREF47 , BIBREF48 . For instance, vulnerable individuals may employ depressive-indicative terms in their Twitter profile descriptions. Others may share their age and gender, e.g., ""16 years old suicidal girl""(see Figure FIGREF15 ). We employ a huge dataset of 45,000 self-reported depressed users introduced in BIBREF46 where a lexicon of depression symptoms consisting of 1500 depression-indicative terms was created with the help of psychologist clinician and employed for collecting self-declared depressed individual's profiles. A subset of 8,770 users (24 million time-stamped tweets) containing 3981 depressed and 4789 control users (that do not show any depressive behavior) were verified by two human judges BIBREF46 . This dataset INLINEFORM0 contains the metadata values of each user such as profile descriptions, followers_count, created_at, and profile_image_url.",by two human judges.,0.3529411728719723,0.249999996953125,0.3529411728719723,10.511846841633776,33.89677499753027,37.00511174431795,0.2678571428571428,0.0036231884057971,0.2344502359628677,0.832127034664154,0.2344502061605453,0.6204541325569153,0.0143456301517857,0.223817229270935,3,0.25,1.0,,0.8427035183543673,0.7044294966705509,0.8177179866822033,0.0,0.223817229270935
64,64,What result from experiments suggest that natural language based agents are more robust?,Average reward across 5 seeds show that NLP representations are robust to changes in the environment as well task-nuisances.,"Results of the DQN-based agent are presented in fig: scenario comparison. Each plot depicts the average reward (across 5 seeds) of all representations methods. It can be seen that the NLP representation outperforms the other methods. This is contrary to the fact that it contains the same information as the semantic segmentation maps. More interestingly, comparing the vision-based and feature-based representations render inconsistent conclusions with respect to their relative performance. NLP representations remain robust to changes in the environment as well as task-nuisances in the state. As depicted in fig: nuisance scenarios, inflating the state space with task-nuisances impairs the performance of all representations. There, a large amount of unnecessary objects were spawned in the level, increasing the state's description length to over 250 words, whilst retaining the same amount of useful information. Nevertheless, the NLP representation outperformed the vision and feature based representations, with high robustness to the applied noise.",NLP outperforms others in nuisance scenarios.,0.159999996352,0.0,0.159999996352,2.51975934424348,27.251202911607624,24.34708614254349,0.0802139037433155,0.003516998827667,0.5510709285736084,0.5475077107548714,0.5510709285736084,0.2807307243347168,0.0138685713274025,0.2347225695848465,4,1.0,3.0,1.0,0.8087418072979267,0.5886739723729547,0.8546958894918187,0.8,0.2347225695848465
65,65,Which datasets are used in the paper?,"Google N-grams
COHA
Moral Foundations Dictionary (MFD)","To ground moral sentiment in text, we leverage the Moral Foundations Dictionary BIBREF27. The MFD is a psycholinguistic resource that associates each MFT category with a set of seed words, which are words that provide evidence for the corresponding moral category in text. We use the MFD for moral polarity classification by dividing seed words into positive and negative sets, and for fine-grained categorization by splitting them into the 10 MFT categories.
To implement the first tier of our framework and detect moral relevance, we complement our morally relevant seed words with a corresponding set of seed words approximating moral irrelevance based on the notion of valence, i.e., the degree of pleasantness or unpleasantness of a stimulus. We refer to the emotional valence ratings collected by BIBREF28 for approximately 14,000 English words, and choose the words with most neutral valence rating that do not occur in the MFD as our set of morally irrelevant seed words, for an equal total number of morally relevant and morally irrelevant words.
We divide historical time into decade-long bins, and use two sets of embeddings provided by BIBREF30, each trained on a different historical corpus of English:
Google N-grams BIBREF31: a corpus of $8.5 \times 10^{11}$ tokens collected from the English literature (Google Books, all-genres) spanning the period 1800–1999.
COHA BIBREF32: a smaller corpus of $4.1 \times 10^8$ tokens from works selected so as to be genre-balanced and representative of American English in the period 1810–2009.",Google N-grams and COHA.,0.5454545408264463,0.2222222177777778,0.5454545408264463,10.552670315936318,52.80295117980387,49.29352880027575,0.297157622739018,0.0079365079365079,0.539180338382721,0.6666666666666666,0.539180338382721,0.3045596480369568,0.2552806245536617,0.990181028842926,4,1.0,4.0,1.0,0.8368530672286759,0.6027726480823223,0.9110905923292892,1.0,0.990181028842926
66,66,How do they quantify moral relevance?,By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence.,"To implement the first tier of our framework and detect moral relevance, we complement our morally relevant seed words with a corresponding set of seed words approximating moral irrelevance based on the notion of valence, i.e., the degree of pleasantness or unpleasantness of a stimulus. We refer to the emotional valence ratings collected by BIBREF28 for approximately 14,000 English words, and choose the words with most neutral valence rating that do not occur in the MFD as our set of morally irrelevant seed words, for an equal total number of morally relevant and morally irrelevant words.",Valence ratings.,0.0,0.0,0.0,1.8160849415439309,19.482559617200696,16.50675771074199,0.0520833333333333,0.0009990009990009,0.5347816944122314,0.7168553191072801,0.5347817540168762,0.1902757138013839,0.006974682955773,0.3189297914505005,3,1.0,1.0,1.0,0.8609888986014491,0.7023484359912822,0.809263531233914,0.8,0.3189297914505005
67,67,How much is proposed model better in perplexity and BLEU score than typical UMT models?,"Perplexity of the best model is 65.58 compared to best baseline 105.79.
Bleu of the best model is 6.57 compared to best baseline 5.50.","As illustrated in Table TABREF12 (ID 1). Given the vernacular translation of each gold poem in test set, we generate five poems using our models. Intuitively, the more the generated poem resembles the gold poem, the better the model is. We report mean perplexity and BLEU scores in Table TABREF19 (Where +Anti OT refers to adding the reinforcement loss to mitigate over-fitting and +Anti UT refers to adding phrase segmentation-based padding to mitigate under-translation), human evaluation results in Table TABREF20.
FLOAT SELECTED: Table 3: Perplexity and BLEU scores of generating poems from vernacular translations. Since perplexity and BLEU scores on the test set fluctuates from epoch to epoch, we report the mean perplexity and BLEU scores over 5 consecutive epochs after convergence.",Not provided in context.,0.0,0.0,0.0,1.444580998177086,10.31330847511042,9.093851468388674,0.0209205020920502,0.0016638935108153,0.0722876861691474,0.4409275650978088,0.0946510657668113,,0.0041495672614834,0.0627562925219535,1,0.0,0.0,,0.7017575206476314,0.1775018047450493,0.7100072189801974,0.0,0.0627562925219535
68,68,Do they train a different training method except from scheduled sampling?,"Answer with content missing: (list missing) 
Scheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.

Yes.","Since the official test set has not been released publicly, results reported on the official test set have been provided by the challenge organizers. For the prototype test set and for the ablation study presented in Table TABREF24 , we use the same code for evaluation metrics as used by BIBREF11 for fairness and comparability. We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below:",Yes,0.0444444440098765,0.0,0.0444444440098765,0.5749785925016119,3.363250453973569,4.414686323925186,0.0088028169014084,0.0001922707171697,0.0883527398109436,1.0,0.8368557691574097,,0.0071757194548642,0.3147139549255371,3,0.0,5.0,,0.7739837594837123,0.6896261034543494,0.7585044138173973,0.0,0.3147139549255371
69,69,How do they define upward and downward reasoning?,"Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific.","A context is upward entailing (shown by [... $\leavevmode {\color {red!80!black}\uparrow }$ ]) that allows an inference from ( ""Introduction"" ) to ( ""Introduction"" ), where French dinner is replaced by a more general concept dinner. On the other hand, a downward entailing context (shown by [... $\leavevmode {\color {blue!80!black}\downarrow }$ ]) allows an inference from ( ""Introduction"" ) to ( ""Introduction"" ), where workers is replaced by a more specific concept new workers. Interestingly, the direction of monotonicity can be reversed again by embedding yet another downward entailing context (e.g., not in ( ""Introduction"" )), as witness the fact that ( ""Introduction"" ) entails ( ""Introduction"" ). To properly handle both directions of monotonicity, NLI models must detect monotonicity operators (e.g., all, not) and their arguments from the syntactic structure.
All [ workers $\leavevmode {\color {blue!80!black}\downarrow }$ ] [joined for a French dinner $\leavevmode {\color {red!80!black}\uparrow }$ ] All workers joined for a dinner All new workers joined for a French dinner Not all [new workers $\leavevmode {\color {red!80!black}\uparrow }$ ] joined for a dinner Not all workers joined for a dinner","Upward: more general, Downward: more specific.",0.1739130400756144,0.0624999973632813,0.1739130400756144,5.3976467790265215,45.08970945055164,43.05547080032956,0.1889970059880239,0.0019960079840319,0.6792929172515869,1.0,0.7123578190803528,0.8161500096321106,0.1141417351571843,0.9512035250663756,3,1.0,5.0,0.5,0.8646530262719994,0.7267596628179804,0.9070386512719216,0.9,0.9512035250663756
70,70,How big are the datasets?,"In-house dataset consists of  3716 documents 
ACE05 dataset consists of  1635 documents.","FLOAT SELECTED: Table 2: Number of documents in the training/dev/test sets of the in-house and ACE05 datasets.
Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. It defines 56 entity types (e.g., Person, Organization, Geo-Political Entity, Location, Facility, Time, Event_Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.).
The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical).",Not specified.,0.0,0.0,0.0,3.089055318156697,8.653779700609562,9.025945483303795,0.0416666666666666,0.0016638935108153,0.183186873793602,0.1908896565437317,0.183186873793602,,0.0039200055232574,0.0774102583527565,1,0.0,0.0,,0.0,0.1809330278351389,0.723691709233655,0.2,0.0774102583527565
71,71,What baselines did they compare their model with?,The baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search.,"The baseline approach is based on BIBREF20 . It divides the task of interpreting commands for behavioral navigation into two steps: path generation, and path verification. For path generation, this baseline uses a standard sequence-to-sequence model augmented with an attention mechanism, similar to BIBREF23 , BIBREF6 . For path verification, the baseline uses depth-first search to find a route in the graph that matches the sequence of predicted behaviors. If no route matches perfectly, the baseline changes up to three behaviors in the predicted sequence to try to turn it into a valid path.","BIBREF20, BIBREF23, BIBREF6",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.001497753369945,0.1713253855705261,0.0,0.1713253855705261,0.0,0.0034581491824348,0.0394429340958595,3,1.0,0.0,,0.8320280600272442,0.1801520110810495,0.7206080443241982,0.0,0.0394429340958595
72,72,What was the performance of their model?,"For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81.","FLOAT SELECTED: Table 3: Performance of different models on the test datasets. EM and GM report percentages, and ED corresponds to average edit distance. The symbol ↑ indicates that higher results are better in the corresponding column; ↓ indicates that lower is better.",Not provided.,0.0,0.0,0.0,0.8733042428534794,4.7764564726887055,4.752170863200814,0.0131233595800524,0.0005878894767783,0.0929452180862426,0.2319958051045736,0.1535114049911499,,0.0071265077314981,0.1078025177121162,1,0.0,0.0,,0.0,0.1796914550306269,0.7187658201225079,0.0,0.1078025177121162
73,73,What evaluation metrics are used?,"Exact match, f1 score, edit distance and goal match.","We compare the performance of translation approaches based on four metrics:
[align=left,leftmargin=0em,labelsep=0.4em,font=]
As in BIBREF20 , EM is 1 if a predicted plan matches exactly the ground truth; otherwise it is 0.
The harmonic average of the precision and recall over all the test set BIBREF26 .
The minimum number of insertions, deletions or swap operations required to transform a predicted sequence of behaviors into the ground truth sequence BIBREF27 .
GM is 1 if a predicted plan reaches the ground truth destination (even if the full sequence of behaviors does not match exactly the ground truth). Otherwise, GM is 0.","EM, F1, ED, GM.",0.0,0.0,0.0,4.456882760699063,4.528985507246377,8.010118043844859,0.1724137931034482,0.0044247787610619,0.3560615181922912,0.4393617510795593,0.3560615479946136,0.1603188812732696,0.007309934778945,0.0524598993360996,4,0.5,5.0,0.5,0.7770617997179183,0.5744637819883645,0.7978551279534576,1.0,0.0524598993360996
74,74,How were the navigation instructions collected?,Using Amazon Mechanical Turk using simulated environments with topological maps.,"This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans.
We created a new dataset for the problem of following navigation instructions under the behavioral navigation framework of BIBREF5 . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation.
As shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different navigation instructions each; the rest has one). The dataset contains two test set variants:
While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort.",Amazon Mechanical Turk.,0.4615384579881657,0.363636360661157,0.4615384579881657,14.991106946711684,61.87296065328979,61.521111492064165,0.3640776699029126,0.0037359900373599,0.750539243221283,1.0,0.750539243221283,0.959729254245758,0.0867939674875877,0.7746209502220154,3,1.0,2.0,1.0,0.8107363356955881,0.725754450211086,0.9029427831477916,1.0,0.7746209502220154
75,75,What additional features are proposed for future work?,Distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort.,"Our current feature set for training a machine learning classifier is relatively small, consisting of paragraph domain scores, bag-of-words, length of stay, and number of previous admissions, but we intend to factor in many additional features that extend beyond the scope of the present study. These include a deeper analysis of clinical narratives in EHRs: our next task will be to extend our EHR data pipeline by distinguishing between clinically positive and negative phenomena within each risk factor domain. This will involve a series of annotation tasks that will allow us to generate lexicon-based and corpus-based sentiment analysis tools. We can then use these clinical sentiment scores to generate a gradient of patient improvement or deterioration over time.",Clinical sentiment scores.,0.0,0.0,0.0,1.6466642419110007,19.41422363305401,16.165123040394768,0.0710900473933649,0.0013617793917385,0.4728406071662903,0.4071078364338194,0.4728405177593231,0.4574986398220062,0.0089330102887106,0.0457823053002357,1,0.6666666666666666,1.0,1.0,0.7795839604933578,0.2050191136889418,0.8200829406926979,0.8,0.0457823053002357
76,76,What are their initial results on this task?,"Achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5), and show consistency in per-domain performance rankings between MLP and RBF models.","FLOAT SELECTED: Table 5: Overall and domain-specific Precision, Recall, and F1 scores for our models. The first row computes similarity directly from the TF-IDF matrix, as in (McCoy et al., 2015). All other rows are classifier outputs.",TF-IDF matrix.,0.0,0.0,0.0,0.9432284668356838,4.4420713416089725,4.582299602051481,0.0141242937853107,0.0006056935190793,0.1469291746616363,0.4069938779651344,0.1469291895627975,,0.0013238273105923,0.0498238317668437,1,1.0,0.0,,0.7911304350081095,0.1977190976365345,0.7909828302793548,0.0,0.0498238317668437
77,77,How is morphology knowledge implemented in the method?,A BPE model is applied to the stem after morpheme segmentation.,"The problem with morpheme segmentation is that the vocabulary of stem units is still very large, which leads to many rare and unknown words at the training time. The problem with BPE is that it do not consider the morpheme boundaries inside words, which might cause a loss of morphological properties and semantic information. Hence, on the analyses of the above popular word segmentation methods, we propose the morphologically motivated segmentation strategy that combines the morpheme segmentation and BPE for further improving the translation performance of NMT.
Compared with the sentence of word surface forms, the corresponding sentence of stem units only contains the structure information without considering morphological information, which can make better generalization over inflectional variants of the same word and reduce data sparseness BIBREF8. Therefore, we learn a BPE model on the stem units in the training corpus rather than the words, and then apply it on the stem unit of each word after morpheme segmentation.",Morpheme segmentation.,0.1538461512426036,0.0,0.1538461512426036,6.772997136689072,70.19882645716912,61.661530210264296,0.2652652652652652,0.0022172949002217,0.6229034662246704,1.0,0.6229034066200256,0.689026415348053,0.1265498920879058,0.7028667330741882,3,1.0,2.0,,0.8246195405132107,0.2167205892842725,0.8667560591645531,0.0,0.7028667330741882
78,78,How is the performance on the task evaluated?,Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors.,"Neural network models, and specifically sequence-to-sequence models, have pushed the state of the art for morphological inflection – the task of learning a mapping from lemmata to their inflected forms – in the last years BIBREF6. Thus, in this work, we experiment on such models, asking not what they learn, but, motivated by the respective research on human subjects, the related question of how what they learn depends on their prior knowledge. We manually investigate the errors made by artificial neural networks for morphological inflection in a target language after pretraining on different source languages. We aim at finding answers to two main questions: (i) Do errors systematically differ between source languages? (ii) Do these differences seem explainable, given the properties of the source and target languages? In other words, we are interested in exploring if and how L2 acquisition of morphological inflection depends on the L1, i.e., the ""native language"", in neural network models.
For our qualitative analysis, we make use of the validation set. Therefore, we show validation set accuracies in Table TABREF19 for comparison. As we can see, the results are similar to the test set results for all language combinations. We manually annotate the outputs for the first 75 development examples for each source–target language combination. All found errors are categorized as belonging to one of the following categories.",Validation set accuracies and manual annotation of errors.,0.3333333288888889,0.0,0.3333333288888889,5.091212823097751,39.48583852729352,36.55520904822864,0.2066666666666666,0.0056818181818181,0.3940590918064117,0.6522271044114056,0.3940590918064117,0.6570743322372437,0.0113487495958197,0.067987360060215,4,1.0,4.0,1.0,0.7900049665352036,0.5169064458066818,0.8676257832267269,0.8,0.067987360060215
79,79,what datasets were used?,"IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German.","We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German.","IWSLT14 German-English, Turkish-English, and WMT14 English-German.",0.9090909041322316,0.3999999950000001,0.9090909041322316,33.9132609103075,84.98180144559568,82.67046158729447,0.8333333333333333,0.029126213592233,0.9662364721298218,0.0,0.9662365317344666,0.0,0.502201620787697,0.988759219646454,3,1.0,5.0,0.6666666666666666,0.91604431809276,0.8478383603780035,0.991386154132068,1.0,0.988759219646454
80,80,Why does not the approach from English work on other languages?,"Because, unlike other languages, English does not mark grammatical genders.","To date, the NLP community has focused primarily on approaches for detecting and mitigating gender stereotypes in English BIBREF5 , BIBREF6 , BIBREF7 . Yet, gender stereotypes also exist in other languages because they are a function of society, not of grammar. Moreover, because English does not mark grammatical gender, approaches developed for English are not transferable to morphologically rich languages that exhibit gender agreement BIBREF8 . In these languages, the words in a sentence are marked with morphological endings that reflect the grammatical gender of the surrounding nouns. This means that if the gender of one word changes, the others have to be updated to match. As a result, simple heuristics, such as augmenting a corpus with additional sentences in which he and she have been swapped BIBREF9 , will yield ungrammatical sentences. Consider the Spanish phrase el ingeniero experto (the skilled engineer). Replacing ingeniero with ingeniera is insufficient—el must also be replaced with la and experto with experta.",English lacks grammatical gender.,0.2857142816326531,0.0,0.2857142816326531,4.065425428798724,57.56687109505799,48.875016295719554,0.3073770491803278,0.0049751243781094,0.7775603532791138,0.6612579707588468,0.7775603532791138,0.7616484761238098,0.1465035590637631,0.7189934849739075,3,1.0,4.0,1.0,0.8569182112663785,0.6037548236642419,0.914992848864856,0.8,0.7189934849739075
81,81,How do they measure grammaticality?,By calculating log ratio of grammatical phrase over ungrammatical phrase.,"Because our approach is specifically intended to yield sentences that are grammatical, we additionally consider the following log ratio (i.e., the grammatical phrase over the ungrammatical phrase): DISPLAYFORM0",Log ratio.,0.1818181788429752,0.0,0.1818181788429752,4.456882760699063,26.112005287576896,25.85287071421165,0.2505446623093682,0.0022172949002217,0.5736380815505981,1.0,0.5736380219459534,0.6305326819419861,0.0229844408981861,0.5141115784645081,3,1.0,4.0,1.0,0.7708427401333612,0.7173879367247856,0.8695517468991424,0.9,0.5141115784645081
82,82,What is the difference in recall score between the systems?,"Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference.","FLOAT SELECTED: Table 2. Comparison with Traditional NERs as reference
FLOAT SELECTED: Table 3. Comparison with Wikipedia titles as reference",Not provided.,0.0,0.0,0.0,1.0885011049519644,5.384809195764538,5.4008094890100615,0.0161812297734627,0.0007137758743754,0.0254221707582473,0.2076329117019971,0.0254221688956022,,0.0076502326606842,0.0591780170798301,1,0.0,0.0,,0.0,0.1794409847842299,0.7177639391369198,0.0,0.0591780170798301
83,83,What is their f1 score and recall?,"F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference.","FLOAT SELECTED: Table 2. Comparison with Traditional NERs as reference
FLOAT SELECTED: Table 3. Comparison with Wikipedia titles as reference",Not provided.,0.0,0.0,0.0,1.6466642419110007,6.555011636026603,6.707479343294407,0.0238095238095238,0.0009990009990009,0.0885802060365676,0.2118360070805801,0.088580198585987,,0.0033229050772089,0.0422787889838218,1,0.0,0.0,,0.0,0.1814832325233717,0.7259329300934869,0.0,0.0422787889838218
84,84,What context modelling methods are evaluated?,"Concat
Turn
Gate
Action Copy
Tree Copy
SQL Attn
Concat + Action Copy
Concat + Tree Copy
Concat + SQL Attn
Turn + Action Copy
Turn + Tree Copy
Turn + SQL Attn
Turn + SQL Attn + Action Copy.","To conduct a thorough comparison, we evaluate 13 different context modeling methods upon the same parser, including 6 methods introduced in Section SECREF2 and 7 selective combinations of them (e.g., Concat+Action Copy). The experimental results are presented in Figure FIGREF37. Taken as a whole, it is very surprising to observe that none of these methods can be consistently superior to the others. The experimental results on BERT-based models show the same trend. Diving deep into the methods only using recent questions as context, we observe that Concat and Turn perform competitively, outperforming Gate by a large margin. With respect to the methods only using precedent SQL as context, Action Copy significantly surpasses Tree Copy and SQL Attn in all metrics. In addition, we observe that there is little difference in the performance of Action Copy and Concat, which implies that using precedent SQL as context gives almost the same effect with using recent questions. In terms of the combinations of different context modeling methods, they do not significantly improve the performance as we expected.
FLOAT SELECTED: Figure 5: Question Match, Interaction Match and Turn i Match on SPARC and COSQL development sets. The numbers are averaged over 5 runs. The first column represents absolute values. The rest are improvements of different context modeling methods over CONCAT.","13 different methods, including 6 introduced methods and 7 selective combinations of them.",0.0,0.0,0.0,0.8954307276600084,9.795386104244292,7.981496114537512,0.0130208333333333,0.00323947171692,0.1346324682235717,0.2057261198060587,0.1346324384212494,0.1658168584108352,0.0024894020021923,0.0888559967279434,3,1.0,2.0,1.0,0.858295734901386,0.9071724370843364,0.7398681481626893,1.0,0.0888559967279434
85,85,Is the baseline a non-heirarchical model like BERT?,There were hierarchical and non-hierarchical baselines; BERT was one of those baselines.,"FLOAT SELECTED: Table 1: Results of various models on the CNNDM test set using full-length F1 ROUGE-1 (R-1), ROUGE-2 (R2), and ROUGE-L (R-L).
Our main results on the CNNDM dataset are shown in Table 1 , with abstractive models in the top block and extractive models in the bottom block. Pointer+Coverage BIBREF9 , Abstract-ML+RL BIBREF10 and DCA BIBREF42 are all sequence to sequence learning based models with copy and coverage modeling, reinforcement learning and deep communicating agents extensions. SentRewrite BIBREF26 and InconsisLoss BIBREF25 all try to decompose the word by word summary generation into sentence selection from document and “sentence” level summarization (or compression). Bottom-Up BIBREF27 generates summaries by combines a word prediction model with the decoder attention model. The extractive models are usually based on hierarchical encoders (SummaRuNNer; BIBREF3 and NeuSum; BIBREF11 ). They have been extended with reinforcement learning (Refresh; BIBREF4 and BanditSum; BIBREF20 ), Maximal Marginal Relevance (NeuSum-MMR; BIBREF21 ), latent variable modeling (LatentSum; BIBREF5 ) and syntactic compression (JECS; BIBREF38 ). Lead3 is a baseline which simply selects the first three sentences. Our model $\text{\sc Hibert}_S$ (in-domain), which only use one pre-training stage on the in-domain CNNDM training set, outperforms all of them and differences between them are all significant with a 0.95 confidence interval (estimated with the ROUGE script). Note that pre-training $\text{\sc Hibert}_S$ (in-domain) is very fast and it only takes around 30 minutes for one epoch on the CNNDM training set. Our models with two pre-training stages ( $\text{\sc Hibert}_S$ ) or larger size ( $\text{\sc Hibert}_M$ ) perform even better and $\text{\sc Hibert}_M$ outperforms BERT by 0.5 ROUGE. We also implemented two baselines. One is the hierarchical transformer summarization model (HeriTransfomer; described in ""Extractive Summarization"" ) without pre-training. Note the setting for HeriTransfomer is ( $L=4$ , $H=300$ and $A=4$ ) . We can see that the pre-training (details in Section ""Pre-training"" ) leads to a +1.25 ROUGE improvement. Another baseline is based on a pre-trained BERT BIBREF0 and finetuned on the CNNDM dataset. We used the $\text{BERT}_{\text{base}}$ model because our 16G RAM V100 GPU cannot fit $\text{BERT}_{\text{large}}$ for the summarization task even with batch size of 1. The positional embedding of BERT supports input length up to 512 words, we therefore split documents with more than 10 sentences into multiple blocks (each block with 10 sentences). We feed each block (the BOS and EOS tokens of each sentence are replaced with [CLS] and [SEP] tokens) into BERT and use the representation at [CLS] token to classify each sentence. Our model $\text{\sc Hibert}_S$1 outperforms BERT by 0.4 to 0.5 ROUGE despite with only half the number of model parameters ( $\text{\sc Hibert}_S$2 54.6M v.s. BERT 110M). Results on the NYT50 dataset show the similar trends (see Table 2 ). EXTRACTION is a extractive model based hierarchical LSTM and we use the numbers reported by xu:2019:arxiv. The improvement of $\text{\sc Hibert}_S$3 over the baseline without pre-training (HeriTransformer) becomes 2.0 ROUGE. $\text{\sc Hibert}_S$4 (in-domain), $\text{\sc Hibert}_S$5 (in-domain), $\text{\sc Hibert}_S$6 and $\text{\sc Hibert}_S$7 all outperform BERT significantly according to the ROUGE script.",No.,0.0,0.0,0.0,2.839838722567789,3.745318352059925,7.3018080667593885,0.0390625,0.0008326394671107,0.0651456788182258,0.12021055072546,0.0651456788182258,,0.0058270322316127,0.0683823823928833,3,0.0,1.0,1.0,0.0,0.6810437047195278,0.7241748188781112,0.8,0.0683823823928833
86,86,How better are results compared to baseline models?,F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61.,"We see that parent quality is a simple yet effective feature and SVM model with this feature can achieve significantly higher ($p<0.001$) F1 score ($46.61\%$) than distance from the thesis and linguistic features. Claims with higher impact parents are more likely to be have higher impact. Similarity with the parent and thesis is not significantly better than the majority baseline. Although the BiLSTM model with attention and FastText baselines performs better than the SVM with distance from the thesis and linguistic features, it has similar performance to the parent quality baseline.
We find that the flat representation of the context achieves the highest F1 score. It may be more difficult for the models with a larger number of parameters to perform better than the flat representation since the dataset is small. We also observe that modeling 3 claims on the argument path before the target claim achieves the best F1 score ($55.98\%$).",significantly.,0.0,0.0,0.0,1.727223799216787,8.638904784962802,8.573962344406848,0.0239234449760765,0.0004759638267491,0.1963922828435897,0.4104789197444916,0.1963922679424286,,0.0052644857449562,0.1217258125543594,3,0.0,0.0,1.0,0.8436983602847032,0.6872596807992424,0.7488819929943464,0.8,0.1217258125543594
87,87,What geometric properties do embeddings display?,Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters.,"The initial analyses of the embedding matrices for both the UK and France revealed that in general, words were grouped by context or influence on the electricity consumption. For instance, we observed that winter words were together and far away from summer ones. Week days were grouped as well and far from week-end days. However considering the vocabulary was reduced to $V^* = 52$ words, those results lacked of consistency. Therefore for both languages we decided to re-train the RNNs using the same architecture, but with a larger vocabulary of the $V=300$ most relevant words (still in the RF sense) and on all the available data (i.e. everything is used as training) to compensate for the increased size of the vocabulary. We then calculated the distance of a few prominent words to the others. The analysis of the average cosine distance over $B=10$ runs for three major words is given by tables TABREF38 and TABREF39, and three other examples are given in the appendix tables TABREF57 and TABREF58. The first row corresponds to the reference word vector $\overrightarrow{w_1}$ used to calculate the distance from (thus the distance is always zero), while the following ones are the 9 closest to it. The two last rows correspond to words we deemed important to check the distance with (an antagonistic one or relevant one not in the top 9 for instance).",Grouping by context or influence.,0.0,0.0,0.0,1.9146030690102511,12.090281464360151,10.488449039884358,0.0268817204301075,0.002770083102493,0.2324903309345245,0.432298112422862,0.3126755654811859,0.1755519956350326,0.0060089583767428,0.059466402977705,4,0.5,1.0,1.0,0.8365536027979189,0.1912410705424766,0.7649642821699065,0.8,0.059466402977705
88,88,What are the state-of-the-art systems?,"For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN.","FLOAT SELECTED: TABLE III COMPARISON WITH THE STATE-OF-THE-ART SYSTEMS OF SEMEVAL 2016 TASK 6 ON SENTIMENT DATASET.
FLOAT SELECTED: TABLE IV COMPARISON WITH THE STATE-OF-THE-ART SYSTEMS PROPOSED BY [16] ON EMOTION DATASET. THE METRICS P, R AND F STAND FOR PRECISION, RECALL AND F1-SCORE.
Table TABREF19 shows the comparison of our proposed system with the existing state-of-the-art system of SemEval 2016 Task 6 for the sentiment dataset. BIBREF7 used feature-based SVM, BIBREF39 used keyword rules, LitisMind relied on hashtag rules on external data, BIBREF38 utilized a combination of sentiment classifiers and rules, whereas BIBREF37 used a maximum entropy classifier with domain-specific features. Our system comfortably surpasses the existing best system at SemEval. Our system manages to improve the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis.
We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22. Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN were the five individual systems used by BIBREF15. Overall, our proposed system achieves an improvement of 5 F-Score points over the existing state-of-the-art system for emotion analysis. Individually, the proposed system improves the existing F-scores for all the emotions except surprise. The findings of BIBREF15 also support this behavior (i.e. worst result for the surprise class). This could be attributed to the data scarcity and a very low agreement between the annotators for the emotion surprise.","BIBREF7, BIBREF39, LitisMind, BIBREF38, BIBREF37, BIBREF15.",0.0833333295833335,0.0,0.0833333295833335,5.71168499906942,16.256081505032387,19.5170992451628,0.1241134751773049,0.0027198549410698,0.314578503370285,0.0,0.3145784735679626,0.0,0.0243599206521944,0.0435803495347499,3,0.8333333333333334,1.0,1.0,0.8342220796180765,0.2900410776650048,0.7861916746591011,0.8,0.0435803495347499
89,89,"What kind of features are used by the HMM models, and how interpretable are those?","A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. 
The interpretability of the model is shown in Figure 2.","We compare a hybrid HMM-LSTM approach with a continuous emission HMM (trained on the hidden states of a 2-layer LSTM), and a discrete emission HMM (trained directly on data).
We interpret the HMM and LSTM states in the hybrid algorithm with 10 LSTM state dimensions and 10 HMM states in Figures 3 and 3 , showing which features are identified by the HMM and LSTM components. In Figures 3 and 3 , we color-code the training data with the 10 HMM states. In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters. The HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data.
FLOAT SELECTED: Figure 2: Visualizing HMM and LSTM states on Linux data for the hybrid with 10 LSTM state dimensions and 10 HMM states. The HMM and LSTM components learn some complementary features in the text related to spaces and comments.","Spaces, indentation, special characters; somewhat interpretable.",0.0,0.0,0.0,0.9964194812460634,24.16203901122163,18.97065637118536,0.0307692307692307,0.0018148820326678,0.197994589805603,0.5231416081472978,0.3861106336116791,0.4685763120651245,0.0039417145387041,0.0639797821640968,1,1.0,0.0,0.5,0.8816581291194033,0.6818116729809366,0.7272466919237465,0.9,0.0639797821640968
90,90,What kind of information do the HMMs learn that the LSTMs don't?,The HMM can identify punctuation or pick up on vowels.,"We interpret the HMM and LSTM states in the hybrid algorithm with 10 LSTM state dimensions and 10 HMM states in Figures 3 and 3 , showing which features are identified by the HMM and LSTM components. In Figures 3 and 3 , we color-code the training data with the 10 HMM states. In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters. The HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data.
FLOAT SELECTED: Figure 2: Visualizing HMM and LSTM states on Linux data for the hybrid with 10 LSTM state dimensions and 10 HMM states. The HMM and LSTM components learn some complementary features in the text related to spaces and comments.",Tail  iturokus peccroft313 Herbrishšak313313urf313 Kear climtha873313urf ｌabay Jessie Sylv clim democr313313urf313 democrＬ313 Chad pads Sylv Sylv thunder tabisíurf Magnus latter Bainoretolv democr postaedacke diss diss tabiabay latter tabihausen tabi_rspAndWaitling gubernＬurfurfreadcrduino-pocketurfurf Bacimosurfimos Bacurfiento-pocketθαoret-pocket daylight gubern,0.0,0.0,0.0,0.0,3.402447213761121,2.551835410320841,0.037593984962406,0.0099009900990099,0.200810506939888,0.2730785951163005,0.2008104771375656,0.1581934094429016,0.0001540079583957,0.0512045584619045,1,0.0,0.0,,0.8732883360772897,0.6797304114759838,0.7188324600961802,0.0,0.0512045584619045
91,91,How large is the gap in performance between the HMMs and the LSTMs?,"With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower.","FLOAT SELECTED: Table 1: Predictive loglikelihood (LL) comparison, sorted by validation set performance.",Not provided.,0.0,0.0,0.0,0.9964194812460634,4.805011474654268,4.880886166476646,0.0148809523809523,0.0006056935190793,-0.0100245438516139,0.1807223272323608,0.0224961247295141,,0.003581859747907,0.1076211184263229,1,0.0,0.0,,0.0,0.1777050073425014,0.7108200293700059,0.0,0.1076211184263229
92,92,what was their system's f1 performance?,"Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.","FLOAT SELECTED: Table 2: F1 Results3
The approach we have developed establishes a new state of the art for classifying hate speech, outperforming previous results by as much as 12 F1 points. Table TABREF10 illustrates the robustness of our method, which often outperform previous results, measured by weighted F1.",12 points higher.,0.0,0.0,0.0,2.0244462660665508,9.802656675015715,9.14047674444902,0.0285714285714285,0.0021382751247327,0.239206776022911,0.3473355025053024,0.239206776022911,0.3707611858844757,0.001157517159797,0.0800697952508926,1,1.0,0.0,,0.8099803640671133,0.1861133939654775,0.74445357586191,0.5,0.0800697952508926
93,93,How was quality measured?,"Inter-annotator agreement, comparison against expert annotation, agreement with PropBank Data annotations.","Dataset Quality Analysis ::: Inter-Annotator Agreement (IAA)
To estimate dataset consistency across different annotations, we measure F1 using our UA metric with 5 generators per predicate. Individual worker-vs-worker agreement yields 79.8 F1 over 10 experiments with 150 predicates, indicating high consistency across our annotators, inline with results by other structured semantic annotations (e.g. BIBREF6). Overall consistency of the dataset is assessed by measuring agreement between different consolidated annotations, obtained by disjoint triplets of workers, which achieves F1 of 84.1 over 4 experiments, each with 35 distinct predicates. Notably, consolidation boosts agreement, suggesting it is a necessity for semantic annotation consistency.
Dataset Quality Analysis ::: Dataset Assessment and Comparison
We assess both our gold standard set and the recent Dense set against an integrated expert annotated sample of 100 predicates. To construct the expert set, we blindly merged the Dense set with our worker annotations and manually corrected them. We further corrected the evaluation decisions, accounting for some automatic evaluation mistakes introduced by the span-matching and question paraphrasing criteria. As seen in Table TABREF19, our gold set yields comparable precision with significantly higher recall, which is in line with our 25% higher yield.
Dataset Quality Analysis ::: Agreement with PropBank Data
It is illuminating to observe the agreement between QA-SRL and PropBank (CoNLL-2009) annotations BIBREF7. In Table TABREF22, we replicate the experiments in BIBREF4 for both our gold set and theirs, over a sample of 200 sentences from Wall Street Journal (agreement evaluation is automatic and the metric is somewhat similar to our UA). We report macro-averaged (over predicates) precision and recall for all roles, including core and adjuncts, while considering the PropBank data as the reference set. Our recall of the PropBank roles is notably high, reconfirming the coverage obtained by our annotation protocol.",F1.,0.0,0.0,0.0,2.839838722567789,1.5432098765432098,5.876068376068376,0.0390625,0.0009082652134423,0.1536645144224167,0.1323658376932144,0.1536645144224167,,0.0029453517001616,0.0593615211546421,1,1.0,1.0,,0.8386104938552966,0.4862524705608446,0.7450831653689294,0.8,0.0593615211546421
94,94,What is different in the improved annotation protocol?,A trained worker consolidates existing annotations.,"We adopt the annotation machinery of BIBREF5 implemented using Amazon's Mechanical Turk, and annotate each predicate by 2 trained workers independently, while a third consolidates their annotations into a final set of roles and arguments. In this consolidation task, the worker validates questions, merges, splits or modifies answers for the same role according to guidelines, and removes redundant roles by picking the more naturally phrased questions. For example, in Table TABREF4 ex. 1, one worker could have chosen “47 people”, while another chose “the councillor”; in this case the consolidator would include both of those answers. In Section SECREF4, we show that this process yields better coverage. For example annotations, please refer to the appendix.",Consolidation by a third worker.,0.1818181768595042,0.0,0.1818181768595042,7.809849842300637,47.10186346681496,39.46187865238743,0.2898550724637681,0.0082644628099173,0.339984118938446,0.5831581971475056,0.339984118938446,0.6199668049812317,0.0184619662498382,0.5125430226325989,4,1.0,4.0,1.0,0.8065968359557889,0.216337927496728,0.8653517099869121,0.8,0.5125430226325989
95,95,What is task success rate achieved? ,96-97.6% using the objects color or shape and 79% using shape alone.,"To test our model, we generated 500 new scenario testing each of the three features to identify the correct target among other bowls. A task is considered to be successfully completed when the cube is withing the boundaries of the targeted bowl. Bowls have a bounding box of 12.5 and 17.5cm edge length for the small and large variant, respectively. Our experiments showed that using the objects color or shape to uniquely identify an object allows the robot successfully complete the binning task in 97.6% and 96.0% of the cases. However, using the shape alone as a unique identifier, the task could only be completed in 79.0% of the cases. We suspect that the loss of accuracy is due to the low image resolution of the input image, preventing the network from reliably distinguishing the object shapes. In general, our approach is able to actuate the robot with an target error well below 5cm, given the target was correctly identified.",97.6%,0.1538461512426036,0.0,0.1538461512426036,4.5739135561238005,22.176772663121337,19.29918687838014,0.0364963503649635,0.0008326394671107,0.6181261539459229,0.0,0.6181261539459229,,0.0492488968338132,0.2835463881492615,4,1.0,1.0,1.0,0.8434862625045185,0.7145769692189365,0.8583078768757464,0.0,0.2835463881492615
96,96,How is performance of this system measured?,Using the BLEU score as a quantitative metric and human evaluation for quality.,"We use the BLEU BIBREF30 metric on the validation set for the VQG model training. BLEU is a measure of similitude between generated and target sequences of words, widely used in natural language processing. It assumes that valid generated responses have significant word overlap with the ground truth responses. We use it because in this case we have five different references for each of the generated questions. We obtain a BLEU score of 2.07.
Our chatbot model instead, only have one reference ground truth in training when generating a sequence of words. We considered that it was not a good metric to apply as in some occasions responses have the same meaning, but do not share any words in common. Thus, we save several models with different hyperparameters and at different number of training iterations and compare them using human evaluation, to chose the model that performs better in a conversation.","BLEU for VQG, human evaluation for chatbot.",0.4210526272576177,0.2222222177777778,0.4210526272576177,12.011055432195764,46.03181764073648,43.59447087258523,0.3303703703703704,0.0077177508269018,0.4860081970691681,0.601037081996245,0.4860081672668457,0.7123921513557434,0.0042733628298754,0.3048781156539917,4,1.0,4.0,1.0,0.8161731356448682,0.7194483017690994,0.8777932070763976,0.8,0.3048781156539917
97,97,How big dataset is used for training this system?,"For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues.","We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions. COCO dataset includes images of complex everyday scenes containing common objects in their natural context, but it is limited in terms of the concepts it covers. Bing dataset contains more event related questions and has a wider range of questions longitudes (between 3 and 20 words), while Flickr questions are shorter (less than 6 words) and the images appear to be more casual.
We use two datasets to train our chatbot model. The first one is the Persona-chat BIBREF15 which contains dialogues between two people with different profiles that are trying to know each other. It is complemented by the Cornell-movie dialogues dataset BIBREF27, which contains a collection of fictional conversations extracted from raw movie scripts. Persona-chat's sentences have a maximum of 15 words, making it easier to learn for machines and a total of 162,064 utterances over 10,907 dialogues. While Cornell-movie dataset contains 304,713 utterances over 220,579 conversational exchanges between 10,292 pairs of movie characters.","75,000 questions and 467,777 utterances.",0.2608695618147448,0.0909090879338843,0.2608695618147448,3.916449665002248,44.75942239890391,40.41126820086315,0.1481807511737089,0.0026246719160104,0.6299735903739929,0.625,0.5722572207450867,,0.050512272142383,0.4160879552364349,4,0.5,2.0,0.5,0.858350343172039,0.8250872981244124,0.9003491924976491,0.9,0.4160879552364349
98,98,How do they obtain word lattices from words?,By considering words as vertices and generating directed edges between neighboring words within a sentence.,"Word Lattice
As shown in Figure FIGREF4 , a word lattice is a directed graph INLINEFORM0 , where INLINEFORM1 represents a node set and INLINEFORM2 represents a edge set. For a sentence in Chinese, which is a sequence of Chinese characters INLINEFORM3 , all of its possible substrings that can be considered as words are treated as vertexes, i.e. INLINEFORM4 . Then, all neighbor words are connected by directed edges according to their positions in the original sentence, i.e. INLINEFORM5 .",Treating substrings as vertexes and connecting neighbor words with edges.,0.3333333284722222,0.0,0.2499999951388889,3.6570159134143823,45.66693981092728,39.45858017384187,0.275089605734767,0.0082644628099173,0.5633320808410645,0.7206682827440614,0.5633320212364197,0.8787235021591187,0.0236220098293319,0.1112313345074653,3,1.0,3.0,1.0,0.8984028955544816,0.7209893287553415,0.8839965303698375,0.8,0.1112313345074653
99,99,How better is proposed method than baselines perpexity wise?,Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set.,"Table TABREF34 gives the perplexity scores obtained by the three models on the two validation sets and the test set. As shown in the table, MEED achieves the lowest perplexity score on all three sets. We also conducted t-test on the perplexity obtained, and results show significant improvements (with $p$-value $<0.05$).
FLOAT SELECTED: Table 2: Perplexity scores achieved by the models. Validation set 1 comes from the Cornell dataset, while validation set 2 comes from the DailyDialog dataset.",significantly.,0.0,0.0,0.0,2.2869567780619007,7.245287274230834,8.041928846362492,0.032258064516129,0.0006246096189881,0.2060561180114746,0.4359203577041626,0.2060561627149582,,0.0021128489337113,0.0829828605055809,3,0.0,1.0,,0.8216249940758488,0.6885382891646823,0.7540572982563734,0.8,0.0829828605055809
