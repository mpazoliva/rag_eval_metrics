,question,context,correct_answer,iris_answer,Rouge1,Rouge2,RougeL,Bleu,Chrf,ChrfPlus,Meteor,Ter,Bert,WMS,SMS,Wisdm,Bleurt,BEM,Bart,Prometheus,Faithfullness,Relevancy,Correctness,RSim,Consistency,TSim,LLM
0,How do these automated metrics for human preferences differ and what factors do they consider when predicting human preferences?,"The ultimate aim of language technology is to interact with humans.However, most language models are trained without direct signals of human preference,with supervised target strings serving as (a sometimes crude) proxy.One option to incorporate user feedback is via human-in-the-loop, i.e., a user would be expected to provide feedback for each sample online as the model trains, but this degree of dense supervision is often prohibitive and inefficient.Automated metrics offer a promising compromise: learned models of human preference like BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020), summarization preferences (Wu et al., 2021) have significantly improved correlation with human judgment compared to earlier metrics (BLEU, METEOR, etc.), and are cheap to evaluate. But — these functions are usually not per-token differentiable: like humans, metrics can only offer quality estimates for full generations.Reinforcement Learning (RL) offers a natural path forward for optimizing non-differentiable, scalar objectives for LM-based generation when it is cast as a sequential decision-making problem. However, Goodhart’s Law222Strathern (1997) paraphrases: When a measure becomes a target, it ceases to be a good measure.looms: particularly in the case of imperfect metrics that use neural networks, it is easy to find nonsense samples that achieve high-quality estimates. Recent works have shown promising results in aligning LMs to human preferences via RL by constraining preference-based rewards to incorporate notions of fluency (Wu et al., 2021; Ouyang et al., 2022) but progress in this line of work is heavily hindered by a lack of open-source benchmarks and algorithmic implementations—resulting in perception that RL is a challenging paradigm for NLP (Choshen et al., 2020; Kreutzer et al., 2021).","The automated metrics that are mentioned while discussing related work are BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020), and Ouyang et al (2022). More information on these automated metrics, including the differences between them, can probably be gleaned by reading these cited works. The current paper does not contain any additional information about these related automated metrics.","These automated metrics for human preferences, such as BERTScore, BLEURT, and summarization preferences, differ in the factors they consider when predicting human preferences. While earlier metrics like BLEU and METEOR rely solely on linguistic features, the newer metrics incorporate human feedback and preferences, such as fluency and coherence, to provide a more comprehensive assessment of language quality",0.1075268767256332,0.0180180130216717,0.1075268767256332,2.0731870502974603,30.87057062543038,26.75670140440155,0.1486486486486486,0.0097319446815776,0.5783581733703613,0.3896329830584675,0.554050475358963,0.4231174886226654,0.1227419003844261,0.0811537504196167,0.0191559660746944,3,0.75,0.9658358002778544,0.4745543954455092,0.8983148115392596,1.0,2.0,0.8
1,What does non-differentiable mean here? If the problem with previous metrics is that they are not per-token differentiable then why are they looking for a way to optimize non-differentiable objectives?,"The ultimate aim of language technology is to interact with humans.However, most language models are trained without direct signals of human preference,with supervised target strings serving as (a sometimes crude) proxy.One option to incorporate user feedback is via human-in-the-loop, i.e., a user would be expected to provide feedback for each sample online as the model trains, but this degree of dense supervision is often prohibitive and inefficient.Automated metrics offer a promising compromise: learned models of human preference like BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020), summarization preferences (Wu et al., 2021) have significantly improved correlation with human judgment compared to earlier metrics (BLEU, METEOR, etc.), and are cheap to evaluate. But — these functions are usually not per-token differentiable: like humans, metrics can only offer quality estimates for full generations.Reinforcement Learning (RL) offers a natural path forward for optimizing non-differentiable, scalar objectives for LM-based generation when it is cast as a sequential decision-making problem. However, Goodhart’s Law222Strathern (1997) paraphrases: When a measure becomes a target, it ceases to be a good measure.looms: particularly in the case of imperfect metrics that use neural networks, it is easy to find nonsense samples that achieve high-quality estimates. Recent works have shown promising results in aligning LMs to human preferences via RL by constraining preference-based rewards to incorporate notions of fluency (Wu et al., 2021; Ouyang et al., 2022) but progress in this line of work is heavily hindered by a lack of open-source benchmarks and algorithmic implementations—resulting in perception that RL is a challenging paradigm for NLP (Choshen et al., 2020; Kreutzer et al., 2021).","A formal definition of non-differentiability has not been provided by the authors. However, authors in this context use this idea of differentiability to explain that most automated metrics cannot provide quality estimates after a language model generates one token or a parietal output - similar to humans, they can provide quality estimates only after a language model generates a full sequence. This non-differentiability is pointed out as a problem by the authors for the new metrics (i.e. things like BLUERT or BERTScore) - it is unclear from this paper alone if previous vanilla metrics (i.e. BLEU) were per token differentiable or not. The authors are looking for ways to use these non-differentiable scores in RL frameworks, potentially as a reward function.","Non-differentiable here refers to the fact that the metrics used to evaluate the quality of generated text, such as BERTScore, BLEURT, and summarization preferences, are not differentiable at the individual token level. Instead, they provide a single quality estimate for a full generation of text. This makes it difficult to optimize these metrics using traditional reinforcement learning methods, which rely on differentiability to update model parameters. The authors are looking for a way to optimize non-differentiable objectives using reinforcement learning despite this limitation",0.3466666617555556,0.0631578898786707,0.3066666617555555,6.047905131969382,46.33480255500867,41.27584243680736,0.2729476763806793,0.0079175808451779,0.5889645218849182,0.6450391623099295,0.5086509436368942,0.6943204998970032,0.2458132952451706,0.2807299792766571,0.0507469810945572,4,1.0,0.8717788625446147,0.7330833684764582,0.9323234750955506,0.75,4.0,0.95
2,"Why is the action space of language modeling particularly large? Is it because of the vocab size? But then, moving in the real world also has a huge action space (degrees of movement).","Language generation action spaces are orders of magnitude larger than what most discrete action space RL algorithms are designed for (Ranzato et al., 2015; Ammanabrolu, 2021), e.g., GPT-2/3 and T5 have a vocabulary size of 50K and 32K respectively.We hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods.To address this issue, we introduce NLPO (Natural Language Policy Optimization), which is inspired by work on action elimination/invalid-action masking (Zahavy et al., 2018; Huang & Ontañón, 2020; Ammanabrolu & Hausknecht, 2020). NLPO, a parameterized-masked extension of PPO, learns to mask out less relevant tokens in-context as it trains. NLPO accomplishes this via top-p sampling, which restricts tokens to the smallest possible set whose cumulative probability is greater than the probability parameter p (Holtzman et al., 2018). RL for Large Action Spaces. MIXER (Ranzato et al., 2015) combined ideas from schedule sampling and REINFORCE (Williams, 1992).Bahdanau et al. (2016) proposed an actor-critic algorithm to address the variance/large action space problems when using REINFORCE for language generation; follow-up works such asKG-A2C (Ammanabrolu & Hausknecht, 2020), TrufLL (Martin et al., 2022), AE-DQN (Zahavy et al., 2018), and GALAD (Ammanabrolu et al., 2022) addressed similar issues by attempting to eliminate and reduce the action space during exploration. Each environment is an NLP task: we are given a supervised dataset \mathcal{D}=\{({\bm{x}}^{i},{\bm{y}}^{i})\}_{i=1}^{N} of N examples, where {\bm{x}}\in\mathcal{X} is an language input and {\bm{y}}\in\mathcal{Y} is the target string. Generation can be viewed as a Markov Decision Process (MDP) \langle\mathcal{S},\mathcal{A},\mathcal{R},P,\gamma,T\rangle using a finite vocabulary \mathcal{V}.Each episode in the MDP begins by sampling a datapoint ({\bm{x}},{\bm{y}}) from our dataset and ends when the current time step t exceeds the horizon T or an end of sentence (EOS) token is generated.The input {\bm{x}}=(x_{0},\cdots,x_{m}) is a task-specific prompt that is used as our initial state \bm{s}_{0}=(x_{0},\cdots,x_{m}), where \bm{s}_{0}\in\mathcal{S} and \mathcal{S} is the state space with x_{m}\in\mathcal{V}. An action in the environment a_{t}\in\mathcal{A} consists of a token from our vocabulary \mathcal{V}. The transition function P:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S} deterministically appends an action a_{t} to the end of the state \bm{s}_{t-1}=(x_{0},\cdots,x_{m},a_{0},\cdots,a_{t-1}). This continues until the end of the horizon t\leq T and we obtain a state \bm{s}_{T}=(x_{0},\cdots,x_{m},a_{0},\cdots,a_{T}).At the end of an episode a reward \mathcal{R}:\mathcal{S}\times\mathcal{A}\times\mathcal{Y}\rightarrow\mathbb{R}^{1} that depends on the (\bm{s}_{T},{\bm{y}}) (e.g., an automated metric like PARENT Dhingra et al. (2019)) is emitted. RL4LMs provides an OpenAI gym (Brockman et al., 2016) styleAPI for an RL environmentthat simulates this LM-Based MDP formulation.Abstracting the details of the MDP environment structure allows for new tasks to be added quickly with compatibility across all implemented algorithms.","The action space for language modeling is equal to the vocabulary set of language models. Since the vocabularies are very large (i.e. tens of thousands of possible tokens), the action space is also very large. In general, locomotion in the real world can be condensed to three quantities - moving across X, Y or Z axes, or a linear combination thereof. The authors mention that typical RL problems have an action space that is an order of magnitude smaller, but do not specifically explain how the action spaces for typical problems is modeled or constructed.","No, the action space of language modeling is not solely due to the vocabulary size. While the vocabulary size is a contributing factor, the action space is larger because of the inherent structure of language.

In language modeling, the action space includes not only the vocabulary size but also the order and sequence of tokens. This means that the number of possible actions is exponentially larger than the vocabulary size, making it challenging to learn and optimize policies.

In contrast, the action space in the real world, such as degrees of movement, is discrete and finite, making it easier to learn and optimize policies. Therefore, the action space of language modeling is particularly large due to the inherent structure of language, rather than just the vocabulary size",0.3007518747063147,0.1176470538271042,0.2706766867363898,7.904009425172115,37.51340038640081,34.18443248657763,0.2977476118652589,0.0121799175218183,0.7524452805519104,0.7025045810890508,0.5916765093803406,0.835239589214325,0.4309799671173095,0.2681870460510254,0.0516696646920383,4,0.8333333333333334,0.9302813285947714,0.4810685162088275,0.9243150653621944,0.75,2.0,0.7
3,What do the equations for Q-value and value represent?,"RL4LMs supports fine-tuning and training LMs from scratch via on-policy actor-critic algorithms on language environments.Formally, this class of algorithms allows us to train a parameterized control policy defined as \pi_{\theta}:\mathcal{S}\rightarrow\mathcal{A}, a function that attempts to select an action in a given state so as to maximize long term discounted rewards over a trajectory \mathbb{E}_{\pi}[\sum_{t=0}^{T}\gamma^{t}\mathcal{R}(\bm{s}_{t},a_{t})].Our benchmark experiments focus on fine-tuning a pre-trained LM denoted as \pi_{0} from which we initial our agent’s policy \pi_{\theta}=\pi_{0}.Similarly, the value network V_{\phi} used to estimate the value function is also initialized from \pi_{0} except for the final layer which is randomly initialized to output a single scalar value.As with other deep RL actor-critic algorithms, we define our value and Q-value functions as V_{t}^{\pi}=\mathbb{E}_{a_{t}\sim\pi}[\sum_{\tau=t}^{T}\gamma R(\bm{s}_{\tau},a_{\tau},{\bm{y}})],Q_{t}^{\pi}(\bm{s}_{t},a_{t})=R(\bm{s}_{t},a_{t},{\bm{y}})+\gamma\mathbb{E}_{s_{t+1}\sim P}[V_{t+1}^{\pi}(\bm{s}_{t+1})]leading to a definition of our advantage function as A_{t}^{\pi}(\bm{s},a)=Q_{t}^{\pi}(\bm{s},a)-V_{t}^{\pi}.To increase training stability, advantage is appoximated using Generalized Advantage Estimation (Schulman et al., 2015).","Q and V are mathematically expressed as: V_{t}^{\pi}=\mathbb{E}_{a_{t}\sim\pi}[\sum_{\tau=t}^{T}\gamma R(\bm{s}_{\tau},a_{\tau},{\bm{y}})],Q_{t}^{\pi}(\bm{s}_{t},a_{t})=R(\bm{s}_{t},a_{t},{\bm{y}})+\gamma\mathbb{E}_{s_{t+1}\sim P}[V_{t+1}^{\pi}(\bm{s}_{t+1})] where R is the reward function, s means states, and the variable 'a' denotes actions. Further information on what these values mean, or their interpretation is not found in this paper.","The equations for Q-value and value represent the expected return or value of taking a particular action in a particular state, and the expected return or value of taking any action in any state, respectively.

Specifically, the Q-value equation, Q_t^π(s_t, a_t) = R(s_t, a_t, y) + γ \* E_s[V_t^π(s_t+1)], represents the expected return of taking action a_t in state s_t and then following policy π thereafter, where E_s[ ] represents the expected value over the next state distribution.

The value equation, V_t^π(s_t) = E_a[Q_t^π(s_t, a_t)], represents the expected value of taking any action in state s_t, where E_a[ ] represents the expected value over all possible actions",0.1290322532315876,0.0168067182261151,0.1290322532315876,5.46967832634927,18.03420928648931,15.384644073926273,0.1047565118912797,0.0103812942660327,0.8276561498641968,0.3926379904954132,0.5937853157520294,0.6798534393310547,0.1149453297257423,0.0889103710651397,0.0313987383157709,4,,0.9999999999999996,,0.936192580426734,0.75,4.0,0.98
4,Why is it helpful to mask out less relevant tokens if these are less likely to be sampled anyways?,"Specifically, NLPOmaintains a masking policy \pi_{\psi}: the masking policy is a copy of the current policy (\pi_{\theta}), but is updated only every \mu steps.A parameterized-invalid-mask is created from \pi_{\psi} by first selecting the top-p tokens from the vocabulary,444\pi_{\psi} could be trained with alternate sampling techniques like top-k or beam search (or even hard-coded via rules by domain experts), though we find top-p sampling to be most effective in practice. and then applying an invalid-mask to the remaining tokens—i.e. setting their probabilities to zero when sampling actions from \pi_{\theta} during training;this periodic updating policy \pi_{\psi} is inspired by off-policy Q-learning algorithms (Andrychowicz et al., 2017),providing the policy \pi_{\theta} with an additional constraint that balances between the benefits of containing more task relevant information than the KL penalty derived from \pi_{0} and the risk of reward hacking.We provide pseudocode in Algorithm 1 (green portions highlight the differences with PPO). PPO vs. NLPO.Figure 2 shows that NLPO generally outperforms PPO and supervised, especially when applied after supervised training.We hypothesize that the primary reason for NLPO’s improved performance and stability is because the masking policy provides an additional constraint for the current policy.This constraint is not based on the initial untuned policy like the KL penalty but of the policy from \mu iterations ago and likely contains more task-relevant information learned during RL training.Table 3 (and Appendix Table 8) shows how performance increases up to a point and then decreases as p in top-p sampling is increased for the masking policy, relaxing the constraint by eliminating less tokens at each step, implying that there is a balance to be found in how much the model should be constrained during RL training.","The authors hypothesize that their dynamic masking function helps because it adds a new constraint that the RL algorithm has to abide by. Additionally, since this is a dynamic function, being updated oftenly (every mu steps), it is likely that the masking function ensures that the selected top-p tokens are more relevant to the current state the RL algorithm needs to analyse and decide on.",Masking out less relevant tokens helps the model learn more task-relevant information and avoid reward hacking,0.18749999625,0.0,0.1249999962500001,0.7849795180306601,27.69961051649875,23.58922424431766,0.0527108433734939,0.0025740025740025,0.4818413853645324,0.6236818450357817,0.4271863996982574,0.1518009901046753,0.1850329935550689,0.2616256773471832,0.005237039604007,3,1.0,0.9319106245993124,0.5095563274170416,0.8382879812520502,1.0,2.0,
5,Why is the masking policy only updated every certain number of steps?,"Language generation action spaces are orders of magnitude larger than what most discrete action space RL algorithms are designed for (Ranzato et al., 2015; Ammanabrolu, 2021), e.g., GPT-2/3 and T5 have a vocabulary size of 50K and 32K respectively.We hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods.To address this issue, we introduce NLPO (Natural Language Policy Optimization), which is inspired by work on action elimination/invalid-action masking (Zahavy et al., 2018; Huang & Ontañón, 2020; Ammanabrolu & Hausknecht, 2020). NLPO, a parameterized-masked extension of PPO, learns to mask out less relevant tokens in-context as it trains. NLPO accomplishes this via top-p sampling, which restricts tokens to the smallest possible set whose cumulative probability is greater than the probability parameter p (Holtzman et al., 2018). Specifically, NLPOmaintains a masking policy \pi_{\psi}: the masking policy is a copy of the current policy (\pi_{\theta}), but is updated only every \mu steps.A parameterized-invalid-mask is created from \pi_{\psi} by first selecting the top-p tokens from the vocabulary,444\pi_{\psi} could be trained with alternate sampling techniques like top-k or beam search (or even hard-coded via rules by domain experts), though we find top-p sampling to be most effective in practice. and then applying an invalid-mask to the remaining tokens—i.e. setting their probabilities to zero when sampling actions from \pi_{\theta} during training;this periodic updating policy \pi_{\psi} is inspired by off-policy Q-learning algorithms (Andrychowicz et al., 2017),providing the policy \pi_{\theta} with an additional constraint that balances between the benefits of containing more task relevant information than the KL penalty derived from \pi_{0} and the risk of reward hacking.We provide pseudocode in Algorithm 1 (green portions highlight the differences with PPO). PPO vs. NLPO.Figure 2 shows that NLPO generally outperforms PPO and supervised, especially when applied after supervised training.We hypothesize that the primary reason for NLPO’s improved performance and stability is because the masking policy provides an additional constraint for the current policy.This constraint is not based on the initial untuned policy like the KL penalty but of the policy from \mu iterations ago and likely contains more task-relevant information learned during RL training.Table 3 (and Appendix Table 8) shows how performance increases up to a point and then decreases as p in top-p sampling is increased for the masking policy, relaxing the constraint by eliminating less tokens at each step, implying that there is a balance to be found in how much the model should be constrained during RL training.","The authors mention that they update the masking function every ""mu"" steps, but the main text of the paper itself does not appear to contain the exact value of mu itself - there is a possibility that the author's model could work with mu=1 instead (i.e. update every step) instead of updating it every couple of steps (i.e. mu > 1), though the authors do not explain in this paper if this were done. However, the authors mention that one reason why NLPO outperforms PPO is probably because of this dynamic masking function that is updated occasionally - this indicates to us that mu is probably not an unbounded or very large number (if mu were very large, tending to infinity, the function would no longer be dynamic).",To balance between the benefits of containing task-relevant information and the risk of reward hacking,0.0412371110808801,0.0,0.0412371110808801,0.3344575135674186,18.18805157314064,14.918055450969838,0.0225733634311512,0.0012180267965895,0.0741159245371818,0.4576068849653033,0.0880975127220153,0.3135266602039337,0.0790308713912963,0.0365874171257019,0.0042167508200353,3,1.0,0.705892030495768,0.3756998350170967,0.7527813329776194,1.0,1.0,
6,How did the token-masking policy help in the results?,"Specifically, NLPOmaintains a masking policy \pi_{\psi}: the masking policy is a copy of the current policy (\pi_{\theta}), but is updated only every \mu steps.A parameterized-invalid-mask is created from \pi_{\psi} by first selecting the top-p tokens from the vocabulary,444\pi_{\psi} could be trained with alternate sampling techniques like top-k or beam search (or even hard-coded via rules by domain experts), though we find top-p sampling to be most effective in practice. and then applying an invalid-mask to the remaining tokens—i.e. setting their probabilities to zero when sampling actions from \pi_{\theta} during training;this periodic updating policy \pi_{\psi} is inspired by off-policy Q-learning algorithms (Andrychowicz et al., 2017),providing the policy \pi_{\theta} with an additional constraint that balances between the benefits of containing more task relevant information than the KL penalty derived from \pi_{0} and the risk of reward hacking.We provide pseudocode in Algorithm 1 (green portions highlight the differences with PPO). PPO vs. NLPO.Figure 2 shows that NLPO generally outperforms PPO and supervised, especially when applied after supervised training.We hypothesize that the primary reason for NLPO’s improved performance and stability is because the masking policy provides an additional constraint for the current policy.This constraint is not based on the initial untuned policy like the KL penalty but of the policy from \mu iterations ago and likely contains more task-relevant information learned during RL training.Table 3 (and Appendix Table 8) shows how performance increases up to a point and then decreases as p in top-p sampling is increased for the masking policy, relaxing the constraint by eliminating less tokens at each step, implying that there is a balance to be found in how much the model should be constrained during RL training.","The use of token-masking policy in the proposed method (NLPO) is theorized by the authors to have been a key reason on why it was able to outperform the PPO based model. They hypothesized that their masking function acts as a dynamic constraint added to the algorithm, that is able to more effectively filter and capture relevant information about the state thanks to its' dynamic nature.","The token-masking policy helped by providing an additional constraint for the current policy, not based on the initial untuned policy like the KL penalty, but of the policy from \mu iterations ago, likely containing more task-relevant information learned during RL training",0.2471910064183816,0.0192307644970425,0.2022471861936625,1.683921026979718,36.05774216976139,31.04346643510916,0.1083815028901734,0.0070193460023968,0.5732196569442749,0.5382587959050215,0.5467941761016846,0.3442212045192718,0.1691690236330032,0.0967616066336631,0.0043926918895519,4,1.0,0.8704170511522857,0.7206950636106463,0.881798260309275,1.0,4.0,0.95
7,What is the combinatorial action space? How is this different to general RL tasks? Are they not combinatorial?,"Language generation action spaces are orders of magnitude larger than what most discrete action space RL algorithms are designed for (Ranzato et al., 2015; Ammanabrolu, 2021), e.g., GPT-2/3 and T5 have a vocabulary size of 50K and 32K respectively.We hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods.To address this issue, we introduce NLPO (Natural Language Policy Optimization), which is inspired by work on action elimination/invalid-action masking (Zahavy et al., 2018; Huang & Ontañón, 2020; Ammanabrolu & Hausknecht, 2020). NLPO, a parameterized-masked extension of PPO, learns to mask out less relevant tokens in-context as it trains. NLPO accomplishes this via top-p sampling, which restricts tokens to the smallest possible set whose cumulative probability is greater than the probability parameter p (Holtzman et al., 2018). Each environment is an NLP task: we are given a supervised dataset \mathcal{D}=\{({\bm{x}}^{i},{\bm{y}}^{i})\}_{i=1}^{N} of N examples, where {\bm{x}}\in\mathcal{X} is an language input and {\bm{y}}\in\mathcal{Y} is the target string. Generation can be viewed as a Markov Decision Process (MDP) \langle\mathcal{S},\mathcal{A},\mathcal{R},P,\gamma,T\rangle using a finite vocabulary \mathcal{V}.Each episode in the MDP begins by sampling a datapoint ({\bm{x}},{\bm{y}}) from our dataset and ends when the current time step t exceeds the horizon T or an end of sentence (EOS) token is generated.The input {\bm{x}}=(x_{0},\cdots,x_{m}) is a task-specific prompt that is used as our initial state \bm{s}_{0}=(x_{0},\cdots,x_{m}), where \bm{s}_{0}\in\mathcal{S} and \mathcal{S} is the state space with x_{m}\in\mathcal{V}. An action in the environment a_{t}\in\mathcal{A} consists of a token from our vocabulary \mathcal{V}. The transition function P:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S} deterministically appends an action a_{t} to the end of the state \bm{s}_{t-1}=(x_{0},\cdots,x_{m},a_{0},\cdots,a_{t-1}). This continues until the end of the horizon t\leq T and we obtain a state \bm{s}_{T}=(x_{0},\cdots,x_{m},a_{0},\cdots,a_{T}).At the end of an episode a reward \mathcal{R}:\mathcal{S}\times\mathcal{A}\times\mathcal{Y}\rightarrow\mathbb{R}^{1} that depends on the (\bm{s}_{T},{\bm{y}}) (e.g., an automated metric like PARENT Dhingra et al. (2019)) is emitted. RL4LMs provides an OpenAI gym (Brockman et al., 2016) styleAPI for an RL environmentthat simulates this LM-Based MDP formulation.Abstracting the details of the MDP environment structure allows for new tasks to be added quickly with compatibility across all implemented algorithms.","The combinatorial action space here probably refers to the set of all possible actions that a RL agent for optimizing a language model could possibly take - here, the action set consists of the entire vocabulary of the language model, which can range to tens of thousands for typical GPT/T5 models used today. This is unlike general RL tasks, where the action space is an order of magnitude smaller.","The combinatorial action space is the set of all possible sequences of tokens that can be generated by the language model, which is orders of magnitude larger than the action spaces of most discrete action space RL algorithms. This is different from general RL tasks, which typically have a finite set of discrete actions. In the context of language generation, the action space is not just a set of discrete actions, but rather a continuous space of possible sequences of tokens",0.4444444394490358,0.2370370320438958,0.3838383788429753,20.26812305056288,46.57949048098889,44.601153544213815,0.4317902676302486,0.0128960356631109,0.8918936252593994,0.732810264412465,0.8359309434890747,0.8781159520149231,0.23682302236557,0.6320191025733948,0.0845626157448447,4,0.25,0.9036639909078902,0.5721419735816123,0.9552326897154224,1.0,2.0,0.8
8,"For the images used for visualization in the paper, were they selected randomly or picked by the authors?","If the above regularization methods are applied individually, they aresomewhat effective at producing more interpretable images; Figure 3 shows the effects of each individual hyperparameter.However, preliminary experiments uncovered that their combinedeffect produces better visualizations. To pick a reasonable set ofhyperparameters for all methods at once, we ran a randomhyperparameter search of 300 possible combinations and settled on fourthat complement each other well. The four selected combinations arelisted in Table 1 and optimized images using each are shown for the “Gorilla” class output unit in Figure 4. Of the four, some show highfrequency information, others low frequency; some contain densepixel data, and others contain only sparse outlines of importantregions.We found the version in the lower-left quadrant to be the best single set of hyperparameters, but often greater intuition canbe gleaned by considering all four at once.Figure 5 shows the optimization results computed for a selection of units on all layers. A single image for every filter of all five convolutional layers is shown in Supplementary Figure S1. Nine images for each filter of all layers, including each of the 1000 ImageNet output classes, can be viewed at http://yosinski.com/deepvis.","Authors best practices were to combine effects of different ways of regularization to produce interpretable images. They first search randomly through 300 different combinations of  hyperparameters, then they pick the best four sets of hyperparameters that are compliments to each other and then these sets would be used to visualize preferred images for different classes.","The images were not selected randomly, but were picked by the authors",0.111111107866941,0.0,0.0370370337928672,0.9359100614362484,25.63393154647688,22.097486403455548,0.0747663551401869,0.0022590361445783,0.387358695268631,0.7642366961999373,0.4457965195178985,0.7091456055641174,0.3076731562614441,0.6191736459732056,0.0371138321400162,3,0.5,0.9096928817869676,0.4168907074836107,0.8100902069745214,1.0,4.0,0.8
9,Does the paper's DNN use a a larger width kernel or multiple smaller width kernels? ,"Gaussian blur: Producing images via gradient ascent tends to produce examples with high frequency information (see Supplementary Section S1 for a possible reason). While these images cause high activations, they are neither realistic nor interpretable (Nguyen et al., 2014). A useful regularization is thus to penalize high frequency information. We implement this as a Gaussian blur step rθ (x) = GaussianBlur(x, θb width). Convolving with a blur kernel is more computationally expensive than the other regularization methods, so we added another hyperparameter θb every to allow, for example, blurring every several optimization steps instead of every step. Blurring an image multiple times with a small width Gaussian kernel is equivalent to blurring once with a larger width kernel, and the effect will be similar even if the image changes slightly during the optimization process. This technique thus lowers computational costs without limiting the expressiveness of the regularization. Mahendran & Vedaldi (2014) used a penalty with a similar effect to blurring, called total variation, in their work reconstructing images from layer codes.",The paper's DNN uses multiple smaller Gaussian kernels iteratively as a way of regularization during the optimization process as seen in equation 2 .,Multiple smaller width kernels,0.1538461512426036,0.0,0.1538461512426036,1.8709718017288024,38.04646038264031,31.683710745156564,0.1115963124696749,0.0019011406844106,0.4199300706386566,0.8294286290248791,0.419930100440979,0.5628637671470642,0.444452166557312,0.9783784747123718,0.0025774747733026,3,0.0,0.7867307413260716,0.7074587571018407,0.8298794419572755,0.75,4.0,0.8
10,Why did the authors choose the four particular regularizations instead of others?,"We investigated the following four regularizations. All are designed to overcome different pathologies commonly encountered by gradient descent without regularization. Gaussian blur: Producing images via gradient ascent tends to produce examples with high frequency information (see Supplementary Section S1 for a possible reason). While these images cause high activations, they are neither realistic nor interpretable (Nguyen et al., 2014). A useful regularization is thus to penalize high frequency information. We implement this as a Gaussian blur step r_{\theta}(\mathbf{x})=\mathrm{GaussianBlur}(\mathbf{x},\theta_{\mathrm{b\_width}}). Convolving with a blur kernel is more computationally expensive than the other regularization methods, so we added another hyperparameter \theta_{\mathrm{b\_every}} to allow, for example, blurring every several optimization steps instead of every step. Blurring an image multiple times with a small width Gaussian kernel is equivalent to blurring once with a larger width kernel, and the effect will be similar even if the image changes slightly during the optimization process. This technique thus lowers computational costs without limiting the expressiveness of the regularization. Mahendran & Vedaldi (2014) used a penalty with a similar effect to blurring, called total variation, in their work reconstructing images from layer codes. The second tool — new regularizations that enable improved, interpretable, optimized visualizations of learned features — will help researchers and practitioners understand, debug, and improve their models. The visualizations also reveal a new twist in an ongoing story. Previous studieshave shown that discriminative networks can easily be fooled or hacked by the addition of certain structurednoise in image space (Szegedy et al., 2013; Nguyen et al., 2014).An oft-cited reason for this property is that discriminative training leads networksto ignore non-discriminative information in their input, e.g. learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs. For this reason it has been seen as a hopeless endeavor to create a generative model in which one randomly samples an x from a broad distribution on the space of all possible imagesand then iteratively transforms x into a recognizable image by moving it to a region that satisfies both a prior p(x) and posterior p(y|x) for some class label y.Past attempts have largely supported this view by producing unrealistic images using this method (Nguyen et al., 2014; Simonyan et al., 2013). Paragraph 10 : L2 decay: A common regularization, L2 decay penalizes large values and is implemented as rθ (x) = (1−θdecay)·x. L2 decay tends to prevent a small number of extreme pixel values from dominating the example image. Such extreme single-pixel values neither occur naturally with great frequency nor are useful for visualization. Paragraph 11 :Clipping pixels with small norm: The first two regularizations suppress high amplitude and high frequency information, so after applying both, we are left with an x∗ that contains somewhat small, somewhat smooth values. However, x∗ will still tend to contain non-zero pixel values everywhere. Even if some pixels in x∗ show the primary object or type of input causing the unit under consideration to activate, the gradient with respect to all other pixels in x∗ will still generally be non-zero, so these pixels will also shift to show some pattern as well, contributing in whatever small way they can to ultimately raise the chosen unit’s activation. We wish to bias the search away from such behavior and instead show only the main object, letting other regions be exactly zero if they are not needed. Paragraph 12 : Clipping pixels with small contribution: Instead of clipping pixels with small norms, we can try something slightly smarter and clip pixels with small contributions to the activation.","Authors mainly introduce four different and newly used regularizations that would help researchers in visualizing responses from different layers. These regularizations are designed to overcome different pathologies commonly encountered by gradient descent without regularization : L2 decay to penalize large pixel values which do not naturally occur, Gaussian blur:a useful regularization to iteratively penalize high frequency information associated with generated images via gradient ascent through each optimization step, Clipping pixels with small norm or Clipping pixels with small contribution.","The authors chose the four regularizations because they address different pathologies in deep learning, specifically high frequency information, extreme pixel values, and non-zero activations in non-discriminative regions. These regularizations help improve the interpretability and realism of the generated images, and they are designed to overcome limitations in previous generative models",0.2962962915432099,0.0967741887682104,0.2592592545061729,6.596285093215256,48.2573355396048,43.03475137614616,0.240673695799458,0.0072992700729927,0.6491338610649109,0.7065537242344553,0.5967794209718704,0.6102944016456604,0.0932356715202331,0.1109694689512252,0.0391530461693614,4,1.0,0.9860927311858602,0.7293458973016804,0.9175584232594972,1.0,4.0,0.95
11,"What is meant by ""linear sweep"" in hyperparameter space?","Figure 3. The effects of each regularization method from Section 3 when used individually. Each of the four rows shows a linear sweep in hyperparameter space from no regularization (left) to strong regularization (right). When applied too strongly, some regularizations cause the optimization to fail (e.g. L2 decay, top row) or the images to be less interpretable (small norm and small contribution clipping, bottom two rows). For this reason, a random hyperparameter search was useful for finding joint hyperparameter settings that worked well together (see Figure 4). Best viewed electronically, zoomed in.",linear sweep can be seen as a regular increment in the values of some regularization hyperparameter (from leftmost where there is no regularization to rightmost where strong regularization occur ) to see the variation of their effects on the corresponding activations.,"A linear sweep in hyperparameter space refers to a gradual increase or decrease in the value of a hyperparameter, starting from a minimum or maximum value, respectively, and ending at the opposite extreme",0.2580645111758586,0.0555555506172843,0.2258064466597295,3.031060490822769,34.74180866597118,30.09262253830844,0.1623585956274544,0.008840075006697,0.7826261520385742,0.5662118288977394,0.7826259136199951,0.3888805210590362,0.3156002163887024,0.479712575674057,0.0630500232966509,3,0.25,0.9782324832060262,0.7242899667704421,0.8969977440707166,0.3333333333333333,4.0,0.8
12,What are the examples of the tools that enable understanding of Neural Networks for newcomers in deep learning? ,"The first tool is software that interactively plots the activations produced on each layer of a trained DNN for user-provided images or video. Static images afford a slow, detailed investigation of a particular input, whereas video input highlights the DNNs changing responses to dynamic input. At present, the videos are processed live from a user’s computer camera, which is especially helpful because users can move different items around the field of view, occlude and combine them, and perform other manipulations to actively learn how different features in the network respond. The second tool — new regularizations that enable improved, interpretable, optimized visualizations of learned features — will help researchers and practitioners understand, debug, and improve their models. The visualizations also reveal a new twist in an ongoing story. Previous studieshave shown that discriminative networks can easily be fooled or hacked by the addition of certain structurednoise in image space (Szegedy et al., 2013; Nguyen et al., 2014).An oft-cited reason for this property is that discriminative training leads networksto ignore non-discriminative information in their input, e.g. learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs. For this reason it has been seen as a hopeless endeavor to create a generative model in which one randomly samples an x from a broad distribution on the space of all possible imagesand then iteratively transforms x into a recognizable image by moving it to a region that satisfies both a prior p(x) and posterior p(y|x) for some class label y.Past attempts have largely supported this view by producing unrealistic images using this method (Nguyen et al., 2014; Simonyan et al., 2013).","The paper talked about two main tools; the first is a software tool to plot activations of each trained layer of a network, for images or videos. Second is introducing new regularization ways to help with understanding learned features through network.These tools are supposed to help newcomers in deep learning to have better intuitions for hidden interpretations of well known structures and give motivations for more new ideas.","The first tool is software that interactively plots the activations produced on each layer of a trained DNN for user-provided images or video, allowing for a slow, detailed investigation of a particular input. The second tool is new regularizations that enable improved, interpretable visualizations of learned features",0.3749999952170139,0.0720720672867465,0.3333333285503472,3.668226507271408,45.90152075393797,41.01353960246489,0.2969829055630238,0.0081781799199582,0.8099290728569031,0.6996798058087523,0.7206337749958038,0.6932125091552734,0.3310208916664123,0.7660347819328308,0.0317968863479124,3,1.0,0.8160394435024131,0.6123603341469978,0.9494641796602356,1.0,4.0,0.9
13,How did the authors compute the contributions of the pixels in order to clip the pixels with smaller contributions?,"Clipping pixels with small contribution: Instead of clipping pixels with small norms, we can try something slightly smarter and clip pixels with small contributions to the activation. One way of computing a pixel’s contribution to an activation is to measure how much the activation increases or decreases when the pixel is set to zero; that is, to compute the contribution as |a_{i}(\mathbf{x})-a_{i}(\mathbf{x}_{-j})|, where \mathbf{x}_{-j} is \mathbf{x} but with the j^{th} pixel set to zero. This approach is straightforward but prohibitively slow, requiring a forward pass for every pixel. Instead, we approximate this process by linearizing a_{i}(\mathbf{x}) around \mathbf{x}, in which case the contribution of each dimension of \mathbf{x} can be estimated as the elementwise product of \mathbf{x} and the gradient. We then sum over all three channels and take the absolute value, computing \left|\sum_{c}\mathbf{x}\circ\nabla_{\mathbf{x}}a_{i}(\mathbf{x})\right|. We use the absolute value to find pixels with small contribution in either direction, positive or negative. While we could choose to keep the pixel transitions where setting the pixel to zero would result in a large activation increase, these shifts are already handled by gradient ascent, and here we prefer to clip only the pixels that are deemed not to matter, not to take large gradient steps outside the region where the linear approximation is most valid. We define this r_{\theta}(\mathbf{x}) as the operation that sets pixels with contribution under the \theta_{\mathrm{c\_pct}} percentile to zero.","Calculating absolute difference between some neuron activation of an input and the activation for same input without certain pixel can be considered a way of measuring the contribution of that pixel in the total response of the neuron. To ensure faster computation, we can estimate activation near the input with 1st order approximation (linear) and hence this leads  to total contribution estimated as the element wise product of the activation gradient and the input x (each element in this product shows how this pixel affects the total response ), we repeat this product for all different channels ,sum them all, and take absolute value  to find pixels with small contribution in either direction, positive or negative to get rid of.","The authors computed the contributions of the pixels using the absolute value of the elementwise product of the gradient and the input, and then used a threshold-based approach to clip pixels with smaller contributions",0.2222222185491277,0.0839160805203189,0.1616161579430671,3.797598755778056,46.49132584873328,41.893169318813285,0.1760098230163435,0.0032276438200113,0.5481663942337036,0.7355564206685775,0.6139730215072632,0.6628809571266174,0.1309606730937957,0.2072422057390213,0.0271278365175641,4,,0.9633980292528248,0.649874353054694,0.8853868493236189,1.0,4.0,0.8
14,"How does a ""network-centric"" approach differ from a ""dataset-centric approach""?","Another approach is to try to interpret the function computed by each individual neuron. Past studies in this vein roughly divide into two different camps: dataset-centric and network-centric. The former requires both a trained DNN and running data through that network; the latter requires only the trained network itself. One dataset-centric approach is to display images from the training or test set that cause high or low activations for individual units. Another is the deconvolution method of Zeiler & Fergus (2013), which highlights the portions of a particular image that are responsible for the firing of each neural unit. Network-centric approaches investigate a network directly without any data from a dataset. For example, Erhan et al. (2009) synthesized images that cause high activations for particular units. Starting with some initial input \mathbf{x}=\mathbf{x_{0}}, the activation a_{i}(\mathbf{x}) caused at some unit i by this input is computed, and then steps are taken in input spacealong the gradient \partial a_{i}(\mathbf{x})/\partial\mathbf{x} to synthesize inputs that cause higher and higher activations of unit i, eventually terminating at some \mathbf{x^{*}} which is deemed to be a preferred input stimulus for the unit in question. In the case where the input space is an image, \mathbf{x^{*}} can be displayed directly for interpretation. Others have followed suit, using the gradient to find images that cause higher activations (Simonyan et al., 2013; Nguyen et al., 2014) or lower activations (Szegedy et al., 2013) for output units.","""Dataset-centric approach"" requires the trained network together with some dataset  to run through the network showing high or low responses of different units while interacting with most significant images of such dataset. This approach can also use deconvolution layers and upsampling to map and highlight the regions of an image that were responsible of the firing of the different units.
""Network-centric approach"" deals only with network without the need to any dataset.You can start with some initial input, compute activations through the forward path and then compute gradients while backprop. You can then ascent or descent the input towards gradient until you reach a preferred input stimulus x* for the unit under consideration. Working with input images, you can visualize that x* if you want.","A ""network-centric"" approach differs from a ""dataset-centric"" approach in that the former does not require any data from a dataset, while the latter requires both a trained DNN and running data through that network",0.1999999962958678,0.0,0.1636363599322315,0.5398546954437098,35.88946699787765,30.850840194717662,0.0936768149882903,0.0028975626384864,0.5573249459266663,0.6200336742203265,0.7552464008331299,0.7187670469284058,0.1578117311000824,0.6057073473930359,0.0704661757197211,3,1.0,0.9883153895911304,0.5289941440545893,0.9159910065264534,1.0,3.0,0.8
15,"What is an example of a ""dataset-centric"" approach?","Another approach is to try to interpret the function computed by each individual neuron. Past studies in this vein roughly divide into two different camps: dataset-centric and network-centric. The former requires both a trained DNN and running data through that network; the latter requires only the trained network itself. One dataset-centric approach is to display images from the training or test set that cause high or low activations for individual units. Another is the deconvolution method of Zeiler & Fergus (2013), which highlights the portions of a particular image that are responsible for the firing of each neural unit.","An example of ""dataset-centric"" approach can be deconvolution method which is used to highlight certain regions of some image that has the highest effects in the response of different units.",Displaying images from the training or test set that cause high or low activations for individual units,0.1395348790481342,0.0,0.0930232511411576,1.4780822562194806,27.387641387098423,22.41579234524745,0.0636942675159235,0.005827905382242,0.3925773799419403,0.567873693333787,0.3925774395465851,0.5526729822158813,0.3076488375663757,0.0782478079199791,0.0090019609385041,3,1.0,0.8670802860844513,0.7031238994318724,0.8124580875662429,1.0,2.0,0.98
16,"What is an example of a ""network-centric"" approach?","Network-centric approaches investigate a network directly without any data from a dataset. For example, Erhan et al. (2009) synthesized images that cause high activations for particular units. Starting with some initial input \mathbf{x}=\mathbf{x_{0}}, the activation a_{i}(\mathbf{x}) caused at some unit i by this input is computed, and then steps are taken in input spacealong the gradient \partial a_{i}(\mathbf{x})/\partial\mathbf{x} to synthesize inputs that cause higher and higher activations of unit i, eventually terminating at some \mathbf{x^{*}} which is deemed to be a preferred input stimulus for the unit in question. In the case where the input space is an image, \mathbf{x^{*}} can be displayed directly for interpretation. Others have followed suit, using the gradient to find images that cause higher activations (Simonyan et al., 2013; Nguyen et al., 2014) or lower activations (Szegedy et al., 2013) for output units.","An example of such approach would be to consider a trained network, start with some initial input and compute the forward path activations. compute gradients through backprop and then move this input towards or against the gradient direction until you have some interesting input that is of much significance in the responses of considered neurons.","An example of a ""network-centric"" approach is synthesizing inputs that cause higher activations for particular units in a network, as done by Erhan et al. (2009) and others",0.3013698583524114,0.0493827116049386,0.2191780775304936,3.720019885344077,34.89655573425849,31.58969059393412,0.2007194244604316,0.0054602184087363,0.5312187671661377,0.5726107650195824,0.3030466213822365,0.6076363921165466,0.2978760004043579,0.3553097546100616,0.0192668332503427,4,1.0,0.9982407247628894,0.4668012036889216,0.8667749155622034,1.0,4.0,0.9
17,"What is meant by ""hacks""?","These gradient-based approaches are attractive in their simplicity, but the optimization process tends to produce images that do not greatly resemble natural images. Instead, they are composed of a collection of “hacks” that happen to cause high (or low) activations: extreme pixel values, structured high frequency patterns, and copies of common motifs without global structure (Simonyan et al., 2013; Nguyen et al., 2014; Szegedy et al., 2013; Goodfellow et al., 2014). The fact that activations may be effected by such hacks is better understood thanks to several recent studies. Specifically, it has been shown that such hacks may be applied to correctly classified images to cause them to be misclassified even via imperceptibly small changes (Szegedy et al., 2013), that such hacks can be found even without the gradient information to produce unrecognizable “fooling examples” (Nguyen et al., 2014), and that the abundance of non-natural looking images that cause extreme activations can be explained by the locally linear behavior of neural nets(Goodfellow et al., 2014).",'Hacks' means that they are not likely to naturally exist (non-natural looking images). However they may even cause harmful changes in the response of the network. Adversarial points for instance are examples of such hacks where slight increments in pixels of even correctly classified images can make them fool the network and tend to go beyond there original part of space and hence be misclassified.,"""hacks"" refer to extreme pixel values, structured high frequency patterns, and copies of common motifs without global structure that are used to cause high (or low) activations in neural networks, but do not greatly resemble natural images",0.2045454497546488,0.0,0.1590909043001034,0.9801851826411272,28.445751756536364,24.175531240118247,0.1244509516837481,0.0060290043995437,0.8327659964561462,0.5793670031068672,0.818353533744812,0.0799755454063415,0.1504595279693603,0.1331617087125778,0.0090931886586378,4,0.75,0.9009622899263589,0.5258458377630411,0.9035036653698768,1.0,2.0,0.95
18,"The paper's pre-trained network is nearly identical to the “AlexNet”. Does it use the same training set as the ""AlexNet""?","Both of our tools are released as open source and are available athttp://yosinski.com/deepvis. While the tools could be adapted to integrate with any DNN software framework, they work out of the box withthe popular Caffe DNN software package (Jia et al., 2014).Users may run visualizations with their own Caffe DNN or our pre-trained DNN, which comes with pre-computed images optimized to activate each neuron in this trained network. Our pre-trained network is nearly identical to the “AlexNet” architecture (Krizhevsky et al., 2012), but with local reponse normalization layers after pooling layers following (Jia et al., 2014). It was trained with the Caffe framework on the ImageNet 2012 dataset (Deng et al., 2009). Our network was trained on ImageNet by first subtracting the per-pixel mean of examples in ImageNet before inputting training examples to the network. Thus, the direct input to the network, \mathbf{x}, can be thought of as a zero-centered input. We may pose the optimization problem as finding an image \mathbf{x^{*}} where","Yes both were trained on ImageNet 2012 dataset but paper's network first subtracted the per-pixel mean of examples in ImageNet before inputting training examples to the network. Hence, direct input to the network, x, can be thought of as a zero-centered input.","No, the paper's pre-trained network does not use the same training set as the ""AlexNet""",0.2127659534449978,0.0,0.2127659534449978,1.3072157844994778,32.41871495349332,28.67003304987229,0.1294790725685034,0.003831417624521,0.5918376445770264,0.5631763838163626,0.6550641059875488,0.7326735854148865,0.1498824954032898,0.6696580052375793,0.0558483850461581,1,1.0,0.9735745196839832,0.503973319977673,0.8160687119973624,1.0,0.0,0.2
19,"What is meant by ""row-major"" order?","Figure 1 shows examples of this type of plot for the \mathsf{conv5} layer.The \mathsf{conv5} layer has size 256\times13\times13, which we depict as 256 separate 13\times13 grayscale images. Each of the 256 small images contains activations in the same spatial x-y spatial layout as the input data, and the 256 images are simply and arbitrarily tiled into a 16\times16 grid in row-major order.Figure 2 shows a zoomed in view of one particular channel, \mathsf{conv5_{151}}, that responds to human and animal faces. All layers can be viewed in the software tool, including pooling and normalization layers. Visualizing these layers provides intuitions about their effects and functions.","""row-major"" means that consecutive small grayscale images of each row reside next to each other unlike ""column-major"" and both are methods of storing elements in memory.","Row-major order refers to the way the 256 small images are arranged in a 16x16 grid, with each image being placed in a row and then stacked on top of each other",0.3846153796449704,0.0727272677685953,0.2307692257988166,4.095476423604123,31.215893065623955,28.52374727724376,0.2364894943019943,0.0109140518417462,0.7887746691703796,0.5133375407532572,0.7887746691703796,0.7057352662086487,0.3867763876914978,0.4442031681537628,0.0476479690631883,3,0.6666666666666666,0.7736443941924908,0.5985327711687654,0.8937204616046972,0.75,2.0,0.8
20,"Why was a zero-centered input used for training the paper's DNN, instead of using the training images as input directly?","Our network was trained on ImageNet by first subtracting the per-pixel mean of examples in ImageNet before inputting training examples to the network. Thus, the direct input to the network, \mathbf{x}, can be thought of as a zero-centered input. We may pose the optimization problem as finding an image \mathbf{x^{*}} where","Zero mean input data and Standardization in general improve the convergence properties of BP training, so it can help to reach desired solution fast. Also, Authors may intend to have centered inputs so that network reduces its biasing towards certain classes or certain large or tiny response values, hence we can have reasonable values for activations and more visualizable responses from different neurons.",To avoid bias towards the mean of the input data,0.1846153822295858,0.0281690118706607,0.1230769206911243,1.3996036421018951,18.5416949330846,18.04317981208167,0.0771047969026839,0.0016638935108153,0.4792337417602539,0.8197114706039428,0.4626325666904449,0.4152556359767914,0.072992019355297,0.3250014185905456,0.0147640451419676,4,1.0,0.8476090793253465,0.5100048427275099,0.8400982063207207,1.0,2.0,0.8
21,"For the paper's pretrained DNN, if the input does not contain a training set class, why does the probability vector show sensitivity towards the noise in input?","•One of the most interesting conclusions so far has been that representations on some layers seem to be surprisingly local. Instead of finding distributed representations on all layers, we see, for example, detectors for text, flowers, fruit, and faces on \mathsf{conv4} and \mathsf{conv5}. These conclusions can be drawn either from the live visualization or the optimized images (or, best, by using both in concert) and suggest several directions for future research (discussed in Section 4).•When using direct file input to classify photos from Flickr or Google Images, classifications are often correct and highly confident (softmax probability for correct class near 1). On the other hand, when using input from a webcam, predictions often cannot be correct because no items from the training set are shown in the image. The training set’s 1000 classes, though numerous, do not cover most common household objects. Thus, when shown a typical webcam view of a person with no ImageNet classes present, the output has no single high probability, as is expected. Surprisingly, however, this probability vector is noisy and varies significantly in response to tiny changes in the input, often changing merely in response to the noise from the webcam. We might have instead expected unchanging and low confidence predictions for a given scene when no object the network has been trained to classify is present. Plotting the fully connected layers (\mathsf{fc6} and \mathsf{fc7}) also reveals a similar sensitivity to small input changes.•Although the last three layers are sensitive to small input changes, much of the lower layer computation is more robust. For example, when visualizing the \mathsf{conv5} layer, one can find many invariant detectors for faces, shoulders, text, etc. by moving oneself or objects in front of the camera. Even though the 1000 classes contain no explicitly labeled faces or text, the network learns to identify these concepts simply because they represent useful partial information for making a later classification decision. One face detector, denoted \mathsf{conv5_{151}} (channel number 151 on \mathsf{conv5}), is shown in Figure 2 activating for human and lion faces and in Figure 1 activating for a cat face. Zhou et al. (2014) recently observed a similar effect where convnets trained only to recognize different scene types — playgrounds, restaurant patios, living rooms, etc. — learn object detectors (e.g. for chairs, books, and sofas) on intermediate layers.","The reason is that convolution layers learn parameters that can extract useful information and relations from the feature map that can help it afterwards to judge and give suitable responses of what this category is. Responses from learned detectors can resemble among some set of categories and can also differ among other set of categories. Input -not being in the training classes- still has a feature map that different layers would respond to according to those different detectors which the network has already learned and would still give a probability vector which may not be accurate. Hence, having noise in the input can stimulate different detectors to respond and fire different activations that would lead to changes in the probability output vector.","The probability vector shows sensitivity towards the noise in input because the pretrained DNN has learned to recognize useful partial information, such as faces, text, and other objects, even if they are not explicitly present in the training set. These detectors are robust to small input changes and can be observed in the lower layers of the network",0.3770491755549583,0.0710059127285462,0.2950819624402043,2.6318446421616843,39.45756382252941,36.338513937951205,0.1616915422885572,0.0053913366796802,0.6637250185012817,0.7335036166793278,0.5912124514579773,0.8095377087593079,0.2585273385047912,0.2710780501365661,0.0210253143508018,3,,0.8985090891989255,0.6509325670345649,0.8893278363349802,1.0,2.0,0.7
22,"The paper wished to only show the main object , letting other regions be exactly zero if they are not needed. How did the authors achieve it?","Clipping pixels with small norm: The first two regularizations suppress high amplitude and high frequency information, so after applying both, we are left with an \mathbf{x^{*}} that contains somewhat small, somewhat smooth values. However, \mathbf{x^{*}} will still tend to contain non-zero pixel values everywhere. Even if some pixels in \mathbf{x^{*}} show the primary object or type of input causing the unit under consideration to activate, the gradient with respect to all other pixels in \mathbf{x^{*}} will still generally be non-zero, so these pixels will also shift to show some pattern as well, contributing in whatever small way they can to ultimately raise the chosen unit’s activation. We wish to bias the search away from such behavior and instead show only the main object, letting other regions be exactly zero if they are not needed. We implement this bias using an r_{\theta}(\mathbf{x}) that computes the norm of each pixel (over red, green, and blue channels) and then sets any pixels with small norm to zero. The threshold for the norm, \theta_{\mathrm{n\_pct}}, is specified as a percentile of all pixel norms in \mathbf{x}.",The paper reaches this goal by calculating each pixel norm over the 3 colour channels and zeroing out small-norm pixels according to some threshold (the percentile of all pixel norms in x).,"By using an r_{\theta}(\mathbf{x}) that sets any pixels with small norm to zero, based on a threshold \theta_{\mathrm{n\_pct}} specified as a percentile of all pixel norms in \mathbf{x}",0.3448275812306777,0.1724137881272296,0.3103448226099881,8.418521548263552,35.394098740093646,32.39694345300083,0.3529772866789441,0.0106544901065449,0.6332104802131653,0.5819116804444859,0.6332103610038757,0.8114334344863892,0.2908808887004852,0.4508783221244812,0.0123614236903203,4,1.0,0.8130698233822741,,0.885539177139053,1.0,5.0,0.95
23,How many hyperparameter combinations were used for the random hyperparameter search?,"If the above regularization methods are applied individually, they aresomewhat effective at producing more interpretable images; Figure 3 shows the effects of each individual hyperparameter.However, preliminary experiments uncovered that their combinedeffect produces better visualizations. To pick a reasonable set ofhyperparameters for all methods at once, we ran a randomhyperparameter search of 300 possible combinations and settled on fourthat complement each other well. The four selected combinations arelisted in Table 1 and optimized images using each are shown for the “Gorilla” class output unit in Figure 4. Of the four, some show highfrequency information, others low frequency; some contain densepixel data, and others contain only sparse outlines of importantregions.We found the version in the lower-left quadrant to be the best single set of hyperparameters, but often greater intuition canbe gleaned by considering all four at once.Figure 5 shows the optimization results computed for a selection of units on all layers. A single image for every filter of all five convolutional layers is shown in Supplementary Figure S1. Nine images for each filter of all layers, including each of the 1000 ImageNet output classes, can be viewed at http://yosinski.com/deepvis.",300 sets of possible hyperparameter combinations then choose four of them that complement each other well.,300,0.124999998828125,0.0,0.124999998828125,2.2869567780619007,10.170130927465191,13.913450338028811,0.0324675324675324,0.0006662225183211,0.3696901500225067,0.0,0.3696900308132171,,0.5066671371459961,0.9897294640541076,0.0041498482418322,4,1.0,0.9724509545543571,0.696978747108919,0.7877260045099195,1.0,2.0,0.8
24,The paper lists tools that enable understanding of neural networks for beginners. Have they mentioned the tools for expert users as well?,"We also note that tools that enable understanding will especially benefit the vast numbers of newcomers to deep learning, who would like to take advantage of off-the-shelf software packages — like Theano (Bergstra et al., 2010), Pylearn2 (Goodfellow et al., 2013), Caffe (Jia et al., 2014), and Torch (Collobert et al., 2011) — in new domains, but who may not have any intuition for why their models work (or do not). Experts can also benefit as they iterate ideas for new models or when they are searching for good hyperparameters. We thus believe that both experts and newcomers will benefit from tools that provide intuitions about the inner workings of DNNs. This paper provides two such tools, both of which are open source so that scientists and practitioners can integrate them with their own DNNs to better understand them.","They didn't mention specific tools for expert users. However, they have thoughts that even experts would benefit from their new ideas such as when experts iterate ideas for new models or while searching for good hyperparameters or maybe from intuitions about the inner workings of DNNs.","Yes, the paper also mentions tools for expert users",0.2083333302864583,0.1132075446066216,0.2083333302864583,5.158249840684866,34.1176504862605,32.06259884145467,0.1042590949423247,0.0020886516593177,0.7097514271736145,0.7666337758302688,0.7402889728546143,0.7279897332191467,0.4416716396808624,0.5627158880233765,0.0365180738299138,3,1.0,0.915385163082122,0.5919667400495782,0.8678059379833856,0.6666666666666666,1.0,0.0
25,"The paper's model implies that the discriminative parameters also contain significant “generative” structure from the training dataset. What is meant by ""generative"" structure?","However, the results presented here suggest an alternate possibility: the previously used priors may simply have been too weak (see Section S1 for one hypothesis of why a strong p(x) model is needed). With the careful design or learning of a p(x) model that biases toward realism,one may be able to harnessthe large number of parameters present in a discriminately learned p(y|x) modelto generate realistic images by enforcing probability under both models simultaneously.Even with the simple, hand-coded p(x) models we use in this paper as regularizers, complex dependencies between distant pixels already arise (cf. the beetles with structure spanning over 100 pixels in Figure 4). This implies that the discriminative parameters also contain significant “generative” structure from thetraining dataset; that is, the parameters encodenot only the jaguar’s spots, but to some extent also its four legs.With better, learned probabilistic models over the input and activations of higher layers, much more structure may be apparent. Work by Dai et al. (2015) shows some interesting results in this direction.While the images generated in this paper are far from being photo-realistic, they do suggest thattransferring discriminatively trained parameters to generative models — opposite the direction of the usual unsupervised pretraining approach — may be a fruitful area for further investigation.","Generative structure is how the data is distributed inside the space where it lives, for example when learning to detect jaguar class, parameters encode not only the jaguar’s spots(Only to distinguish it through a rare property), but to some extent also its four legs(to learn the pattern with which the whole creature can be found). So, discriminative parameters also contain significant “generative” structure.","The ""generative"" structure refers to the ability of the model to generate realistic images, beyond just recognizing the training data",0.1159420252720018,0.0,0.086956518025625,0.809078048613971,28.644448447879245,24.66818694128696,0.1074601357560904,0.0034364261168384,0.7500528693199158,0.562450190768739,0.754930853843689,0.5112212896347046,0.2803402543067932,0.5864516496658325,0.0214555313571435,3,0.6666666666666666,0.8803221605222071,0.5170526146221273,0.8683351787864129,1.0,1.0,
26,How does the performance change when a dense retriever is evaluated on out-of-domain queries and documents that are different from the domain on which the retriever was trained?,"However, as shown in Thakur et al. (2021b), dense retrieval methods require large amounts of training data to work well.333For reference, the popular MS MARCO dataset (Nguyen et al., 2016) has about 500k training instances; the Natural Questions dataset (Kwiatkowski et al., 2019) has more than 100k training instances.  Most importantly, dense retrieval methods are extremely sensitive to domain shifts: Models trained on MS MARCO perform rather poorly for questions for COVID-19 scientific literature (Wang et al., 2020; Voorhees et al., 2021). The MS MARCO dataset was created before COVID-19, hence, it does not include any COVID-19 related topics and models did not learn how to represent this topic well in a vector space. We use the MS MARCO passage ranking dataset Nguyen et al. (2016) as the data from the source domain. It has 8.8M passages and 532.8K query-passage pairs labeled as relevant in the training set. As Table 1 shows, a state-of-the-art dense retrieval model, achieving an MRR@10 of 33.2 points on the MS MARCO passage ranking dataset, performs poorly on the six selected domain-specific retrieval datasets when compared to simple BM25 lexical search. So far, ICT and CD have only been studied on in-domain performance, i.e. a large in-domain labeled dataset is available which is used for subsequent supervised fine-tuning. SimCSE, CT, and TSDAE have been only studied for unsupervised sentence embedding learning. As our results show in Appendix E, they do not work at all for purely unsupervised dense retrieval. If these pre-training approaches can be used for unsupervised domain adaptation for dense retrieval was so far unclear. In this work, we transfer the setup from Wang et al. (2021) to dense retrieval and first pre-train on the target corpus, followed by supervised training on labeled data from MS MARCO Nguyen et al. (2016). Performance is then measured on the target corpus.","It is said that when evaluating a retriever trained on a source domain in an out-of-domain setting, the performance is obtained lower than BM25. Also, dense retrievers are said to be sensitive to domain shift and models that perform well on MS MARCO do not perform well on COVID-19 data. There have been many studies on unsupervised sentence embedding learning, but it is said that they do not work well in unsupervised dense retrieval. Therefore, the performance of the retriever in out-of-domain may be worse.",Poorly,0.0,0.0,0.0,0.0,1.1109659106262426,0.9522564948224936,0.0,0.0001176332196212,0.1889553219079971,0.6874154210090637,0.1653053015470504,,0.078846551477909,0.0388428643345832,0.0009193727583199,2,1.0,0.0,0.4017053300047696,0.7494819195368312,1.0,1.0,0.8
27,"What kinds of relevant documents are missing, when lexical matching is used for retrieval?","Information Retrieval (IR) is a central component of many natural language applications. Traditionally, lexical methods (Robertson et al., 1994) have been used to search through text content. However, these methods suffer from the lexical gap (Berger et al., 2000) and are not able to recognize synonyms and distinguish between ambiguous words.",Using lexical matching makes it difficult to identify synonyms or to distinguish between ambiguous words.,"Sure! Here's the answer to the question based on the provided context:

Documents that contain synonyms or ambiguous words that are not recognized by lexical matching methods",0.3684210479778393,0.1499999954500001,0.2631578900831025,3.907035910657579,31.35261684264943,28.311454400639946,0.3647665962936898,0.0116029222174473,0.6758891344070435,0.814492404720811,0.7826161980628967,0.7218063473701477,0.6260356903076172,0.2323693633079528,0.0159910670606782,4,,0.8991755226968424,0.7145693410229617,0.8582431273873283,1.0,5.0,0.9
28,What are the factors that should be considered for memory footprint for indexing?,"To evaluate the latency of neural re-ranking models in §4.2, we use a single Tesla V100 GPU that has 32 GiBs of memory on a server with two Intel Xeon Gold 6132 CPUs, each with 14 physical cores (24 hyperthreads), and 469 GiBs of RAM. For the mostly CPU-based retrieval experiments in §4.3 and the indexing experiments in §4.5, we use another server with the same CPU and system memory specifications but which has four Titan V GPUs attached, each with 12 GiBs of memory. Across all experiments, only one GPU is dedicated per query for retrieval (i.e., for methods with neural computations) but we use up to all four GPUs during indexing.","During indexing, we use another server with the same CPU and system memory specifications but which has four Titan V GPUs attached, each with 12 GiBs of memory. Across all experiments, only one GPU is dedicated per query for retrieval (i.e., for methods with neural computations) but we use up to all four GPUs during indexing.","Memory footprint for indexing should consider the number of documents, the size of the index, the amount of neural computations required, and the number of GPUs available",0.2089552198173313,0.0,0.1791044735486746,1.653475092954637,33.57521759492354,29.297628135818925,0.1301579862128224,0.0051654868949684,0.6821839213371277,0.5426000544105667,0.6065645217895508,0.507175087928772,0.2706979215145111,0.1960524767637252,0.0270901490939554,4,0.25,0.9939750466122732,0.7109750505480285,0.843941613870685,0.25,1.0,0.8
29,"What are pros and cons of these models illustrated in Figure 2, and what are distinctions of the proposed model?  ","These increasingly expressive architectures are in tension. While interaction-based models (i.e., Figure 2 (b) and (c)) tend to be superior for IR tasks (Guo et al., 2019; Mitraet al., 2018), a representation-focused model—by isolating the computations among q and d—makes it possible to pre-compute document representations offline (Zamani et al., 2018), greatly reducing the computational load per query. In this work, we observe that the fine-grained matching of interaction-based models and the pre-computation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query–document interaction. Figure 2 (d) illustrates an architecture that precisely does so. As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. This paradigm allows ColBERT to exploit deep LM-based representations while shifting the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., (Johnsonet al., 2017; Abuzaidet al., 2019)) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval. Our main contributions are as follows.(1)We propose late interaction (§3.1) as a paradigm for efficient and effective neural ranking.(2)We present ColBERT (§3.2 & 3.3), a highly-effective model that employs novel BERT-based query and document encoders within the late interaction paradigm.(3)We show how to leverage ColBERT both for re-ranking on top of a term-based retrieval model (§3.5) and for searching a full collection using vector similarity indexes (§3.6).(4)We evaluate ColBERT on MS MARCO and TREC CAR, two recent passage search collections.","Using figure 2, 
These increasingly expressive architectures are in tension. While interaction-based models (i.e., Figure 2 (b) and (c)) tend to be superior for IR tasks (Guo et al., 2019; Mitraet al., 2018), a representation-focused model—by isolating the computations among q and d—makes it possible to pre-compute document representations offline (Zamani et al., 2018), greatly reducing the computational load per query. In this work, we observe that the fine-grained matching of interaction-based models and the pre-computation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query–document interaction. Figure 2 (d) illustrates an architecture that precisely does so. As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. This paradigm allows ColBERT to exploit deep LM-based representations while shifting the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., (Johnsonet al., 2017; Abuzaidet al., 2019)) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval.






The distinction of proposed model : 1) a highly-effective model is proposed that employs novel BERT-based query and document encoders within the late interaction paradigm.(2) We show how to leverage ColBERT both for re-ranking on top of a term-based retrieval model (3) and for searching a full collection using vector similarity indexes.(4)We evaluate ColBERT on MS MARCO and TREC CAR, two recent passage search collections.","Pros of the interaction-based models (Figure 2(b) and (c)):

* Superior for IR tasks (Guo et al., 2019; Mitra et al., 2018)

Cons of the interaction-based models:

* Computationally expensive
* Limited by the number of documents that can be processed in a single batch

Pros of the representation-focused model (Figure 2(d)):

* Pre-computes document representations offline, reducing computational load per query
* Allows for efficient vector-similarity search indexes (Johnson et al., 2017; Abuzaid et al., 2019)

Cons of the representation-focused model:

* May not be as effective for IR tasks as interaction-based models

Distinctions of the proposed model (ColBERT):

* Combines the fine-grained matching of interaction-based models with the pre-computation of document representations of representation-based models
* Allows for efficient retrieval of top-k results directly from a large document collection, improving recall over models that only re-rank the output of term-based retrieval",0.386861309561511,0.2313624635232387,0.3722627694155256,22.049572319501472,63.98921563320037,58.61321858407626,0.2854403100162125,0.0063154175683434,0.7619077563285828,0.7722808708600879,0.8208861947059631,0.6382620334625244,,,0.0597877283184481,4,0.7142857142857143,0.8530374321577039,0.3585066188871584,0.933811918756645,0.8461538461538461,4.0,0.9
30,"What if a query term can be matched to multiple document terms? Does MaxSim suffice for capturing query-document relevance, for this case too?","Using E_{q} and E_{d}, ColBERT computes the relevance score between q and d via late interaction, which we define as a summation of maximum similarity (MaxSim) operators. In particular, we find the maximum cosine similarity of each v\in E_{q} with vectors in E_{d}, and combine the outputs via summation. Besides cosine, we also evaluate squared L2 distance as a measure of vector similarity. Intuitively, this interaction mechanism softly searches for each query term t_{q}—in a manner that reflects its context in the query—against the document’s embeddings, quantifying the strength of the “match” via the largest similarity score between t_{q} and a document term t_{d}. Given these term scores, it then estimates the document relevance by summing the matching evidence across all query terms. Given the representation of a query q and a document d, the relevance score of d to q, denoted as S_{q,d}, is estimated via late interaction between their bags of contextualized embeddings. As mentioned before, this is conducted as a sum of maximum similarity computations, namely cosine similarity (implemented as dot-products due to the embedding normalization) or squared L2 distance.","if a query term can be matched to multiple document terms, MaxSim suffice for capturing query-document relevance.  ColBERT computes the relevance score between q and d via late interaction, which we define as a summation of maximum similarity (MaxSim) operators. In particular, we find the maximum cosine similarity of each v\in E_{q} with vectors in E_{d}, and combine the outputs via summation.","No, MaxSim may not suffice for capturing query-document relevance when a query term can be matched to multiple document terms",0.4225352072207896,0.299999996378125,0.2816901367982544,15.5857915291758,60.08033853154403,57.76166734812433,0.232861310614211,0.0040650406504065,0.6868599057197571,0.8091025045213773,0.934676468372345,0.6079936027526855,0.9479606747627258,0.8286710381507874,0.1117423978143761,3,1.0,0.0,0.4633400179484528,0.8534551963191228,0.5,1.0,0.5
31,What are the metrics they used for measuring efficiency and effectiveness?,"Diving deeper into the quality–cost tradeoff between BERT and ColBERT, Figure 4 demonstrates the relationships between FLOPs and effectiveness (MRR@10) as a function of the re-ranking depth k when re-ranking the top-k results by BM25, comparing ColBERT and BERT{}_{\textnormal{base}} (our training). We conduct this experiment on MS MARCO (Dev). We note here that as the official top-1000 ranking does not provide the BM25 order (and also lacks documents beyond the top-1000 per query), the models in this experiment re-rank the Anserini (Yanget al., 2018) toolkit’s BM25 output. Consequently, both MRR@10 values at k=1000 are slightly higher from those reported in Table 1.",They used (MRR@10) for measuring efficiency and effectiveness .,"Sure! Here's the answer to your question based on the provided context:

MRR@10",0.0,0.0,0.0,8.453215125294388,17.180678524118946,12.88550889308921,0.2181069958847737,0.0099009900990099,0.2444480508565902,0.4398624463992959,0.3514485657215118,0.2675998508930206,0.7864586114883423,0.7251616716384888,0.0084735127549365,4,,0.8875972255589525,0.9620231385607192,0.8480023749719481,1.0,5.0,1.0
32,"What are the different aspects that MRR@10 and Recall@50/200/1000 capture, as evaluation metrics for end-to-end retrieval performance ?","Shifting our attention to ColBERT’s end-to-end retrieval effectiveness, we see its major gains in MRR@10 over all of these end-to-end models. In fact, using ColBERT in the end-to-end setup is superior in terms of MRR@10 to re-ranking with the same model due to the improved recall. Moving beyond MRR@10, we also see large gains in Recall@k for k equals to 50, 200, and 1000. For instance, its Recall@50 actually exceeds the official BM25’s Recall@1000 and even all but docTTTTTquery’s Recall@200, emphasizing the value of end-to-end retrieval (instead of just re-ranking) with ColBERT.","In fact, using ColBERT in the end-to-end setup is superior in terms of MRR@10 to re-ranking with the same model due to the improved recall. Moving beyond MRR@10, large gains in Recall@k for k equals to 50, 200, and 1000. For instance, its Recall@50 actually exceeds the official BM25’s Recall@1000 and even all but docTTTTTquery’s Recall@200, emphasizing the value of end-to-end retrieval (instead of just re-ranking) with ColBERT.","MRR@10 and Recall@50/200/1000 capture different aspects of end-to-end retrieval performance, with MRR@10 focusing on precision at the top 10 retrieval positions, while Recall@50/200/1000 emphasizes the overall recall of relevant documents in the top N retrieval positions (where N = 50, 200, or 1000)",0.2499999952582645,0.0740740693638548,0.2499999952582645,7.959082208450317,36.47270889529693,33.76149009847224,0.2146772367964656,0.006999837213088,0.6485401391983032,0.6076716351662388,0.6736586093902588,0.6813220381736755,0.2256049066781997,0.3760406970977783,0.0286737266002957,4,1.0,0.9689757976999248,0.5994768179304576,0.8979017060801506,0.6666666666666666,2.0,0.8
33,"If both queries and documents are short, is still the fine-granular interaction required?","These increasingly expressive architectures are in tension. While interaction-based models (i.e., Figure 2 (b) and (c)) tend to be superior for IR tasks (Guo et al., 2019; Mitraet al., 2018), a representation-focused model—by isolating the computations among q and d—makes it possible to pre-compute document representations offline (Zamani et al., 2018), greatly reducing the computational load per query. In this work, we observe that the fine-grained matching of interaction-based models and the pre-computation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query–document interaction. Figure 2 (d) illustrates an architecture that precisely does so. As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. This paradigm allows ColBERT to exploit deep LM-based representations while shifting the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., (Johnsonet al., 2017; Abuzaidet al., 2019)) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval.","During indexing, we use another server with the same CPU and system memory specifications but which has four Titan V GPUs attached, each with 12 GiBs of memory. Across all experiments, only one GPU is dedicated per query for retrieval (i.e., for methods with neural computations) but we use up to all four GPUs during indexing.",No,0.0,0.0,0.0,0.0,0.892857142857143,0.5952380952380951,0.0,0.0001785395465095,0.0975634977221489,0.2622675001621246,0.0868942439556121,,0.0971280932426452,0.0690280646085739,0.0005963919205931,1,,0.0,0.1812512904959554,0.7252931152924189,0.0,0.0,
34,"Targeting memory-efficient indexing, can we also prune out redundant tokens in documents while preserving a sufficient level of fine granularity?","Document Encoder. Our document encoder has a very similar architecture. We first segment a document d into its constituent tokens d_{1}d_{2}...d_{m}, to which we prepend BERT’s start token [CLS] followed by our special token [D] that indicates a document sequence. Unlike queries, we do not append [mask] tokens to documents. After passing this input sequence through BERT and the subsequent linear layer, the document encoder filters out the embeddings corresponding to punctuation symbols, determined via a pre-defined list. This filtering is meant to reduce the number of embeddings per document, as we hypothesize that (even contextualized) embeddings of punctuation are unnecessary for effectiveness. ","Targeting memory-efficient indexing, tokens are not appended in documents. We first segment a document d into its constituent tokens d_{1}d_{2}...d_{m}, to which we prepend BERT’s start token [CLS] followed by our special token [D] that indicates a document sequence. Unlike queries, we do not append [mask] tokens to documents. After passing this input sequence through BERT and the subsequent linear layer, the document encoder filters out the embeddings corresponding to punctuation symbols, determined via a pre-defined list. This filtering is meant to reduce the number of embeddings per document, as we hypothesize that (even contextualized) embeddings of punctuation are unnecessary for effectiveness.","Yes, we can prune out redundant tokens in documents while preserving a sufficient level of fine granularity",0.1458333304188368,0.0170940147329976,0.1458333304188368,0.8712432297564716,22.129853189662462,19.905217444539257,0.0364963503649635,0.0017316899256391,0.5636609196662903,0.6377292837106809,0.6538758873939514,0.5974200367927551,0.0541275031864643,0.0595339275896549,0.0105224868862034,3,0.5,0.9541282587733624,0.3694258587857756,0.8112081519381078,0.5,1.0,0.8
35,"How much does the late interaction decrease computational costs, and how close is the performance of the late interaction model to the early interaction model?","In contrast with this trend, ColBERT (which employs late interaction over BERT{}_{\textnormal{base}}) performs no worse than the original adaptation of BERT{}_{\textnormal{base}} for ranking by Nogueira and Cho (Nogueira and Cho, 2019; Nogueiraet al., 2019b) and is only marginally less effective than BERT{}_{\textnormal{large}} and our training of BERT{}_{\textnormal{base}} (described above). While highly competitive in effectiveness, ColBERT is orders of magnitude cheaper than BERT{}_{\textnormal{base}}, in particular, by over 170\times in latency and 13,900\times in FLOPs. This highlights the expressiveness of our proposed late interaction mechanism, particularly when coupled with a powerful pre-trained LM like BERT. While ColBERT’s re-ranking latency is slightly higher than the non-BERT re-ranking models shown (i.e., by 10s of milliseconds), this difference is explained by the time it takes to gather, stack, and transfer the document embeddings to the GPU. In particular, the query encoding and interaction in ColBERT consume only 13 milliseconds of its total execution time. We note that ColBERT’s latency and FLOPs can be considerably reduced by padding queries to a shorter length, using smaller vector dimensions (the MRR@10 of which is tested in §4.5), employing quantization of the document vectors, and storing the embeddings on GPU if sufficient memory exists. We leave these directions for future work.","In contrast with this trend, ColBERT (which employs late interaction over BERT performs no worse than the original adaptation of BERT for ranking and is only marginally less effective than BERT and our training of BERT. While highly competitive in effectiveness, ColBERT is orders of magnitude cheaper than BERT, in particular, by over 170\times in latency and 13,900\times in FLOPs.","The late interaction mechanism decreases computational costs by over 170 times in latency and 13,900 times in FLOPs compared to the early interaction model, while maintaining similar performance",0.2571428525510204,0.1190476147732427,0.2285714239795919,8.722778492512923,43.46782869191621,38.14594546859999,0.171569545352408,0.0051584377302873,0.4103821218013763,0.5049749792373194,0.3478900194168091,0.6501472592353821,0.168068990111351,0.4485958516597748,0.0577364423232659,4,0.6666666666666666,0.9392273420975265,0.5114885528241251,0.8461650193337852,1.0,5.0,0.9
36,What kinds of distribution shifts are considered for evaluating retrievers on out-of-distribution datasets?,"3. Dense retrieval models with issues for out-of-distribution data. Dense retrieval models (esp. ANCE and TAS-B), that map queries and documents independently to vector spaces, perform strongly on certain datasets, while on many other datasets perform significantly worse than BM25. For example, dense retrievers are observed to underperform on datasets with a large domain shift compared from what they have been trained on, like in BioASQ, or task-shifts like in Touché-2020. DPR, the only non-MSMARCO trained dataset overall performs the worst in generalization on the benchmark.","BioASQ, or task-shifts like in Touché-2020 distribution shifts are considered for evaluating retrievers on out-of-distribution datasets","Sure! Here's the answer to your question based on the provided context:

Out-of-distribution distribution shifts, including domain shifts and task-shifts",0.2285714236081633,0.0,0.1142857093224491,4.041034416763137,42.91177593668824,35.67050644101024,0.1694915254237288,0.0104166666666666,0.5897957682609558,0.4324193595535647,0.6126131415367126,0.4757649004459381,0.0880726650357246,0.2697340548038482,0.0148453837816781,3,1.0,0.8911311972701208,0.5873738795910027,0.8496941761270156,1.0,4.0,0.8
37,Does a zero-shot scenario in this context refer to cases where relevance annotations are not available? Or are you referring to the case where the query set is also unavailable?,"However, creating a large training corpus is often time-consuming and expensive and hence many retrieval systems are applied in a zero-shot setup, with no available training data to train the system. So far, it is unclear how well existing trained neural models will perform for other text domains or textual retrieval tasks. Even more important, it is unclear how well different approaches, like sparse embeddings vs. dense embeddings, generalize to out-of-distribution data.","creating a large training corpus is often time-consuming and expensive and hence many retrieval systems are applied in a zero-shot setup, with no available training data to train the system. Hence, a zero-shot scenario in this context refer to cases where relevance annotations are not available and  does not refer to unavailability of query set.",Zero-shot scenario in this context refers to cases where relevance annotations are not available,0.4285714248214285,0.3076923044923077,0.4285714248214285,15.153742348516834,62.38720455351775,59.07971930450246,0.2510297697060475,0.0033222591362126,0.847188413143158,0.9305757606351698,0.8852077126502991,0.645069420337677,0.1978504657745361,0.9916969537734984,0.2267004507011548,4,,0.912363395706292,0.7278003412524456,0.911279937947693,0.5,4.0,0.8
38,"If dense/sparse retrievers are pre-trained on target corpus to enable the retrievers to be corpus-aware, can the fine-tuned retrievers outperform lexical models?","We use beir to evaluate ten diverse retrieval methods from five broad architectures: lexical, sparse, dense, late interaction, and re-ranking. From our analysis, we find that no single approach consistently outperforms other approaches on all datasets. Further, we notice that the in-domain performance of a model does not correlate well with its generalization capabilities: models fine-tuned with identical training data might generalize differently. In terms of efficiency, we find a trade-off between the performances and the computational cost: computationally expensive models, like re-ranking models and late interaction model perform the best. More efficient approaches e.g. based on dense or sparse embeddings can substantially underperform traditional lexical models like BM25. Overall, BM25 remains a strong baseline for zero-shot text retrieval.","If dense/sparse retrievers are pre-trained on target corpus to enable the retrievers to be corpus-aware, the fine-tuned retrievers underperform lexical models","Sure! Here's the answer to your question based on the provided context:

NO.

According to the context, even when dense/sparse retrievers are pre-trained on the target corpus to enable corpus-awareness, they do not consistently outperform lexical models like BM25",0.4799999955120001,0.2807017498307172,0.4799999955120001,15.627183183666007,45.703795581375736,41.63498756739364,0.5514564043209876,0.0132698196665532,0.689672589302063,0.7475464936108032,0.7677463889122009,0.8808659315109253,0.8869037628173828,0.4810589849948883,0.0201025806501447,3,1.0,0.0,0.4704879001107326,0.8821129599980594,0.5,1.0,0.85
39,"How is the ""relevance"" defined in TREC-COVID dataset? ","Finally, we notice that there can be a strong lexical bias present in datasets included within the benchmark, likely as lexical models are pre-dominantly used during the annotation or creation of datasets. This can give an unfair disadvantage to non-lexical approaches. We analyze this for the TREC-COVID Voorhees et al. (2021) dataset: We manually annotate the missing relevance judgements for the tested systems and see a significant performance improvement for non-lexical approaches. Hence, future work requires better unbiased datasets that allow a fair comparison for all types of retrieval systems.","relevance"" is defined as judgements in TREC-COVID dataset","Sure! Here's the answer to your question based on the provided context:

Relevance in the TREC-COVID dataset is defined manually by annotators",0.3571428530612245,0.1428571391071429,0.2142857102040816,2.6931337349958566,28.20913272100578,25.386524094278464,0.4821802935010482,0.0120746432491767,0.7927599549293518,0.6290640754084433,0.8569422364234924,0.7294688820838928,0.9216108918190002,0.3549850285053253,0.0169666575033149,4,1.0,0.9854871415438008,0.7262299933422054,0.904741557360214,1.0,4.0,0.8
40,"What does ""speed"" mean in retrieval contexts? ","Models need to potentially compare a single query against millions of documents at inference, hence, a high computational speed for retrieving results in real-time is desired. Besides speed, index sizes are vital and are often stored entirely in memory. We randomly sample 1 million documents from DBPedia Hasibi et al. (2017) and evaluate latency. For dense models, we use exact search, while for ColBERT we follow the original setup Khattab and Zaharia (2020) and use approximate nearest neighbor search. Performances on CPU were measured with an 8 core Intel Xeon Platinum 8168 CPU @ 2.70GHz and on GPU using a single Nvidia Tesla V100, CUDA 11.0.",Index are important as speed in retrieval system.,Time taken to retrieve results,0.0,0.0,0.0,0.0,28.6026283276456,21.4519712457342,0.0581395348837209,0.0062111801242236,0.4267708659172058,0.5237091599867262,0.4267707765102386,0.4289385080337524,0.0997500643134117,0.0555797182023525,0.0051652246898932,4,1.0,0.8481972660845339,0.2048339363081798,0.8192526208143589,1.0,2.0,0.8
41,What are examples where we have annotation holes?,"The results reveal large differences between approaches: Lexical approaches like BM25 and docT5query have a rather low Hole@10 value of 6.4% and 2.8%, indicating that the annotation pool contained the top-hits from lexical retrieval systems. In contrast, dense retrieval systems like ANCE and TAS-B have a much higher Hole@10 of 14.4% and 31.8%, indicating that a large fraction of hits found by these systems have not been judged by annotators.Next, we manually added for all systems, the missing annotation (or holes) following the original annotation guidelines. During annotation, we were unaware of the system who retrieved the missing annotation to avoid a preference bias. In total, we annotated 980 query-document pairs in TREC-COVID. We then re-computed nDCG@10 for all systems with this additional annotations.","Lexical approaches like BM25 and docT5query have a rather low Hole@10 value of 6.4% and 2.8%, indicating that the annotation pool contained the top-hits from lexical retrieval systems. In contrast, dense retrieval systems like ANCE and TAS-B have a much higher Hole@10 of 14.4% and 31.8%, indicating that a large fraction of hits found by these systems have not been judged by annotators","The examples of annotation holes are the hits found by dense retrieval systems like ANCE and TAS-B, which have not been judged by annotators",0.485714281302041,0.2857142817375284,0.485714281302041,15.804500046137806,54.76846733237085,53.42771281268035,0.2818624557260921,0.0051903114186851,0.7693530321121216,0.8264383599162102,0.8248314261436462,0.7486264705657959,0.4438742399215698,0.9842678308486938,0.0742872041538978,4,1.0,0.9844011989881186,0.4697562916788459,0.8791230819923148,1.0,4.0,0.8
42,What are the motivation behind choosing TREC-COVID for analysis on annotation bias?,"Finally, we notice that there can be a strong lexical bias present in datasets included within the benchmark, likely as lexical models are pre-dominantly used during the annotation or creation of datasets. This can give an unfair disadvantage to non-lexical approaches. We analyze this for the TREC-COVID Voorhees et al. (2021) dataset: We manually annotate the missing relevance judgements for the tested systems and see a significant performance improvement for non-lexical approaches. Hence, future work requires better unbiased datasets that allow a fair comparison for all types of retrieval systems.",This dataset could be made unbiased by manual annotations,"Sure! Here's the answer to your question based on the provided context:

The motivation behind choosing TREC-COVID for analysis on annotation bias is to examine the potential unfair advantage of lexical models in the dataset, and to demonstrate the significance of non-lexical approaches in improving performance",0.0,0.0,0.0,0.0559901798401804,10.32433543421856,8.048135559555563,0.1145038167938931,0.0099009900990099,0.4529993236064911,0.6023725436793435,0.3980250060558319,0.5666022300720215,0.1583223044872284,0.143032044172287,0.0047311311312382,3,0.3333333333333333,0.9984264720170408,0.8054713721526187,0.8217406525270512,1.0,1.0,0.8
43,Did the authors have an experiment showing the self-supervised learning can mitigate data sparsity issue?,"Sequential recommendation aimsto accurately characterize users’ dynamic interestsby modeling their past behavior sequences (Rendle, 2010; Rendle et al., 2010; Kang and McAuley, 2018; Chen et al., 2021; Liet al., 2021a; Liuet al., 2021a).Early works on SR usually modelanitem-to-item transaction patternbased on MarkovChains (Rendle, 2010; He and McAuley, 2016).FPMC (Rendle et al., 2010)combines the advantages of Markov Chainsand matrix factorizationto fuse both sequential patternsand users’ general interest.With the recent advances of deep learning,many deep sequential recommendation modelsare also developed (Tang and Wang, 2018; Hidasi et al., 2015; Kang and McAuley, 2018; Sunet al., 2019). Such as Convolutional Neural Networks (CNN)-based (Tang and Wang, 2018) andRNN-based (Hidasi et al., 2015) models.The recent success of Transformer (Vaswani et al., 2017)also motivatesthe developments of pure Transformer-based SR models.SASRec (Kang and McAuley, 2018)utilizes unidirectional Transformerto assign weights to each interacted item adaptively.BERT4Rec (Sunet al., 2019) improves that byutilizing a bidirectional Transformerwith a Cloze task (Taylor, 1953)to fuse user behaviors information fromleft and right directions into each item.LSAN (Liet al., 2021a) improves SASRecon reducing model size perspective.It proposes a temporal context-aware embeddingand twin-attention network, which are light weighted.ASReP (Liuet al., 2021b) further alleviatesthe data-sparsity issueby leveraging a pre-trained Transformeron the revised user behavior sequences toaugment short sequences.In this paper, we study thepotential of addressing data sparsity issuesand improving SR via self-supervised learning.","No, they do not.",Yes. The authors of the paper have conducted experiments to demonstrate the effectiveness of self-supervised learning in mitigating data sparsity issues in sequential recommendation,0.0,0.0,0.0,0.3420688455566386,1.9431245253327831,2.0601294461434287,0.0632911392405063,0.0099009900990099,0.0873042419552803,0.1935934871435165,0.2272972017526626,,0.2822860777378082,0.3476283252239227,0.0026216594284825,3,0.0,0.927183108841318,0.1883030220357933,0.7530752688711927,0.8,0.0,
44,Does the paper show that the learnt user intents by clustering are orthogonal to the sequence embeddings?,"Effectively modeling latent intentsfrom user behaviorsposes two challenges.First,it is extremely difficult to learn latent intents accuratelybecause we have no labelling data for intents.The only available supervision signals for intents are the user behavior data. Nevertheless, as aforementioned example indicates, distinct behaviors may reflect the same intent.Besides,effectively fusing intent informationinto a SR model is non-trivial.The target in SR is to predict next items in sequences, which is solved by encoding sequences.Leveraging latent intents of sequences into the model requires the intentfactors to be orthogonal to the sequence embeddings,which otherwise would induce redundant information. To discover the benefits oflatent intentsand address challenges,we propose theIntent Contrastive Learning (ICL),a general learning paradigm thatleverages the latent intentfactor into SR.It learns users’ intentdistributionsfrom all user behavior sequencesvia clustering.And it leveragesthe learnt intentsinto the SR modelvia a new contrastive SSL,whichmaximizes the agreementbetween a view of sequenceand its corresponding intent.The intent representation learning moduleand the contrastive SSL module are mutually reinforcedto train a more expressivesequence encoder.We tackle the challenge of intentmining problem byintroducing alatent variable to represent users’ intentsand learn them alternatelyalong with the SR model optimization throughan expectation-maximization (EM) frameworkto ensure convergence.We suggest fusing learnt intent informationinto SR via the proposed contrastive SSL,as it can improve model’s performance as wellas robustness.Extensive experiments conducted on four real-world datasetsfurther verify the effectiveness of the proposed learning paradigm,which improves performance and robustness,even when recommender systemsfaceheavy data sparsity issues.","No, it does not.","Yes.

The paper shows that the learnt user intents by clustering are orthogonal to the sequence embeddings by ensuring that the intent factors are orthogonal to the sequence embeddings, which is necessary to avoid redundant information and improve the performance of the SR model",0.0,0.0,0.0,0.0122840148902133,1.3130470677126482,1.6481741330754407,0.1,0.0099009900990099,0.1958924382925033,0.2426745295524597,0.2228019088506698,,0.1090396493673324,0.2062619775533676,0.0053136243899755,3,1.0,0.9476669735523584,0.1863328345285612,0.7453505771659598,1.0,0.0,0.2
45,What metrics are used to measure the robustness of the model?,"We follow (Wanget al., 2019a; Krichene andRendle, 2020) to rank the prediction on the whole item set without negative sampling.Performance isevaluated ona variety of evaluation metrics, including Hit Ratio@k (\mathrm{HR}@k), and Normalized DiscountedCumulative Gain@k (\mathrm{NDCG}@k) where k\in\{5,20\}.",No metrics are mentioned for explicitly measuring robustness.,"Sure! Here's the answer to your question based on the provided context:

HR@k and NDCG@k",0.0,0.0,0.0,0.0,14.043881924850623,10.532911443637968,0.0,0.0099009900990099,-0.0018395640654489,0.3499521183279844,0.001953633967787,0.3313245773315429,0.0495039895176887,0.0543949082493782,0.0037236498220859,1,1.0,0.8932997427692033,0.1803941978161935,0.7215979006874567,1.0,0.0,0.0
46,Does the parallelization of transformer part of the proposed method reduce time complexity effectively?,"In every iteration of the training phase,the computation costsof our proposed methodare mainly fromthe E-step estimation of Q(\cdot) and M-step optimization of \thetawith multi-tasks training.For the E-step,the time complexity is O(|U|mKd) from clustering, where d is the dimensionalityof the embedding and m is themaximum iteration number in clustering (m=20 in this paper).For the M-step,since we have three objectivesto optimize the network f_{\theta}(\cdot),the time complexity is O(3⋅(|U|2d+|U|d2)O(3\cdot(|U|^{2}d+|U|d^{2})italic_O ( 3 ⋅ ( | italic_U | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d + | italic_U | italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ).The overall complexity is dominated by the term O(3\cdot(|U|^{2}d)),which is 3 times of Transformer-based SR with only next item prediction objective, e.g., SASRec.Fortunately,the model can be effectively parallelized becausef_{\theta} is Transformer and we leave it in future work.In the testing phase,the proposed ICL as wellas the SeqCL objectivesare no longer needed, which yields themodel to have the sametime complexity as SASRec (O(d|V|)).The empirical time spending comparisonsare reported in Sec. 5.2.The convergence of ICL is guaranteedunder the generalized EM framework.Proof is provided in Appendix B.","Logically, yes. Empirically, no.",Yes,0.0,0.0,0.0,0.0,12.838770930433284,9.62907819782496,0.0684931506849315,0.0024937655860349,0.2393469363451004,1.0,0.3504924178123474,,0.3885182440280914,0.9898709058761596,0.0012378971537217,3,,0.8341826785569623,0.7016493895015752,0.8064800459004512,0.0,1.0,
47,What are some examples of the SR model that uses deep neural network to encode user behavior sequences?,"•Non-sequential models:BPR-MF (Rendle et al., 2012) characterizesthe pairwise interactionsvia a matrix factorization model andoptimizes through apair-wiseBayesian Personalized Ranking loss.•Standard sequential models.We include solutionsthat train the models with a next-itemprediction objective.Caser (Tang and Wang, 2018) is a CNN-based approach, GRU4Rec (Hidasi et al., 2015)is an RNN-based method, and SASRec (Kang and McAuley, 2018) is one of the state-of-the-art Transformer-basedbaselines for SR.•Sequential models with additional SSL:BERT4Rec (Sunet al., 2019)replaces the next-item prediction witha Cloze task (Taylor, 1953)tofuse informationbetween an item (a view) in a userbehavior sequence and its contextual information.S{}^{3}\text{-Rec} (Zhou et al., 2020) uses SSL to capture correlation-ship among item, sub-sequence, and associated attributes from the given user behavior sequence.Its modules for mining on attributes are removedbecause we don’t have attributes for items,namely S{}^{3}\text{-Rec}_{ISP}.CL4SRec (Xieet al., 2020) fusescontrastive SSL with aTransformer-based SR model.•Sequential models considering latent factors:We include DSSRec(Maet al., 2020),which utilizes seq2seq trainingand performs optimization inlatent space.We do not directly compare ASLI (Tanjim et al., 2020),as it requires user action type information (e.g.,click, add-to-favorite, etc).Instead, we provide a case study inSec. 5.6 toevaluate the benefits of the learnt intent factorwith additional item category information. Sequential recommendation aimsto accurately characterize users’ dynamic interestsby modeling their past behavior sequences (Rendle, 2010; Rendle et al., 2010; Kang and McAuley, 2018; Chen et al., 2021; Liet al., 2021a; Liuet al., 2021a).Early works on SR usually modelanitem-to-item transaction patternbased on MarkovChains (Rendle, 2010; He and McAuley, 2016).FPMC (Rendle et al., 2010)combines the advantages of Markov Chainsand matrix factorizationto fuse both sequential patternsand users’ general interest.With the recent advances of deep learning,many deep sequential recommendation modelsare also developed (Tang and Wang, 2018; Hidasi et al., 2015; Kang and McAuley, 2018; Sunet al., 2019). Such as Convolutional Neural Networks (CNN)-based (Tang and Wang, 2018) andRNN-based (Hidasi et al., 2015) models.The recent success of Transformer (Vaswani et al., 2017)also motivatesthe developments of pure Transformer-based SR models.SASRec (Kang and McAuley, 2018)utilizes unidirectional Transformerto assign weights to each interacted item adaptively.BERT4Rec (Sunet al., 2019) improves that byutilizing a bidirectional Transformerwith a Cloze task (Taylor, 1953)to fuse user behaviors information fromleft and right directions into each item.LSAN (Liet al., 2021a) improves SASRecon reducing model size perspective.It proposes a temporal context-aware embeddingand twin-attention network, which are light weighted.ASReP (Liuet al., 2021b) further alleviatesthe data-sparsity issueby leveraging a pre-trained Transformeron the revised user behavior sequences toaugment short sequences.In this paper, we study thepotential of addressing data sparsity issuesand improving SR via self-supervised learning.",BERT4Rec and S3-Rec are two examples,"1. SASRec (Kang and McAuley, 2018)
2. BERT4Rec (Sunet al., 2019)
3. LSAN (Liet al., 2021a)
4. ASReP (Liuet al., 2021b)

These are all examples of deep sequential recommendation models that use neural networks to encode user behavior sequences",0.1818181794628099,0.0,0.1818181794628099,0.0150765569313865,9.58197167682922,8.295290956706745,0.2293577981651376,0.010717230008244,0.3587892055511474,0.3190538819347109,0.6112767457962036,0.0806717351078987,0.4697350561618805,0.2156728953123092,0.0051020498515097,4,1.0,0.905172318584361,0.5113693103817265,0.8455074650038442,1.0,4.0,
48,How sparse is the real-world dataset used in the experiment?,"We follow (Zhou et al., 2020; Xieet al., 2020) to prepare the datasets. In detail, we only keep the ‘5-core’ datasets, in which all users and items have at least 5 interactions. The statistics of the prepared datasets are summarized inAppendix C.",They are about 99.95% sparse.,"Sure! Here's the answer to your question based on the provided context:

The real-world dataset used in the experiment is very sparse",0.0769230733727812,0.0,0.0769230733727812,0.5789886025125507,7.193869557779293,6.004613073042287,0.0568181818181818,0.0099009900990099,0.4497523903846741,0.4788732394366197,0.4728824198246002,,0.4173903167247772,0.4075272381305694,0.0087061070044327,4,0.0,0.9181783547754158,0.70672404637212,0.8250301447496913,0.0,4.0,0.6
49,What was the value of maximum length T used for the experiment and how was the ratio of sequences that longer than length T?,"Assume that a recommender system has a set of usersand items denoted by \mathcal{U} and \mathcal{V} respectively.Each user u\in\mathcal{U} has a sequence of interacted itemssorted in chronological order S^{u}=[s^{u}_{1},\dots,s^{u}_{t},\dots,s^{u}_{|S^{u}|}]where |S^{u}| is the number of interacted itemsand s^{u}_{t} is the item u interacted atstep t. We denote \mathbf{S}^{u}as embedded representation of S^{u},where \mathbf{s}^{u}_{t} is the d-dimensional embedding of item s^{u}_{t}.In practice, sequences are truncated withmaximum length T.If the sequence length is greater than T, the mostrecent T actions are considered. If the sequence length is less than T, ‘padding’ items will beadded to the left until the length isT (Tang and Wang, 2018; Hidasi et al., 2015; Kang and McAuley, 2018).For each user u,the goal of next item prediction task is to predictthe next item that the user uis most likely to interact withat the |S_{u}|+1 step among the item set \mathcal{V},given sequence \mathbf{S}^{u}.","The value of T is not mentioned, and neither is the ratio of sequences that exceed T in length.","Sure! Here's the answer to your question based on the provided context:

Maximum length T used for the experiment was 10.

According to the context, the sequences were truncated to a maximum length of T=10, and if the sequence length was greater than T, the most recent T actions were considered. This means that for the experiment, the maximum length of the sequences was 10, and any sequences longer than 10 were truncated to 10.

As for the ratio of sequences that were longer than length T, the context does not provide direct information on this. However, based on the fact that the sequences were truncated to T=10, it can be inferred that some sequences were longer than 10, but the exact ratio of such sequences is not specified in the context",0.2650602378516475,0.0775193774436633,0.1927710812251415,0.0527262201175735,8.3118803494067,8.395180188975095,0.2791751669139466,0.0107038598767434,0.6373951435089111,0.8220328020624875,0.8394376039505005,0.8439546823501587,0.1732037365436554,0.3458870351314544,0.0380129888478241,3,0.0,0.9149681799166162,0.426684733000887,0.849529213434399,0.75,1.0,0.2
50,How does the paper show that the clustering result can be interpreted as users' intent?,"The main goal of next item prediction task is to optimizeEq. (1).Assume that there are also K different user intents (e.g., purchasing holiday gifts, preparing for fishing activity, etc.)in a recommender system that formsthe intent variable c=\left\{c_{i}\right\}_{i=1}^{K}, thenthe probability of a user interacting with a certainitem can be rewritten as follows:(7)\begin{split}P_{\theta}(s^{u})=\mathbb{E}_{(c)}\left[P_{\theta}(s^{u},c)\right].\end{split}However, users intents are latent by definition.Because of the missing observation of variable c,we are in a ‘chicken-and-eggs’ situation thatwithout c, we cannot estimate parameter \theta,and without \theta we cannot inferwhat the value of c might be. The larger of the intent class number K means users can havemore diverseintentions.The larger value of the strength of SeqCL objective \betameans the ICL task contributes more tothe final model.The results on Yelp is shown in Figure 5.We find that: (1)ICLRec reaches itsbest performance when increasing K to 512,and then it starts to deteriorateas K become larger.When K is very small,the number of users undereach intent prototype can potentially be large.As a result, false-positive samples(i.e., users that actually have different intentsare considered as having the same intent erroneously)are introduced to the contrastive SSL,thus affecting learning.On the other hand, when K is too large,the number of users undereach intent prototype is small,the introduced false-negative sampleswill also impair contrastive SSL.In Yelp, 512 user intents summarizeusers’ distinct behaviors best.(2) A ‘sweet-spot’ of \lambda=0.5 canalso be found.It indicatesthat the ICL task can benefitthe recommendation predictionas an auxiliary task.The impact of the batch size and \beta are provided in Appendix D. Recently, many approaches have been proposed to studyusers’ intents forimproving recommendations (Wanget al., 2019b; Cenet al., 2020; Li et al., 2019; Liet al., 2021b).MCPRN (Wanget al., 2019b)designs mixture-channel purposerouting networks to adaptivelylearnusers’ different purchase purposesof each itemunder different channels (sub-sequences) for session-based recommendation.MITGNN(Liuet al., 2020a)proposes amulti-intenttranslation graph neural networkto mine users’ multiple intentsby considering the correlations of the intents.ICM-SR (Panet al., 2020)designs anintent-guided neighbor detectorto retrieve correctneighbor sessionsfor neighbor representation.Different from session-based recommendation,another line of worksfocus on modeling the sequentialdynamics of users’ interaction behaviorsin a longer time span.DSSRec (Maet al., 2020)proposes a seq2seq trainingstrategy using multiple future interactions as supervision and introducing an intent variable from her historical and future behavior sequences.The intent variable is used to capture mutual information between an individual user’s historical and future behavior sequences.Two users of similar intentsmight be far away in representation space.Unlike this work, our intent variable is learned over all users’ sequences and is used to maximize mutual information across different users with similar learned intents.ASLI (Tanjim et al., 2020)captures intentvia a temporal convolutionalnetwork with side information (e.g., user action types such asclick, add-to-favorite, etc.),and then use the learnedintents toguide SR model to predictthe next item.Instead, our methodcan learn users’ intentsbased on user interactiondata only.",It does not. K as a hyperparameter is only best believed as the number of user intents and does not necessarily equal the actual number of user intents.,"The paper shows that the clustering result can be interpreted as users' intent by demonstrating that the learned intent variables can capture users' distinct behaviors and preferences, and that the number of intent classes (K) affects the performance of the recommendation model. The paper also compares the learned intent variables with other methods of modeling users' intents, such as using side information (e.g., user action types) or incorporating intent into the recommendation model directly",0.1690140804602262,0.0444444406395065,0.1690140804602262,1.066029016189431,13.345500700986497,12.26383503940478,0.2112676056338028,0.0109241216415707,0.4263162016868591,0.7543613175761501,0.2884108945727348,0.6707748174667358,0.0888591259717941,0.1097476780414581,0.0175033840576778,3,,0.8855198431940897,0.8118009283966425,0.8472847381751908,1.0,1.0,
51,Why does the proposed method introduced EM framework to optimize the model (instead of directly optimizing the loss)?,"Later, we will show that a generalized Expectation-Maximization frameworkprovides a direction to address above problemwith a convergence guarantee.The basic idea of optimizing Eq. (7) via EMis to start with an initial guessof the model parameter \thetaand estimate the expected valuesof the missing variable c, i.e., the E-step.And once we have the values of c,we can maximize the Eq. (7) w.r.t theparameter \theta, i.e., the M step.We can repeat this iterative process until the likelihood cannot increase anymore. To discover the benefits oflatent intentsand address challenges,we propose theIntent Contrastive Learning (ICL),a general learning paradigm thatleverages the latent intentfactor into SR.It learns users’ intentdistributionsfrom all user behavior sequencesvia clustering.And it leveragesthe learnt intentsinto the SR modelvia a new contrastive SSL,whichmaximizes the agreementbetween a view of sequenceand its corresponding intent.The intent representation learning moduleand the contrastive SSL module are mutually reinforcedto train a more expressivesequence encoder.We tackle the challenge of intentmining problem byintroducing alatent variable to represent users’ intentsand learn them alternatelyalong with the SR model optimization throughan expectation-maximization (EM) frameworkto ensure convergence.We suggest fusing learnt intent informationinto SR via the proposed contrastive SSL,as it can improve model’s performance as wellas robustness.Extensive experiments conducted on four real-world datasetsfurther verify the effectiveness of the proposed learning paradigm,which improves performance and robustness,even when recommender systemsfaceheavy data sparsity issues. In this work,we propose a new learning paradigm ICL that canmodel latent intent factorsfrom user interactionsand fuse them into a sequential recommendationmodel via a newcontrastive SSL objective.ICL is formulated withinan EM framework, which guaranteesconvergence. Detailed analyses showthe superiority of ICL andexperiments conducted onfour datasets further demonstratethe effectiveness of the proposed method.",EM guarantees convergence.,To ensure convergence,0.3333333283333334,0.0,0.3333333283333334,15.97357760615681,53.07935322710594,43.71622456662513,0.1282051282051282,0.0099009900990099,0.6068649888038635,0.5711221277713775,0.606864869594574,,0.9627068042755128,0.9832093119621276,0.0025904083030121,3,1.0,0.887284274400148,0.7113771329571921,0.8455312758648954,1.0,4.0,0.9
52,How does temporal context-aware embedding and twin-attention network enable LSAN to be lightweighted compared to SASRec?,"Sequential recommendation aimsto accurately characterize users’ dynamic interestsby modeling their past behavior sequences (Rendle, 2010; Rendle et al., 2010; Kang and McAuley, 2018; Chen et al., 2021; Liet al., 2021a; Liuet al., 2021a).Early works on SR usually modelanitem-to-item transaction patternbased on MarkovChains (Rendle, 2010; He and McAuley, 2016).FPMC (Rendle et al., 2010)combines the advantages of Markov Chainsand matrix factorizationto fuse both sequential patternsand users’ general interest.With the recent advances of deep learning,many deep sequential recommendation modelsare also developed (Tang and Wang, 2018; Hidasi et al., 2015; Kang and McAuley, 2018; Sunet al., 2019). Such as Convolutional Neural Networks (CNN)-based (Tang and Wang, 2018) andRNN-based (Hidasi et al., 2015) models.The recent success of Transformer (Vaswani et al., 2017)also motivatesthe developments of pure Transformer-based SR models.SASRec (Kang and McAuley, 2018)utilizes unidirectional Transformerto assign weights to each interacted item adaptively.BERT4Rec (Sunet al., 2019) improves that byutilizing a bidirectional Transformerwith a Cloze task (Taylor, 1953)to fuse user behaviors information fromleft and right directions into each item.LSAN (Liet al., 2021a) improves SASRecon reducing model size perspective.It proposes a temporal context-aware embeddingand twin-attention network, which are light weighted.ASReP (Liuet al., 2021b) further alleviatesthe data-sparsity issueby leveraging a pre-trained Transformeron the revised user behavior sequences toaugment short sequences.In this paper, we study thepotential of addressing data sparsity issuesand improving SR via self-supervised learning.",Authors do not discuss how.,"Temporal context-aware embedding and twin-attention network in LSAN enable it to be lightweighted compared to SASRec by capturing contextual information and attention weights in a more efficient manner, reducing the model size and computational complexity",0.0,0.0,0.0,0.0,2.299510584759496,1.724632938569622,0.0,0.0099009900990099,-0.0071269562467932,0.451064212096704,-0.007126881275326,,0.0557846389710903,0.032748032361269,0.0024025588720293,3,0.3333333333333333,0.9093721934456552,0.1759509803176212,0.7038039672823985,0.6666666666666666,0.0,0.7
53,What properties of costrastive self-supervised learning have attracted attention from researchers in the recommendation field?,"Recent advances in contrastive SSLhave inspired therecommendation communityto leverage contrastive SSL tofuse correlations amongdifferent views of one sequence (Chenet al., 2020; Yao et al., 2020; Wuet al., 2021),following themutual information maximization (MIM) principle.Existing approaches in SRcan be seen asinstance discrimination tasksthat optimize a lower bound of MIM,such as InfoNCE (Oordet al., 2018; Heet al., 2020b; Chenet al., 2020; Liet al., 2020b).It aims to optimize theproportion of gap of positive pairs and negative pairs (Liu et al., 2021c).In such an instance discrimination task,sequence augmentations such as ‘mask’, ‘crop’, or ‘reorder’ are required tocreatedifferent views of the unlabeled data in SR (Sunet al., 2019; Zhou et al., 2020; Xieet al., 2020; Zhou et al., 2021).Formally, given a sequence S^{u},and a pre-defined data transformationfunction set \mathcal{G}, we can createtwo positive views of S^{u} as follows:(4)\tilde{S}^{u}_{1}=g_{1}^{u}(S^{u}),\tilde{S}^{u}_{2}=g_{2}^{u}(S^{u}),\text{ s.t. }g_{1}^{u},g_{2}^{u}\sim\mathcal{G},where g_{1}^{u} and g_{2}^{u} are transformation functions sampledfrom \mathcal{G} to createa different view of sequence s_{u}.Commonly, views created from the same sequenceare treated as positive pairs,and the views of any different sequencesare considered as negative pairs.The augmented views are first encoded with thesequence encoder f_{\theta}(\cdot) to\mathbf{\tilde{H}}^{u}_{1} and \mathbf{\tilde{H}}^{u}_{2},and then be fed into an ‘Aggregation’layer to get vector representationsof sequences, denoted as \mathbf{\tilde{h}}^{u}_{1} and \mathbf{\tilde{h}}^{u}_{2}. In this paper,we ‘concatenate’ users’ interest representations over time stepsfor simplicity. Note that sequences are prepossessed to have the same length (See Sec. 3.1), thustheir vector representations after concatenationhave the same length too.After that,we can optimize \theta via InfoNCE loss:(5)\mathcal{L}_{\mathrm{SeqCL}}=\mathcal{L}_{\mathrm{SeqCL}}(\mathbf{\tilde{h}}^{u}_{1},\mathbf{\tilde{h}}^{u}_{2})+\mathcal{L}_{\mathrm{SeqCL}}(\mathbf{\tilde{h}}^{u}_{2},\mathbf{\tilde{h}}^{u}_{1}),and(6)\mathcal{L}_{\mathrm{SeqCL}}(\mathbf{\tilde{h}}^{u}_{1},\mathbf{\tilde{h}}^{u}_{2})=-\log\frac{\exp(\text{sim}(\mathbf{\tilde{h}}^{u}_{1},\mathbf{\tilde{h}}^{u}_{2}))}{\sum_{neg}\exp(\text{sim}(\mathbf{\tilde{h}}^{u}_{1},\mathbf{\tilde{h}}_{neg}))},where sim(\cdot) is dot product and\mathbf{\tilde{h}}_{neg} are negativeviews’ representations of sequence S^{u}.Figure 2(a) illustrates how SeqCL works. Sequential recommendation aimsto accurately characterize users’ dynamic interestsby modeling their past behavior sequences (Rendle, 2010; Rendle et al., 2010; Kang and McAuley, 2018; Chen et al., 2021; Liet al., 2021a; Liuet al., 2021a).Early works on SR usually modelanitem-to-item transaction patternbased on MarkovChains (Rendle, 2010; He and McAuley, 2016).FPMC (Rendle et al., 2010)combines the advantages of Markov Chainsand matrix factorizationto fuse both sequential patternsand users’ general interest.With the recent advances of deep learning,many deep sequential recommendation modelsare also developed (Tang and Wang, 2018; Hidasi et al., 2015; Kang and McAuley, 2018; Sunet al., 2019). Such as Convolutional Neural Networks (CNN)-based (Tang and Wang, 2018) andRNN-based (Hidasi et al., 2015) models.The recent success of Transformer (Vaswani et al., 2017)also motivatesthe developments of pure Transformer-based SR models.SASRec (Kang and McAuley, 2018)utilizes unidirectional Transformerto assign weights to each interacted item adaptively.BERT4Rec (Sunet al., 2019) improves that byutilizing a bidirectional Transformerwith a Cloze task (Taylor, 1953)to fuse user behaviors information fromleft and right directions into each item.LSAN (Liet al., 2021a) improves SASRecon reducing model size perspective.It proposes a temporal context-aware embeddingand twin-attention network, which are light weighted.ASReP (Liuet al., 2021b) further alleviatesthe data-sparsity issueby leveraging a pre-trained Transformeron the revised user behavior sequences toaugment short sequences.In this paper, we study thepotential of addressing data sparsity issuesand improving SR via self-supervised learning. Contrastive Self-Supervised Learning (SSL)has brought much attentions bydifferent research communitiesincluding CV (Chenet al., 2020; Liet al., 2020b; Heet al., 2020b; Caron et al., 2020; Khosla et al., 2020) andNLP (Gao et al., 2021; Gunelet al., 2020; Mnih andKavukcuoglu, 2013; Zhanget al., 2020),as well asrecommendation(Yao et al., 2020; Zhou et al., 2020; Wuet al., 2021; Xieet al., 2020).The fundamental goal of contrastive SSLis to maximize mutual informationamong the positive transformationsof the data itself whileimprovingdiscrimination abilityto the negatives.In reccommendation,A two-tower DNN-basedcontrastive SSLmodel is proposed in (Yao et al., 2020).Itaimstoimproving collaborative filteringbasedrecommendation leveraging item attributes.SGL (Wuet al., 2021) adoptsa multi-task framework withcontrastive SSL to improve thegraph neural networks (GCN)-basedcollaborative filtering methods (Heet al., 2020a; Wanget al., 2019a; Liuet al., 2020b; Zhang and McAuley, 2020)with only item IDs as features.Specific to SR,S{}^{3}\text{-Rec} (Zhou et al., 2020)adopts a pre-training andfine-tuning strategy, and utilizescontrastive SSL during pre-trainingto incorporate correlationsamong items, sub-sequences, and attributes of a givenuser behavior sequence.However, the two-stage training strategyprevents the information sharing between next-item prediction and SSL tasks and restrictsthe performance improvement.CL4SRec (Xieet al., 2020) andCoSeRec (Liuet al., 2021a)insteadutilize a multi-task training frameworkwith a contrastive objectiveto enhance user representations.Different from them, our work is aware ofusers’ latent intent factor whenleveraging contrastive SSL,which we show to bebeneficial forimproving recommendationperformanceand robustness.",Sequences of user behavior can be maximally separated or brought together by means of contrastive SSL.,"The properties of contrastive self-supervised learning that have attracted attention from researchers in the recommendation field include maximizing mutual information among positive transformations of the data itself, improving discrimination ability to negatives, and incorporating correlations among items, sub-sequences, and attributes of a given user behavior sequence",0.142857138934949,0.0666666629166668,0.1071428532206634,0.9817213230095388,15.40133892514396,13.587254252196416,0.2362521363225093,0.0105844454670961,0.2764268219470978,0.6336698388571248,0.2764268219470978,0.5706557035446167,0.3600407540798187,0.1151683330535888,0.0084215062952249,4,0.6666666666666666,0.9877358136887024,0.8518894749345409,0.836163615916903,1.0,2.0,0.9
54,Can the proposed methodology utilize user intent information associated with user interaction data if available?,"Robustness w.r.t. user interaction frequency.Theuser ‘cold-start’ problem (Caiet al., 2021; Yinet al., 2020) is one of thetypical data-sparsity issues thatrecommender systems often face, i.e.,most users have limited historical behaviors.To check whether ICL improves the robustnessunder such a scenario,wesplit user behavior sequences into three groupsbased on their behavior sequences’ length, and keepthe total number of behavior sequencesthe same.Models are trained and evaluated on each group of usersindependently. Figure 3shows the comparison results on four datasets.We observe that:(1) The proposed ICLRec canconsistentlyperforms better thanSASRec among all user groups whileCL4SRec fails to outperform SASRecin Beauty and Yelpwhen user behavior sequences areshort.This demonstrates thatCL4SRec requires individual userbehavior sequences long enough to provide‘complete’ informationfor auxiliary supervisionwhileICLRec reducesthe need by leveraginguser intent information,thus consistently benefiting userrepresentation learning evenwhen users have limited historical interactions.(2) Compared with CL4SRec,we observe thatthe improvement of ICLRec is mainly becauseit provides betterrecommendations tousers with low interaction frequency.Thisverifies thatuser intent informationis beneficial, especiallywhen the recommender system facesdata-sparsity issues whereinformationin each individual user sequenceis limited.","Yes, it can.","Sure! Here's the answer to your question based on the provided context:

YES.

The proposed methodology can utilize user intent information associated with user interaction data if available, as demonstrated by the improved performance of ICLRec compared to CL4SRec in the low interaction frequency group",0.047619046292517,0.0,0.047619046292517,0.0021190859299801,1.2019788908361675,1.8411343462760683,0.2105263157894737,0.0099009900990099,0.2055504024028778,1.0,0.4581083059310913,,0.9160704612731934,0.3123067915439605,0.0019069040556648,4,1.0,0.9640872901630372,0.4953724573010422,0.7815196671219858,0.8,5.0,
55,What does the proposed method BUIR require instead of negative sampling for training?,"For all the datasets, BUIRid shows the substantially higher performance than the discriminative methods taking only user-id/item-id (i.e., BPR, NeuMF, CML, and SML).In particular, the sparser the training set becomes, the larger the performance improvement of BUIRid is achieved over the best baseline (denoted by Improvid).It is obvious that BUIRid is more robust to the extreme sparsity compared to the other baselines that are more likely to explicitly use “positive but unobserved” interactions as negative interactions when positive user-item interactions are more rarely observed.BUIRid is not affected by such inconsistent supervision from uncertain negative interactions because it directly optimizes the representations of users and items by using only positive interactions. As a solution to the aforementioned limitations, this paper proposes a novel OCCF framework, named as BUIR, which does not require the negative sampling at all for training the model.The main idea is, given a positive user-item interaction (u, v), to make representations for u and v similar to each other, in order to encode the preference information into the representations.However, a naive end-to-end learning framework that guides positive user-item pairs to be similar to each other without any negative supervision can easily converge to a collapsed solution – the encoder network outputs the same representations for all the users and items. First of all, the BPR framework that optimizes the cross-prediction score, q\left(f(u)\right)^{\top}f(v)+f(u)^{\top}q\left(f(v)\right), is not as effective as ours;it is even worse compared to the conventional BPR, which optimizes the inner-product score f(u)^{\top}f(v).This implies that the performance improvement of BUIR is mainly caused by our learning framework rather than its score modeling based on the predictor.In addition, even without the stochastic augmentation, the neighbor-based encoder (i.e., LGCN) based on the BUIR framework beats LGCN based on the BPR framework, which demonstrates that BUIR successfully addresses the issue of incorrect negative sampling.Lastly, our framework with the stochastic neighbor augmentation further improves the performance by taking benefits from various views of the positive user-item interactions for the optimization. This paper proposes a novel framework for learning the representations of users and items, termed as BUIR, to address the main challenges of the OCCF problem: the implicit assumption about negative interactions, and high sparsity of observed (positively-labeled) interactions.First, BUIR directly bootstraps the representations of users and items by minimizing their cross-prediction error.This makes BUIR use only partially-observed positive interactions for training the model, and accordingly, it can eliminate the need for negative sampling.In addition, BUIR is able to learn the augmented views of each positive interaction obtained from the neighborhood information, which further relieves the data sparsity issue of the OCCF problem.Through the extensive comparison with a wide range of OCCF methods, we demonstrate that BUIR consistently outperforms all the other baselines in terms of top-K recommendation.In particular, the effectiveness of BUIR becomes more significant for much sparse datasets in which the positively-labeled interactions are not enough to optimize the model as well as the assumption about negative interactions becomes less valid.Based on its great compatibility with existing user/item encoder networks, we expect that our BUIR framework can be a major solution for the OCCF problem, replacing the conventional BPR framework. Our extensive evaluation on real-world implicit feedback datasets shows that BUIR consistently performs the best for top-K recommendation among a wide range of OCCF methods.In particular, the performance improvement becomes more significant in sparser datasets, with the help of utilizing augmented views of positive interactions as well as eliminating the effect of uncertain negative interactions.In addition, comparison results on a downstream task, which classifies the items into their category, support that BUIR learns more effective representations than other OCCF baselines.",BUIR requires positive user-item pairs instead of negative sampling for training.,"BUIR requires only positive interactions for training, eliminating the need for negative sampling",0.5217391254442345,0.1818181768595042,0.434782603705104,10.320382724810017,50.73239297310286,47.46012613947264,0.469220475075276,0.0142387732749178,0.8479787111282349,0.599288913110892,0.8479788899421692,0.9044939875602722,0.8218333721160889,0.7116220593452454,0.0460977525756782,4,1.0,0.8627195172455712,0.6039844468811221,0.9159825592451452,1.0,5.0,0.95
56,Is it true that approximating the online encoder slowly make the target encoder keep from converging to the collapsed solution?,"Bypassing the collapsed solution.  It is obvious that the loss in Equation (3) admits the collapsed solution with respect to \theta and \xi, which means both the encoders generate the same representations for all users and items.For this reason, the conventional end-to-end learning strategy, which optimizes both f_{\theta} and f_{\xi} to minimize the loss (i.e., cross-prediction error), may easily lead to such collapsed solution.In contrast, our proposed framework updates each of the encoders in different ways.From Equation (4), the online encoder is optimized to minimize the loss, while the target encoder is updated to slowly approximate the online encoder.That is, the direction of updating the target encoder (\theta-\xi) totally differs from that of updating the online encoder (-\nabla_{\theta}\mathcal{L}_{\theta,\xi}),and this effectively keeps both the encoders from converging to the collapsed solution.Several recent work on bootstrapping-based representation learning (Grill et al., 2020; Chen and He, 2021) empirically demonstrated that this kind of dynamics (i.e., updating two networks differently) allows to avoid the collapsed solution without any explicit term to prevent it.",Approximating the online encoder keep the target encoder from converging to the collapsed solution.,Yes,0.0,0.0,0.0,0.0,3.401360544217687,2.5510204081632653,0.0,0.0007137758743754,0.0362202152609825,0.30049929022789,0.0362202040851116,,0.0539646036922931,0.0799378380179405,0.0004445363719685,3,1.0,0.8362303388647679,0.9240128681128942,0.6960703996204778,0.0,1.0,0.9
57,What component of the model eliminates the effect of uncertain negative interactions after the positive interaction augmentation?,"We argue that the above collapsed solution is incurred by the si\x02multaneous optimization of 𝑢 and 𝑣 within the end-to-end learning framework of a single encoder. Hence, we instead adopt the student\x02teacher-like network [6, 29] in which only the student’s output 𝑢 (and 𝑣) is optimized to predict the target 𝑣 (and 𝑢) presented by the teacher. Specifically, BUIR directly bootstraps1 the representations of users and items by employing two distinct encoder networks, referred to as online encoder and target encoder. The high-level idea is training only the online encoder for the prediction task between 𝑢 and 𝑣, where the target for its prediction is provided by the target encoder. That is, the online encoder is optimized so that its user (and item) vectors get closer to the item (and user) vectors com\x02puted by the target encoder. At the same time, the target encoder is updated based on momentum-based moving average [6, 8, 29] to slowly approximate the online encoder, which encourages to pro\x02vide enhanced representations as the target for the online encoder. By doing so, the online encoder can capture the positive relation\x02ship between 𝑢 and 𝑣 into the representations, while preventing the model from collapsing to the trivial solution without explicitly using any negative interactions for the optimization.",Online encoders prevent models from collapsing into trivial solutions without explicitly using negative interactions for optimization.,The target encoder,0.0,0.0,0.0,0.0,17.12883709896418,12.846627824223136,0.032051282051282,0.0018714909544603,0.4756548404693603,0.5211325153708457,0.4756549298763275,0.3842657804489136,0.1262364238500595,0.4390668869018554,0.0022804399410223,3,0.0,0.7437311863123547,0.200567652975712,0.8023492655737653,0.0,1.0,
58,What are the benefits of using the predictor to calculate user-item interaction score instead of directly encoding into their inner product?,"Existing discriminative OCCF methods (Rendle et al., 2009; Hsieh et al., 2017) have tried to optimize the latent space where the user-item interactions are directly encoded into their inner product (or Euclidean distance).On the contrary, BUIR additionally uses the predictor to model their interaction, which results in the capability of encoding the high-level relationship between users and items into the representations.In conclusion, with the help of the predictor, BUIR accurately computes the user-item interaction scores as well as optimizes the representation without explicitly using negative samples.",Using predictor can optimize the representation without any negative sample.,"Using the predictor to calculate user-item interaction scores instead of directly encoding them into their inner product allows for the capture of high-level relationships between users and items, resulting in more accurate representation and optimization of the latent space",0.1818181783057851,0.0,0.1818181783057851,0.3796037808761127,16.971368599646414,14.19052110878748,0.1798561151079136,0.0110200621644532,0.2997903823852539,0.669922719956187,0.299790471792221,0.4729765057563782,0.3190077543258667,0.2605347037315368,0.0101140992480615,3,,0.9594378497454256,0.8132365066842346,0.8528072546481337,1.0,4.0,0.9
59,"What does ""stochastic"" mean in the stochastic data augmentation technique that the author introduced?","In Figure 4, our stochastic data augmentation (i.e., P>0) brings a significant improvement compared to the case of using the fixed neighborhood information (i.e., P=0) as encoder inputs.This result shows that the augmented views of positive interactions encourage BUIR to effectively learn users’ preference on items even in much sparse dataset.Interestingly, in case of the Ciao dataset which is less sparse than CiteULike, the benefit of our augmentation linearly increases with the maximum drop probability.This is because there is room for producing more various views (i.e., larger perturbation) based on a relatively more number of neighbors, and it eventually helps to boost the recommendation performance.To sum up, our framework that adopts the neighbor augmentation function successfully relieves the data sparsity issue of the OCCF problem, by leveraging the augmented views of few positive interactions. Furthermore, we introduce a stochastic data augmentation technique to relieve the data sparsity problem in our framework.Motivated by the recent success of self-supervised learning in various domains (Chenet al., 2020; Devlinet al., 2019), we exploit augmented views of an input interaction, which are generated based on the neighborhood information of each user and item (i.e., the set of the items interacted with a user, and the users interacted with an item).The stochastic augmentation is applied to positive user-item pairs when they are passed to the encoder, so as to produce the different views of the pairs.To be precise, by making our encoder use a random subset of a user’s (and item’s) neighbors for the input features, it produces a similar effect to increasing the number of positive pairs from the data itself without any human intervention.In the end, BUIR is allowed to learn various views of each positive user-item pair.",Stochastic means it use random neighborhood information of each user and item during data augmentation.,"Random.

In the context of the stochastic data augmentation technique introduced by the author, ""stochastic"" refers to the fact that the augmentation is applied randomly to positive user-item pairs. Specifically, a random subset of a user's (and item's) neighbors is used as input features to the encoder, producing different views of the pairs. This randomness allows for the generation of various views of each positive user-item pair, effectively relieving the data sparsity issue in the OCCF problem",0.1449275328292376,0.0465116251811791,0.1449275328292376,0.088381971748578,15.228173290779868,12.972224158394436,0.3413505686232959,0.0105812834959461,0.7264724969863892,0.8269593144550976,0.8006033301353455,0.6975932121276855,0.8299333453178406,0.8527397513389587,0.0189189351063625,4,1.0,0.9712252056436926,0.7310796341175625,0.9242359488842128,1.0,4.0,0.8
60,What value of momentum coefficient (τ) makes the BULR model perform best?,"Implementation Details.  We implement the proposed framework and all the baselines by using PyTorch, and use the Adam optimizer to train them.For BUIR, we fix the momentum coefficient \tau to 0.995, and adopt a single linear layer for the predictor q_{\theta}.666We empirically found that these hyperparameters hardly affect the final performance of BUIR, and the sensitivity analysis on the parameters is provided in Section 4.6.The augmentation function \psi simply uses a uniform distribution for drawing a drop probability p\sim\mathcal{U}(0,1), where each user’s (item’s) neighbor is independently deleted with the probability p. Figure 6 clearly shows that the performance is hardly affected by \tau in the range of [0.9, 1.0).In other words, any values of \tau larger than 0.9 allow the target encoder to successfully provide the target representations to the online encoder, by slowly approximating the online encoder;on the contrary, BUIR cannot learn the effective representations at all in case that the target encoder is fixed (i.e., \tau=1).This observation is consistent with previous work on momentum-based moving average (Tarvainen andValpola, 2017; Heet al., 2020b; Grill et al., 2020) that showed all values of \tau between 0.9 and 0.999 can yield the best performance.Furthermore, BUIR performs the best with a single-layer predictor, because a multi-layer predictor makes it difficult to optimize the relationship between outputs of the two encoder networks.In conclusion, BUIR is more powerful even with fewer hyperparameters, compared to existing OCCF methods that include a variety of regularization terms or modeling components.",Model gets best performance when the value of parameter tau is larger or equal than 0.9 and smaller than 1.,τ=0.995,0.0,0.0,0.0,0.0,4.453254661342438,3.817075424007804,0.0,0.0004997501249375,0.4846746921539306,0.0,0.4846747815608978,,0.1316044926643371,0.3764951825141907,0.0032274682213824,4,1.0,0.8550029963890339,0.7048165579539898,0.8192209452723636,1.0,4.0,0.9
61,"In BUIR, how does the online encoder updated compared to the target encoder?","BUIR makes use of two distinct encoder networks that have the same structure: online encoder f_{\theta} and target encoder f_{\xi}.They are parameterized by \theta and \xi, respectively.The key idea of BUIR is to train the online encoder by using outputs of the target encoder as its target, while gradually improving the target encoder as well.The main difference of BUIR from existing end-to-end learning frameworks is that f_{\theta} and f_{\xi} are updated in different ways.The online encoder is trained to minimize the error between its output and the target, whereas the target network is slowly updated based on the momentum update (Heet al., 2020b) so as to keep its output consistent. To sum up, the parameters of the online encoder and target encoder are optimized by(4)\begin{split}\theta&\leftarrow\theta-\eta\cdot\nabla_{\theta}\mathcal{L}_{\theta,\xi}\\\xi&\leftarrow\tau\cdot\xi+(1-\tau)\cdot\theta.\end{split}\eta is the learning rate for stochastic optimization, and \tau\in[0,1] is a momentum coefficient (also called as target decay) for momentum-based moving average.The online encoder f_{\theta} (and the predictor q_{\theta}) is effectively optimized by the gradients back-propagated from the loss (Equation (3)), while the target encoder f_{\xi} is updated as the moving average of the online encoder.By taking a large value of \tau, the target encoder slowly approximates the online encoder.This momentum-based update makes \xi evolve more slowly than \theta, which enables to bootstrap the representations by providing enhanced but consistent targets to the online encoders (Heet al., 2020b; Grill et al., 2020).Figure 1 illustrates the overall framework of BUIR with the simple one-hot encoders. Bypassing the collapsed solution.  It is obvious that the loss in Equation (3) admits the collapsed solution with respect to \theta and \xi, which means both the encoders generate the same representations for all users and items.For this reason, the conventional end-to-end learning strategy, which optimizes both f_{\theta} and f_{\xi} to minimize the loss (i.e., cross-prediction error), may easily lead to such collapsed solution.In contrast, our proposed framework updates each of the encoders in different ways.From Equation (4), the online encoder is optimized to minimize the loss, while the target encoder is updated to slowly approximate the online encoder.That is, the direction of updating the target encoder (\theta-\xi) totally differs from that of updating the online encoder (-\nabla_{\theta}\mathcal{L}_{\theta,\xi}),and this effectively keeps both the encoders from converging to the collapsed solution.Several recent work on bootstrapping-based representation learning (Grill et al., 2020; Chen and He, 2021) empirically demonstrated that this kind of dynamics (i.e., updating two networks differently) allows to avoid the collapsed solution without any explicit term to prevent it. Similarly to Section 3.2, the online encoder is trained by minimizing \mathcal{L}_{\theta,\xi}(\psi(u,\mathcal{V}_{u}),\psi(v,\mathcal{U}_{v})), and the target encoder is slowly updated by the momentum mechanism.After the optimization is finished, the interaction score is inferred by f_{\theta}(u,\mathcal{V}_{u}) and f_{\theta}(v,\mathcal{U}_{v}) (Equation (5)).Figure 2 shows an example of our data augmentation which injects a certain level of perturbations to the neighbors.","The online encoder is updated to minimize the error between the output and the target and updated by the gradients back-propagated from the loss, but target network is updated based on the momentum update and updated as the moving average of the online encoder .","The online encoder is updated to minimize the loss, while the target encoder is updated slowly to approximate the online encoder",0.4878048737180251,0.3508771887965528,0.4878048737180251,21.79667239969489,57.10254593681422,57.05800388584391,0.2810228246722326,0.0069513406156901,0.849577009677887,0.8810786893112319,0.8495771288871765,0.6289705634117126,0.2578378617763519,0.7924059629440308,0.0734504109722313,4,1.0,0.9668313702439462,0.5321329776600522,0.9287519222338833,1.0,4.0,0.9
62,Why does assumning unobserved user-item pairs negative leads to limited performance for generative methods?,"Nevertheless, the negative sampling approach has critical limitations in the following aspects.First, the underlying assumption about negative interactions becomes less valid as user-item interactions get sparser.This is because as fewer positive interactions are observed, the number of ”positive but unobserved” interactions increases, which consequently makes it even harder to sample correct negative ones.Such uncertainty of supervision eventually degrades the performance for top-K recommendation.Second, the convergence speed and the final performance depend on the specific choice of distributions for negative sampling.For example, sampling negative pairs from a non-uniform distribution (Rendle andFreudenthaler, 2014; Dinget al., 2019) (e.g., the multinomial distribution which models the probability of each interaction being actually negative) can improve the final performance, but inevitably incurs high computational costs, especially when a lot of users and items should be considered.","Assuming unobserved user-item pairs negative leads to limited performance since there are some cases of positive but unobserved, and the number of this case is increased.","Assuming unobserved user-item pairs negative leads to limited performance for generative methods because it becomes less valid as user-item interactions get sparser, and the uncertainty of supervision degrades the performance for top-K recommendation",0.4444444394718793,0.3214285664859694,0.4444444394718793,27.094070474766408,43.19596810860718,41.32286169428659,0.4015806678528572,0.0154711673699015,0.6598605513572693,0.7838259016287357,0.6598604321479797,0.6131335496902466,0.7080655694007874,0.427906334400177,0.0160794953468255,4,0.6666666666666666,0.8556235529670527,0.535053808314687,0.9404460675559544,1.0,4.0,0.9
63,How did previous OCCF studies mitigated the problem of performance being largely depend on negative sampling distribution?,"However, since the negative interactions are not available in the OCCF problem, previous discriminative methods assume that all unobserved interactions are negative.In other words, for each user, the items that have not been interacted yet are regarded to be less preferred to positive items.In this sense, they either use all unobserved user-item interactions as negative or adopt a negative sampling, which randomly samples unobserved user-item interactions in a stochastic manner to alleviate the computational burden.For better recommendation performance and faster convergence, advanced negative sampling strategies (Rendle andFreudenthaler, 2014; Dinget al., 2019) are also proposed to sample from non-uniform distributions.",Previous OCCF studies assume that all unobserved interactions are negative to mitigate the problem of performance being largely depend on negative sampling distribution,"By assuming all unobserved interactions are negative and using negative sampling strategies, such as random sampling or non-uniform distributions, to alleviate the computational burden and improve recommendation performance",0.3829787184246266,0.204081627705123,0.3404255269352649,14.297291781542466,46.54872303850444,41.54729506874465,0.4313173933427098,0.0110759493670886,0.6812689304351807,0.6598607250686231,0.6812689900398254,0.6765391230583191,0.4164502322673797,0.2486368268728256,0.0207901740951617,4,,0.8587121098999285,0.7270923206353639,0.9086628546148056,1.0,3.0,0.9
64,How does the negative pairs prevent the problem of collapsed solution during optimization in contrastive learning methods?,"Pointing out that the contrastive methods need to carefully treat the negative instances during the training for effectiveness and efficiency, the most recent work proposed a bootstrapping-based self-supervised learning framework (Grill et al., 2020; Chen and He, 2021), which is capable of avoiding the collapsed solution without the help of negative instances.Inspired by bootstrapping methods in deep reinforcement learning (Mnihet al., 2015; Mnih et al., 2016), it directly bootstraps the representation of images by using two neural networks that iteratively learn from each other.This approach achieves the state-of-the-art performance for various downstream tasks in computer vision, and also shows better robustness to the choice of data augmentations used for self-supervision. Bypassing the collapsed solution.  It is obvious that the loss in Equation (3) admits the collapsed solution with respect to \theta and \xi, which means both the encoders generate the same representations for all users and items.For this reason, the conventional end-to-end learning strategy, which optimizes both f_{\theta} and f_{\xi} to minimize the loss (i.e., cross-prediction error), may easily lead to such collapsed solution.In contrast, our proposed framework updates each of the encoders in different ways.From Equation (4), the online encoder is optimized to minimize the loss, while the target encoder is updated to slowly approximate the online encoder.That is, the direction of updating the target encoder (\theta-\xi) totally differs from that of updating the online encoder (-\nabla_{\theta}\mathcal{L}_{\theta,\xi}),and this effectively keeps both the encoders from converging to the collapsed solution.Several recent work on bootstrapping-based representation learning (Grill et al., 2020; Chen and He, 2021) empirically demonstrated that this kind of dynamics (i.e., updating two networks differently) allows to avoid the collapsed solution without any explicit term to prevent it.","To prevent the problem of collapsed sollution, they update target encoder and online encoder differently.","The negative pairs do not prevent the problem of collapsed solution during optimization in contrastive learning methods. Instead, the bootstrapping-based self-supervised learning framework proposed in recent works (Grill et al., 2020; Chen and He, 2021) avoids the collapsed solution by directly bootstrapping the representation of images using two neural networks that iteratively learn from each other. This approach achieves state-of-the-art performance for various downstream tasks in computer vision and shows better robustness to the choice of data augmentations used for self-supervision",0.1445783104485411,0.0851063804436397,0.1204819249063725,0.3286371523220208,11.652269433874231,10.759454966793117,0.1859504132231405,0.0106846062524732,0.2285271137952804,0.6160892944728189,0.2985386252403259,0.4076887965202331,0.0711808428168296,0.0957515612244606,0.0104882130040199,1,1.0,0.8865894769951587,0.4972925561794838,0.7890846161490555,1.0,1.0,0.8
65,"How does the authors show utilizing augmented views of positive interactions can lead the performance improvement, especially in sparser datasets?","In Figure 4, our stochastic data augmentation (i.e., P>0) brings a significant improvement compared to the case of using the fixed neighborhood information (i.e., P=0) as encoder inputs.This result shows that the augmented views of positive interactions encourage BUIR to effectively learn users’ preference on items even in much sparse dataset.Interestingly, in case of the Ciao dataset which is less sparse than CiteULike, the benefit of our augmentation linearly increases with the maximum drop probability.This is because there is room for producing more various views (i.e., larger perturbation) based on a relatively more number of neighbors, and it eventually helps to boost the recommendation performance.To sum up, our framework that adopts the neighbor augmentation function successfully relieves the data sparsity issue of the OCCF problem, by leveraging the augmented views of few positive interactions. Our extensive evaluation on real-world implicit feedback datasets shows that BUIR consistently performs the best for top-K recommendation among a wide range of OCCF methods.In particular, the performance improvement becomes more significant in sparser datasets, with the help of utilizing augmented views of positive interactions as well as eliminating the effect of uncertain negative interactions.In addition, comparison results on a downstream task, which classifies the items into their category, support that BUIR learns more effective representations than other OCCF baselines.","They show augmented views of positive interactions can lead the performance improvement, especially in sparser datasets by showing the experimental result of stochastic data augmentation achieved a big improvement compared to the case of using the fixed neighborhood information as encoder inputs.","By comparing the performance of BUIR with and without the neighbor augmentation function, the authors show that utilizing augmented views of positive interactions leads to performance improvement, especially in sparser datasets",0.4687499951220703,0.2816901359650864,0.3749999951220703,23.556366957615364,61.35474788674173,56.78482431873672,0.4278865431984524,0.0105765950187649,0.7987727522850037,0.7894185133402706,0.7987726926803589,0.6959692239761353,0.8027980327606201,0.3770548403263092,0.0433638560008711,4,1.0,0.8565815243776607,0.6041272187378451,0.9165000920725812,1.0,4.0,0.9
66,Does utilizing the multi-hop neighbor information in meta-graph help improve the performance of the proposed model?,"Knowledge propagation via meta-graph.It is worth noting that, the above-defined knowledge injection process only leverages knowledge embeddings learned by TransE on the global graph \overline{\mathcal{G}}. Particularly, it lacks considering the knowledge that bridges the semantics between query and passage. To this end, we introduce a Graph Meta Network (GMN) module that refines knowledge with the constructed meta-graph \mathbf{G}_{\mathbf{q},\mathbf{p}}, The multi-hop paths of \mathbf{G}_{\mathbf{q},\mathbf{p}} allow the knowledge to be propagated between query and passage, which can enhance the relevance signal to be captured by the model, and thus alleviate the semantic gap. By applying a K-layer GMN in each layer of the knowledge injector, the output entity representation \hat{\mathbf{E}}_{e_{h}}^{(K)} can ensemble knowledge from all the K-hop neighbors. As described in Section 4.1.2 that all the paths of \mathbf{G}_{\mathbf{q},\mathbf{p}} between \mathbf{q} and \mathbf{p} is within K hops, the GMN module can attentively propagate knowledge along the paths from entities in \mathbf{p} to those in \mathbf{q}, and vice versa, which can enrich the semantics of the entities that benefit the relevance modeling. Table 3 shows the performance comparisons between different settings of knowledge injector, which is statistically significant. From this table, we can observe the following phenomena. (1) MRR@10 of KERM without interaction and propagation process decreases at least 1\% respectively. This indicates both knowledge interaction and propagation processes play an indispensable role in ranking performance. (2) The performance of KERM without propagation is comparable to vanilla ERNIE. Not only query and passage entities, but also their multi-hop neighbors are essential for the ranking performance. (3) MRR@10 of KERM without knowledge interaction drops the most. It suggests the simple and straightforward way to aggregate knowledge graph with text does not work in the passage re-ranking scenario. The text and knowledge graph need to be refined with each other mutually in the interaction, which will be further analyzed in detail as follows.","Through experiments, the authors demonstrated that the performance of the model (i.e., MRR@10) decreased without knowledge propagation and that it was comparable to vanilla ERNIE, which demonstrated that multi-hop neighbors were essential for ranking performance. This result can be attributed to how using multi-hope neighbors allows for knowledge to propagate between query and passage.",Yes,0.0,0.0,0.0,0.0,1.4494782208723214,1.087108665654241,0.0,0.0001851508979818,0.1056836619973182,0.3005533218383789,0.1062486544251442,,0.0528813600540161,0.0689630582928657,0.0004799672074539,3,,0.8775649509533484,0.9358100588583548,0.7433801050646673,0.0,0.0,0.8
67,What components of the proposed method aggregate explicit knowledge into implicit knowledge for query and passage embedding?,"Knowledge propagation via meta-graph.It is worth noting that, the above-defined knowledge injection process only leverages knowledge embeddings learned by TransE on the global graph \overline{\mathcal{G}}. Particularly, it lacks considering the knowledge that bridges the semantics between query and passage. To this end, we introduce a Graph Meta Network (GMN) module that refines knowledge with the constructed meta-graph \mathbf{G}_{\mathbf{q},\mathbf{p}}, The multi-hop paths of \mathbf{G}_{\mathbf{q},\mathbf{p}} allow the knowledge to be propagated between query and passage, which can enhance the relevance signal to be captured by the model, and thus alleviate the semantic gap. For knowledge aggregation, we design a novel interaction module between text and knowledge graph to combine the implicit and explicit knowledge. To derive implicit knowledge from text, we employ PLM as text encoder. To be aligned with implicit knowledge, knowledge meta graph is encoded with a multi-layer graph neural network (i.e. k-hop), namely Graph Meta Network (GMN). Each transformer layer outputs word representations. Each graph meta network layer outputs entity representations. Both word and entity representations are aggregated as the input of the following transformer and GMN layer, respectively in a novelly designed module, namely knowledge injector. Therefore through knowledge aggregation, implicit knowledge from text corpus and explicit knowledge from existing knowledge graph can mutually boost each other to achieve a better re-ranking performance, in which the issues in Challenge 2. could be mitigated. Overall, our contributions can be summarized as follows:•It is the first attempt to solve the knowledge enhanced PLMs problem for passage re-ranking. The key motivation lies in that bridging the semantic gap between the query and passage with the help of both kinds of knowledge.•We design a novel knowledge graph distillation method. It refines a reliable knowledge graph from the existing one globally and constructs a knowledge meta graph based on the refined graph locally.•We propose a novel aggregation of PLM and graph neural network framework to model the interaction between explicit knowledge and implicit knowledge.•Experimental results show the effectiveness of KERM on both general and domain specific data, achieving state-of-the-art performance for passage re-ranking. We also conduct a comprehensive study for the effects of each module in our method. The code is available at https://github.com/DQ0408 /KERM.","This work proposes an aggregation module that employs a PLM and a Graph Neural Network (GMN) to model the interaction between explicit and implicit knowledge. The PLM encodes text to obtain word representations (i.e., implicit knowledge), and the Graph Neural Network (GMN) encodes knowledge meta-graphs to obtain entity representations (i.e., explicit knowledge). This module aggregates the word and entity representations to aggregate the implicit and explicit knowledge.","The proposed method aggregates explicit knowledge into implicit knowledge for query and passage embedding through the following components:

1. Graph Meta Network (GMN) module, which refines knowledge with the constructed meta-graph \mathbf{G}_{\mathbf{q},\mathbf{p}} and propagates knowledge between query and passage.
2. Novel interaction module between text and knowledge graph, which combines implicit and explicit knowledge.
3. Knowledge injector module, which aggregates word and entity representations from PLM and GMN, respectively, to inject implicit knowledge from text corpus into the passage embedding",0.4269662872415099,0.1333333283665296,0.4269662872415099,10.173030173953329,43.678012931856095,39.62947150470709,0.3251242736119862,0.0119760479041916,0.7717984914779663,0.7487999292658063,0.6871476769447327,0.7805246710777283,0.7371002435684204,0.1897273510694503,0.018992166651909,4,1.0,0.8591439295471988,0.7804600044049392,0.9399462626194768,1.0,4.0,0.95
68,Does the paper show how each component of KERM can contribute to passage re-ranking performance quantitatively and qualitatively?,"Here we compare ranking performances of KERM and other PLMs based re-rankers on the first two widely used query sets. Moreover, ablation studies for each component of KERM are also explored. All experimental results were reported under the same BM25 setting. Table 3 shows the performance comparisons between different settings of knowledge injector, which is statistically significant. From this table, we can observe the following phenomena. (1) MRR@10 of KERM without interaction and propagation process decreases at least 1\% respectively. This indicates both knowledge interaction and propagation processes play an indispensable role in ranking performance. (2) The performance of KERM without propagation is comparable to vanilla ERNIE. Not only query and passage entities, but also their multi-hop neighbors are essential for the ranking performance. (3) MRR@10 of KERM without knowledge interaction drops the most. It suggests the simple and straightforward way to aggregate knowledge graph with text does not work in the passage re-ranking scenario. The text and knowledge graph need to be refined with each other mutually in the interaction, which will be further analyzed in detail as follows. Knowledge graph distillation is performed in both global and local perspectives. To explore their roles in the ranking performance, we remove the graph pruning globally and sentence selection locally respectively, keep other settings unchanged, and derive KERM without graph pruning and sentence selection respectively. From results on TREC 2019 DL in Table 5, observations are listed as below. (1) Without global graph pruning, MRR@10 and the average edge score, calculated through Eq.(3), decrease the most, and the time efficiency drops slightly. This indicates the original knowledge graph exists noise data that affect performance. (2) Without sentence selection, the time efficiency drops the most and the average edge score decreases slightly, which proves that not every sentence in a passage has a positive effect on semantic matching. Overall, knowledge graph distillation is significant to KERM.","This work conducted ablation studies to investigate the contribution of each component in the performance of KERM. By testing different settings for the knowledge injector, this work found that performance decreases without knowledge interaction and also without knowledge propagation. By testing the model without global or local distillation, they also demonstrated that performance decreases without global distillation and efficiency decreases without either global or local distillation. These experiments demonstrate that each component of KERM contributes to passage re-ranking performance quantitatively.","Yes. The paper shows how each component of KERM contributes to passage re-ranking performance through ablation studies, which demonstrate the importance of both knowledge interaction and propagation processes, as well as the effectiveness of knowledge graph distillation in improving performance",0.4390243853896491,0.1869158832177483,0.365853653682332,12.932447761525529,53.81875344829818,49.62504491563429,0.2774717397980197,0.0056022408963585,0.8294733762741089,0.8171043708637156,0.3341071233153343,0.7223728895187378,0.5974774956703186,0.1566676050424575,0.048834379002854,4,1.0,0.9198713305099876,0.6820020895231633,0.9279169072180088,1.0,5.0,0.95
69,Does the author showed that the distillation on the knowledge graph can be useful for re-ranking task?,"Existing knowledge graphs are usually incomplete and noisy. It is unsuitable for direct introduction of them to the current model. Specially, there is no knowledge base particularly for passage re-ranking task. For example, ConceptNet (Speeret al., 2017) is a general knowledge graph that contains common sense knowledge, where the information might not be useful for our passage re-ranking task. Therefore, it is critical for us to propose a knowledge graph distillation process from both global and local perspectives. Knowledge graph distillation is performed in both global and local perspectives. To explore their roles in the ranking performance, we remove the graph pruning globally and sentence selection locally respectively, keep other settings unchanged, and derive KERM without graph pruning and sentence selection respectively. From results on TREC 2019 DL in Table 5, observations are listed as below. (1) Without global graph pruning, MRR@10 and the average edge score, calculated through Eq.(3), decrease the most, and the time efficiency drops slightly. This indicates the original knowledge graph exists noise data that affect performance. (2) Without sentence selection, the time efficiency drops the most and the average edge score decreases slightly, which proves that not every sentence in a passage has a positive effect on semantic matching. Overall, knowledge graph distillation is significant to KERM. The main goal of this paper is to reasonably introduce external knowledge graph to PLMs for passage re-ranking. We first design a novel knowledge meta graph construction method to distill reliable and query related knowledge from a general and noisy knowledge graph. The knowledge meta graph bridges the semantic gap between each query and passage. Then we propose a knowledge injector layer for mutually updating text and knowledge representations, which transformers word to entity representations for graph meta network, vice versa. Knowledge Enhanced Ranking Model is pretrained with Masked Language Model (MLM) Sentence Relation Prediction (SRP) [38] tasks, and fine-tuned with cross entropy loss function for passage re-ranking task. Experimental results on public benchmark datasets show the effectiveness of the proposed method compared with state-of-the-art baselines without external knowledge due to its first attempt. The role of each module in KERM is also comprehensively analyzed. Since this work was limited to the one-to-one meta-graph of a query-passage pair built online, continued efforts are needed to make knowledge enhancement more efficient for both retrieval and re-ranking stage. Despite that the knowledge graph distillation in our method is empirically shown to be effective for the final performance, the implementation of graph pruning and meta-graph construction is still based on simple heuristics. A more promising way of formulating a useful meta-graph is to jointly learn a graph generator with the reranker in an end-to-end fashion, which enables more flexibility.Besides, it is currently infeasible to exploit the external knowledge in the retrieval stage, which needs to exhaustively build massive meta-graphs for a large scale of candidates. A further study could focus on how to use external knowledge in PLM based retriever. For knowledge graph distillation, we propose a novel pipeline to establish knowledge meta graphs, which only retain informative knowledge for passage re-ranking. Specifically, we first distill a graph globally for passage re-ranking scenario from an existing knowledge graph by pruning some unreliable or noisy relations based on TransE embedding. Then for a specific query-passage pair, we extract entities from both the query and passage, and construct a query-document bipartite entity graph based on query and passage entities and their k-hop neighbors, namely knowledge meta graph. Challenge 1. could be addressed in this distillation process.","This work proposes using knowledge graph distillation as it can help retain only informative knowledge needed for passage re-ranking. By investigating the effect of global and local distillation separately, this work found that the MRR@10 score and efficiency decreased slightly without global distillation, and that time efficiency decreased the most without local distillation. Therefore, this work demonstrates that both global and local distillation of knowledge graphs is useful for re-ranking tasks in terms of performance and efficiency.","Yes. The author showed that knowledge graph distillation can be useful for the re-ranking task by experimenting with and without global graph pruning and sentence selection, and observing the results on TREC 2019 DL. The results indicate that knowledge graph distillation is significant to KERM and can improve the ranking performance",0.3218390755397015,0.068965512421225,0.298850569792575,3.3676152320047525,45.23159590183105,40.2178748364285,0.2112070780322861,0.0079057510463494,0.8428150415420532,0.6631258148532544,0.5445745304847758,0.7995726466178894,0.4256433844566345,0.0622754283249378,0.0128381779148193,4,1.0,0.9337745725397676,0.6068011466635528,0.9271282662735376,1.0,4.0,0.95
70,Who collected the queries from MSMARCO-Passage dataset to make MSMARCO-TRAIN query set?,"We use a large-scale public available corpus, i.e., MSMARCO-Passage collection (Nguyen et al., 2016), as our passage collection. This collection contains approximately 8.8 million passages extracted from 3.2 million web documents covering multiple fields. We train our model on the MSMARCO-TRAIN query set of 502,939 queries and evaluate KERM on three query sets. Table 1 provides the detailed information of these query sets.The first test set is MSMARCO-DEV, which includes 6,980 sparsely-judged queries mixed with multiple domains. Each query has an average of 1.1 relevant passages with binary relevance label.The second test set is TREC 2019 DL (Craswell et al., 2020), which contains 43 densely-judged queries with fine-grained relevance labels, i.e., irrelevant, relevant, highly relevant and perfectly relevant. On average, a query has 95.4 relevant passages, and most queries have more than 10 relevant passages. With fine-grained labels and multiple relevant passages per query, TREC 2019 DL can be used to reflect the fine-grained ranking performance between relevant passages.To evaluate KERM on specific domains, we further introduce Ohsumed 111http://disi.unitn.it/moschitti/corpora.htm query set, which contains 63 queries on bio-medical domain.The collection of Ohsumed is constructed from the first 20,000 passages in Mesh categories of the year 1991.Following the previous work (Joachims, 1998), the test collection including 10,000 passages are utilized for performance comparison on Ohsumed query set.Each query has an average of 50.9 relevant passages with three graded relevance labels. In section 6.4, we demonstrate that the quality of external knowledge constructed by KERM in such domain could be more useful.",MARCO-Passage collection is a large-scale publicly available corpus and two query sets derived from this corpus are used in the paper: MSMARCO-TRAIN and MSMARCO-DEV. How and who collected the queries from MARCO-Passage to construct MSMARCO-TRAIN cannot be answered from this paper.,Nguyen et al. (2016) collected the queries from MSMARCO-Passage dataset to make MSMARCO-TRAIN query set,0.2978723360796741,0.113207543282307,0.2553191445903124,6.195434067782696,46.75341048295149,41.28528759366933,0.2216312056737588,0.0041493775933609,0.7534575462341309,0.5406806716950548,0.426699586212635,0.4790093600749969,0.3187157213687897,0.7146604657173157,0.0765860397566183,1,0.5,0.9265017998273276,0.729356781076346,0.9174442125384854,0.5,0.0,0.7
71,What methods refine the graph containing external knowledge in 1) global and 2) local way?,"For knowledge graph distillation, we propose a novel pipeline to establish knowledge meta graphs, which only retain informative knowledge for passage re-ranking. Specifically, we first distill a graph globally for passage re-ranking scenario from an existing knowledge graph by pruning some unreliable or noisy relations based on TransE embedding. Then for a specific query-passage pair, we extract entities from both the query and passage, and construct a query-document bipartite entity graph based on query and passage entities and their k-hop neighbors, namely knowledge meta graph. Challenge 1. could be addressed in this distillation process.","The knowledge graph is distilled globally by taking an existing knowledge graph and pruning unreliable or noise relations based on TransE embeddings. The graph is then distilled locally by extracting entities from both the given query and passage, and constructing a query-document bipartite entity graph based on the extracted entities and their k-hop neighbors.","1. Global refinement: TransE embedding pruning.
2. Local refinement: Query-document bipartite entity graph construction",0.1923076885576923,0.0634920602166794,0.1538461500961539,2.9313635073729056,45.37332204208682,38.29666740033695,0.1921470342522974,0.0028490028490028,0.6958785653114319,0.7272009354061876,0.6340049803256989,0.7878252267837524,0.229871392250061,0.5743858218193054,0.0444397474808825,4,1.0,0.8463725575176783,0.7247458147276696,0.898741962642253,1.0,4.0,0.9
72,What is the example of unreliable relations in knowledge graph for passage re-ranking scenario?,"•Challenge 1. Existing knowledge graph are not constructed for re-ranking task. They usually contain trivial factual triples, which can hardly bring information gain. The inappropriate selection of external knowledge could even jeopardize the re-ranker performance. How to utilize existing knowledge graph to re-ranking task is remain a challenge.•Challenge 2.The explicit knowledge and implicit knowledge are highly heterogeneous due to the different sources, which makes the aggregation of the two difficult.How to mutually refine each other and effectively aggregate explicit knowledge into implicit knowledge to alleviate the semantic gap between query and passage is still a challenge. Fig. 2 shows a real case of our global graph pruning method on ConceptNet, i.e., a general knowledge graph. In this case, the entity hepatitis has various relations to disease, infectious disease, adult, etc. From the distance of nodes in Fig. 2, we can clearly observe that the knowledge hepatitis is an infectious disease is more reliable and informative than hepatitis is located at adult. To hepatitis, the concept adult is more general than infectious disease. This indicates that our pruning method can effectively eliminate less informative knowledge.","Unreliable relations in a knowledge graph involve trivial factual triplets that do not bring substantial information gain. For example, in ConceptNet, the entity “hepatitis” has relations with both “infectious disease” and “adult”. To the concept “hepatitis”,  the concept “adults” is more general than “infectious disease” and thus the relationship between “hepatitis” and “infectious disease” is more reliable and informative.","Unreliable relations in knowledge graphs for passage re-ranking scenarios can be seen in the form of trivial factual triples, such as the relation between hepatitis and adult, which is more general than infectious disease",0.3466666617742223,0.1395348789913468,0.2933333284408889,6.587550601652169,53.02721255327752,46.70189283774629,0.2431372549019607,0.0073370738023305,0.7832459211349487,0.81658452217068,0.7766258716583252,0.5928729176521301,0.4325848817825317,0.4155230224132538,0.047965276460351,3,1.0,0.9432391095541895,0.6645811116544766,0.9440319714525576,0.75,4.0,0.9
73,"What does ""meta"" means in the term graph meta network (GMN)?","Knowledge propagation via meta-graph.It is worth noting that, the above-defined knowledge injection process only leverages knowledge embeddings learned by TransE on the global graph \overline{\mathcal{G}}. Particularly, it lacks considering the knowledge that bridges the semantics between query and passage. To this end, we introduce a Graph Meta Network (GMN) module that refines knowledge with the constructed meta-graph \mathbf{G}_{\mathbf{q},\mathbf{p}}, The multi-hop paths of \mathbf{G}_{\mathbf{q},\mathbf{p}} allow the knowledge to be propagated between query and passage, which can enhance the relevance signal to be captured by the model, and thus alleviate the semantic gap. For knowledge aggregation, we design a novel interaction module between text and knowledge graph to combine the implicit and explicit knowledge. To derive implicit knowledge from text, we employ PLM as text encoder. To be aligned with implicit knowledge, knowledge meta graph is encoded with a multi-layer graph neural network (i.e. k-hop), namely Graph Meta Network (GMN). Each transformer layer outputs word representations. Each graph meta network layer outputs entity representations. Both word and entity representations are aggregated as the input of the following transformer and GMN layer, respectively in a novelly designed module, namely knowledge injector. Therefore through knowledge aggregation, implicit knowledge from text corpus and explicit knowledge from existing knowledge graph can mutually boost each other to achieve a better re-ranking performance, in which the issues in Challenge 2. could be mitigated. Different from existing knowledge-enhanced PLMs for other NLP tasks, our aim for the re-ranking task is particularly on the relevance modeling between query and passage. Thus, we further leverage the knowledge in the global graph G to construct “bridges” between query and passage, which alleviates the semantic gap and improves semantic modeling. More specifically, for a given query-passage pair (i.e., (q, p)), we propose to construct a bipartite meta-graph that connects those entities in the q and those in p.",The Graph Meta Network (GMN) refines knowledge in a meta-graph. A meta-graph is a graph that is constructed by constructing multi-hop paths between the entities in a query and a passage using the knowledge from a global graph. The meaning for “meta” in both graph meta network (GMN) and meta-graph is not explicitly defined in this paper.,"""meta"" in graph meta network (GMN) refers to the fact that the network operates on a higher level of abstraction, leveraging knowledge from the global graph to refine the knowledge embeddings and alleviate the semantic gap between query and passage",0.4285714236367347,0.1505376295386751,0.342857137922449,11.043335098211704,46.188921879097016,42.17216403829833,0.2660522636145604,0.0086206896551724,0.9056100845336914,0.6771172043758834,0.8834742903709412,0.938501238822937,0.9807783365249634,0.7370716333389282,0.0414867610000304,4,,0.9837282603521356,0.5347172271180197,0.9387778665632652,1.0,4.0,0.8
74,What is the other example of frameworks that can be used in PaddlePaddle like Paddle Graph Learning?,"We use the traditional sparse retriever BM25 (Yanget al., 2017) as our first stage method. All experiments are conducted under the same BM25 setting with 1000 retrieved candidates. We conduct experiments with the deep learning framework PaddlePaddle (Maet al., 2019) on up to 4 NVIDIA Tesla A100 GPUs (with 40G RAM). For the GMN module, we use Paddle Graph Learning (PGL) 222https://github.com/PaddlePaddle/PGL, an efficient and flexible graph learning framework based on PaddlePaddle. For training, we used the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e-5 for text encoder and 1e-4 for knowledge injector. The model is trained up to 5 epochs with a batch size of 640 and 240 for base and large models respectively.In our experiments, the PLM small, base and large models have 6, 12 and 24 Transformer layers respectively.The text encoder has 9 layers and 21 layers for base and large model respectively, and the knowledge injector both has 3 layers in our experiment. The dropout rates are set to 0.1. The ratio of the positive to the hard negative is set to 1:19.All transformer layers in KERM’s backbone are initialized from ERNIE-2.0 base (Sunet al., 2020b), which is a BERT-like model pre-trained with a continual pre-training framework on multiple tasks. We perform Knowledge-enhanced pre-training on MARCO passage collection to warm up the parameters in knowledge injector, which has 60,000 iterations under the batch size of 256.For a fair comparison, the same pre-training without knowledge enhancement is also conducted on \textrm{ERNIE}_{\textrm{base}} re-ranker and all models in ablation studies.",This work mentions using the Paddle Graph Learning (PGL) framework from the deep learning framework PaddlePaddle. Other examples of frameworks in PaddlePaddle are not mentioned in this paper.,PaddleSphere,0.0,0.0,0.0,0.0,13.466753621748015,11.542931675784011,0.0,0.0003570153516601,0.5220917463302612,0.0,0.4927297234535217,,0.3799096941947937,0.2210894674062729,0.0048521407232068,4,0.0,0.8669837199455234,0.1956179537158038,0.7826363553420568,0.0,1.0,
75,How many entities and relations does ConceptNet has?,"We use ConceptNet (Speeret al., 2017), a general knowledge graph as our external knowledge base \mathcal{G}. Following KagNet (Linet al., 2019), we merge relation types to increase graph density and construct a multi-relational graph with 17 relation types, including atlocation, causes, createdby, etc.","ConceptNet is a general knowledge graph and, in this work, they merged relation types in the graph to construct a multi-relational graph with 17 relation types. The full number of entities and relations that are found in ConceptNet cannot be answered in this paper.",ConceptNet has approximately 1.1 million entities and 3.3 million relations,0.1860465083180097,0.0384615351257399,0.1860465083180097,1.808902424657176,31.93647220751456,27.646050716875013,0.0699833702882483,0.0024330900243309,0.7351815700531006,0.4206988075946239,0.8664948344230652,0.2563925683498382,0.1072126924991607,0.6116886138916016,0.0638987259282001,3,0.0,0.9979550805543108,0.2229611684017004,0.8919555565834352,0.0,1.0,0.8
76,How is next sentence prediction (NSP) different from sentence relation prediction (SRP)?,"Knowledge-enhanced pre-training.Following previous studies (Nogueiraet al., 2019a; Yanet al., 2021; Kim and Ko, 2021), we conduct continual pre-training on MSMARCO corpus to warm up the parameters of GMN module.We apply Masked Language Model (MLM) (Devlinet al., 2018) and Sentence Relation Prediction (SRP) (Wang et al., 2019) as the pre-training tasks in KERM.Compared to conventional Next Sentence Prediction (NSP) (Devlinet al., 2018), the task of SRP is to predict whether a given sentence is the next sentence, previous sentence relation or no relation with another sentence. To incorporate knowledge during the pre-training stage, we construct a meta-graph for each sentence pair, and apply the knowledge aggregation process as introduced above.The pre-training loss is defined as\mathcal{L}_{p}=\mathcal{L}_{MLM}+\mathcal{L}_{SRP}.","Compared to conventional Next Sentence Prediction (NSP), Sentence Relation Prediction (SRP) aims to predict whether a given sentence is the next sentence, previous sentence relation, or no relation with another sentence.","NSP predicts the next sentence, while SRP predicts the relationship between a sentence and another sentence",0.3076923032478633,0.1363636320247935,0.256410251965812,7.978741405529227,50.29874499510459,45.058276643202575,0.2317330917874396,0.0061162079510703,0.845615565776825,0.8596729811093982,0.8456154465675354,0.7840591669082642,0.7969024181365967,0.8925214409828186,0.1044013510121865,3,0.5,0.8869952198329903,0.8350427925801407,0.9401484412338246,0.5,4.0,0.8
77,How is DPR retriever different from BM25?,"The low-dimensional dense representations for query and passage are computed by PLMs based retrievers from the dual-encoder architecture. Afterward, the candidate passage set could be retrieved efficiently via approximate nearest neighbor algorithms.Existing studies could be categorized into two parts:(1) By optimizing the matching stage: DPR (Karpukhin et al., 2020) is the first study to leverage PLM to empower the retriever by a single vector. Other researches, such asRepBERT (Zhanet al., 2020), ColBERT (Khattab andZaharia, 2020), COIL (Gaoet al., 2021) and Interactor (Yeet al., 2022), obtain multiple vectors for query and passage for matching.(2) By optimizing the representation learning module: RocketQAv1 (Qu et al., 2021) and RocketQAv2 (Ren et al., 2021) boost the representation learning of retriever by leveraging the power of cross-encoder in a cascade or joint manner. Other studies boost the representation learning by designed IR-oriented pre-training tasks.ICT (Leeet al., 2019) treats sentences as pseudo-queries and matched them to the passage they originate from. Condenser (Gao and Callan, 2021) utilizes a novel pre-training task, which can produces an information-rich representation to condense an input sequence. We use the traditional sparse retriever BM25 (Yanget al., 2017) as our first stage method. All experiments are conducted under the same BM25 setting with 1000 retrieved candidates. We conduct experiments with the deep learning framework PaddlePaddle (Maet al., 2019) on up to 4 NVIDIA Tesla A100 GPUs (with 40G RAM). For the GMN module, we use Paddle Graph Learning (PGL) 222https://github.com/PaddlePaddle/PGL, an efficient and flexible graph learning framework based on PaddlePaddle. For training, we used the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e-5 for text encoder and 1e-4 for knowledge injector. The model is trained up to 5 epochs with a batch size of 640 and 240 for base and large models respectively.In our experiments, the PLM small, base and large models have 6, 12 and 24 Transformer layers respectively.The text encoder has 9 layers and 21 layers for base and large model respectively, and the knowledge injector both has 3 layers in our experiment. The dropout rates are set to 0.1. The ratio of the positive to the hard negative is set to 1:19.All transformer layers in KERM’s backbone are initialized from ERNIE-2.0 base (Sunet al., 2020b), which is a BERT-like model pre-trained with a continual pre-training framework on multiple tasks. We perform Knowledge-enhanced pre-training on MARCO passage collection to warm up the parameters in knowledge injector, which has 60,000 iterations under the batch size of 256.For a fair comparison, the same pre-training without knowledge enhancement is also conducted on \textrm{ERNIE}_{\textrm{base}} re-ranker and all models in ablation studies. Given a query q, passage re-ranking aims at ordering a set of 𝜘 passages, i.e., P =  p𝜅 	𝜘 𝜅=1 , which is usually retrieved from a large-scale passage collection by a retriever, e.g. BM25 [48], DPR [16] etc. In particular, a passage is a sequence of words p = {𝑤𝑝 } |p| 𝑝=1 , where |p| is the length of passage p. Similarly, a query is a sequence of words q = {𝑤𝑞 } |q| 𝑞=1 . Note that a passage p consists of 𝑇 sentences p = {s𝜏 } 𝑇 𝜏=1.",BM25 and DPR are both examples of retrievers used in large-scale passage collection. BM25 is described as a traditional sparse retriever and DPR leverages PLM to empower the retriever by a single vector. How both BM25 and DPR function is not described in detail in this paper and thus their differences cannot be answered in this paper.,"DPR retriever is different from BM25 in that it uses a PLM to empower the retriever with a single vector, while BM25 uses a traditional sparse retriever",0.426229503681806,0.2105263113746538,0.3278688479441011,12.38552602369171,54.172168678609886,50.17754779681677,0.2834663617035491,0.0060989383329568,0.7686167359352112,0.779816569720418,0.8602914810180664,0.767598032951355,0.5368837118148804,0.8513455986976624,0.1465111912862888,4,1.0,1.0,0.7345023130106378,0.9382694858438184,1.0,4.0,0.8
78,What is the difference of RocketQAv1 and RocketQAv2 model?,"Existing PLMs based re-rankers typically improve ranking performance from two aspects: (1) By optimizing the ranking procedure: monoBERT (Nogueira and Cho, 2019) is the first work that re-purposed BERT as a passage re-ranker and achieves state-of-the-art results. duoBERT (Nogueiraet al., 2019a) integrates monoBERT in a multistage ranking architecture and adopts a pairwise classification approach to passage relevance computation. UED (Yanet al., 2021) proposes a cascade pre-training manner that can jointly enhance the retrieval stage through passage expansion with a pre-trained query generator and thus elevate the re-ranking stage with a pre-trained transformer encoder. The two stages can facilitate each other in a unified pre-training framework. H-ERNIE (Chuet al., 2022) proposes a multi-granularity PLM for web search.(2) By designing rational distillation procedure: LM Distill + Fine-Tuning (Gaoet al., 2020) explores a variety of distillation methods to equip a smaller re-ranker with both general-purpose language modeling knowledge learned in pre-training and search- specific relevance modeling knowledge learned in fine-tuning, and produces a faster re-ranker with better ranking performance. CAKD (Hofstätter et al., 2020) proposes a cross-architecture knowledge distillation procedure with a Margin-MSE loss, which can distill knowledge from multiple teachers at the same time. RocketQAv1 (Qu et al., 2021) trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful cross-encoder to empower the dual-encoder. RocketQAv2 (Ren et al., 2021) proposes a novel approach that jointly trains the dense passage retriever and passage re-ranker. The parameters of RocketQAv2 are inherited from RocketQAv1. Besides, RocketQAv2 utilizes a large PLM for data augmentation and denoising, which can also be regarded as a distillation procedure. Notably, these two types of studies anticipate more insightful information to be captured by the advanced ranking and training procedures, while neglecting the limitations of implicit knowledge extracted from noisy and heterogeneous data. Therefore, in this paper, we proposed the first knowledge-enhanced PLM based re-ranker, which thoughtfully leverages explicit external knowledge that improve the effectiveness of the model. We include several PLMs based re-rankers in our evaluation, including the state-of-the-art:•monoBERT (Nogueira and Cho, 2019): The first study that re-purposes BERT as a re-ranker and achieves state-of-the-art results.•duoBERT (Nogueiraet al., 2019a):This work proposes a pairwise classification approach using BERT, which obtains the ability to be more sensitive to semantics through greater computation.•UED (Yanet al., 2021): A unified pre-training framework that jointly refines re-ranker and query generator. For a fair comparison, we only use the re-ranker in UED without passage expansion.•LM Distill+Fine-Tuning (LDFT) (Gaoet al., 2020):A variety of distillation methods are compared in this paper. The experimental results indicate that a proper distillation procedure (i.e. first distill the language model, and then fine-tune on the ranking task) could produce a faster re-ranker with better ranking performance.•CAKD (Hofstätter et al., 2020): This work proposes a cross-architecture knowledge distillation procedure with Margin-MSE loss, which can distill knowledge from multiple teachers.•RocketQAv1 (Qu et al., 2021): This work mainly focuses on the training of PLM based retriever, where the re-ranker is an intermediate product of its training process.•RocketQAv2 (Ren et al., 2021): Based on RocketQAv1, this work proposes a novel approach that jointly trains the PLM based retriever and re-ranker.To compare the performance of different methods, we resort to two ranking metrics.For MSMARCO-DEV, We adopt Mean Reciprocal Rank (i.e., MRR@10).For TREC 2019 DL, we use Mean Average Precision, i.e., MAP@10 and MAP@30.For Ohsumed, both Mean Reciprocal Rank and Mean Average Precision (i.e., MRR@10 and MAP@10) are employed for comprehensive performance analysis in queries requiring in-depth domain knowledge.","RocketQAv1 trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful cross-encoder to empower the dual-encoder. While it inherits the parameters from RocketQAv1, RocketQAv2 extends the first version through a novel approach that jointly trains the dense passage retriever and passage re-ranker, and by using a large PLM for data augmentation and denoising (i.e.,  a distillation procedure).","RocketQAv2 is an improved version of RocketQAv1.

RocketQAv1 trains dual-encoder and cross-encoder in a cascade manner, while RocketQAv2 jointly trains the dense passage retriever and passage re-ranker. Additionally, RocketQAv2 utilizes a large PLM for data augmentation and denoising, which can be regarded as a distillation procedure. This allows RocketQAv2 to improve the effectiveness of the model by leveraging explicit external knowledge",0.5263157844742382,0.3666666616666666,0.5263157844742382,38.36610461468432,60.7376998536639,58.39436844837592,0.5618536944905105,0.0150209308052203,0.9281622171401978,0.5903320005521993,0.7944404780864716,0.9369184970855712,0.3758136928081512,0.949727177619934,0.1580323808085442,4,0.8,0.97902908069302,0.4570116837894422,0.9709179493425748,1.0,4.0,0.7
79,Would the performance be improved if the PLM model is pre-trained or fine-tuned on bio-medical domain datasets?,"However, implicit knowledge still has some inherent weaknesses, which limits the applicability of PLMs based re-rankers. First,queries and passages are usually created by different persons and have different expression ways (Nogueiraet al., 2019b), such as word usage and language style.Worse still, the data distributions of search queries and web contents are highly heterogeneous (Liuet al., 2021), where various specialized domains (e.g., bio-medical) may only have few training examples in a general corpus. Domain-specific knowledge can hardly be revealed and captured by the model, and thus the processing of domain-specific queries is often inaccurate. Results are obtained from Table 6. (1) Poor ranking performances of all models on bio-medical domain indicates that it is more challenging in the data scarcity scenario, where textual data is not covered widely in the PLMs’ pretraining datasets. (2) Compared with ERNIE, KERM has a higher relative improvement in bio-medical domain than general domain. This demonstrates that the incorporation of knowledge graph is more useful for a data scarcity domain. To verify this idea, we compare the size of knowledge meta graph used for different domains as follows.","In their experiments, the authors showed that all of the models performed poorly on the bio-medical domain due to the textual data of the domain not being covered widely in the PLMs’ pretraining dataset. This lack of data can cause the PLM to struggle to reveal and capture knowledge specific to that domain. These results suggest that further training on bio-medical data could increase performance.",Yes,0.0,0.0,0.0,0.0,1.4452883844951463,1.0839662883713597,0.0,0.0001538224888478,0.0202166903764009,0.2800340354442596,0.0264758430421352,,0.0584235154092311,0.0749721080064773,0.0002831648623498,3,,0.7812842714578198,0.4812439652461502,0.7252213782567838,0.0,0.0,0.8
80,What characteristics of large-scale pre-trained language models made it remarkable successful for passage re-ranking task?,"Passage Re-ranking is a crucial stage in modern information retrieval systems, which aims to reorder a small set of candidate passages to be presented to users. To put the most relevant passages on top of a ranking list, a re-ranker is usually designed with powerful capacity in modeling semantic relevance, which attracted a wealth of research studies in the past decade (Guo et al., 2020). Recently,large-scale pre-trained language models (PLMs), e.g. BERT (Devlinet al., 2018), ERNIE (Sun et al., 2019) and RoBERTa (Liu et al., 2019), have dominated many natural language processing tasks, and have also achieved remarkable success on passage re-ranking.For example, PLM based re-rankers (MacAvaney et al., 2019; Liet al., 2020; Dong and Niu, 2021; Donget al., 2022) have achieved state-of-the-art performance, which takes the concatenation of query-passage pair as input, and applies multi-layer full-attention to model their semantic relevance. Their superiority can be attributed to the expressive transformer structure and the pretrain-then-finetune paradigm, which allow the model to learn useful implicit knowledge (i.e., semantic relevance in the latent space) from massive textual corpus (Fan et al., 2021).",Large-scale pre-trained language models (PLMs) have been found to be successful for passage re-ranking due to their ability to learn semantic relevance in the latent space from massive textual corpus. PLMs obtain this ability from their expressive transformer architecture and the pretrain-then-finetune paradigm.,The expressive transformer structure and the pretrain-then-finetune paradigm of large-scale pre-trained language models (PLMs) have contributed to their remarkable success on the passage re-ranking task,0.4918032739156142,0.3030302984022039,0.4262295034238108,20.499828552353325,69.0279559231807,63.64139515328469,0.3964444444444445,0.0075187969924812,0.8747223019599915,0.6641772453201145,0.7364842891693115,0.637157142162323,0.7887670993804932,0.9614017009735109,0.1567242244253213,4,,0.9721817944332218,0.6139459858163876,0.9558005479595738,1.0,5.0,0.95
81,How does the knowledge distilation works if meta-graph can't be constructed (i.e. there is no corresponding entities in knowledge graph for query/passage)?,"(1)Key sentence selection. The actual information need of a user usually concentrates on a small part of a relevant passage (Guo et al., 2020). To this end, we mimic human judgment and only focus on the sentence of each passage that is the most related to a query (Zou et al., 2021).In particular, we define the relevance score between a query q and a sentence \textbf{s}_{i} as(7)Rel_{qs}(\textbf{q},\textbf{s}_{i})=\frac{\sum_{q=1}^{|\textbf{q}|}\textbf{E}(w_{q})}{|\textbf{q}|}\cdot\frac{\sum_{s=1}^{|\textbf{s}_{i}|}\textbf{E}(w_{s})}{|\textbf{s}_{i}|}.For the sake of efficiency, we initialize \textbf{E}(w) from Word2Vec (Mikolovet al., 2013) embedding.Based on Eq.(7), we select the most relevant sentence \textbf{s}^{*} in p to build the meta-graph for \mathbf{q} and \mathbf{p}.(2)Target entity recognition.Next, we select the entities in q and \textbf{s}^{*} to construct the meta-graph. Specifically, we only consider the entities that exactly match in \mathcal{E}. Meanwhile, we omit those entity phrases that are sub-sequences of other recognized entities.For example, in the query ""what causes low liver enzymes"", both ""liver"" and ""liver enzyme"" are entities, but the entity ""liver enzyme"" is more informative to be recognized as the target entity, and ""liver"" should be omitted.(3)Path discovery. Finally, given the target entities of q and \textbf{s}^{*} (denoted as \phi_{\mathbf{q}} and \phi_{\mathbf{s}^{*}}, respectively), we perform Breadth First Search (BFS) on \overline{\mathcal{G}} to discover the paths within K-hop between \phi_{\mathbf{q}} and \phi_{\mathbf{s}^{*}}. Note that we only keep the within-K-hop paths that might be the most useful for the downstream re-ranking task. Meanwhile, the knowledge could be complemented from the K-hop paths.","Entities that exactly match entities in E are selected from q and s* to construct the meta-graph. Also, entities that are sub-sequences of other recognized entities are omitted. This process assumes that entities are identified in the query and passage. The process for handling cases where no entities are identified cannot be answered in this paper.","The knowledge distillation process does not rely on the construction of the meta-graph. Instead, it uses the pre-trained word embeddings to capture the semantic relationships between the query, passage, and target entities. Even if there are no corresponding entities in the knowledge graph, the distillation process can still leverage the word embeddings to learn the relevant information from the passage",0.3095238045351475,0.0555555505624147,0.2857142807256236,4.121376644135902,33.82758545402154,30.18411642404165,0.1895734597156398,0.0109890109890109,0.455671101808548,0.511072205102397,0.4429392317930857,0.703478217124939,0.1205771937966346,0.2983922958374023,0.0415055981478748,3,0.0,0.8444462001533427,0.5851317762969296,0.8405351409874113,0.5,1.0,
82,Does this method likely to show similar tendency of performance improvement when other backbone model (like BERT_large) is used?,"We use the traditional sparse retriever BM25 (Yanget al., 2017) as our first stage method. All experiments are conducted under the same BM25 setting with 1000 retrieved candidates. We conduct experiments with the deep learning framework PaddlePaddle (Maet al., 2019) on up to 4 NVIDIA Tesla A100 GPUs (with 40G RAM). For the GMN module, we use Paddle Graph Learning (PGL) 222https://github.com/PaddlePaddle/PGL, an efficient and flexible graph learning framework based on PaddlePaddle. For training, we used the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e-5 for text encoder and 1e-4 for knowledge injector. The model is trained up to 5 epochs with a batch size of 640 and 240 for base and large models respectively.In our experiments, the PLM small, base and large models have 6, 12 and 24 Transformer layers respectively.The text encoder has 9 layers and 21 layers for base and large model respectively, and the knowledge injector both has 3 layers in our experiment. The dropout rates are set to 0.1. The ratio of the positive to the hard negative is set to 1:19.All transformer layers in KERM’s backbone are initialized from ERNIE-2.0 base (Sunet al., 2020b), which is a BERT-like model pre-trained with a continual pre-training framework on multiple tasks. We perform Knowledge-enhanced pre-training on MARCO passage collection to warm up the parameters in knowledge injector, which has 60,000 iterations under the batch size of 256.For a fair comparison, the same pre-training without knowledge enhancement is also conducted on \textrm{ERNIE}_{\textrm{base}} re-ranker and all models in ablation studies. (4) Compared with \textrm{ERNIE}_{\textrm{base}} we trained, \textrm{KERM}_{\textrm{base}} shows a significant improvement on both two query sets. This indicates the explicit introduction of external knowledge can alleviate the semantic gap and heterogeneity between query and passage, and improve the semantic matching performance.","Through the experiments, this work demonstrated that the KERM model was able to significantly improve on the performance of its backbone model, ERNIE. The authors posit that this is due to how KERM explicitly introduces external knowledge which can improve semantic matching performance. This suggests that KERM models with other backbone models will be able to improve on the performance of their backbone models. However, the likelihood of performance improvements with other backbone models cannot be answered from this paper.","Yes.

The use of a different backbone model, such as BERT_large, is likely to result in similar performance improvements as the KERM model, due to the explicit introduction of external knowledge alleviating the semantic gap and heterogeneity between query and passage",0.3333333285147393,0.0909090862809919,0.2857142808956916,4.307997839692618,41.74352691278014,38.601185419302944,0.1763075799814151,0.0059069298371992,0.6713731288909912,0.6582621013243612,0.3859569877386093,0.6774297952651978,0.2768489420413971,0.0646719262003898,0.0142601398631473,3,0.5,0.8658244370476407,0.7218124633718761,0.8871784382067075,0.5,4.0,0.8
83,Why does existing knowledge enhanced PLMs (such as CokeBERT and CoLake) cannot be used directly for re-ranking tasks?,"Existing KE-PLMs can be categorized by the granularity of knowledge they incorporate from knowledge graph (KG), as text-based knowledge, entity knowledge and KG meta-graphs.To integrate text-based knowledge, RAG (Lewiset al., 2020) and KIF (Fanet al., 2020) first retrieve top-k documents from Wikipedia using KNN-based retrieval, and the PLM model is employed to generate the output conditioned on these retrieved documents. Entity-level information can be highly useful for a variety of natural language understanding tasks. Hence, many existing KE-PLMs target this type of simple yet powerful knowledge. ERNIE(BAIDU) (Sun et al., 2019) introduces a new pre-training strategy of language model which masking phrases or entities in order to implicitly learn both synaptic and semantic knowledge from these units. ERNIE(THU) (Zhanget al., 2019) integrates informative entity representations in the knowledge module into the underlying layers of the semantic module based on the alignments between text and entity to equip the model with the ability of knowledge awareness. As knowledge graphs provide richer information than simply entity, more and more researchers start to explore integration of more sophisticated knowledge, such as meta-graphs in KG. CokeBERT (Su et al., 2021) proposes a novel semantic-driven Graph Neural Network (GNN) to dynamically select contextual knowledge and embed knowledge context according to textual context for PLMs, which can avoid the effect of redundant and ambiguous knowledge in KGs that cannot match the input text.CoLake (Sunet al., 2020a) also uses GNN to aggregate information from the constructed meta-graph in both pre-training and inference. CoLake converts the meta-graph into token sequence and appends it to input sequence for PLMs, which is distinctive to CokeBERT. Although extensive research has been proposed up to now to address the knowledge-aware problem, none exists which constrained on how to use knowledge to empower PLMs particularly for re-ranking tasks. Different from existing knowledge-enhanced PLMs for other NLP tasks, our aim for the re-ranking task is particularly on the relevance modeling between query and passage.Thus, we further leverage the knowledge in the global graph \overline{\mathcal{G}} to construct “bridges” between query and passage, which alleviates the semantic gap and improves semantic modeling.More specifically, for a given query-passage pair (i.e., (\mathbf{q},\mathbf{p})), we propose to construct a bipartite meta-graph that connects those entities in the \mathbf{q} and those in \mathbf{p}.","While approaches like CokeBERT and CoLake integrate sophisticated knowledge into PLMs through knowledge graphs, they did not focus specifically on using knowledge to empower PLMs for re-ranking tasks. The reasons for why CokeBERT or CoLake cannot be directly used in re-ranking cannot be answered from this paper.","Existing knowledge-enhanced PLMs, such as CokeBERT and CoLake, are not directly suitable for re-ranking tasks because they are designed primarily for other NLP tasks, such as semantic relevance and entity-level information, and do not specifically target the relevance modeling between query and passage",0.2499999950154321,0.0697674368712821,0.1944444394598766,5.538275311043418,36.758099476949006,32.05637308610256,0.174757281553398,0.0094651111600264,0.825920820236206,0.4257312569197667,0.7405765056610107,0.7162345051765442,0.1768991202116012,0.397619366645813,0.026625573842443,3,0.6666666666666666,0.9664568344495018,0.4513317351822881,0.9479523617444732,0.75,4.0,0.9
84,Would there be a performance gain if the model utilizes the IE (information extraction) model instead of the exact match for target entity recognition?,"(1)Key sentence selection. The actual information need of a user usually concentrates on a small part of a relevant passage (Guo et al., 2020). To this end, we mimic human judgment and only focus on the sentence of each passage that is the most related to a query (Zou et al., 2021).In particular, we define the relevance score between a query q and a sentence \textbf{s}_{i} as(7)Rel_{qs}(\textbf{q},\textbf{s}_{i})=\frac{\sum_{q=1}^{|\textbf{q}|}\textbf{E}(w_{q})}{|\textbf{q}|}\cdot\frac{\sum_{s=1}^{|\textbf{s}_{i}|}\textbf{E}(w_{s})}{|\textbf{s}_{i}|}.For the sake of efficiency, we initialize \textbf{E}(w) from Word2Vec (Mikolovet al., 2013) embedding.Based on Eq.(7), we select the most relevant sentence \textbf{s}^{*} in p to build the meta-graph for \mathbf{q} and \mathbf{p}.(2)Target entity recognition.Next, we select the entities in q and \textbf{s}^{*} to construct the meta-graph. Specifically, we only consider the entities that exactly match in \mathcal{E}. Meanwhile, we omit those entity phrases that are sub-sequences of other recognized entities.For example, in the query ""what causes low liver enzymes"", both ""liver"" and ""liver enzyme"" are entities, but the entity ""liver enzyme"" is more informative to be recognized as the target entity, and ""liver"" should be omitted.(3)Path discovery. Finally, given the target entities of q and \textbf{s}^{*} (denoted as \phi_{\mathbf{q}} and \phi_{\mathbf{s}^{*}}, respectively), we perform Breadth First Search (BFS) on \overline{\mathcal{G}} to discover the paths within K-hop between \phi_{\mathbf{q}} and \phi_{\mathbf{s}^{*}}. Note that we only keep the within-K-hop paths that might be the most useful for the downstream re-ranking task. Meanwhile, the knowledge could be complemented from the K-hop paths. • Challenge 1. Existing knowledge graph are not constructed for re-ranking task. They usually contain trivial factual triples, which can hardly bring information gain. The inappropriate selection of external knowledge could even jeopardize the re-ranker performance. How to utilize existing knowledge graph to re-ranking task is remain a challenge. • Challenge 2. The explicit knowledge and implicit knowledge are highly heterogeneous due to the different sources, which makes the aggregation of the two difficult. How to mutually refine each other and effectively aggregate explicit knowledge into implicit knowledge to alleviate the semantic gap between query and passage is still a challenge.","This work’s approach aims at focusing mostly on informative factors. For example, the key sentence selection module focused on extracting only the most relevant sentences and the target entity recognition module focused on identifying only the most informative entities. Further, this work argues that, to use knowledge graphs for re-ranking tasks, it is important that the graphs contain triplets with substantial information gain. The effect on information gain from using IE models, instead of exact match, for target entity recognition cannot be answered from this paper.","Yes.

By utilizing an IE model instead of exact match for target entity recognition, the model can potentially gain performance by capturing more informative entities that may not be exactly matched in the query or passage. This can lead to more accurate and informative paths being discovered in the knowledge graph, which can improve the re-ranking performance",0.3214285665577169,0.0751879650743403,0.2857142808434312,6.885462085538328,46.62092752606742,41.60581952580328,0.2559937549238446,0.0072546773577701,0.7145088911056519,0.7188299560191062,0.559300101051728,0.7700898051261902,0.2603433430194855,0.2779231071472168,0.0245665997417328,4,1.0,0.8517109763449363,0.5577373548858864,0.897668601676896,0.6,1.0,
85,How does TransE learns entity and relatio embeddings in unsupervised way?,"Given a global knowledge graph \mathcal{G}, the first step is to eliminate those knowledge that might be noisy to be applied. To achieve this, we use TransE (Bordes et al., 2013) to measure the reliability of a given knowledge triplet. In particular, TransE is an unsupervised learning method that learns latent representations for a knowledge triplet (e_{h},r,e_{t}). Intuitively, it models the latent distribution of knowledge in a given knowledge graph, and those who are out of this distribution can be viewed as less informative knowledge, which should not be used. Based on this,we use the entity embeddings pre-trained by TransE to calculate a distance metric between two linked entities as(3)Rel_{e}(e_{h},r,e_{t})=\mathbf{E}({e_{h}})\cdot\mathbf{E}(r)+\mathbf{E}({e_{h}})\cdot\mathbf{E}({e_{t}})+\mathbf{E}({r})\cdot\mathbf{E}({e_{t}}),(4)Dist(e_{h},e_{t})=\frac{1}{Rel_{e}(e_{h},r,e_{t})},where \mathbf{E}({e}) and \mathbf{E}({r}) are the TransE embeddings of entity and relation, respectively, and the inner product measures the relevance between two vectors. As the objective of TranE is aligned with minimizing the distance shown in Eq.(4), we can consider those knowledge triplets with small distance values as informative knowledge.",TransE is an unsupervised learning method that learns latent representations for a knowledge triplet. The method in which TransE learns these latent representations cannot be answered from this paper.,TransE learns entity and relation embeddings in an unsupervised way by modeling the latent distribution of knowledge in a given knowledge graph,0.3636363586776859,0.0833333284114586,0.3181818132231406,4.0341101701202575,38.85651007750958,34.44058830868425,0.2220230983671843,0.0087232355273592,0.7800800800323486,0.6426444147893029,0.7659541368484497,0.7558284997940063,0.7335315942764282,0.3480929732322693,0.0195219625559056,3,1.0,0.9685526884681064,0.4795746639031093,0.9183133968479752,1.0,2.0,0.8
86,What is the maximum memory capacity of FPGA? ,"Much of the recent research on deep convolutional neural networks (CNNs) has focused on increasing accuracy on computer vision datasets.For a given accuracy level, there typically exist multiple CNN architectures that achieve that accuracy level.Given equivalent accuracy, a CNN architecture with fewer parameters has several advantages:\bulletMore efficient distributed training.Communication among servers is the limiting factor to the scalability of distributed CNN training.For distributed data-parallel training, communication overhead is directly proportional to the number of parameters in the model Iandola et al. (2016).In short, small models train faster due to requiring less communication.\bulletLess overhead when exporting new models to clients. For autonomous driving, companies such as Tesla periodically copy new models from their servers to customers’ cars. This practice is often referred to as an over-the-air update. Consumer Reports has found that the safety of Tesla’s Autopilot semi-autonomous driving functionality has incrementally improved with recent over-the-air updates Consumer Reports (2016). However, over-the-air updates of today’s typical CNN/DNN models can require large data transfers. With AlexNet, this would require 240MB of communication from the server to the car. Smaller models require less communication, making frequent updates more feasible.\bulletFeasible FPGA and embedded deployment. FPGAs often have less than 10MB111For example, the Xilinx Vertex-7 FPGA has a maximum of 8.5 MBytes (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory. of on-chip memory and no off-chip memory or storage. For inference, a sufficiently small model could be stored directly on the FPGA instead of being bottlenecked by memory bandwidth Qiu et al. (2016), while video frames stream through the FPGA in real time.Further, when deploying CNNs on Application-Specific Integrated Circuits (ASICs), a sufficiently small model could be stored directly on-chip, and smaller models may enable the ASIC to fit on a smaller die.","Near 10 MB of on-chip memory and no off-chip memory or storage(For example, the Xilinx Vertex-7 FPGA has a maximum of 8.5 MB (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory)",8.5 MBytes (68 Mbits) of on-chip memory,0.3243243209349891,0.1999999971125,0.3243243209349891,11.017212838991655,42.72133600276055,42.38271931688617,0.1631136950904392,0.0023279015630196,0.6918707489967346,0.4074074074074073,0.8652797937393188,0.9488627314567566,0.213091492652893,0.98710697889328,0.1001052901544773,4,0.5,0.9022025075845764,0.4741553198575645,0.8968490774226061,1.0,4.0,0.9
87,What is an example of an autonomous car that uses CNN?,"Much of the recent research on deep convolutional neural networks (CNNs) has focused on increasing accuracy on computer vision datasets.For a given accuracy level, there typically exist multiple CNN architectures that achieve that accuracy level.Given equivalent accuracy, a CNN architecture with fewer parameters has several advantages:\bulletMore efficient distributed training.Communication among servers is the limiting factor to the scalability of distributed CNN training.For distributed data-parallel training, communication overhead is directly proportional to the number of parameters in the model Iandola et al. (2016).In short, small models train faster due to requiring less communication.\bulletLess overhead when exporting new models to clients. For autonomous driving, companies such as Tesla periodically copy new models from their servers to customers’ cars. This practice is often referred to as an over-the-air update. Consumer Reports has found that the safety of Tesla’s Autopilot semi-autonomous driving functionality has incrementally improved with recent over-the-air updates Consumer Reports (2016). However, over-the-air updates of today’s typical CNN/DNN models can require large data transfers. With AlexNet, this would require 240MB of communication from the server to the car. Smaller models require less communication, making frequent updates more feasible.\bulletFeasible FPGA and embedded deployment. FPGAs often have less than 10MB111For example, the Xilinx Vertex-7 FPGA has a maximum of 8.5 MBytes (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory. of on-chip memory and no off-chip memory or storage. For inference, a sufficiently small model could be stored directly on the FPGA instead of being bottlenecked by memory bandwidth Qiu et al. (2016), while video frames stream through the FPGA in real time.Further, when deploying CNNs on Application-Specific Integrated Circuits (ASICs), a sufficiently small model could be stored directly on-chip, and smaller models may enable the ASIC to fit on a smaller die.",Tesla ( Model S for example ) autopilot system uses a convolutional neural network to detect objects on its way.,Tesla's Autopilot semi-autonomous driving functionality,0.0,0.0,0.0,0.0,31.607264841019425,23.705448630764565,0.0769230769230769,0.0026246719160104,0.5963187217712402,0.5751675951598895,0.5963187217712402,0.2153009325265884,0.6864705681800842,0.4008577466011047,0.047697115728538,3,0.0,0.8311785499859795,0.7224147037821678,0.8894771303368988,0.5,2.0,0.8
88,High accuracy is crucial for safety in autonomous vehicles. Would deploying smaller models using over-the-air updates in Tesla result in a trade-off with accuracy(and hence safety)?,"Much of the recent research on deep convolutional neural networks (CNNs) has focused on increasing accuracy on computer vision datasets.For a given accuracy level, there typically exist multiple CNN architectures that achieve that accuracy level.Given equivalent accuracy, a CNN architecture with fewer parameters has several advantages:\bulletMore efficient distributed training.Communication among servers is the limiting factor to the scalability of distributed CNN training.For distributed data-parallel training, communication overhead is directly proportional to the number of parameters in the model Iandola et al. (2016).In short, small models train faster due to requiring less communication.\bulletLess overhead when exporting new models to clients. For autonomous driving, companies such as Tesla periodically copy new models from their servers to customers’ cars. This practice is often referred to as an over-the-air update. Consumer Reports has found that the safety of Tesla’s Autopilot semi-autonomous driving functionality has incrementally improved with recent over-the-air updates Consumer Reports (2016). However, over-the-air updates of today’s typical CNN/DNN models can require large data transfers. With AlexNet, this would require 240MB of communication from the server to the car. Smaller models require less communication, making frequent updates more feasible.\bulletFeasible FPGA and embedded deployment. FPGAs often have less than 10MB111For example, the Xilinx Vertex-7 FPGA has a maximum of 8.5 MBytes (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory. of on-chip memory and no off-chip memory or storage. For inference, a sufficiently small model could be stored directly on the FPGA instead of being bottlenecked by memory bandwidth Qiu et al. (2016), while video frames stream through the FPGA in real time.Further, when deploying CNNs on Application-Specific Integrated Circuits (ASICs), a sufficiently small model could be stored directly on-chip, and smaller models may enable the ASIC to fit on a smaller die. So far, we have proposed architectural design strategies for small models, followed these principles to create SqueezeNet, and discovered that SqueezeNet is 50x smaller than AlexNet with equivalent accuracy.However, SqueezeNet and other models reside in a broad and largely unexplored design space of CNN architectures.Now, in Sections 5 and 6, we explore several aspects of the design space. We divide this architectural exploration into two main topics: microarchitectural exploration (per-module layer dimensions and configurations) and macroarchitectural exploration (high-level end-to-end organization of modules and other layers). ","Accuracy is crucial for safety but it's not only accuracy vs size relation. We should consider more aspects. For example, response time of a driving car system is very crucial for safety. Communication overhead between servers while model training increases with the size of the model so smaller models train faster. Updating models from company servers to the car or over-the-air updates based on AlexNet at that time would require 240MB of communication from the server to the car. Hence, smaller models require less communication, making frequent updates more feasible. Also, keeping in mind architectural designs such as adjusting some functionalities, introducing new ways of extracting features, or using different objectives and optimizers may make a small model achieve the same level of accuracy or even surpass the larger model; for instance, SqueezeNet is 50x smaller than AlexNet with equivalent accuracy.","NO.

Smaller models with equivalent accuracy as AlexNet can be deployed using over-the-air updates in Tesla without compromising safety. The advantages of smaller models, such as more efficient distributed training, less overhead when exporting new models to clients, and feasible FPGA and embedded deployment, outweigh any potential accuracy trade-offs",0.2857142814660558,0.0437158431246084,0.2176870705817021,2.010035024980693,38.70786962867469,35.207489097540154,0.1350612371892177,0.0037265191269298,0.6157276630401611,0.6955276377228516,0.4692668716112772,0.6884636878967285,0.1733310669660568,0.3015468716621399,0.015947845311382,4,0.2,0.0,0.3602033485307591,0.8953217164165426,1.0,2.0,0.8
89,What is an example of an FPGA?,"Much of the recent research on deep convolutional neural networks (CNNs) has focused on increasing accuracy on computer vision datasets.For a given accuracy level, there typically exist multiple CNN architectures that achieve that accuracy level.Given equivalent accuracy, a CNN architecture with fewer parameters has several advantages:\bulletMore efficient distributed training.Communication among servers is the limiting factor to the scalability of distributed CNN training.For distributed data-parallel training, communication overhead is directly proportional to the number of parameters in the model Iandola et al. (2016).In short, small models train faster due to requiring less communication.\bulletLess overhead when exporting new models to clients. For autonomous driving, companies such as Tesla periodically copy new models from their servers to customers’ cars. This practice is often referred to as an over-the-air update. Consumer Reports has found that the safety of Tesla’s Autopilot semi-autonomous driving functionality has incrementally improved with recent over-the-air updates Consumer Reports (2016). However, over-the-air updates of today’s typical CNN/DNN models can require large data transfers. With AlexNet, this would require 240MB of communication from the server to the car. Smaller models require less communication, making frequent updates more feasible.\bulletFeasible FPGA and embedded deployment. FPGAs often have less than 10MB111For example, the Xilinx Vertex-7 FPGA has a maximum of 8.5 MBytes (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory. of on-chip memory and no off-chip memory or storage. For inference, a sufficiently small model could be stored directly on the FPGA instead of being bottlenecked by memory bandwidth Qiu et al. (2016), while video frames stream through the FPGA in real time.Further, when deploying CNNs on Application-Specific Integrated Circuits (ASICs), a sufficiently small model could be stored directly on-chip, and smaller models may enable the ASIC to fit on a smaller die.",Xilinx Vertex-7 FPGA which has a maximum of 8.5 MB (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory.,Xilinx Vertex-7 FPGA,0.239999997888,0.159999998528,0.239999997888,4.970745472800839,48.23884648555169,45.53338260085734,0.1242381622128457,0.001576458223857,0.7365744113922119,0.6,0.8046629428863525,0.0802843645215034,0.5441359281539917,0.9908363819122314,0.3079388847243108,3,1.0,0.9129809372637656,0.599779689850404,0.8990163521836791,1.0,4.0,0.8
90,What is an example of model compression approaches?,"The overarching goal of our work is to identify a model that has very few parameters while preserving accuracy.To address this problem, a sensible approach is to take an existing CNN model and compress it in a lossy fashion.In fact, a research community has emerged around the topic of model compression, and several approaches have been reported.A fairly straightforward approach by Denton et al. is to apply singular value decomposition (SVD) to a pretrained CNN model Denton et al. (2014).Han et al. developed Network Pruning, which begins with a pretrained model, then replaces parameters that are below a certain threshold with zeros to form a sparse matrix, and finally performs a few iterations of training on the sparse CNN Han et al. (2015b).Recently, Han et al. extended their work by combining Network Pruning with quantization (to 8 bits or less) and huffman encoding to create an approach called Deep Compression Han et al. (2015a), and further designed a hardware accelerator called EIE Han et al. (2016a) that operates directly on the compressed model, achieving substantial speedups and energy savings.","different examples can be: Applying SVD to a pretrained CNN model through which we can obtain most effective parameters or features of largest singular values of this factorization if we want. Information reconstruction of a matrix factorized with SVD  allow decreasing its rank, hence decreasing the memory allocated to save the vectors of these parameters . Also Network Pruning, which begins with a pretrained model, then replaces parameters that are below a certain threshold with zeros to form a sparse matrix, and finally performs a few iterations of training on the sparse CNN Maybe seen as another example . Deep compression -utilizing Huffman encoding, Network Pruning and quantization- yet is a third example.","Model compression approaches include SVD-based compression, Network Pruning, quantization, and huffman encoding",0.0851063807514713,0.016666665001389,0.0851063807514713,1.9903919122973324,29.34279411742103,26.568776895348662,0.0673913043478261,0.0011098779134295,0.5915745496749878,0.8629731382517254,0.7376848459243774,0.5389003753662109,0.2346570491790771,0.6650747060775757,0.0531518430180892,4,1.0,0.9650617142946012,0.7192036668111782,0.8767967106194742,1.0,5.0,0.8
91,"What is an example of a ""module"" in CNN?","With the trend of designing very deep CNNs, it becomes cumbersome to manually select filter dimensions for each layer.To address this, various higher level building blocks, or modules, comprised of multiple convolution layers with a specific fixed organization have been proposed.For example, the GoogLeNet papers propose Inception modules, which are comprised of a number of different dimensionalities of filters, usually including 1x1 and 3x3, plus sometimes 5x5 Szegedy et al. (2014) and sometimes 1x3 and 3x1 Szegedy et al. (2015).Many such modules are then combined, perhaps with additional ad-hoc layers, to form a complete network.We use the term CNN microarchitecture to refer to the particular organization and dimensions of the individual modules.","a module can be thought of as a block of some several layers may be of different filter sizes and dimensions to perform some specific functionality. Many such modules are then combined to form a complete network. For example, Inception modules, which are comprised of a number of different dimensionalities of filters, like 1x1 and 3x3, sometimes 5x5, 1x3 and 3x1.","An example of a ""module"" in CNN is the Inception module, which is a building block composed of multiple convolutional layers with different fixed dimensions, such as 1x1, 3x3, 5x5, and 1x3, 3x1",0.3421052583795014,0.0219780174181871,0.289473679432133,2.78601414987878,42.197102016696334,39.59912393815211,0.2190332326283988,0.0060739922694643,0.7804293036460876,0.6056238939966325,0.7218546867370605,0.7242287397384644,0.4709964990615845,0.8052758574485779,0.1118836868209503,4,1.0,0.9933469705579302,0.5322526918400214,0.9290978913675012,1.0,5.0,0.98
92,What is an example of a DSE approach?,"Neural networks (including deep and convolutional NNs) have a large design space, with numerous options for microarchitectures, macroarchitectures, solvers, and other hyperparameters.It seems natural that the community would want to gain intuition about how these factors impact a NN’s accuracy (i.e. the shape of the design space).Much of the work on design space exploration (DSE) of NNs has focused on developing automated approaches for finding NN architectures that deliver higher accuracy.These automated DSE approaches include bayesian optimization Snoek et al. (2012), simulated annealing Ludermir et al. (2006), randomized search Bergstra & Bengio (2012), and genetic algorithms Stanley & Miikkulainen (2002).To their credit, each of these papers provides a case in which the proposed DSE approach produces a NN architecture that achieves higher accuracy compared to a representative baseline.However, these papers make no attempt to provide intuition about the shape of the NN design space.Later in this paper, we eschew automated approaches – instead, we refactor CNNs in such a way that we can do principled A/B comparisons to investigate how CNN architectural decisions influence model size and accuracy.","An example of DSE approach can be  Bayesian optimization, simulated annealing, randomized search or genetic algorithms and all tend to develop automated approaches to find NN architectures exhibiting higher accuracy.",Bayesian optimization,0.0645161278251821,0.0,0.0645161278251821,2.246199289207964,34.49622830529167,31.14181399601937,0.0627090301003344,0.0006891798759476,0.4033109843730926,1.0,0.4033111929893493,0.6948093175888062,0.2681411504745483,0.9897518754005432,0.0406648907505414,4,1.0,0.7985292227532175,0.5787081555610333,0.8148234256981217,0.5714285714285714,4.0,0.75
93,What is the ratio of 1x1 filters in the total number of filters?,"We define the Fire module as follows.A Fire module is comprised of: a squeeze convolution layer (which has only 1x1 filters), feeding into an expand layer that has a mix of 1x1 and 3x3 convolution filters; we illustrate this in Figure 1.The liberal use of 1x1 filters in Fire modules is an application of Strategy 1 from Section 3.1.We expose three tunable dimensions (hyperparameters) in a Fire module: s_{1x1}, e_{1x1}, and e_{3x3}.In a Fire module, s_{1x1} is the number of filters in the squeeze layer (all 1x1), e_{1x1} is the number of 1x1 filters in the expand layer, and e_{3x3} is the number of 3x3 filters in the expand layer.When we use Fire modules we set s_{1x1} to be less than (e_{1x1} + e_{3x3}), so the squeeze layer helps to limit the number of input channels to the 3x3 filters, as per Strategy 2 from Section 3.1.","The question needs to be related to some certain context but if we consider asking about the ratio of 1*1 filters in each fire module then the answer would be as follows: for a fire module ratio of 1*1 filters w.r.t. all filters can be calculated as (s1x1+e1x1)/(s1x1+e1x1+e3x3) where; s1x1 is the number of filters in the squeeze layer,e1x1 is the number of 1x1 filters in the expand layer, and e3x3 is the number of 3x3 filters in the expand layer. It is also worth to mention that s1x1 is to be less than (e1x1 + e3x3), so the squeeze layer helps to limit the number of input channels to the 3x3 filters.",The ratio of 1x1 filters in the total number of filters is e_{1x1}/(e_{1x1} + e_{3x3}),0.253164554212466,0.1320754694054824,0.2025316428200609,3.9698444712896106,27.06504504428249,26.942495822635195,0.0904448918474816,0.0014829461196243,0.6549941301345825,0.73411189893476,0.711905837059021,0.6729288101196289,0.253562867641449,0.775286078453064,0.0981398262663638,3,1.0,1.000000000000001,0.39076542065972,0.8964553768175614,0.5,4.0,0.7
94,"How does the choice of layers, in which to downsample, affect the size of activation maps?","Strategy 3. Downsample late in the network so that convolution layers have large activation maps.In a convolutional network, each convolution layer produces an output activation map with a spatial resolution that is at least 1x1 and often much larger than 1x1.The height and width of these activation maps are controlled by: (1) the size of the input data (e.g. 256x256 images) and (2) the choice of layers in which to downsample in the CNN architecture.Most commonly, downsampling is engineered into CNN architectures by setting the (stride > 1) in some of the convolution or pooling layers (e.g. Szegedy et al. (2014); Simonyan & Zisserman (2014); Krizhevsky et al. (2012)).If early333In our terminology, an “early” layer is close to the input data. layers in the network have large strides, then most layers will have small activation maps.Conversely, if most layers in the network have a stride of 1, and the strides greater than 1 are concentrated toward the end444In our terminology, the “end” of the network is the classifier. of the network, then many layers in the network will have large activation maps.Our intuition is that large activation maps (due to delayed downsampling) can lead to higher classification accuracy, with all else held equal.Indeed, K. He and H. Sun applied delayed downsampling to four different CNN architectures, and in each case delayed downsampling led to higher classification accuracy He & Sun (2015).","As we can see, downsampling aim to collect summary about statistics of different regions of some feature map, and this can be addressed with stride >1 in convolution or pooling layers which affects the size of activation map. If early layers  have large strides, then most layers will have small activation maps and if most layers in the network have a stride of 1, and the strides greater than 1 are later in the network, then many layers in the network will have large activation maps.","The choice of layers in which to downsample affects the size of activation maps by controlling the spatial resolution of the output. Downsampling early in the network results in smaller activation maps, while downsampling late in the network preserves larger activation maps",0.3023255769253651,0.1391304303576561,0.2790697629718767,8.054297623377934,48.90692703442811,45.82917417273053,0.3087418802668539,0.0060501296456352,0.7619210481643677,0.7795315077826804,0.6643487811088562,0.7864681482315063,0.9009963274002076,0.2952786684036255,0.0638226589617302,4,1.0,0.9363471502419012,0.4139986061470709,0.9061775988148126,1.0,5.0,0.9
95,Why did the authors use a mix of 1x1 and 3x3 filters in the expand layer of fire module?,"Strategy 2. Decrease the number of input channels to 3x3 filters.Consider a convolution layer that is comprised entirely of 3x3 filters.The total quantity of parameters in this layer is (number of input channels) * (number of filters) * (3*3).So, to maintain a small total number of parameters in a CNN, it is important not only to decrease the number of 3x3 filters (see Strategy 1 above), but also to decrease the number of input channels to the 3x3 filters.We decrease the number of input channels to 3x3 filters using squeeze layers, which we describe in the next section.  We define the Fire module as follows.A Fire module is comprised of: a squeeze convolution layer (which has only 1x1 filters), feeding into an expand layer that has a mix of 1x1 and 3x3 convolution filters; we illustrate this in Figure 1.The liberal use of 1x1 filters in Fire modules is an application of Strategy 1 from Section 3.1.We expose three tunable dimensions (hyperparameters) in a Fire module: s_{1x1}, e_{1x1}, and e_{3x3}.In a Fire module, s_{1x1} is the number of filters in the squeeze layer (all 1x1), e_{1x1} is the number of 1x1 filters in the expand layer, and e_{3x3} is the number of 3x3 filters in the expand layer.When we use Fire modules we set s_{1x1} to be less than (e_{1x1} + e_{3x3}), so the squeeze layer helps to limit the number of input channels to the 3x3 filters, as per Strategy 2 from Section 3.1.","Authors used a mix of 1x1 and 3x3 filters in the expand layer of the fire module to reduce the number of parameters while still getting benefits from the desired properties of having reasonable scope of the input receptive field and extracting correlations and useful information by applying the 3*3 filters of the CNN. To have a small number of parameters in a CNN, we need to decrease the number of input channels to the 3x3 filters and here comes the role of 1*1 filters, while the 3x3 filters are used to capture larger spatial features (Assuming only 3*3 and 1*1 kernels). This way, the model get its wide fame of achieving a high level of accuracy with fewer parameters than other networks.","To maintain a small total number of parameters in a CNN, the authors use a mix of 1x1 and 3x3 filters in the expand layer of the Fire module, as described in Strategy 2 of Section 3.1. By using 1x1 filters, the number of input channels to the 3x3 filters is decreased, which helps to limit the total number of parameters in the layer",0.3418803373803784,0.2823529367190311,0.3247863202863613,20.804894636038945,52.975510504757054,51.521053140391246,0.2826516190713721,0.0069838498472282,0.9026752710342408,0.8111016796023216,0.7895431816577911,0.6984023451805115,0.2568669319152832,0.7052677869796753,0.1134433773374545,4,1.0,0.992714410685942,0.6697909824658932,0.9648846017914214,1.0,4.0,0.95
96,What is the total number of filters in squeeze convolution layer?,"We define the Fire module as follows.A Fire module is comprised of: a squeeze convolution layer (which has only 1x1 filters), feeding into an expand layer that has a mix of 1x1 and 3x3 convolution filters; we illustrate this in Figure 1.The liberal use of 1x1 filters in Fire modules is an application of Strategy 1 from Section 3.1.We expose three tunable dimensions (hyperparameters) in a Fire module: s_{1x1}, e_{1x1}, and e_{3x3}.In a Fire module, s_{1x1} is the number of filters in the squeeze layer (all 1x1), e_{1x1} is the number of 1x1 filters in the expand layer, and e_{3x3} is the number of 3x3 filters in the expand layer.When we use Fire modules we set s_{1x1} to be less than (e_{1x1} + e_{3x3}), so the squeeze layer helps to limit the number of input channels to the 3x3 filters, as per Strategy 2 from Section 3.1.",s1x1 is the number of filters in the squeeze layer and it is set s1x1 to be less than (e1x1 + e3x3) -the total number of filters in expand layer of the fire module- to limit the number of input channels to the 3x3 filters.,1,0.0,0.0,0.0,0.0,2.747252747252747,1.3736263736263734,0.0,0.0002221728504776,0.2253756821155548,0.191225990653038,0.2253757119178772,,0.1390906572341919,0.0820789709687233,0.0017079795567009,1,0.0,0.7345648361209811,0.5678393823895934,0.7715673341248931,0.1666666666666666,0.0,0.4
97,"The Caffe framework does not natively support a convolution layer that contains multiple filter resolutions .To get around this, the authors implement the expand layer with two separate convolution layers. What is the additional cost incurred by using two convolution layers?","\bulletSo that the output activations from 1x1 and 3x3 filters have the same height and width, we add a 1-pixel border of zero-padding in the input data to 3x3 filters of expand modules.\bulletReLU Nair & Hinton (2010) is applied to activations from squeeze and expand layers.\bulletDropout Srivastava et al. (2014) with a ratio of 50% is applied after the fire9 module.\bulletNote the lack of fully-connected layers in SqueezeNet; this design choice was inspired by the NiN Lin et al. (2013) architecture.\bulletWhen training SqueezeNet, we begin with a learning rate of 0.04, and we linearly decrease the learning rate throughout training, as described in Mishkin et al. (2016).For details on the training protocol (e.g. batch size, learning rate, parameter initialization), please refer to our Caffe-compatible configuration files located here: https://github.com/DeepScale/SqueezeNet.\bulletThe Caffe framework does not natively support a convolution layer that contains multiple filter resolutions (e.g. 1x1 and 3x3) Jia et al. (2014). To get around this, we implement our expand layer with two separate convolution layers: a layer with 1x1 filters, and a layer with 3x3 filters. Then, we concatenate the outputs of these layers together in the channel dimension. This is numerically equivalent to implementing one layer that contains both 1x1 and 3x3 filters.","The additional cost of using 2 convolutional layers may be that the parameters of the 2 layers are now trained separately; they are not benefiting from each other being jointly optimized to perform some task and share useful information between each other while training, but output shape is still not affected by the separation i.e.,this is numerically equivalent to have one layer that contains both 1x1 and 3x3 filters.",Increased computational cost due to concatenating the outputs of two separate convolution layers,0.1408450674310653,0.0,0.1408450674310653,0.7003226658264669,31.7734022518456,26.388293207885404,0.0662739322533137,0.0019658248903674,0.7705753445625305,0.7020596369493718,0.770575225353241,0.7428770065307617,0.2358236014842987,0.1397313326597213,0.0289059492804285,3,0.5,0.7797668827201081,0.5224730657383869,0.8898440103273395,1.0,2.0,
98,Did the authors use AlexNet for evaluation of SqueezeNet?,"We now turn our attention to evaluating SqueezeNet.In each of the CNN model compression papers reviewed in Section 2.1, the goal was to compress an AlexNet Krizhevsky et al. (2012) model that was trained to classify images using the ImageNet Deng et al. (2009) (ILSVRC 2012) dataset.Therefore, we use AlexNet555Our baseline is bvlc_alexnet from the Caffe codebase Jia et al. (2014). and the associated model compression results as a basis for comparison when evaluating SqueezeNet.","Yes, as told by authors that they used AlexNet and the associated model compression results as a basis for comparison when evaluating SqueezeNet.",Yes,0.0,0.0,0.0,1.506189323093867,7.672902655507709,10.220869181685986,0.0221238938053097,0.0004345936549326,0.1641569882631302,1.0,0.1641570180654525,,0.5731861591339111,0.9883264303207396,0.0105234201849612,4,,0.8723928721435622,0.6964957048766203,0.7858937459222991,0.0,2.0,1.0
99,How would the effectiveness of SqueezeNet's model compression be affected if a significantly smaller CNN is used instead of AlexNet?,"In addition, these results demonstrate that Deep Compression Han et al. (2015a) not only works well on CNN architectures with many parameters (e.g. AlexNet and VGG), but it is also able to compress the already compact, fully convolutional SqueezeNet architecture.Deep Compression compressed SqueezeNet by 10×10\times10 × while preserving the baseline accuracy.In summary: by combining CNN architectural innovation (SqueezeNet) with state-of-the-art compression techniques (Deep Compression), we achieved a 510×510\times510 × reduction in model size with no decrease in accuracy compared to the baseline.","by combining CNN architectural innovation (SqueezeNet) with state-of-the-art compression techniques (Deep Compression), we achieved a 510× reduction in model size with no decrease in accuracy compared
to the baseline.",The effectiveness of SqueezeNet's model compression would likely be even more significant if a significantly smaller CNN is used instead of AlexNet,0.1666666617447918,0.0,0.0833333284114586,1.493369524819332,27.897979661219747,22.989604321290173,0.0887573964497041,0.0077958894401133,0.7780088186264038,0.4655779816359809,0.7780088186264038,0.5491448044776917,0.3940216600894928,0.3591561317443847,0.0214025297983902,3,0.0,0.9623811797291644,0.7275053946125292,0.9094928877027156,0.5,2.0,0.8
