{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qasper_aligned = pd.read_csv(\"/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/computing scores/FULL ALIGNED/qasper_aligned_iris_100.csv\")\n",
    "qasper_8b= pd.read_csv(\"/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/computing scores/Qasper 8-70b/qasper_8b_100.csv\")\n",
    "qasper_70b = pd.read_csv(\"/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/computing scores/Qasper 8-70b/qasper_70b_100.csv\")\n",
    "qasa_aligned = pd.read_csv(\"/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/computing scores/FULL ALIGNED/qasa_aligned_iris_100.csv\")\n",
    "qasa_8b = pd.read_csv(\"/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/computing scores/Qasa 8-70b/qasa_8b_100.csv\")\n",
    "qasa_70b = pd.read_csv(\"/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/computing scores/Qasa 8-70b/qasa_70b_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qasper_8b = qasper_8b.rename(columns={'answer_8b': 'generated_answer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qasa_8b = qasa_8b.rename(columns={'answer_8b': 'generated_answer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qasper_70b = qasper_70b.rename(columns={'answer_70b': 'generated_answer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qasa_70b = qasa_70b.rename(columns={'answer_70b': 'generated_answer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned = pd.concat([qasper_aligned, qasa_aligned], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallarge = pd.concat([qasper_8b, qasper_70b, qasa_8b, qasa_70b], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned['Wisdm'] = pd.to_numeric(aligned['Wisdm'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0          int64\n",
       "question           object\n",
       "context            object\n",
       "correct_answer     object\n",
       "iris_answer        object\n",
       "Rouge1            float64\n",
       "Rouge2            float64\n",
       "RougeL            float64\n",
       "Bleu              float64\n",
       "Chrf              float64\n",
       "ChrfPlus          float64\n",
       "Meteor            float64\n",
       "Ter               float64\n",
       "Bert              float64\n",
       "WMS               float64\n",
       "SMS               float64\n",
       "Wisdm             float64\n",
       "Bleurt            float64\n",
       "BEM               float64\n",
       "Bart              float64\n",
       "Prometheus          int64\n",
       "Faithfullness     float64\n",
       "Relevancy         float64\n",
       "Correctness       float64\n",
       "RSim              float64\n",
       "Consistency       float64\n",
       "TSim              float64\n",
       "LLM               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'question', 'correct_answer', 'context',\n",
       "       'generated_answer', 'Rouge1', 'Rouge2', 'RougeL', 'Bleu', 'Chrf',\n",
       "       'Chrfplus', 'Meteor', 'Ter', 'Bert', 'WMS', 'SMS', 'Wisdm', 'Bart',\n",
       "       'BEM', 'Prometheus', 'Consistency', 'TSim', 'Faithfullness',\n",
       "       'Relevancy', 'Correctness', 'RSim', 'LLM', 'Bleurt', 'ChrfPlus'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallarge.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Metric  Spearman Correlation     p-value\n",
      "0          Rouge1               -0.1580      0.0255\n",
      "1          Rouge2               -0.0527      0.4589\n",
      "2          RougeL               -0.1330      0.0605\n",
      "3            Bleu               -0.4182  7.1993e-10\n",
      "4            Chrf               -0.4389  7.9954e-11\n",
      "5        ChrfPlus               -0.4396  7.3899e-11\n",
      "6          Meteor                0.1273      0.0725\n",
      "7             Ter                0.5379  2.1606e-16\n",
      "8            Bert               -0.0732      0.3032\n",
      "9             WMS               -0.0255      0.7195\n",
      "10            SMS                0.0142      0.8416\n",
      "11          Wisdm                0.0181      0.8102\n",
      "12         Bleurt                0.0290      0.6853\n",
      "13            BEM               -0.0349      0.6258\n",
      "14           Bart                0.0359      0.6137\n",
      "15     Prometheus                0.1119      0.1145\n",
      "16  Faithfullness                0.0293      0.7079\n",
      "17      Relevancy                0.2378      0.0007\n",
      "18    Correctness                0.0393      0.5846\n",
      "19           RSim               -0.0996      0.1607\n",
      "20    Consistency                0.0458      0.5193\n",
      "21           TSim                0.0722      0.3098\n",
      "22            LLM                0.0601      0.4475\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of words in each 'answer_8b' and store in a new column 'word_count'\n",
    "aligned['word_count'] = aligned['iris_answer'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# List of metric columns\n",
    "metric_columns = ['Rouge1', 'Rouge2', 'RougeL', 'Bleu', 'Chrf', 'ChrfPlus', 'Meteor',\n",
    "       'Ter', 'Bert', 'WMS', 'SMS', 'Wisdm', 'Bleurt', 'BEM', 'Bart',\n",
    "       'Prometheus', 'Faithfullness', 'Relevancy', 'Correctness', 'RSim',\n",
    "       'Consistency', 'TSim', 'LLM']\n",
    "\n",
    "# Initialize a dictionary to store the correlations and p-values\n",
    "results = {'Metric': [], 'Spearman Correlation': [], 'p-value': []}\n",
    "\n",
    "# Loop over each metric column to compute the Spearman correlation and p-value\n",
    "for metric in metric_columns:\n",
    "    # Spearman correlation automatically handles NaN values by ignoring them\n",
    "    corr, p_val = spearmanr(aligned['word_count'], aligned[metric], nan_policy='omit')\n",
    "    results['Metric'].append(metric)\n",
    "    results['Spearman Correlation'].append(corr)\n",
    "    results['p-value'].append(p_val)\n",
    "\n",
    "# Convert results to a DataFrame for easier visualization\n",
    "a_result = pd.DataFrame(results)\n",
    "\n",
    "# Format the p-values and Spearman correlation for readability\n",
    "a_result['Spearman Correlation'] = a_result['Spearman Correlation'].apply(lambda x: round(x, 4))\n",
    "a_result['p-value'] = a_result['p-value'].apply(lambda x: \"{:.4e}\".format(x) if x < 0.0001 else round(x, 4))\n",
    "\n",
    "# Display the updated results\n",
    "print(a_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Metric  Spearman Correlation     p-value\n",
      "0          Rouge1                0.5603  1.9185e-34\n",
      "1          Rouge2                0.4545  8.7244e-22\n",
      "2          RougeL                0.5408  9.1883e-32\n",
      "3            Bleu                0.2864  5.4228e-09\n",
      "4            Chrf                0.5968  5.8411e-40\n",
      "5        ChrfPlus                0.5849  1.6544e-10\n",
      "6          Meteor                0.4599  2.4892e-22\n",
      "7             Ter                0.6854  8.5885e-57\n",
      "8            Bert                0.5193  5.2303e-29\n",
      "9             WMS                0.2867  5.2527e-09\n",
      "10            SMS                0.4984  1.6565e-26\n",
      "11          Wisdm                0.2543  6.0940e-05\n",
      "12         Bleurt                0.1702      0.0007\n",
      "13            BEM                0.1702      0.0007\n",
      "14           Bart                0.3699  2.0541e-14\n",
      "15     Prometheus                0.2866  5.3386e-09\n",
      "16  Faithfullness                0.0870      0.1579\n",
      "17      Relevancy                0.3652  4.5852e-14\n",
      "18    Correctness                0.1280      0.0105\n",
      "19           RSim                0.4748  7.0325e-24\n",
      "20    Consistency                0.4069  2.2235e-17\n",
      "21           TSim                0.3914  4.2746e-16\n",
      "22            LLM                0.2283  3.9543e-06\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of words in each 'answer_8b' and store in a new column 'word_count'\n",
    "smallarge['word_count'] = smallarge['generated_answer'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# List of metric columns\n",
    "metric_columns = ['Rouge1', 'Rouge2', 'RougeL', 'Bleu', 'Chrf', 'ChrfPlus', 'Meteor',\n",
    "       'Ter', 'Bert', 'WMS', 'SMS', 'Wisdm', 'Bleurt', 'BEM', 'Bart',\n",
    "       'Prometheus', 'Faithfullness', 'Relevancy', 'Correctness', 'RSim',\n",
    "       'Consistency', 'TSim', 'LLM']\n",
    "\n",
    "# Initialize a dictionary to store the correlations and p-values\n",
    "results = {'Metric': [], 'Spearman Correlation': [], 'p-value': []}\n",
    "\n",
    "# Loop over each metric column to compute the Spearman correlation and p-value\n",
    "for metric in metric_columns:\n",
    "    # Spearman correlation automatically handles NaN values by ignoring them\n",
    "    corr, p_val = spearmanr(smallarge['word_count'], smallarge[metric], nan_policy='omit')\n",
    "    results['Metric'].append(metric)\n",
    "    results['Spearman Correlation'].append(corr)\n",
    "    results['p-value'].append(p_val)\n",
    "\n",
    "# Convert results to a DataFrame for easier visualization\n",
    "smallarge_result = pd.DataFrame(results)\n",
    "\n",
    "# Format the p-values and Spearman correlation for readability\n",
    "smallarge_result['Spearman Correlation'] = smallarge_result['Spearman Correlation'].apply(lambda x: round(x, 4))\n",
    "smallarge_result['p-value'] = smallarge_result['p-value'].apply(lambda x: \"{:.4e}\".format(x) if x < 0.0001 else round(x, 4))\n",
    "\n",
    "# Display the updated results\n",
    "print(smallarge_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
