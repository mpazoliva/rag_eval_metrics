{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/computing scores/Qasa 8-70b/qasa_70b.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_100 = \"/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/computing scores/Qasa 8-70b/qasa_70b_100.csv\"\n",
    "df_100 = pd.read_csv(path_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'question', 'correct_answer', 'context', 'answer_70b',\n",
       "       'Rouge1', 'Rouge2', 'RougeL', 'Bleu', 'Chrf', 'Chrfplus', 'Meteor',\n",
       "       'Ter', 'Bert', 'WMS', 'SMS', 'Wisdm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'question', 'correct_answer', 'context',\n",
       "       'answer_70b', 'Rouge1', 'Rouge2', 'RougeL', 'Bleu', 'Chrf', 'Chrfplus',\n",
       "       'Meteor', 'Ter', 'Bert', 'WMS', 'SMS', 'Wisdm', 'Bart', 'BEM',\n",
       "       'Prometheus', 'Consistency', 'TSim', 'Faithfullness', 'Relevancy',\n",
       "       'Correctness', 'RSim', 'LLM', 'Bleurt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_100.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0          int64\n",
      "question           object\n",
      "correct_answer     object\n",
      "context            object\n",
      "answer_70b         object\n",
      "Rouge1            float64\n",
      "Rouge2            float64\n",
      "RougeL            float64\n",
      "Bleu              float64\n",
      "Chrf              float64\n",
      "Chrfplus          float64\n",
      "Meteor            float64\n",
      "Ter               float64\n",
      "Bert              float64\n",
      "WMS               float64\n",
      "SMS               float64\n",
      "Wisdm             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   1554.000\n",
      "mean       0.109\n",
      "std        0.153\n",
      "min        0.000\n",
      "25%        0.000\n",
      "50%        0.053\n",
      "75%        0.167\n",
      "max        1.000\n",
      "Name: Rouge1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_rouge1 = df['Rouge1'].describe()\n",
    "print(statistics_rouge1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   1554.000\n",
      "mean       0.041\n",
      "std        0.108\n",
      "min        0.000\n",
      "25%        0.000\n",
      "50%        0.000\n",
      "75%        0.021\n",
      "max        1.000\n",
      "Name: Rouge2, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_rouge2 = df['Rouge2'].describe()\n",
    "print(statistics_rouge2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   1554.000\n",
      "mean       0.102\n",
      "std        0.147\n",
      "min        0.000\n",
      "25%        0.000\n",
      "50%        0.051\n",
      "75%        0.151\n",
      "max        1.000\n",
      "Name: RougeL, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_rougel = df['RougeL'].describe()\n",
    "print(statistics_rougel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   1554.000\n",
      "mean       4.145\n",
      "std        8.934\n",
      "min        0.000\n",
      "25%        0.777\n",
      "50%        1.601\n",
      "75%        3.583\n",
      "max      100.000\n",
      "Name: Bleu, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_bleu = df['Bleu'].describe()\n",
    "print(statistics_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   1554.000\n",
      "mean       0.094\n",
      "std        0.130\n",
      "min        0.000\n",
      "25%        0.018\n",
      "50%        0.045\n",
      "75%        0.116\n",
      "max        0.992\n",
      "Name: Meteor, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_meteor = df['Meteor'].describe()\n",
    "print(statistics_meteor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   1554.000\n",
      "mean      21.414\n",
      "std       19.585\n",
      "min        0.000\n",
      "25%        5.477\n",
      "50%       15.710\n",
      "75%       31.608\n",
      "max      100.000\n",
      "Name: Chrf, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_chrf = df['Chrf'].describe()\n",
    "print(statistics_chrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   1554.000\n",
      "mean      20.144\n",
      "std       18.166\n",
      "min        0.000\n",
      "25%        6.160\n",
      "50%       14.707\n",
      "75%       28.509\n",
      "max      100.000\n",
      "Name: Chrfplus, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_chrfplus = df['Chrfplus'].describe()\n",
    "print(statistics_chrfplus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   1554.000\n",
      "mean       0.004\n",
      "std        0.044\n",
      "min        0.000\n",
      "25%        0.000\n",
      "50%        0.001\n",
      "75%        0.003\n",
      "max        1.000\n",
      "Name: Ter, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_ter = df['Ter'].describe()\n",
    "print(statistics_ter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   1554.000\n",
      "mean       0.316\n",
      "std        0.242\n",
      "min       -0.145\n",
      "25%        0.101\n",
      "50%        0.275\n",
      "75%        0.511\n",
      "max        1.000\n",
      "Name: Bert, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_bert = df['Bert'].describe()\n",
    "print(statistics_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   1554.000\n",
      "mean       0.545\n",
      "std        0.316\n",
      "min       -0.000\n",
      "25%        0.275\n",
      "50%        0.534\n",
      "75%        0.820\n",
      "max        1.000\n",
      "Name: WMS, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_wms = df['WMS'].describe()\n",
    "print(statistics_wms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   1554.000\n",
      "mean       0.356\n",
      "std        0.246\n",
      "min       -0.076\n",
      "25%        0.136\n",
      "50%        0.329\n",
      "75%        0.546\n",
      "max        1.000\n",
      "Name: SMS, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_sms = df['SMS'].describe()\n",
    "print(statistics_sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   838.000\n",
      "mean      0.490\n",
      "std       0.241\n",
      "min       0.000\n",
      "25%       0.322\n",
      "50%       0.505\n",
      "75%       0.664\n",
      "max       1.000\n",
      "Name: Wisdm, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_wisdm = df['Wisdm'].describe()\n",
    "print(statistics_wisdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   99.000\n",
      "mean     0.208\n",
      "std      0.222\n",
      "min      0.040\n",
      "25%      0.066\n",
      "50%      0.110\n",
      "75%      0.248\n",
      "max      0.973\n",
      "Name: Bleurt, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_bleurt = df_100['Bleurt'].describe()\n",
    "print(statistics_bleurt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   99.000\n",
      "mean     0.208\n",
      "std      0.222\n",
      "min      0.040\n",
      "25%      0.066\n",
      "50%      0.110\n",
      "75%      0.248\n",
      "max      0.973\n",
      "Name: BEM, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_bem = df_100['BEM'].describe()\n",
    "print(statistics_bem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      0.033\n",
      "std       0.072\n",
      "min       0.000\n",
      "25%       0.004\n",
      "50%       0.009\n",
      "75%       0.026\n",
      "max       0.446\n",
      "Name: Bart, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_bart = df_100['Bart'].describe()\n",
    "print(statistics_bart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      2.740\n",
      "std       1.070\n",
      "min       1.000\n",
      "25%       2.750\n",
      "50%       3.000\n",
      "75%       3.000\n",
      "max       4.000\n",
      "Name: Prometheus, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_prometheus = df_100['Prometheus'].describe()\n",
    "print(statistics_prometheus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      0.575\n",
      "std       0.407\n",
      "min       0.000\n",
      "25%       0.000\n",
      "50%       0.800\n",
      "75%       0.847\n",
      "max       1.000\n",
      "Name: LLM, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_llm = df_100['LLM'].describe()\n",
    "print(statistics_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   66.000\n",
      "mean     0.792\n",
      "std      0.382\n",
      "min      0.000\n",
      "25%      0.750\n",
      "50%      1.000\n",
      "75%      1.000\n",
      "max      1.000\n",
      "Name: Faithfullness, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_faithfulness = df_100['Faithfullness'].describe()\n",
    "print(statistics_faithfulness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      0.716\n",
      "std       0.284\n",
      "min       0.000\n",
      "25%       0.756\n",
      "50%       0.813\n",
      "75%       0.857\n",
      "max       0.972\n",
      "Name: Relevancy, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_rrel = df_100['Relevancy'].describe()\n",
    "print(statistics_rrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      0.548\n",
      "std       0.226\n",
      "min       0.179\n",
      "25%       0.402\n",
      "50%       0.562\n",
      "75%       0.704\n",
      "max       0.975\n",
      "Name: Correctness, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_correctness = df_100['Correctness'].describe()\n",
    "print(statistics_correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      0.798\n",
      "std       0.055\n",
      "min       0.680\n",
      "25%       0.752\n",
      "50%       0.795\n",
      "75%       0.841\n",
      "max       0.915\n",
      "Name: RSim, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_rsim = df_100['RSim'].describe()\n",
    "print(statistics_rsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      0.590\n",
      "std       0.466\n",
      "min       0.000\n",
      "25%       0.000\n",
      "50%       1.000\n",
      "75%       1.000\n",
      "max       1.000\n",
      "Name: Consistency, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_consistency = df_100['Consistency'].describe()\n",
    "print(statistics_consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      1.650\n",
      "std       1.617\n",
      "min       0.000\n",
      "25%       0.000\n",
      "50%       1.000\n",
      "75%       2.000\n",
      "max       5.000\n",
      "Name: TSim, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_tsim = df_100['TSim'].describe()\n",
    "print(statistics_tsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0   Rouge1   Rouge2   RougeL     Bleu     Chrf  Chrfplus  \\\n",
      "count    1554.000 1554.000 1554.000 1554.000 1554.000 1554.000  1554.000   \n",
      "mean      776.500    0.109    0.041    0.102    4.145   21.414    20.144   \n",
      "std       448.745    0.153    0.108    0.147    8.934   19.585    18.166   \n",
      "min         0.000    0.000    0.000    0.000    0.000    0.000     0.000   \n",
      "25%       388.250    0.000    0.000    0.000    0.777    5.477     6.160   \n",
      "50%       776.500    0.053    0.000    0.051    1.601   15.710    14.707   \n",
      "75%      1164.750    0.167    0.021    0.151    3.583   31.608    28.509   \n",
      "max      1553.000    1.000    1.000    1.000  100.000  100.000   100.000   \n",
      "\n",
      "        Meteor      Ter     Bert      WMS      SMS   Wisdm  \n",
      "count 1554.000 1554.000 1554.000 1554.000 1554.000 838.000  \n",
      "mean     0.094    0.004    0.316    0.545    0.356   0.490  \n",
      "std      0.130    0.044    0.242    0.316    0.246   0.241  \n",
      "min      0.000    0.000   -0.145   -0.000   -0.076   0.000  \n",
      "25%      0.018    0.000    0.101    0.275    0.136   0.322  \n",
      "50%      0.045    0.001    0.275    0.534    0.329   0.505  \n",
      "75%      0.116    0.003    0.511    0.820    0.546   0.664  \n",
      "max      0.992    1.000    1.000    1.000    1.000   1.000  \n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0.1  Unnamed: 0  Rouge1  Rouge2  RougeL    Bleu    Chrf  \\\n",
      "count       100.000     100.000 100.000 100.000 100.000 100.000 100.000   \n",
      "mean         49.500      49.500   0.070   0.019   0.066   2.556  17.391   \n",
      "std          29.011      29.011   0.092   0.051   0.089   3.645  15.193   \n",
      "min           0.000       0.000   0.000   0.000   0.000   0.000   0.634   \n",
      "25%          24.750      24.750   0.000   0.000   0.000   0.546   5.243   \n",
      "50%          49.500      49.500   0.033   0.000   0.033   1.169  13.458   \n",
      "75%          74.250      74.250   0.120   0.000   0.111   2.544  26.751   \n",
      "max          99.000      99.000   0.431   0.328   0.431  17.850  73.854   \n",
      "\n",
      "       Chrfplus  Meteor     Ter  ...    BEM  Prometheus  Consistency    TSim  \\\n",
      "count   100.000 100.000 100.000  ... 99.000     100.000      100.000 100.000   \n",
      "mean     16.055   0.066   0.002  ...  0.208       2.740        0.590   1.650   \n",
      "std      13.795   0.088   0.002  ...  0.222       1.070        0.466   1.617   \n",
      "min       0.667   0.000   0.000  ...  0.040       1.000        0.000   0.000   \n",
      "25%       5.079   0.012   0.000  ...  0.066       2.750        0.000   0.000   \n",
      "50%      12.806   0.036   0.001  ...  0.110       3.000        1.000   1.000   \n",
      "75%      23.455   0.085   0.002  ...  0.248       3.000        1.000   2.000   \n",
      "max      59.309   0.516   0.013  ...  0.973       4.000        1.000   5.000   \n",
      "\n",
      "       Faithfullness  Relevancy  Correctness    RSim     LLM  Bleurt  \n",
      "count         66.000    100.000      100.000 100.000 100.000  99.000  \n",
      "mean           0.792      0.716        0.548   0.798   0.575   0.208  \n",
      "std            0.382      0.284        0.226   0.055   0.407   0.222  \n",
      "min            0.000      0.000        0.179   0.680   0.000   0.040  \n",
      "25%            0.750      0.756        0.402   0.752   0.000   0.066  \n",
      "50%            1.000      0.813        0.562   0.795   0.800   0.110  \n",
      "75%            1.000      0.857        0.704   0.841   0.847   0.248  \n",
      "max            1.000      0.972        0.975   0.915   1.000   0.973  \n",
      "\n",
      "[8 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_100.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = pd.DataFrame(df.describe())\n",
    "statistics_100 = pd.DataFrame(df_100.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_statistics_100 = statistics_100[['Bart', 'BEM', 'Prometheus',\n",
    "       'Consistency', 'TSim', 'Faithfullness', 'Relevancy', 'Correctness',\n",
    "       'RSim', 'LLM']]\n",
    "final_df = pd.concat([statistics, sub_statistics_100], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>RougeL</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Chrf</th>\n",
       "      <th>Chrfplus</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Ter</th>\n",
       "      <th>Bert</th>\n",
       "      <th>...</th>\n",
       "      <th>Bart</th>\n",
       "      <th>BEM</th>\n",
       "      <th>Prometheus</th>\n",
       "      <th>Consistency</th>\n",
       "      <th>TSim</th>\n",
       "      <th>Faithfullness</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>Correctness</th>\n",
       "      <th>RSim</th>\n",
       "      <th>LLM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1554.000</td>\n",
       "      <td>1554.000</td>\n",
       "      <td>1554.000</td>\n",
       "      <td>1554.000</td>\n",
       "      <td>1554.000</td>\n",
       "      <td>1554.000</td>\n",
       "      <td>1554.000</td>\n",
       "      <td>1554.000</td>\n",
       "      <td>1554.000</td>\n",
       "      <td>1554.000</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000</td>\n",
       "      <td>99.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>776.500</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.102</td>\n",
       "      <td>4.145</td>\n",
       "      <td>21.414</td>\n",
       "      <td>20.144</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.208</td>\n",
       "      <td>2.740</td>\n",
       "      <td>0.590</td>\n",
       "      <td>1.650</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>448.745</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.147</td>\n",
       "      <td>8.934</td>\n",
       "      <td>19.585</td>\n",
       "      <td>18.166</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.222</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.466</td>\n",
       "      <td>1.617</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.040</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>388.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.777</td>\n",
       "      <td>5.477</td>\n",
       "      <td>6.160</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.066</td>\n",
       "      <td>2.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>776.500</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.051</td>\n",
       "      <td>1.601</td>\n",
       "      <td>15.710</td>\n",
       "      <td>14.707</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.110</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1164.750</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.151</td>\n",
       "      <td>3.583</td>\n",
       "      <td>31.608</td>\n",
       "      <td>28.509</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.248</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1553.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.973</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.915</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0   Rouge1   Rouge2   RougeL     Bleu     Chrf  Chrfplus  \\\n",
       "count    1554.000 1554.000 1554.000 1554.000 1554.000 1554.000  1554.000   \n",
       "mean      776.500    0.109    0.041    0.102    4.145   21.414    20.144   \n",
       "std       448.745    0.153    0.108    0.147    8.934   19.585    18.166   \n",
       "min         0.000    0.000    0.000    0.000    0.000    0.000     0.000   \n",
       "25%       388.250    0.000    0.000    0.000    0.777    5.477     6.160   \n",
       "50%       776.500    0.053    0.000    0.051    1.601   15.710    14.707   \n",
       "75%      1164.750    0.167    0.021    0.151    3.583   31.608    28.509   \n",
       "max      1553.000    1.000    1.000    1.000  100.000  100.000   100.000   \n",
       "\n",
       "        Meteor      Ter     Bert  ...    Bart    BEM  Prometheus  Consistency  \\\n",
       "count 1554.000 1554.000 1554.000  ... 100.000 99.000     100.000      100.000   \n",
       "mean     0.094    0.004    0.316  ...   0.033  0.208       2.740        0.590   \n",
       "std      0.130    0.044    0.242  ...   0.072  0.222       1.070        0.466   \n",
       "min      0.000    0.000   -0.145  ...   0.000  0.040       1.000        0.000   \n",
       "25%      0.018    0.000    0.101  ...   0.004  0.066       2.750        0.000   \n",
       "50%      0.045    0.001    0.275  ...   0.009  0.110       3.000        1.000   \n",
       "75%      0.116    0.003    0.511  ...   0.026  0.248       3.000        1.000   \n",
       "max      0.992    1.000    1.000  ...   0.446  0.973       4.000        1.000   \n",
       "\n",
       "         TSim  Faithfullness  Relevancy  Correctness    RSim     LLM  \n",
       "count 100.000         66.000    100.000      100.000 100.000 100.000  \n",
       "mean    1.650          0.792      0.716        0.548   0.798   0.575  \n",
       "std     1.617          0.382      0.284        0.226   0.055   0.407  \n",
       "min     0.000          0.000      0.000        0.179   0.680   0.000  \n",
       "25%     0.000          0.750      0.756        0.402   0.752   0.000  \n",
       "50%     1.000          1.000      0.813        0.562   0.795   0.800  \n",
       "75%     2.000          1.000      0.857        0.704   0.841   0.847  \n",
       "max     5.000          1.000      0.972        0.975   0.915   1.000  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/analyse results /statistics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfinal_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/analyse results /statistics/qasa_70b.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py:3964\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3953\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3955\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3956\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3957\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3961\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3962\u001b[0m )\n\u001b[0;32m-> 3964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/analyse results /statistics'"
     ]
    }
   ],
   "source": [
    "final_df.to_csv('/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/analyse results /statistics/qasa_70b.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAL STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100['std'] = df_100.select_dtypes(include=['number']).std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>context</th>\n",
       "      <th>answer_70b</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>RougeL</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Chrf</th>\n",
       "      <th>Chrfplus</th>\n",
       "      <th>...</th>\n",
       "      <th>BEM</th>\n",
       "      <th>Prometheus</th>\n",
       "      <th>Consistency</th>\n",
       "      <th>TSim</th>\n",
       "      <th>Faithfullness</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>Correctness</th>\n",
       "      <th>RSim</th>\n",
       "      <th>LLM</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do these automated metrics for human prefe...</td>\n",
       "      <td>The automated metrics that are mentioned while...</td>\n",
       "      <td>The ultimate aim of language technology is to ...</td>\n",
       "      <td>They differ in models and consider factors lik...</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.114</td>\n",
       "      <td>18.174</td>\n",
       "      <td>15.897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056</td>\n",
       "      <td>3</td>\n",
       "      <td>0.750</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.800</td>\n",
       "      <td>4.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does non-differentiable mean here? If the...</td>\n",
       "      <td>A formal definition of non-differentiability h...</td>\n",
       "      <td>The ultimate aim of language technology is to ...</td>\n",
       "      <td>Flat ThompsonavaVER Outside Thompsonlingerling...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.259</td>\n",
       "      <td>17.918</td>\n",
       "      <td>13.439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why is the action space of language modeling p...</td>\n",
       "      <td>The action space for language modeling is equa...</td>\n",
       "      <td>Language generation action spaces are orders o...</td>\n",
       "      <td>vocab size.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.329</td>\n",
       "      <td>3.507</td>\n",
       "      <td>3.170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are actor-critic algorithms and how do th...</td>\n",
       "      <td>Actor critic models are a class of reinforceme...</td>\n",
       "      <td>RL4LMs supports fine-tuning and training LMs f...</td>\n",
       "      <td>Actor-critic algorithms combine policy and val...</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.108</td>\n",
       "      <td>1.756</td>\n",
       "      <td>30.339</td>\n",
       "      <td>25.307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.800</td>\n",
       "      <td>7.893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What do the equations for Q-value and value re...</td>\n",
       "      <td>Q and V are mathematically expressed as: V_{t}...</td>\n",
       "      <td>RL4LMs supports fine-tuning and training LMs f...</td>\n",
       "      <td>Expected discounted rewards.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.162</td>\n",
       "      <td>9.110</td>\n",
       "      <td>7.804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.700</td>\n",
       "      <td>2.483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>How does the choice of layers, in which to dow...</td>\n",
       "      <td>As we can see, downsampling aim to collect sum...</td>\n",
       "      <td>Strategy 3. Downsample late in the network so ...</td>\n",
       "      <td>It controls their height and width.</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.455</td>\n",
       "      <td>8.851</td>\n",
       "      <td>7.663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Why did the authors use a mix of 1x1 and 3x3 f...</td>\n",
       "      <td>Authors used a mix of 1x1 and 3x3 filters in t...</td>\n",
       "      <td>Strategy 2. Decrease the number of input chann...</td>\n",
       "      <td>To limit input channels to 3x3 filters.</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1.309</td>\n",
       "      <td>16.294</td>\n",
       "      <td>16.121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.800</td>\n",
       "      <td>4.541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>What is the total number of filters in squeeze...</td>\n",
       "      <td>s1x1 is the number of filters in the squeeze l...</td>\n",
       "      <td>We define the Fire module as follows.A Fire mo...</td>\n",
       "      <td>s_{1x1}.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.761</td>\n",
       "      <td>3.218</td>\n",
       "      <td>3.491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.825</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>The Caffe framework does not natively support ...</td>\n",
       "      <td>The additional cost of using 2 convolutional l...</td>\n",
       "      <td>\\bulletSo that the output activations from 1x1...</td>\n",
       "      <td>Zero-padding.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.468</td>\n",
       "      <td>4.767</td>\n",
       "      <td>4.360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Did the authors use AlexNet for evaluation of ...</td>\n",
       "      <td>Yes, as told by authors that they used AlexNet...</td>\n",
       "      <td>We now turn our attention to evaluating Squeez...</td>\n",
       "      <td>yes.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.506</td>\n",
       "      <td>4.536</td>\n",
       "      <td>5.719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.782</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   How do these automated metrics for human prefe...   \n",
       "1   What does non-differentiable mean here? If the...   \n",
       "2   Why is the action space of language modeling p...   \n",
       "3   What are actor-critic algorithms and how do th...   \n",
       "4   What do the equations for Q-value and value re...   \n",
       "..                                                ...   \n",
       "95  How does the choice of layers, in which to dow...   \n",
       "96  Why did the authors use a mix of 1x1 and 3x3 f...   \n",
       "97  What is the total number of filters in squeeze...   \n",
       "98  The Caffe framework does not natively support ...   \n",
       "99  Did the authors use AlexNet for evaluation of ...   \n",
       "\n",
       "                                       correct_answer  \\\n",
       "0   The automated metrics that are mentioned while...   \n",
       "1   A formal definition of non-differentiability h...   \n",
       "2   The action space for language modeling is equa...   \n",
       "3   Actor critic models are a class of reinforceme...   \n",
       "4   Q and V are mathematically expressed as: V_{t}...   \n",
       "..                                                ...   \n",
       "95  As we can see, downsampling aim to collect sum...   \n",
       "96  Authors used a mix of 1x1 and 3x3 filters in t...   \n",
       "97  s1x1 is the number of filters in the squeeze l...   \n",
       "98  The additional cost of using 2 convolutional l...   \n",
       "99  Yes, as told by authors that they used AlexNet...   \n",
       "\n",
       "                                              context  \\\n",
       "0   The ultimate aim of language technology is to ...   \n",
       "1   The ultimate aim of language technology is to ...   \n",
       "2   Language generation action spaces are orders o...   \n",
       "3   RL4LMs supports fine-tuning and training LMs f...   \n",
       "4   RL4LMs supports fine-tuning and training LMs f...   \n",
       "..                                                ...   \n",
       "95  Strategy 3. Downsample late in the network so ...   \n",
       "96  Strategy 2. Decrease the number of input chann...   \n",
       "97  We define the Fire module as follows.A Fire mo...   \n",
       "98  \\bulletSo that the output activations from 1x1...   \n",
       "99  We now turn our attention to evaluating Squeez...   \n",
       "\n",
       "                                           answer_70b  Rouge1  Rouge2  RougeL  \\\n",
       "0   They differ in models and consider factors lik...   0.033   0.000   0.033   \n",
       "1   Flat ThompsonavaVER Outside Thompsonlingerling...   0.000   0.000   0.000   \n",
       "2                                         vocab size.   0.000   0.000   0.000   \n",
       "3   Actor-critic algorithms combine policy and val...   0.162   0.000   0.108   \n",
       "4                        Expected discounted rewards.   0.000   0.000   0.000   \n",
       "..                                                ...     ...     ...     ...   \n",
       "95                It controls their height and width.   0.032   0.000   0.032   \n",
       "96            To limit input channels to 3x3 filters.   0.143   0.050   0.143   \n",
       "97                                           s_{1x1}.   0.000   0.000   0.000   \n",
       "98                                      Zero-padding.   0.000   0.000   0.000   \n",
       "99                                               yes.   0.000   0.000   0.000   \n",
       "\n",
       "    Bleu   Chrf  Chrfplus  ...   BEM  Prometheus  Consistency  TSim  \\\n",
       "0  1.114 18.174    15.897  ... 0.056           3        0.750 2.000   \n",
       "1  0.259 17.918    13.439  ... 0.070           1        0.000 0.000   \n",
       "2  0.329  3.507     3.170  ... 0.156           3        1.000 1.000   \n",
       "3  1.756 30.339    25.307  ... 0.476           3        0.500 2.000   \n",
       "4  0.162  9.110     7.804  ... 0.069           3        1.000 1.000   \n",
       "..   ...    ...       ...  ...   ...         ...          ...   ...   \n",
       "95 0.455  8.851     7.663  ... 0.047           3        0.500 1.000   \n",
       "96 1.309 16.294    16.121  ... 0.111           3        1.000 1.000   \n",
       "97 0.761  3.218     3.491  ... 0.189           3        1.000 2.000   \n",
       "98 0.468  4.767     4.360  ... 0.052           1        1.000 0.000   \n",
       "99 1.506  4.536     5.719  ... 0.606           3        0.000 4.000   \n",
       "\n",
       "    Faithfullness  Relevancy  Correctness  RSim   LLM   std  \n",
       "0           1.000      0.847        0.447 0.789 0.800 4.819  \n",
       "1           1.000      0.733        0.554 0.716 0.000 4.529  \n",
       "2           1.000      0.811        0.628 0.800 0.000 1.103  \n",
       "3           0.333      0.959        0.520 0.880 0.800 7.893  \n",
       "4             NaN      0.743        0.204 0.817 0.700 2.483  \n",
       "..            ...        ...          ...   ...   ...   ...  \n",
       "95          1.000      0.784        0.395 0.721 0.000 2.377  \n",
       "96          1.000      0.862        0.859 0.864 0.800 4.541  \n",
       "97          1.000      0.718        0.806 0.825 1.000 1.149  \n",
       "98          0.500      0.779        0.438 0.752 0.000 1.349  \n",
       "99            NaN      0.872        0.695 0.782 1.000 1.726  \n",
       "\n",
       "[100 rows x 28 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df_100.sort_values(by='std')\n",
    "\n",
    "lowest_std_examples = sorted_df.head(3)\n",
    "\n",
    "# Get the 3 examples with the highest std\n",
    "highest_std_examples = sorted_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options to ensure full display\n",
    "pd.set_option('display.max_columns', None)      # Show all columns\n",
    "pd.set_option('display.max_rows', None)         # Show all rows\n",
    "pd.set_option('display.max_colwidth', None)     # No truncation of column content\n",
    "pd.set_option('display.width', 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Examples with Lowest Standard Deviation:\n",
      "                                                                                                                                                                                 question  \\\n",
      "89  High accuracy is crucial for safety in autonomous vehicles. Would deploying smaller models using over-the-air updates in Tesla result in a trade-off with accuracy(and hence safety)?   \n",
      "85                                Would there be a performance gain if the model utilizes the IE (information extraction) model instead of the exact match for target entity recognition?   \n",
      "34                                                                                              If both queries and documents are short, is still the fine-granular interaction required?   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           correct_answer  \\\n",
      "89  Accuracy is crucial for safety but it's not only accuracy vs size relation. We should consider more aspects. For example, response time of a driving car system is very crucial for safety. Communication overhead between servers while model training increases with the size of the model so smaller models train faster. Updating models from company servers to the car or over-the-air updates based on AlexNet at that time would require 240MB of communication from the server to the car. Hence, smaller models require less communication, making frequent updates more feasible. Also, keeping in mind architectural designs such as adjusting some functionalities, introducing new ways of extracting features, or using different objectives and optimizers may make a small model achieve the same level of accuracy or even surpass the larger model; for instance, SqueezeNet is 50x smaller than AlexNet with equivalent accuracy.   \n",
      "85                                                                                                                                                                                                                                                                                                                                                        This workâ€™s approach aims at focusing mostly on informative factors. For example, the key sentence selection module focused on extracting only the most relevant sentences and the target entity recognition module focused on identifying only the most informative entities. Further, this work argues that, to use knowledge graphs for re-ranking tasks, it is important that the graphs contain triplets with substantial information gain. The effect on information gain from using IE models, instead of exact match, for target entity recognition cannot be answered from this paper.   \n",
      "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                During indexing, we use another server with the same CPU and system memory specifications but which has four Titan V GPUs attached, each with 12 GiBs of memory. Across all experiments, only one GPU is dedicated per query for retrieval (i.e., for methods with neural computations) but we use up to all four GPUs during indexing.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 context  \\\n",
      "89  Much of the recent research on deep convolutional neural networks (CNNs) has focused on increasing accuracy on computer vision datasets.For a given accuracy level, there typically exist multiple CNN architectures that achieve that accuracy level.Given equivalent accuracy, a CNN architecture with fewer parameters has several advantages:\\bulletMore efficient distributed training.Communication among servers is the limiting factor to the scalability of distributed CNN training.For distributed data-parallel training, communication overhead is directly proportional to the number of parameters in the modelÂ Iandola etÂ al. (2016).In short, small models train faster due to requiring less communication.\\bulletLess overhead when exporting new models to clients. For autonomous driving, companies such as Tesla periodically copy new models from their servers to customersâ€™ cars. This practice is often referred to as an over-the-air update. Consumer Reports has found that the safety of Teslaâ€™s Autopilot semi-autonomous driving functionality has incrementally improved with recent over-the-air updatesÂ Consumer Reports (2016). However, over-the-air updates of todayâ€™s typical CNN/DNN models can require large data transfers. With AlexNet, this would require 240MB of communication from the server to the car. Smaller models require less communication, making frequent updates more feasible.\\bulletFeasible FPGA and embedded deployment. FPGAs often have less than 10MB111For example, the Xilinx Vertex-7 FPGA has a maximum of 8.5 MBytes (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory. of on-chip memory and no off-chip memory or storage. For inference, a sufficiently small model could be stored directly on the FPGA instead of being bottlenecked by memory bandwidthÂ Qiu etÂ al. (2016), while video frames stream through the FPGA in real time.Further, when deploying CNNs on Application-Specific Integrated Circuits (ASICs), a sufficiently small model could be stored directly on-chip, and smaller models may enable the ASIC to fit on a smaller die.\\nSo far, we have proposed architectural design strategies for small models, followed these principles to create SqueezeNet, and discovered that SqueezeNet is 50x smaller than AlexNet with equivalent accuracy.However, SqueezeNet and other models reside in a broad and largely unexplored design space of CNN architectures.Now, in SectionsÂ 5 andÂ 6, we explore several aspects of the design space. We divide this architectural exploration into two main topics: microarchitectural exploration (per-module layer dimensions and configurations) and macroarchitectural exploration (high-level end-to-end organization of modules and other layers).    \n",
      "85                                                                                                                                                                                                                                              (1)Key sentence selection. The actual information need of a user usually concentrates on a small part of a relevant passageÂ (Guo etÂ al., 2020). To this end, we mimic human judgment and only focus on the sentence of each passage that is the most related to a queryÂ (Zou etÂ al., 2021).In particular, we define the relevance score between a query q and a sentence \\textbf{s}_{i} as(7)Rel_{qs}(\\textbf{q},\\textbf{s}_{i})=\\frac{\\sum_{q=1}^{|\\textbf{q}|}\\textbf{E}(w_{q})}{|\\textbf{q}|}\\cdot\\frac{\\sum_{s=1}^{|\\textbf{s}_{i}|}\\textbf{E}(w_{s})}{|\\textbf{s}_{i}|}.For the sake of efficiency, we initialize \\textbf{E}(w) from Word2VecÂ (MikolovetÂ al., 2013) embedding.Based on Eq.(7), we select the most relevant sentence \\textbf{s}^{*} in p to build the meta-graph for \\mathbf{q} and \\mathbf{p}.(2)Target entity recognition.Next, we select the entities in q and \\textbf{s}^{*} to construct the meta-graph. Specifically, we only consider the entities that exactly match in \\mathcal{E}. Meanwhile, we omit those entity phrases that are sub-sequences of other recognized entities.For example, in the query \"what causes low liver enzymes\", both \"liver\" and \"liver enzyme\" are entities, but the entity \"liver enzyme\" is more informative to be recognized as the target entity, and \"liver\" should be omitted.(3)Path discovery. Finally, given the target entities of q and \\textbf{s}^{*} (denoted as \\phi_{\\mathbf{q}} and \\phi_{\\mathbf{s}^{*}}, respectively), we perform Breadth First Search (BFS) on \\overline{\\mathcal{G}} to discover the paths within K-hop between \\phi_{\\mathbf{q}} and \\phi_{\\mathbf{s}^{*}}. Note that we only keep the within-K-hop paths that might be the most useful for the downstream re-ranking task. Meanwhile, the knowledge could be complemented from the K-hop paths.\\nâ€¢ Challenge 1. Existing knowledge graph are not constructed for re-ranking task. They usually contain trivial factual triples, which can hardly bring information gain. The inappropriate selection of external knowledge could even jeopardize the re-ranker performance. How to utilize existing knowledge graph to re-ranking task is remain a challenge. â€¢ Challenge 2. The explicit knowledge and implicit knowledge are highly heterogeneous due to the different sources, which makes the aggregation of the two difficult. How to mutually refine each other and effectively aggregate explicit knowledge into implicit knowledge to alleviate the semantic gap between query and passage is still a challenge.   \n",
      "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 These increasingly expressive architectures are in tension. While interaction-based models (i.e., FigureÂ 2Â (b) and (c)) tend to be superior for IR tasks (Guo etÂ al., 2019; MitraetÂ al., 2018), a representation-focused modelâ€”by isolating the computations among q and dâ€”makes it possible to pre-compute document representations offline (Zamani etÂ al., 2018), greatly reducing the computational load per query. In this work, we observe that the fine-grained matching of interaction-based models and the pre-computation of document representations of representation-based models can be combined by retaining yet judiciously delaying the queryâ€“document interaction. FigureÂ 2Â (d) illustrates an architecture that precisely does so. As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. This paradigm allows ColBERT to exploit deep LM-based representations while shifting the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., (JohnsonetÂ al., 2017; AbuzaidetÂ al., 2019)) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval.   \n",
      "\n",
      "   answer_70b  Rouge1  Rouge2  RougeL  Bleu  Chrf  Chrfplus  Meteor   Ter  Bert   WMS   SMS  Wisdm  Bleurt  Bart   BEM  Prometheus  Consistency  TSim  Faithfullness  Relevancy  Correctness  RSim   LLM   std  \n",
      "89        No.   0.000   0.000   0.000 0.229 0.634     1.000   0.004 0.000 0.036 0.220 0.143    NaN  -0.826 0.002 0.058           1        0.000 0.000          0.000      0.000        0.315 0.716 0.000 0.394  \n",
      "85       Yes.   0.000   0.000   0.000 0.374 1.494     1.821   0.006 0.000 0.042 0.512 0.092    NaN  -0.823 0.005 0.060           1        0.000 0.000            NaN      0.786        0.437 0.747 0.000 0.590  \n",
      "34        No.   0.000   0.000   0.000 0.540 1.174     2.151   0.009 0.000 0.087 0.262 0.089    NaN  -0.801 0.002 0.070           1        0.000 0.000          1.000      0.000        0.179 0.714 0.700 0.606  \n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"3 Examples with Lowest Standard Deviation:\")\n",
    "print(lowest_std_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples with Highest Standard Deviation:\n",
      "                                                                                                                 question                                                                                                                                                                                                                                                                                                                                                                                                                        correct_answer  \\\n",
      "78                                                                              How is DPR retriever different from BM25?                                                                                      BM25 and DPR are both examples of retrievers used in large-scale passage collection. BM25 is described as a traditional sparse retriever and DPR leverages PLM to empower the retriever by a single vector. How both BM25 and DPR function is not described in detail in this paper and thus their differences cannot be answered in this paper.   \n",
      "79                                                             What is the difference of RocketQAv1 and RocketQAv2 model?  RocketQAv1 trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful cross-encoder to empower the dual-encoder. While it inherits the parameters from RocketQAv1, RocketQAv2 extends the first version through a novel approach that jointly trains the dense passage retriever and passage re-ranker, and by using a large PLM for data augmentation and denoising (i.e.,  a distillation procedure).   \n",
      "52  Why does the proposed method introduced EM framework to optimize the model (instead of directly optimizing the loss)?                                                                                                                                                                                                                                                                                                                                                                                                            EM guarantees convergence.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   context  \\\n",
      "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The low-dimensional dense representations for query and passage are computed by PLMs based retrievers from the dual-encoder architecture. Afterward, the candidate passage set could be retrieved efficiently via approximate nearest neighbor algorithms.Existing studies could be categorized into two parts:(1) By optimizing the matching stage: DPRÂ (Karpukhin etÂ al., 2020) is the first study to leverage PLM to empower the retriever by a single vector. Other researches, such asRepBERTÂ (ZhanetÂ al., 2020), ColBERTÂ (Khattab andZaharia, 2020), COILÂ (GaoetÂ al., 2021) and InteractorÂ (YeetÂ al., 2022), obtain multiple vectors for query and passage for matching.(2) By optimizing the representation learning module: RocketQAv1Â (Qu etÂ al., 2021) and RocketQAv2Â (Ren etÂ al., 2021) boost the representation learning of retriever by leveraging the power of cross-encoder in a cascade or joint manner. Other studies boost the representation learning by designed IR-oriented pre-training tasks.ICTÂ (LeeetÂ al., 2019) treats sentences as pseudo-queries and matched them to the passage they originate from. CondenserÂ (Gao and Callan, 2021) utilizes a novel pre-training task, which can produces an information-rich representation to condense an input sequence.\\nWe use the traditional sparse retriever BM25Â (YangetÂ al., 2017) as our first stage method. All experiments are conducted under the same BM25 setting with 1000 retrieved candidates. We conduct experiments with the deep learning framework PaddlePaddleÂ (MaetÂ al., 2019) on up to 4 NVIDIA Tesla A100 GPUs (with 40G RAM). For the GMN module, we use Paddle Graph Learning (PGL)Â 222https://github.com/PaddlePaddle/PGL, an efficient and flexible graph learning framework based on PaddlePaddle. For training, we used the Adam optimizerÂ (Kingma and Ba, 2014) with a learning rate of 1e-5 for text encoder and 1e-4 for knowledge injector. The model is trained up to 5 epochs with a batch size of 640 and 240 for base and large models respectively.In our experiments, the PLM small, base and large models have 6, 12 and 24 Transformer layers respectively.The text encoder has 9 layers and 21 layers for base and large model respectively, and the knowledge injector both has 3 layers in our experiment. The dropout rates are set to 0.1. The ratio of the positive to the hard negative is set to 1:19.All transformer layers in KERMâ€™s backbone are initialized from ERNIE-2.0 baseÂ (SunetÂ al., 2020b), which is a BERT-like model pre-trained with a continual pre-training framework on multiple tasks. We perform Knowledge-enhanced pre-training on MARCO passage collection to warm up the parameters in knowledge injector, which has 60,000 iterations under the batch size of 256.For a fair comparison, the same pre-training without knowledge enhancement is also conducted on \\textrm{ERNIE}_{\\textrm{base}} re-ranker and all models in ablation studies.\\nGiven a query q, passage re-ranking aims at ordering a set of ðœ˜ passages, i.e., P =  pðœ… \\tðœ˜ ðœ…=1 , which is usually retrieved from a large-scale passage collection by a retriever, e.g. BM25 [48], DPR [16] etc. In particular, a passage is a sequence of words p = {ð‘¤ð‘ } |p| ð‘=1 , where |p| is the length of passage p. Similarly, a query is a sequence of words q = {ð‘¤ð‘ž } |q| ð‘ž=1 . Note that a passage p consists of ð‘‡ sentences p = {sðœ } ð‘‡ ðœ=1.   \n",
      "79  Existing PLMs based re-rankers typically improve ranking performance from two aspects: (1) By optimizing the ranking procedure: monoBERTÂ (Nogueira and Cho, 2019) is the first work that re-purposed BERT as a passage re-ranker and achieves state-of-the-art results. duoBERTÂ (NogueiraetÂ al., 2019a) integrates monoBERT in a multistage ranking architecture and adopts a pairwise classification approach to passage relevance computation. UEDÂ (YanetÂ al., 2021) proposes a cascade pre-training manner that can jointly enhance the retrieval stage through passage expansion with a pre-trained query generator and thus elevate the re-ranking stage with a pre-trained transformer encoder. The two stages can facilitate each other in a unified pre-training framework. H-ERNIEÂ (ChuetÂ al., 2022) proposes a multi-granularity PLM for web search.(2) By designing rational distillation procedure: LM Distill + Fine-TuningÂ (GaoetÂ al., 2020) explores a variety of distillation methods to equip a smaller re-ranker with both general-purpose language modeling knowledge learned in pre-training and search- specific relevance modeling knowledge learned in fine-tuning, and produces a faster re-ranker with better ranking performance. CAKDÂ (HofstÃ¤tter etÂ al., 2020) proposes a cross-architecture knowledge distillation procedure with a Margin-MSE loss, which can distill knowledge from multiple teachers at the same time. RocketQAv1Â (Qu etÂ al., 2021) trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful cross-encoder to empower the dual-encoder. RocketQAv2Â (Ren etÂ al., 2021) proposes a novel approach that jointly trains the dense passage retriever and passage re-ranker. The parameters of RocketQAv2 are inherited from RocketQAv1. Besides, RocketQAv2 utilizes a large PLM for data augmentation and denoising, which can also be regarded as a distillation procedure. Notably, these two types of studies anticipate more insightful information to be captured by the advanced ranking and training procedures, while neglecting the limitations of implicit knowledge extracted from noisy and heterogeneous data. Therefore, in this paper, we proposed the first knowledge-enhanced PLM based re-ranker, which thoughtfully leverages explicit external knowledge that improve the effectiveness of the model.\\nWe include several PLMs based re-rankers in our evaluation, including the state-of-the-art:â€¢monoBERTÂ (Nogueira and Cho, 2019): The first study that re-purposes BERT as a re-ranker and achieves state-of-the-art results.â€¢duoBERTÂ (NogueiraetÂ al., 2019a):This work proposes a pairwise classification approach using BERT, which obtains the ability to be more sensitive to semantics through greater computation.â€¢UEDÂ (YanetÂ al., 2021): A unified pre-training framework that jointly refines re-ranker and query generator. For a fair comparison, we only use the re-ranker in UED without passage expansion.â€¢LM Distill+Fine-Tuning (LDFT)Â (GaoetÂ al., 2020):A variety of distillation methods are compared in this paper. The experimental results indicate that a proper distillation procedure (i.e. first distill the language model, and then fine-tune on the ranking task) could produce a faster re-ranker with better ranking performance.â€¢CAKDÂ (HofstÃ¤tter etÂ al., 2020): This work proposes a cross-architecture knowledge distillation procedure with Margin-MSE loss, which can distill knowledge from multiple teachers.â€¢RocketQAv1Â (Qu etÂ al., 2021): This work mainly focuses on the training of PLM based retriever, where the re-ranker is an intermediate product of its training process.â€¢RocketQAv2Â (Ren etÂ al., 2021): Based on RocketQAv1, this work proposes a novel approach that jointly trains the PLM based retriever and re-ranker.To compare the performance of different methods, we resort to two ranking metrics.For MSMARCO-DEV, We adopt Mean Reciprocal Rank (i.e., MRR@10).For TREC 2019 DL, we use Mean Average Precision, i.e., MAP@10 and MAP@30.For Ohsumed, both Mean Reciprocal Rank and Mean Average Precision (i.e., MRR@10 and MAP@10) are employed for comprehensive performance analysis in queries requiring in-depth domain knowledge.   \n",
      "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Later, we will show that a generalized Expectation-Maximization frameworkprovides a direction to address above problemwith a convergence guarantee.The basic idea of optimizing Eq.Â (7) via EMis to start with an initial guessof the model parameter \\thetaand estimate the expected valuesof the missing variable c, i.e., the E-step.And once we have the values of c,we can maximize the Eq.Â (7) w.r.t theparameter \\theta, i.e., the M step.We can repeat this iterative process until the likelihood cannot increase anymore.\\nTo discover the benefits oflatent intentsand address challenges,we propose theIntent Contrastive Learning (ICL),a general learning paradigm thatleverages the latent intentfactor into SR.It learns usersâ€™ intentdistributionsfrom all user behavior sequencesvia clustering.And it leveragesthe learnt intentsinto the SR modelvia a new contrastive SSL,whichmaximizes the agreementbetween a view of sequenceand its corresponding intent.The intent representation learning moduleand the contrastive SSL module are mutually reinforcedto train a more expressivesequence encoder.We tackle the challenge of intentmining problem byintroducing alatent variable to represent usersâ€™ intentsand learn them alternatelyalong with the SR model optimization throughan expectation-maximization (EM) frameworkto ensure convergence.We suggest fusing learnt intent informationinto SR via the proposed contrastive SSL,as it can improve modelâ€™s performance as wellas robustness.Extensive experiments conducted on four real-world datasetsfurther verify the effectiveness of the proposed learning paradigm,which improves performance and robustness,even when recommender systemsfaceheavy data sparsity issues.\\nIn this work,we propose a new learning paradigm ICL that canmodel latent intent factorsfrom user interactionsand fuse them into a sequential recommendationmodel via a newcontrastive SSL objective.ICL is formulated withinan EM framework, which guaranteesconvergence. Detailed analyses showthe superiority of ICL andexperiments conducted onfour datasets further demonstratethe effectiveness of the proposed method.   \n",
      "\n",
      "                                                                                                      answer_70b  Rouge1  Rouge2  RougeL   Bleu   Chrf  Chrfplus  Meteor   Ter  Bert   WMS   SMS  Wisdm  Bleurt  Bart   BEM  Prometheus  Consistency  TSim  Faithfullness  Relevancy  Correctness  RSim   LLM    std  \n",
      "78                                                DPR leverages PLM to empower the retriever by a single vector.   0.431   0.328   0.431 17.850 52.045    52.673   0.212 0.002 0.623 1.000 0.690  0.577  -0.405 0.446 0.379           3        1.000 1.000          0.667      0.877        0.521 0.885 0.700 15.099  \n",
      "79  RocketQAv2 jointly trains retriever and re-ranker, and uses a large PLM for data augmentation and denoising.   0.426   0.267   0.426 13.132 53.475    53.085   0.265 0.004 0.782 0.762 0.782  0.364  -0.302 0.389 0.353           4        1.000 4.000            NaN      0.894        0.524 0.897 1.000 15.494  \n",
      "52                                                                                        Convergence guarantee.   0.000   0.000   0.000 15.974 73.854    59.309   0.385 0.007 0.699 0.928 0.699  0.838   0.656 0.030 0.873           4        1.000 5.000            NaN      0.847        0.720 0.879 0.800 19.519  \n"
     ]
    }
   ],
   "source": [
    "print(\"Examples with Highest Standard Deviation:\")\n",
    "print(highest_std_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAL SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100['general_score'] = df_100.select_dtypes(include=['number']).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df_100.sort_values(by='general_score')\n",
    "\n",
    "lowest_score_examples = sorted_df.head(3)\n",
    "\n",
    "# Get the 3 examples with the highest std\n",
    "highest_score_examples = sorted_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Examples with Lowest Score:\n",
      "                                                                                                                                                                                 question  \\\n",
      "89  High accuracy is crucial for safety in autonomous vehicles. Would deploying smaller models using over-the-air updates in Tesla result in a trade-off with accuracy(and hence safety)?   \n",
      "85                                Would there be a performance gain if the model utilizes the IE (information extraction) model instead of the exact match for target entity recognition?   \n",
      "34                                                                                              If both queries and documents are short, is still the fine-granular interaction required?   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           correct_answer  \\\n",
      "89  Accuracy is crucial for safety but it's not only accuracy vs size relation. We should consider more aspects. For example, response time of a driving car system is very crucial for safety. Communication overhead between servers while model training increases with the size of the model so smaller models train faster. Updating models from company servers to the car or over-the-air updates based on AlexNet at that time would require 240MB of communication from the server to the car. Hence, smaller models require less communication, making frequent updates more feasible. Also, keeping in mind architectural designs such as adjusting some functionalities, introducing new ways of extracting features, or using different objectives and optimizers may make a small model achieve the same level of accuracy or even surpass the larger model; for instance, SqueezeNet is 50x smaller than AlexNet with equivalent accuracy.   \n",
      "85                                                                                                                                                                                                                                                                                                                                                        This workâ€™s approach aims at focusing mostly on informative factors. For example, the key sentence selection module focused on extracting only the most relevant sentences and the target entity recognition module focused on identifying only the most informative entities. Further, this work argues that, to use knowledge graphs for re-ranking tasks, it is important that the graphs contain triplets with substantial information gain. The effect on information gain from using IE models, instead of exact match, for target entity recognition cannot be answered from this paper.   \n",
      "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                During indexing, we use another server with the same CPU and system memory specifications but which has four Titan V GPUs attached, each with 12 GiBs of memory. Across all experiments, only one GPU is dedicated per query for retrieval (i.e., for methods with neural computations) but we use up to all four GPUs during indexing.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 context  \\\n",
      "89  Much of the recent research on deep convolutional neural networks (CNNs) has focused on increasing accuracy on computer vision datasets.For a given accuracy level, there typically exist multiple CNN architectures that achieve that accuracy level.Given equivalent accuracy, a CNN architecture with fewer parameters has several advantages:\\bulletMore efficient distributed training.Communication among servers is the limiting factor to the scalability of distributed CNN training.For distributed data-parallel training, communication overhead is directly proportional to the number of parameters in the modelÂ Iandola etÂ al. (2016).In short, small models train faster due to requiring less communication.\\bulletLess overhead when exporting new models to clients. For autonomous driving, companies such as Tesla periodically copy new models from their servers to customersâ€™ cars. This practice is often referred to as an over-the-air update. Consumer Reports has found that the safety of Teslaâ€™s Autopilot semi-autonomous driving functionality has incrementally improved with recent over-the-air updatesÂ Consumer Reports (2016). However, over-the-air updates of todayâ€™s typical CNN/DNN models can require large data transfers. With AlexNet, this would require 240MB of communication from the server to the car. Smaller models require less communication, making frequent updates more feasible.\\bulletFeasible FPGA and embedded deployment. FPGAs often have less than 10MB111For example, the Xilinx Vertex-7 FPGA has a maximum of 8.5 MBytes (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory. of on-chip memory and no off-chip memory or storage. For inference, a sufficiently small model could be stored directly on the FPGA instead of being bottlenecked by memory bandwidthÂ Qiu etÂ al. (2016), while video frames stream through the FPGA in real time.Further, when deploying CNNs on Application-Specific Integrated Circuits (ASICs), a sufficiently small model could be stored directly on-chip, and smaller models may enable the ASIC to fit on a smaller die.\\nSo far, we have proposed architectural design strategies for small models, followed these principles to create SqueezeNet, and discovered that SqueezeNet is 50x smaller than AlexNet with equivalent accuracy.However, SqueezeNet and other models reside in a broad and largely unexplored design space of CNN architectures.Now, in SectionsÂ 5 andÂ 6, we explore several aspects of the design space. We divide this architectural exploration into two main topics: microarchitectural exploration (per-module layer dimensions and configurations) and macroarchitectural exploration (high-level end-to-end organization of modules and other layers).    \n",
      "85                                                                                                                                                                                                                                              (1)Key sentence selection. The actual information need of a user usually concentrates on a small part of a relevant passageÂ (Guo etÂ al., 2020). To this end, we mimic human judgment and only focus on the sentence of each passage that is the most related to a queryÂ (Zou etÂ al., 2021).In particular, we define the relevance score between a query q and a sentence \\textbf{s}_{i} as(7)Rel_{qs}(\\textbf{q},\\textbf{s}_{i})=\\frac{\\sum_{q=1}^{|\\textbf{q}|}\\textbf{E}(w_{q})}{|\\textbf{q}|}\\cdot\\frac{\\sum_{s=1}^{|\\textbf{s}_{i}|}\\textbf{E}(w_{s})}{|\\textbf{s}_{i}|}.For the sake of efficiency, we initialize \\textbf{E}(w) from Word2VecÂ (MikolovetÂ al., 2013) embedding.Based on Eq.(7), we select the most relevant sentence \\textbf{s}^{*} in p to build the meta-graph for \\mathbf{q} and \\mathbf{p}.(2)Target entity recognition.Next, we select the entities in q and \\textbf{s}^{*} to construct the meta-graph. Specifically, we only consider the entities that exactly match in \\mathcal{E}. Meanwhile, we omit those entity phrases that are sub-sequences of other recognized entities.For example, in the query \"what causes low liver enzymes\", both \"liver\" and \"liver enzyme\" are entities, but the entity \"liver enzyme\" is more informative to be recognized as the target entity, and \"liver\" should be omitted.(3)Path discovery. Finally, given the target entities of q and \\textbf{s}^{*} (denoted as \\phi_{\\mathbf{q}} and \\phi_{\\mathbf{s}^{*}}, respectively), we perform Breadth First Search (BFS) on \\overline{\\mathcal{G}} to discover the paths within K-hop between \\phi_{\\mathbf{q}} and \\phi_{\\mathbf{s}^{*}}. Note that we only keep the within-K-hop paths that might be the most useful for the downstream re-ranking task. Meanwhile, the knowledge could be complemented from the K-hop paths.\\nâ€¢ Challenge 1. Existing knowledge graph are not constructed for re-ranking task. They usually contain trivial factual triples, which can hardly bring information gain. The inappropriate selection of external knowledge could even jeopardize the re-ranker performance. How to utilize existing knowledge graph to re-ranking task is remain a challenge. â€¢ Challenge 2. The explicit knowledge and implicit knowledge are highly heterogeneous due to the different sources, which makes the aggregation of the two difficult. How to mutually refine each other and effectively aggregate explicit knowledge into implicit knowledge to alleviate the semantic gap between query and passage is still a challenge.   \n",
      "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 These increasingly expressive architectures are in tension. While interaction-based models (i.e., FigureÂ 2Â (b) and (c)) tend to be superior for IR tasks (Guo etÂ al., 2019; MitraetÂ al., 2018), a representation-focused modelâ€”by isolating the computations among q and dâ€”makes it possible to pre-compute document representations offline (Zamani etÂ al., 2018), greatly reducing the computational load per query. In this work, we observe that the fine-grained matching of interaction-based models and the pre-computation of document representations of representation-based models can be combined by retaining yet judiciously delaying the queryâ€“document interaction. FigureÂ 2Â (d) illustrates an architecture that precisely does so. As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. This paradigm allows ColBERT to exploit deep LM-based representations while shifting the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., (JohnsonetÂ al., 2017; AbuzaidetÂ al., 2019)) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval.   \n",
      "\n",
      "   answer_70b  Rouge1  Rouge2  RougeL      Bleu      Chrf  Chrfplus    Meteor       Ter      Bert       WMS       SMS  Wisdm    Bleurt      Bart       BEM  Prometheus  Consistency  TSim  Faithfullness  Relevancy  Correctness      RSim  LLM  general_score  \n",
      "89        No.     0.0     0.0     0.0  0.228843  0.633714  0.999743  0.003534  0.000071  0.036112  0.220234  0.143435    NaN -0.825678  0.001806  0.058424           1          0.0   0.0            0.0   0.000000     0.315331  0.715819  0.0       0.160518  \n",
      "85       Yes.     0.0     0.0     0.0  0.374157  1.494459  1.821484  0.005599  0.000116  0.042034  0.511566  0.091721    NaN -0.823361  0.005346  0.059834           1          0.0   0.0            NaN   0.786205     0.436834  0.747399  0.0       0.312066  \n",
      "34        No.     0.0     0.0     0.0  0.539855  1.173709  2.151285  0.008651  0.000179  0.087326  0.262268  0.088525    NaN -0.800648  0.002384  0.069666           1          0.0   0.0            1.0   0.000000     0.178610  0.714439  0.7       0.326193  \n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"3 Examples with Lowest Score:\")\n",
    "print(lowest_score_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Examples with Hihgest Score:\n",
      "                                                                                                                 question                                                                                                                                                                                                                                                                                                                                                                                                                        correct_answer  \\\n",
      "78                                                                              How is DPR retriever different from BM25?                                                                                      BM25 and DPR are both examples of retrievers used in large-scale passage collection. BM25 is described as a traditional sparse retriever and DPR leverages PLM to empower the retriever by a single vector. How both BM25 and DPR function is not described in detail in this paper and thus their differences cannot be answered in this paper.   \n",
      "79                                                             What is the difference of RocketQAv1 and RocketQAv2 model?  RocketQAv1 trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful cross-encoder to empower the dual-encoder. While it inherits the parameters from RocketQAv1, RocketQAv2 extends the first version through a novel approach that jointly trains the dense passage retriever and passage re-ranker, and by using a large PLM for data augmentation and denoising (i.e.,  a distillation procedure).   \n",
      "52  Why does the proposed method introduced EM framework to optimize the model (instead of directly optimizing the loss)?                                                                                                                                                                                                                                                                                                                                                                                                            EM guarantees convergence.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   context  \\\n",
      "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The low-dimensional dense representations for query and passage are computed by PLMs based retrievers from the dual-encoder architecture. Afterward, the candidate passage set could be retrieved efficiently via approximate nearest neighbor algorithms.Existing studies could be categorized into two parts:(1) By optimizing the matching stage: DPRÂ (Karpukhin etÂ al., 2020) is the first study to leverage PLM to empower the retriever by a single vector. Other researches, such asRepBERTÂ (ZhanetÂ al., 2020), ColBERTÂ (Khattab andZaharia, 2020), COILÂ (GaoetÂ al., 2021) and InteractorÂ (YeetÂ al., 2022), obtain multiple vectors for query and passage for matching.(2) By optimizing the representation learning module: RocketQAv1Â (Qu etÂ al., 2021) and RocketQAv2Â (Ren etÂ al., 2021) boost the representation learning of retriever by leveraging the power of cross-encoder in a cascade or joint manner. Other studies boost the representation learning by designed IR-oriented pre-training tasks.ICTÂ (LeeetÂ al., 2019) treats sentences as pseudo-queries and matched them to the passage they originate from. CondenserÂ (Gao and Callan, 2021) utilizes a novel pre-training task, which can produces an information-rich representation to condense an input sequence.\\nWe use the traditional sparse retriever BM25Â (YangetÂ al., 2017) as our first stage method. All experiments are conducted under the same BM25 setting with 1000 retrieved candidates. We conduct experiments with the deep learning framework PaddlePaddleÂ (MaetÂ al., 2019) on up to 4 NVIDIA Tesla A100 GPUs (with 40G RAM). For the GMN module, we use Paddle Graph Learning (PGL)Â 222https://github.com/PaddlePaddle/PGL, an efficient and flexible graph learning framework based on PaddlePaddle. For training, we used the Adam optimizerÂ (Kingma and Ba, 2014) with a learning rate of 1e-5 for text encoder and 1e-4 for knowledge injector. The model is trained up to 5 epochs with a batch size of 640 and 240 for base and large models respectively.In our experiments, the PLM small, base and large models have 6, 12 and 24 Transformer layers respectively.The text encoder has 9 layers and 21 layers for base and large model respectively, and the knowledge injector both has 3 layers in our experiment. The dropout rates are set to 0.1. The ratio of the positive to the hard negative is set to 1:19.All transformer layers in KERMâ€™s backbone are initialized from ERNIE-2.0 baseÂ (SunetÂ al., 2020b), which is a BERT-like model pre-trained with a continual pre-training framework on multiple tasks. We perform Knowledge-enhanced pre-training on MARCO passage collection to warm up the parameters in knowledge injector, which has 60,000 iterations under the batch size of 256.For a fair comparison, the same pre-training without knowledge enhancement is also conducted on \\textrm{ERNIE}_{\\textrm{base}} re-ranker and all models in ablation studies.\\nGiven a query q, passage re-ranking aims at ordering a set of ðœ˜ passages, i.e., P =  pðœ… \\tðœ˜ ðœ…=1 , which is usually retrieved from a large-scale passage collection by a retriever, e.g. BM25 [48], DPR [16] etc. In particular, a passage is a sequence of words p = {ð‘¤ð‘ } |p| ð‘=1 , where |p| is the length of passage p. Similarly, a query is a sequence of words q = {ð‘¤ð‘ž } |q| ð‘ž=1 . Note that a passage p consists of ð‘‡ sentences p = {sðœ } ð‘‡ ðœ=1.   \n",
      "79  Existing PLMs based re-rankers typically improve ranking performance from two aspects: (1) By optimizing the ranking procedure: monoBERTÂ (Nogueira and Cho, 2019) is the first work that re-purposed BERT as a passage re-ranker and achieves state-of-the-art results. duoBERTÂ (NogueiraetÂ al., 2019a) integrates monoBERT in a multistage ranking architecture and adopts a pairwise classification approach to passage relevance computation. UEDÂ (YanetÂ al., 2021) proposes a cascade pre-training manner that can jointly enhance the retrieval stage through passage expansion with a pre-trained query generator and thus elevate the re-ranking stage with a pre-trained transformer encoder. The two stages can facilitate each other in a unified pre-training framework. H-ERNIEÂ (ChuetÂ al., 2022) proposes a multi-granularity PLM for web search.(2) By designing rational distillation procedure: LM Distill + Fine-TuningÂ (GaoetÂ al., 2020) explores a variety of distillation methods to equip a smaller re-ranker with both general-purpose language modeling knowledge learned in pre-training and search- specific relevance modeling knowledge learned in fine-tuning, and produces a faster re-ranker with better ranking performance. CAKDÂ (HofstÃ¤tter etÂ al., 2020) proposes a cross-architecture knowledge distillation procedure with a Margin-MSE loss, which can distill knowledge from multiple teachers at the same time. RocketQAv1Â (Qu etÂ al., 2021) trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful cross-encoder to empower the dual-encoder. RocketQAv2Â (Ren etÂ al., 2021) proposes a novel approach that jointly trains the dense passage retriever and passage re-ranker. The parameters of RocketQAv2 are inherited from RocketQAv1. Besides, RocketQAv2 utilizes a large PLM for data augmentation and denoising, which can also be regarded as a distillation procedure. Notably, these two types of studies anticipate more insightful information to be captured by the advanced ranking and training procedures, while neglecting the limitations of implicit knowledge extracted from noisy and heterogeneous data. Therefore, in this paper, we proposed the first knowledge-enhanced PLM based re-ranker, which thoughtfully leverages explicit external knowledge that improve the effectiveness of the model.\\nWe include several PLMs based re-rankers in our evaluation, including the state-of-the-art:â€¢monoBERTÂ (Nogueira and Cho, 2019): The first study that re-purposes BERT as a re-ranker and achieves state-of-the-art results.â€¢duoBERTÂ (NogueiraetÂ al., 2019a):This work proposes a pairwise classification approach using BERT, which obtains the ability to be more sensitive to semantics through greater computation.â€¢UEDÂ (YanetÂ al., 2021): A unified pre-training framework that jointly refines re-ranker and query generator. For a fair comparison, we only use the re-ranker in UED without passage expansion.â€¢LM Distill+Fine-Tuning (LDFT)Â (GaoetÂ al., 2020):A variety of distillation methods are compared in this paper. The experimental results indicate that a proper distillation procedure (i.e. first distill the language model, and then fine-tune on the ranking task) could produce a faster re-ranker with better ranking performance.â€¢CAKDÂ (HofstÃ¤tter etÂ al., 2020): This work proposes a cross-architecture knowledge distillation procedure with Margin-MSE loss, which can distill knowledge from multiple teachers.â€¢RocketQAv1Â (Qu etÂ al., 2021): This work mainly focuses on the training of PLM based retriever, where the re-ranker is an intermediate product of its training process.â€¢RocketQAv2Â (Ren etÂ al., 2021): Based on RocketQAv1, this work proposes a novel approach that jointly trains the PLM based retriever and re-ranker.To compare the performance of different methods, we resort to two ranking metrics.For MSMARCO-DEV, We adopt Mean Reciprocal Rank (i.e., MRR@10).For TREC 2019 DL, we use Mean Average Precision, i.e., MAP@10 and MAP@30.For Ohsumed, both Mean Reciprocal Rank and Mean Average Precision (i.e., MRR@10 and MAP@10) are employed for comprehensive performance analysis in queries requiring in-depth domain knowledge.   \n",
      "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Later, we will show that a generalized Expectation-Maximization frameworkprovides a direction to address above problemwith a convergence guarantee.The basic idea of optimizing Eq.Â (7) via EMis to start with an initial guessof the model parameter \\thetaand estimate the expected valuesof the missing variable c, i.e., the E-step.And once we have the values of c,we can maximize the Eq.Â (7) w.r.t theparameter \\theta, i.e., the M step.We can repeat this iterative process until the likelihood cannot increase anymore.\\nTo discover the benefits oflatent intentsand address challenges,we propose theIntent Contrastive Learning (ICL),a general learning paradigm thatleverages the latent intentfactor into SR.It learns usersâ€™ intentdistributionsfrom all user behavior sequencesvia clustering.And it leveragesthe learnt intentsinto the SR modelvia a new contrastive SSL,whichmaximizes the agreementbetween a view of sequenceand its corresponding intent.The intent representation learning moduleand the contrastive SSL module are mutually reinforcedto train a more expressivesequence encoder.We tackle the challenge of intentmining problem byintroducing alatent variable to represent usersâ€™ intentsand learn them alternatelyalong with the SR model optimization throughan expectation-maximization (EM) frameworkto ensure convergence.We suggest fusing learnt intent informationinto SR via the proposed contrastive SSL,as it can improve modelâ€™s performance as wellas robustness.Extensive experiments conducted on four real-world datasetsfurther verify the effectiveness of the proposed learning paradigm,which improves performance and robustness,even when recommender systemsfaceheavy data sparsity issues.\\nIn this work,we propose a new learning paradigm ICL that canmodel latent intent factorsfrom user interactionsand fuse them into a sequential recommendationmodel via a newcontrastive SSL objective.ICL is formulated withinan EM framework, which guaranteesconvergence. Detailed analyses showthe superiority of ICL andexperiments conducted onfour datasets further demonstratethe effectiveness of the proposed method.   \n",
      "\n",
      "                                                                                                      answer_70b    Rouge1    Rouge2    RougeL       Bleu       Chrf   Chrfplus    Meteor       Ter      Bert       WMS       SMS     Wisdm    Bleurt      Bart       BEM  Prometheus  Consistency  TSim  Faithfullness  Relevancy  Correctness      RSim  LLM  general_score  \n",
      "78                                                DPR leverages PLM to empower the retriever by a single vector.  0.431373  0.327869  0.431373  17.849563  52.045173  52.673008  0.212233  0.002386  0.622661  1.000000  0.689887  0.577283 -0.405083  0.445627  0.378898           3          1.0   1.0       0.666667   0.876577     0.521224  0.884895  0.7       5.910070  \n",
      "79  RocketQAv2 jointly trains retriever and re-ranker, and uses a large PLM for data augmentation and denoising.  0.426230  0.266667  0.426230  13.132418  53.474707  53.084692  0.265005  0.003543  0.782243  0.762239  0.781695  0.364447 -0.301530  0.389264  0.353159           4          1.0   4.0            NaN   0.894262     0.524176  0.896739  1.0       6.205736  \n",
      "52                                                                                        Convergence guarantee.  0.000000  0.000000  0.000000  15.973578  73.853946  59.309019  0.384615  0.006623  0.699150  0.928057  0.699150  0.837550  0.656420  0.029515  0.873098           4          1.0   5.0            NaN   0.847223     0.719825  0.879300  0.8       7.613503  \n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"3 Examples with Hihgest Score:\")\n",
    "print(highest_score_examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_eval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
