{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/computing scores/FULL ALIGNED/qasa_aligned_iris.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_100 = \"/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/computing scores/FULL ALIGNED/qasa_aligned_iris_100.csv\"\n",
    "df_100 = pd.read_csv(path_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'question', 'iris_answer', 'correct_answer', 'context',\n",
       "       'Rouge1', 'Rouge2', 'RougeL', 'Bleu', 'Chrf', 'ChrfPlus', 'Meteor',\n",
       "       'Ter', 'Bert', 'WMS', 'SMS', 'Wisdm', 'Bart', 'Prometheus',\n",
       "       'Faithfullness', 'Relevancy', 'RSim'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0          int64\n",
      "question           object\n",
      "iris_answer        object\n",
      "correct_answer     object\n",
      "context            object\n",
      "Rouge1            float64\n",
      "Rouge2            float64\n",
      "RougeL            float64\n",
      "Bleu              float64\n",
      "Chrf              float64\n",
      "ChrfPlus          float64\n",
      "Meteor            float64\n",
      "Ter               float64\n",
      "Bert              float64\n",
      "WMS               float64\n",
      "SMS               float64\n",
      "Wisdm             float64\n",
      "Bart              float64\n",
      "Prometheus          int64\n",
      "Faithfullness     float64\n",
      "Relevancy         float64\n",
      "RSim              float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1541.000000\n",
      "mean        0.271767\n",
      "std         0.171827\n",
      "min         0.000000\n",
      "25%         0.160000\n",
      "50%         0.264151\n",
      "75%         0.372093\n",
      "max         1.000000\n",
      "Name: Rouge1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_rouge1 = df['Rouge1'].describe()\n",
    "print(statistics_rouge1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1541.000000\n",
      "mean        0.109738\n",
      "std         0.137374\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.065574\n",
      "75%         0.157895\n",
      "max         1.000000\n",
      "Name: Rouge2, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_rouge2 = df['Rouge2'].describe()\n",
    "print(statistics_rouge2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1541.000000\n",
      "mean        0.236762\n",
      "std         0.160315\n",
      "min         0.000000\n",
      "25%         0.131148\n",
      "50%         0.222222\n",
      "75%         0.321429\n",
      "max         1.000000\n",
      "Name: RougeL, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_rougel = df['RougeL'].describe()\n",
    "print(statistics_rougel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1542.000000\n",
      "mean        7.157982\n",
      "std        10.383510\n",
      "min         0.000000\n",
      "25%         1.388299\n",
      "50%         3.395838\n",
      "75%         8.512099\n",
      "max       100.000000\n",
      "Name: Bleu, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_bleu = df['Bleu'].describe()\n",
    "print(statistics_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1542.000000\n",
      "mean        0.245151\n",
      "std         0.182204\n",
      "min         0.000000\n",
      "25%         0.109885\n",
      "50%         0.216744\n",
      "75%         0.333308\n",
      "max         0.937500\n",
      "Name: Meteor, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_meteor = df['Meteor'].describe()\n",
    "print(statistics_meteor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1542.000000\n",
      "mean       33.854310\n",
      "std        17.561821\n",
      "min         0.000000\n",
      "25%        22.054063\n",
      "50%        33.650002\n",
      "75%        44.405941\n",
      "max       100.000000\n",
      "Name: Chrf, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_chrf = df['Chrf'].describe()\n",
    "print(statistics_chrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1542.000000\n",
      "mean       30.681159\n",
      "std        16.741305\n",
      "min         0.000000\n",
      "25%        19.558579\n",
      "50%        30.165231\n",
      "75%        40.165899\n",
      "max       100.000000\n",
      "Name: ChrfPlus, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_chrfplus = df['ChrfPlus'].describe()\n",
    "print(statistics_chrfplus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1542.000000\n",
      "mean        0.009059\n",
      "std         0.025953\n",
      "min         0.000010\n",
      "25%         0.004289\n",
      "50%         0.009901\n",
      "75%         0.011351\n",
      "max         1.000000\n",
      "Name: Ter, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_ter = df['Ter'].describe()\n",
    "print(statistics_ter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1542.000000\n",
      "mean        0.591952\n",
      "std         0.238171\n",
      "min        -0.054496\n",
      "25%         0.464883\n",
      "50%         0.645052\n",
      "75%         0.769851\n",
      "max         1.000000\n",
      "Name: Bert, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_bert = df['Bert'].describe()\n",
    "print(statistics_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1542.000000\n",
      "mean        0.627994\n",
      "std         0.203390\n",
      "min         0.000000\n",
      "25%         0.530802\n",
      "50%         0.643955\n",
      "75%         0.753887\n",
      "max         1.000000\n",
      "Name: WMS, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_wms = df['WMS'].describe()\n",
    "print(statistics_wms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1542.000000\n",
      "mean        0.574361\n",
      "std         0.229593\n",
      "min        -0.029573\n",
      "25%         0.428774\n",
      "50%         0.608697\n",
      "75%         0.748751\n",
      "max         1.000000\n",
      "Name: SMS, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_sms = df['SMS'].describe()\n",
    "print(statistics_sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1358.000000\n",
      "mean        0.618083\n",
      "std         0.196987\n",
      "min         0.000000\n",
      "25%         0.501396\n",
      "50%         0.639642\n",
      "75%         0.767332\n",
      "max         1.000000\n",
      "Name: Wisdm, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_wisdm = df['Wisdm'].describe()\n",
    "print(statistics_wisdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    99.000000\n",
      "mean      0.354585\n",
      "std       0.261246\n",
      "min       0.049504\n",
      "25%       0.150171\n",
      "50%       0.276849\n",
      "75%       0.457094\n",
      "max       0.980778\n",
      "Name: Bleurt, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_bleurt = df_100['Bleurt'].describe()\n",
    "print(statistics_bleurt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    99.000000\n",
      "mean      0.432032\n",
      "std       0.308222\n",
      "min       0.032748\n",
      "25%       0.173197\n",
      "50%       0.355310\n",
      "75%       0.687463\n",
      "max       0.991697\n",
      "Name: BEM, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_bem = df_100['BEM'].describe()\n",
    "print(statistics_bem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1541.000000\n",
      "mean        0.037770\n",
      "std         0.053588\n",
      "min         0.000196\n",
      "25%         0.009817\n",
      "50%         0.021566\n",
      "75%         0.043479\n",
      "max         0.705363\n",
      "Name: Bart, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_bart = df['Bart'].describe()\n",
    "print(statistics_bart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1542.000000\n",
      "mean        3.446174\n",
      "std         0.811594\n",
      "min         1.000000\n",
      "25%         3.000000\n",
      "50%         4.000000\n",
      "75%         4.000000\n",
      "max         5.000000\n",
      "Name: Prometheus, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_prometheus = df['Prometheus'].describe()\n",
    "print(statistics_prometheus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    86.000000\n",
      "mean      0.795233\n",
      "std       0.197863\n",
      "min       0.000000\n",
      "25%       0.800000\n",
      "50%       0.800000\n",
      "75%       0.900000\n",
      "max       1.000000\n",
      "Name: LLM, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_llm = df_100['LLM'].describe()\n",
    "print(statistics_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1327.00000\n",
      "mean        0.74885\n",
      "std         0.36290\n",
      "min         0.00000\n",
      "25%         0.50000\n",
      "50%         1.00000\n",
      "75%         1.00000\n",
      "max         1.00000\n",
      "Name: Faithfullness, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_faithfulness = df['Faithfullness'].describe()\n",
    "print(statistics_faithfulness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1541.000000\n",
      "mean        0.883879\n",
      "std         0.163772\n",
      "min         0.000000\n",
      "25%         0.857341\n",
      "50%         0.916041\n",
      "75%         0.965479\n",
      "max         1.000000\n",
      "Name: Relevancy, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_rrel = df['Relevancy'].describe()\n",
    "print(statistics_rrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    98.000000\n",
      "mean      0.572012\n",
      "std       0.179301\n",
      "min       0.175951\n",
      "25%       0.474255\n",
      "50%       0.586253\n",
      "75%       0.713771\n",
      "max       0.962023\n",
      "Name: Correctness, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_correctness = df_100['Correctness'].describe()\n",
    "print(statistics_correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1542.000000\n",
      "mean        0.871125\n",
      "std         0.059897\n",
      "min         0.664231\n",
      "25%         0.840857\n",
      "50%         0.882042\n",
      "75%         0.914954\n",
      "max         1.000000\n",
      "Name: RSim, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_rsim = df['RSim'].describe()\n",
    "print(statistics_rsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    100.000000\n",
      "mean       0.778676\n",
      "std        0.330803\n",
      "min        0.000000\n",
      "25%        0.592857\n",
      "50%        1.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: Consistency, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_consistency = df_100['Consistency'].describe()\n",
    "print(statistics_consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    100.000000\n",
      "mean       2.740000\n",
      "std        1.624435\n",
      "min        0.000000\n",
      "25%        1.000000\n",
      "50%        3.500000\n",
      "75%        4.000000\n",
      "max        5.000000\n",
      "Name: TSim, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_tsim = df_100['TSim'].describe()\n",
    "print(statistics_tsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0       Rouge1       Rouge2       RougeL         Bleu  \\\n",
      "count  1542.000000  1541.000000  1541.000000  1541.000000  1542.000000   \n",
      "mean    770.500000     0.271767     0.109738     0.236762     7.157982   \n",
      "std     445.281372     0.171827     0.137374     0.160315    10.383510   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%     385.250000     0.160000     0.000000     0.131148     1.388299   \n",
      "50%     770.500000     0.264151     0.065574     0.222222     3.395838   \n",
      "75%    1155.750000     0.372093     0.157895     0.321429     8.512099   \n",
      "max    1541.000000     1.000000     1.000000     1.000000   100.000000   \n",
      "\n",
      "              Chrf     ChrfPlus       Meteor          Ter         Bert  \\\n",
      "count  1542.000000  1542.000000  1542.000000  1542.000000  1542.000000   \n",
      "mean     33.854310    30.681159     0.245151     0.009059     0.591952   \n",
      "std      17.561821    16.741305     0.182204     0.025953     0.238171   \n",
      "min       0.000000     0.000000     0.000000     0.000010    -0.054496   \n",
      "25%      22.054063    19.558579     0.109885     0.004289     0.464883   \n",
      "50%      33.650002    30.165231     0.216744     0.009901     0.645052   \n",
      "75%      44.405941    40.165899     0.333308     0.011351     0.769851   \n",
      "max     100.000000   100.000000     0.937500     1.000000     1.000000   \n",
      "\n",
      "               WMS          SMS        Wisdm         Bart   Prometheus  \\\n",
      "count  1542.000000  1542.000000  1358.000000  1541.000000  1542.000000   \n",
      "mean      0.627994     0.574361     0.618083     0.037770     3.446174   \n",
      "std       0.203390     0.229593     0.196987     0.053588     0.811594   \n",
      "min       0.000000    -0.029573     0.000000     0.000196     1.000000   \n",
      "25%       0.530802     0.428774     0.501396     0.009817     3.000000   \n",
      "50%       0.643955     0.608697     0.639642     0.021566     4.000000   \n",
      "75%       0.753887     0.748751     0.767332     0.043479     4.000000   \n",
      "max       1.000000     1.000000     1.000000     0.705363     5.000000   \n",
      "\n",
      "       Faithfullness    Relevancy         RSim  \n",
      "count     1327.00000  1541.000000  1542.000000  \n",
      "mean         0.74885     0.883879     0.871125  \n",
      "std          0.36290     0.163772     0.059897  \n",
      "min          0.00000     0.000000     0.664231  \n",
      "25%          0.50000     0.857341     0.840857  \n",
      "50%          1.00000     0.916041     0.882042  \n",
      "75%          1.00000     0.965479     0.914954  \n",
      "max          1.00000     1.000000     1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0      Rouge1      Rouge2      RougeL        Bleu        Chrf  \\\n",
      "count  100.000000  100.000000  100.000000  100.000000  100.000000  100.000000   \n",
      "mean    49.500000    0.225488    0.081375    0.195275    5.574618   31.891763   \n",
      "std     29.011492    0.156572    0.099399    0.141746    7.201356   17.718136   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.892857   \n",
      "25%     24.750000    0.110215    0.000000    0.084663    0.569205   17.089470   \n",
      "50%     49.500000    0.225397    0.047947    0.193608    2.739574   33.972618   \n",
      "75%     74.250000    0.345287    0.134091    0.293770    7.963997   45.973371   \n",
      "max     99.000000    0.526316    0.366667    0.526316   38.366105   69.027956   \n",
      "\n",
      "         ChrfPlus      Meteor         Ter        Bert  ...        BEM  \\\n",
      "count  100.000000  100.000000  100.000000  100.000000  ...  99.000000   \n",
      "mean    28.740070    0.180891    0.006462    0.572618  ...   0.432032   \n",
      "std     16.572863    0.131687    0.004094    0.244931  ...   0.308222   \n",
      "min      0.595238    0.000000    0.000118   -0.007127  ...   0.032748   \n",
      "25%     13.831901    0.073571    0.002554    0.424720  ...   0.173197   \n",
      "50%     30.138369    0.173163    0.006263    0.642968  ...   0.355310   \n",
      "75%     41.378970    0.258508    0.009901    0.769659  ...   0.687463   \n",
      "max     63.641395    0.561854    0.015471    0.928162  ...   0.991697   \n",
      "\n",
      "             Bart  Prometheus  Faithfullness   Relevancy  Correctness  \\\n",
      "count  100.000000  100.000000       84.00000  100.000000    98.000000   \n",
      "mean     0.038691    3.380000        0.73767    0.861457     0.572012   \n",
      "std      0.048716    0.788554        0.36614    0.209422     0.179301   \n",
      "min      0.000283    1.000000        0.00000    0.000000     0.175951   \n",
      "25%      0.008461    3.000000        0.50000    0.855468     0.474255   \n",
      "50%      0.020908    4.000000        1.00000    0.904418     0.586253   \n",
      "75%      0.049313    4.000000        1.00000    0.965991     0.713771   \n",
      "max      0.307939    4.000000        1.00000    1.000000     0.962023   \n",
      "\n",
      "             RSim  Consistency        TSim        LLM  \n",
      "count  100.000000   100.000000  100.000000  86.000000  \n",
      "mean     0.865061     0.778676    2.740000   0.795233  \n",
      "std      0.065352     0.330803    1.624435   0.197863  \n",
      "min      0.696070     0.000000    0.000000   0.000000  \n",
      "25%      0.821119     0.592857    1.000000   0.800000  \n",
      "50%      0.883750     1.000000    3.500000   0.800000  \n",
      "75%      0.916118     1.000000    4.000000   0.900000  \n",
      "max      0.970918     1.000000    5.000000   1.000000  \n",
      "\n",
      "[8 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_100.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = pd.DataFrame(df.describe())\n",
    "statistics_100 = pd.DataFrame(df_100.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_statistics_100 = statistics_100[['BEM', 'LLM', 'Correctness', 'Consistency', 'TSim']]\n",
    "final_df = pd.concat([statistics, sub_statistics_100], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>RougeL</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Chrf</th>\n",
       "      <th>ChrfPlus</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Ter</th>\n",
       "      <th>Bert</th>\n",
       "      <th>...</th>\n",
       "      <th>Bart</th>\n",
       "      <th>Prometheus</th>\n",
       "      <th>Faithfullness</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>RSim</th>\n",
       "      <th>BEM</th>\n",
       "      <th>LLM</th>\n",
       "      <th>Correctness</th>\n",
       "      <th>Consistency</th>\n",
       "      <th>TSim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1542.000000</td>\n",
       "      <td>1541.000000</td>\n",
       "      <td>1541.000000</td>\n",
       "      <td>1541.000000</td>\n",
       "      <td>1542.000000</td>\n",
       "      <td>1542.000000</td>\n",
       "      <td>1542.000000</td>\n",
       "      <td>1542.000000</td>\n",
       "      <td>1542.000000</td>\n",
       "      <td>1542.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1541.000000</td>\n",
       "      <td>1542.000000</td>\n",
       "      <td>1327.00000</td>\n",
       "      <td>1541.000000</td>\n",
       "      <td>1542.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>770.500000</td>\n",
       "      <td>0.271767</td>\n",
       "      <td>0.109738</td>\n",
       "      <td>0.236762</td>\n",
       "      <td>7.157982</td>\n",
       "      <td>33.854310</td>\n",
       "      <td>30.681159</td>\n",
       "      <td>0.245151</td>\n",
       "      <td>0.009059</td>\n",
       "      <td>0.591952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037770</td>\n",
       "      <td>3.446174</td>\n",
       "      <td>0.74885</td>\n",
       "      <td>0.883879</td>\n",
       "      <td>0.871125</td>\n",
       "      <td>0.432032</td>\n",
       "      <td>0.795233</td>\n",
       "      <td>0.572012</td>\n",
       "      <td>0.778676</td>\n",
       "      <td>2.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>445.281372</td>\n",
       "      <td>0.171827</td>\n",
       "      <td>0.137374</td>\n",
       "      <td>0.160315</td>\n",
       "      <td>10.383510</td>\n",
       "      <td>17.561821</td>\n",
       "      <td>16.741305</td>\n",
       "      <td>0.182204</td>\n",
       "      <td>0.025953</td>\n",
       "      <td>0.238171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053588</td>\n",
       "      <td>0.811594</td>\n",
       "      <td>0.36290</td>\n",
       "      <td>0.163772</td>\n",
       "      <td>0.059897</td>\n",
       "      <td>0.308222</td>\n",
       "      <td>0.197863</td>\n",
       "      <td>0.179301</td>\n",
       "      <td>0.330803</td>\n",
       "      <td>1.624435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.054496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664231</td>\n",
       "      <td>0.032748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>385.250000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>1.388299</td>\n",
       "      <td>22.054063</td>\n",
       "      <td>19.558579</td>\n",
       "      <td>0.109885</td>\n",
       "      <td>0.004289</td>\n",
       "      <td>0.464883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009817</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.857341</td>\n",
       "      <td>0.840857</td>\n",
       "      <td>0.173197</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.474255</td>\n",
       "      <td>0.592857</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>770.500000</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.065574</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>3.395838</td>\n",
       "      <td>33.650002</td>\n",
       "      <td>30.165231</td>\n",
       "      <td>0.216744</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.645052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021566</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.916041</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.355310</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.586253</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1155.750000</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>8.512099</td>\n",
       "      <td>44.405941</td>\n",
       "      <td>40.165899</td>\n",
       "      <td>0.333308</td>\n",
       "      <td>0.011351</td>\n",
       "      <td>0.769851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043479</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.965479</td>\n",
       "      <td>0.914954</td>\n",
       "      <td>0.687463</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.713771</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1541.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705363</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991697</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962023</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0       Rouge1       Rouge2       RougeL         Bleu  \\\n",
       "count  1542.000000  1541.000000  1541.000000  1541.000000  1542.000000   \n",
       "mean    770.500000     0.271767     0.109738     0.236762     7.157982   \n",
       "std     445.281372     0.171827     0.137374     0.160315    10.383510   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%     385.250000     0.160000     0.000000     0.131148     1.388299   \n",
       "50%     770.500000     0.264151     0.065574     0.222222     3.395838   \n",
       "75%    1155.750000     0.372093     0.157895     0.321429     8.512099   \n",
       "max    1541.000000     1.000000     1.000000     1.000000   100.000000   \n",
       "\n",
       "              Chrf     ChrfPlus       Meteor          Ter         Bert  ...  \\\n",
       "count  1542.000000  1542.000000  1542.000000  1542.000000  1542.000000  ...   \n",
       "mean     33.854310    30.681159     0.245151     0.009059     0.591952  ...   \n",
       "std      17.561821    16.741305     0.182204     0.025953     0.238171  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000010    -0.054496  ...   \n",
       "25%      22.054063    19.558579     0.109885     0.004289     0.464883  ...   \n",
       "50%      33.650002    30.165231     0.216744     0.009901     0.645052  ...   \n",
       "75%      44.405941    40.165899     0.333308     0.011351     0.769851  ...   \n",
       "max     100.000000   100.000000     0.937500     1.000000     1.000000  ...   \n",
       "\n",
       "              Bart   Prometheus  Faithfullness    Relevancy         RSim  \\\n",
       "count  1541.000000  1542.000000     1327.00000  1541.000000  1542.000000   \n",
       "mean      0.037770     3.446174        0.74885     0.883879     0.871125   \n",
       "std       0.053588     0.811594        0.36290     0.163772     0.059897   \n",
       "min       0.000196     1.000000        0.00000     0.000000     0.664231   \n",
       "25%       0.009817     3.000000        0.50000     0.857341     0.840857   \n",
       "50%       0.021566     4.000000        1.00000     0.916041     0.882042   \n",
       "75%       0.043479     4.000000        1.00000     0.965479     0.914954   \n",
       "max       0.705363     5.000000        1.00000     1.000000     1.000000   \n",
       "\n",
       "             BEM        LLM  Correctness  Consistency        TSim  \n",
       "count  99.000000  86.000000    98.000000   100.000000  100.000000  \n",
       "mean    0.432032   0.795233     0.572012     0.778676    2.740000  \n",
       "std     0.308222   0.197863     0.179301     0.330803    1.624435  \n",
       "min     0.032748   0.000000     0.175951     0.000000    0.000000  \n",
       "25%     0.173197   0.800000     0.474255     0.592857    1.000000  \n",
       "50%     0.355310   0.800000     0.586253     1.000000    3.500000  \n",
       "75%     0.687463   0.900000     0.713771     1.000000    4.000000  \n",
       "max     0.991697   1.000000     0.962023     1.000000    5.000000  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df.to_csv('/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/analyse results /statistics/qasa_aligned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAL STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_100['std'] = df_100.select_dtypes(include=['number']).std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>iris_answer</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>RougeL</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Chrf</th>\n",
       "      <th>...</th>\n",
       "      <th>BEM</th>\n",
       "      <th>Bart</th>\n",
       "      <th>Prometheus</th>\n",
       "      <th>Faithfullness</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>Correctness</th>\n",
       "      <th>RSim</th>\n",
       "      <th>Consistency</th>\n",
       "      <th>TSim</th>\n",
       "      <th>LLM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How do these automated metrics for human prefe...</td>\n",
       "      <td>The ultimate aim of language technology is to ...</td>\n",
       "      <td>The automated metrics that are mentioned while...</td>\n",
       "      <td>These automated metrics for human preferences,...</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>2.073187</td>\n",
       "      <td>30.870571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081154</td>\n",
       "      <td>0.019156</td>\n",
       "      <td>3</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.965836</td>\n",
       "      <td>0.474554</td>\n",
       "      <td>0.898315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What does non-differentiable mean here? If the...</td>\n",
       "      <td>The ultimate aim of language technology is to ...</td>\n",
       "      <td>A formal definition of non-differentiability h...</td>\n",
       "      <td>Non-differentiable here refers to the fact tha...</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.306667</td>\n",
       "      <td>6.047905</td>\n",
       "      <td>46.334803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280730</td>\n",
       "      <td>0.050747</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871779</td>\n",
       "      <td>0.733083</td>\n",
       "      <td>0.932323</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why is the action space of language modeling p...</td>\n",
       "      <td>Language generation action spaces are orders o...</td>\n",
       "      <td>The action space for language modeling is equa...</td>\n",
       "      <td>No, the action space of language modeling is n...</td>\n",
       "      <td>0.300752</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.270677</td>\n",
       "      <td>7.904009</td>\n",
       "      <td>37.513400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268187</td>\n",
       "      <td>0.051670</td>\n",
       "      <td>4</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.930281</td>\n",
       "      <td>0.481069</td>\n",
       "      <td>0.924315</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What do the equations for Q-value and value re...</td>\n",
       "      <td>RL4LMs supports fine-tuning and training LMs f...</td>\n",
       "      <td>Q and V are mathematically expressed as: V_{t}...</td>\n",
       "      <td>The equations for Q-value and value represent ...</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>5.469678</td>\n",
       "      <td>18.034209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088910</td>\n",
       "      <td>0.031399</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.936193</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Why is it helpful to mask out less relevant to...</td>\n",
       "      <td>Specifically, NLPOmaintains a masking policy \\...</td>\n",
       "      <td>The authors hypothesize that their dynamic mas...</td>\n",
       "      <td>Masking out less relevant tokens helps the mod...</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.784980</td>\n",
       "      <td>27.699611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261626</td>\n",
       "      <td>0.005237</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931911</td>\n",
       "      <td>0.509556</td>\n",
       "      <td>0.838288</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>Why did the authors use a mix of 1x1 and 3x3 f...</td>\n",
       "      <td>Strategy 2. Decrease the number of input chann...</td>\n",
       "      <td>Authors used a mix of 1x1 and 3x3 filters in t...</td>\n",
       "      <td>To maintain a small total number of parameters...</td>\n",
       "      <td>0.341880</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.324786</td>\n",
       "      <td>20.804895</td>\n",
       "      <td>52.975511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705268</td>\n",
       "      <td>0.113443</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992714</td>\n",
       "      <td>0.669791</td>\n",
       "      <td>0.964885</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>What is the total number of filters in squeeze...</td>\n",
       "      <td>We define the Fire module as follows.A Fire mo...</td>\n",
       "      <td>s1x1 is the number of filters in the squeeze l...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.747253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082079</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734565</td>\n",
       "      <td>0.567839</td>\n",
       "      <td>0.771567</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>The Caffe framework does not natively support ...</td>\n",
       "      <td>\\bulletSo that the output activations from 1x1...</td>\n",
       "      <td>The additional cost of using 2 convolutional l...</td>\n",
       "      <td>Increased computational cost due to concatenat...</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.700323</td>\n",
       "      <td>31.773402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139731</td>\n",
       "      <td>0.028906</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.779767</td>\n",
       "      <td>0.522473</td>\n",
       "      <td>0.889844</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>Did the authors use AlexNet for evaluation of ...</td>\n",
       "      <td>We now turn our attention to evaluating Squeez...</td>\n",
       "      <td>Yes, as told by authors that they used AlexNet...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.506189</td>\n",
       "      <td>7.672903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.988326</td>\n",
       "      <td>0.010523</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.872393</td>\n",
       "      <td>0.696496</td>\n",
       "      <td>0.785894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>How would the effectiveness of SqueezeNet's mo...</td>\n",
       "      <td>In addition, these results demonstrate that De...</td>\n",
       "      <td>by combining CNN architectural innovation (Squ...</td>\n",
       "      <td>The effectiveness of SqueezeNet's model compre...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.493370</td>\n",
       "      <td>27.897980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359156</td>\n",
       "      <td>0.021403</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962381</td>\n",
       "      <td>0.727505</td>\n",
       "      <td>0.909493</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                           question  \\\n",
       "0            0  How do these automated metrics for human prefe...   \n",
       "1            1  What does non-differentiable mean here? If the...   \n",
       "2            2  Why is the action space of language modeling p...   \n",
       "3            3  What do the equations for Q-value and value re...   \n",
       "4            4  Why is it helpful to mask out less relevant to...   \n",
       "..         ...                                                ...   \n",
       "95          95  Why did the authors use a mix of 1x1 and 3x3 f...   \n",
       "96          96  What is the total number of filters in squeeze...   \n",
       "97          97  The Caffe framework does not natively support ...   \n",
       "98          98  Did the authors use AlexNet for evaluation of ...   \n",
       "99          99  How would the effectiveness of SqueezeNet's mo...   \n",
       "\n",
       "                                              context  \\\n",
       "0   The ultimate aim of language technology is to ...   \n",
       "1   The ultimate aim of language technology is to ...   \n",
       "2   Language generation action spaces are orders o...   \n",
       "3   RL4LMs supports fine-tuning and training LMs f...   \n",
       "4   Specifically, NLPOmaintains a masking policy \\...   \n",
       "..                                                ...   \n",
       "95  Strategy 2. Decrease the number of input chann...   \n",
       "96  We define the Fire module as follows.A Fire mo...   \n",
       "97  \\bulletSo that the output activations from 1x1...   \n",
       "98  We now turn our attention to evaluating Squeez...   \n",
       "99  In addition, these results demonstrate that De...   \n",
       "\n",
       "                                       correct_answer  \\\n",
       "0   The automated metrics that are mentioned while...   \n",
       "1   A formal definition of non-differentiability h...   \n",
       "2   The action space for language modeling is equa...   \n",
       "3   Q and V are mathematically expressed as: V_{t}...   \n",
       "4   The authors hypothesize that their dynamic mas...   \n",
       "..                                                ...   \n",
       "95  Authors used a mix of 1x1 and 3x3 filters in t...   \n",
       "96  s1x1 is the number of filters in the squeeze l...   \n",
       "97  The additional cost of using 2 convolutional l...   \n",
       "98  Yes, as told by authors that they used AlexNet...   \n",
       "99  by combining CNN architectural innovation (Squ...   \n",
       "\n",
       "                                          iris_answer    Rouge1    Rouge2  \\\n",
       "0   These automated metrics for human preferences,...  0.107527  0.018018   \n",
       "1   Non-differentiable here refers to the fact tha...  0.346667  0.063158   \n",
       "2   No, the action space of language modeling is n...  0.300752  0.117647   \n",
       "3   The equations for Q-value and value represent ...  0.129032  0.016807   \n",
       "4   Masking out less relevant tokens helps the mod...  0.187500  0.000000   \n",
       "..                                                ...       ...       ...   \n",
       "95  To maintain a small total number of parameters...  0.341880  0.282353   \n",
       "96                                                  1  0.000000  0.000000   \n",
       "97  Increased computational cost due to concatenat...  0.140845  0.000000   \n",
       "98                                                Yes  0.000000  0.000000   \n",
       "99  The effectiveness of SqueezeNet's model compre...  0.166667  0.000000   \n",
       "\n",
       "      RougeL       Bleu       Chrf  ...       BEM      Bart  Prometheus  \\\n",
       "0   0.107527   2.073187  30.870571  ...  0.081154  0.019156           3   \n",
       "1   0.306667   6.047905  46.334803  ...  0.280730  0.050747           4   \n",
       "2   0.270677   7.904009  37.513400  ...  0.268187  0.051670           4   \n",
       "3   0.129032   5.469678  18.034209  ...  0.088910  0.031399           4   \n",
       "4   0.125000   0.784980  27.699611  ...  0.261626  0.005237           3   \n",
       "..       ...        ...        ...  ...       ...       ...         ...   \n",
       "95  0.324786  20.804895  52.975511  ...  0.705268  0.113443           4   \n",
       "96  0.000000   0.000000   2.747253  ...  0.082079  0.001708           1   \n",
       "97  0.140845   0.700323  31.773402  ...  0.139731  0.028906           3   \n",
       "98  0.000000   1.506189   7.672903  ...  0.988326  0.010523           4   \n",
       "99  0.083333   1.493370  27.897980  ...  0.359156  0.021403           3   \n",
       "\n",
       "    Faithfullness  Relevancy  Correctness      RSim  Consistency  TSim   LLM  \n",
       "0        0.750000   0.965836     0.474554  0.898315     1.000000   2.0  0.80  \n",
       "1        1.000000   0.871779     0.733083  0.932323     0.750000   4.0  0.95  \n",
       "2        0.833333   0.930281     0.481069  0.924315     0.750000   2.0  0.70  \n",
       "3             NaN   1.000000          NaN  0.936193     0.750000   4.0  0.98  \n",
       "4        1.000000   0.931911     0.509556  0.838288     1.000000   2.0   NaN  \n",
       "..            ...        ...          ...       ...          ...   ...   ...  \n",
       "95       1.000000   0.992714     0.669791  0.964885     1.000000   4.0  0.95  \n",
       "96       0.000000   0.734565     0.567839  0.771567     0.166667   0.0  0.40  \n",
       "97       0.500000   0.779767     0.522473  0.889844     1.000000   2.0   NaN  \n",
       "98            NaN   0.872393     0.696496  0.785894     0.000000   2.0  1.00  \n",
       "99       0.000000   0.962381     0.727505  0.909493     0.500000   2.0  0.80  \n",
       "\n",
       "[100 rows x 28 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_100.to_csv('/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/analyse results /statistics/qasa_aligned_std.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'std'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2883/3604761373.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_100\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'std'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlowest_std_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   7172\u001b[0m             )\n\u001b[1;32m   7173\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7174\u001b[0m             \u001b[0;31m# len(by) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7176\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7178\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7179\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1906\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1907\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1910\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'std'"
     ]
    }
   ],
   "source": [
    "sorted_df = df_100.sort_values(by='std')\n",
    "\n",
    "lowest_std_examples = sorted_df.head(3)\n",
    "\n",
    "# Get the 3 examples with the highest std\n",
    "highest_std_examples = sorted_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options to ensure full display\n",
    "pd.set_option('display.max_columns', None)      # Show all columns\n",
    "pd.set_option('display.max_rows', None)         # Show all rows\n",
    "pd.set_option('display.max_colwidth', None)     # No truncation of column content\n",
    "pd.set_option('display.width', 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Examples with Lowest Standard Deviation:\n",
      "                                                                                                                                                                             question  \\\n",
      "33                                                                                          If both queries and documents are short, is still the fine-granular interaction required?   \n",
      "26  How does the performance change when a dense retriever is evaluated on out-of-domain queries and documents that are different from the domain on which the retriever was trained?   \n",
      "96                                                                                                                  What is the total number of filters in squeeze convolution layer?   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       context  \\\n",
      "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                       These increasingly expressive architectures are in tension. While interaction-based models (i.e., Figure 2 (b) and (c)) tend to be superior for IR tasks (Guo et al., 2019; Mitraet al., 2018), a representation-focused model—by isolating the computations among q and d—makes it possible to pre-compute document representations offline (Zamani et al., 2018), greatly reducing the computational load per query. In this work, we observe that the fine-grained matching of interaction-based models and the pre-computation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query–document interaction. Figure 2 (d) illustrates an architecture that precisely does so. As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. This paradigm allows ColBERT to exploit deep LM-based representations while shifting the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., (Johnsonet al., 2017; Abuzaidet al., 2019)) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval.   \n",
      "26  However, as shown in Thakur et al. (2021b), dense retrieval methods require large amounts of training data to work well.333For reference, the popular MS MARCO dataset (Nguyen et al., 2016) has about 500k training instances; the Natural Questions dataset (Kwiatkowski et al., 2019) has more than 100k training instances.  Most importantly, dense retrieval methods are extremely sensitive to domain shifts: Models trained on MS MARCO perform rather poorly for questions for COVID-19 scientific literature (Wang et al., 2020; Voorhees et al., 2021). The MS MARCO dataset was created before COVID-19, hence, it does not include any COVID-19 related topics and models did not learn how to represent this topic well in a vector space. We use the MS MARCO passage ranking dataset Nguyen et al. (2016) as the data from the source domain. It has 8.8M passages and 532.8K query-passage pairs labeled as relevant in the training set. As Table 1 shows, a state-of-the-art dense retrieval model, achieving an MRR@10 of 33.2 points on the MS MARCO passage ranking dataset, performs poorly on the six selected domain-specific retrieval datasets when compared to simple BM25 lexical search. So far, ICT and CD have only been studied on in-domain performance, i.e. a large in-domain labeled dataset is available which is used for subsequent supervised fine-tuning. SimCSE, CT, and TSDAE have been only studied for unsupervised sentence embedding learning. As our results show in Appendix E, they do not work at all for purely unsupervised dense retrieval. If these pre-training approaches can be used for unsupervised domain adaptation for dense retrieval was so far unclear. In this work, we transfer the setup from Wang et al. (2021) to dense retrieval and first pre-train on the target corpus, followed by supervised training on labeled data from MS MARCO Nguyen et al. (2016). Performance is then measured on the target corpus.   \n",
      "96                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We define the Fire module as follows.A Fire module is comprised of: a squeeze convolution layer (which has only 1x1 filters), feeding into an expand layer that has a mix of 1x1 and 3x3 convolution filters; we illustrate this in Figure 1.The liberal use of 1x1 filters in Fire modules is an application of Strategy 1 from Section 3.1.We expose three tunable dimensions (hyperparameters) in a Fire module: s_{1x1}, e_{1x1}, and e_{3x3}.In a Fire module, s_{1x1} is the number of filters in the squeeze layer (all 1x1), e_{1x1} is the number of 1x1 filters in the expand layer, and e_{3x3} is the number of 3x3 filters in the expand layer.When we use Fire modules we set s_{1x1} to be less than (e_{1x1} + e_{3x3}), so the squeeze layer helps to limit the number of input channels to the 3x3 filters, as per Strategy 2 from Section 3.1.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     correct_answer iris_answer  Rouge1  Rouge2  RougeL  Bleu      Chrf  ChrfPlus  Meteor       Ter      Bert       WMS       SMS  Wisdm    Bleurt       BEM      Bart  Prometheus  Faithfullness  Relevancy  Correctness      RSim  Consistency  TSim  LLM       std  \n",
      "33                                                                                                                                                                                          During indexing, we use another server with the same CPU and system memory specifications but which has four Titan V GPUs attached, each with 12 GiBs of memory. Across all experiments, only one GPU is dedicated per query for retrieval (i.e., for methods with neural computations) but we use up to all four GPUs during indexing.          No     0.0     0.0     0.0   0.0  0.892857  0.595238     0.0  0.000179  0.097563  0.262268  0.086894    NaN -0.807949  0.069028  0.000596           1            NaN   0.000000     0.181251  0.725293     0.000000   0.0  NaN  0.395521  \n",
      "26  It is said that when evaluating a retriever trained on a source domain in an out-of-domain setting, the performance is obtained lower than BM25. Also, dense retrievers are said to be sensitive to domain shift and models that perform well on MS MARCO do not perform well on COVID-19 data. There have been many studies on unsupervised sentence embedding learning, but it is said that they do not work well in unsupervised dense retrieval. Therefore, the performance of the retriever in out-of-domain may be worse.      Poorly     0.0     0.0     0.0   0.0  1.110966  0.952256     0.0  0.000118  0.188955  0.687415  0.165305    NaN -0.881442  0.038843  0.000919           2            1.0   0.000000     0.401705  0.749482     1.000000   1.0  0.8  0.615619  \n",
      "96                                                                                                                                                                                                                                                                                                   s1x1 is the number of filters in the squeeze layer and it is set s1x1 to be less than (e1x1 + e3x3) -the total number of filters in expand layer of the fire module- to limit the number of input channels to the 3x3 filters.           1     0.0     0.0     0.0   0.0  2.747253  1.373626     0.0  0.000222  0.225376  0.191226  0.225376    NaN -0.902474  0.082079  0.001708           1            0.0   0.734565     0.567839  0.771567     0.166667   0.0  0.4  0.704530  \n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"3 Examples with Lowest Standard Deviation:\")\n",
    "print(lowest_std_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples with Highest Standard Deviation:\n",
      "                                                                                                                      question  \\\n",
      "29          What are pros and cons of these models illustrated in Figure 2, and what are distinctions of the proposed model?     \n",
      "78                                                                  What is the difference of RocketQAv1 and RocketQAv2 model?   \n",
      "80  What characteristics of large-scale pre-trained language models made it remarkable successful for passage re-ranking task?   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  context  \\\n",
      "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         These increasingly expressive architectures are in tension. While interaction-based models (i.e., Figure 2 (b) and (c)) tend to be superior for IR tasks (Guo et al., 2019; Mitraet al., 2018), a representation-focused model—by isolating the computations among q and d—makes it possible to pre-compute document representations offline (Zamani et al., 2018), greatly reducing the computational load per query. In this work, we observe that the fine-grained matching of interaction-based models and the pre-computation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query–document interaction. Figure 2 (d) illustrates an architecture that precisely does so. As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. This paradigm allows ColBERT to exploit deep LM-based representations while shifting the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., (Johnsonet al., 2017; Abuzaidet al., 2019)) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval. Our main contributions are as follows.(1)We propose late interaction (§3.1) as a paradigm for efficient and effective neural ranking.(2)We present ColBERT (§3.2 & 3.3), a highly-effective model that employs novel BERT-based query and document encoders within the late interaction paradigm.(3)We show how to leverage ColBERT both for re-ranking on top of a term-based retrieval model (§3.5) and for searching a full collection using vector similarity indexes (§3.6).(4)We evaluate ColBERT on MS MARCO and TREC CAR, two recent passage search collections.   \n",
      "78  Existing PLMs based re-rankers typically improve ranking performance from two aspects: (1) By optimizing the ranking procedure: monoBERT (Nogueira and Cho, 2019) is the first work that re-purposed BERT as a passage re-ranker and achieves state-of-the-art results. duoBERT (Nogueiraet al., 2019a) integrates monoBERT in a multistage ranking architecture and adopts a pairwise classification approach to passage relevance computation. UED (Yanet al., 2021) proposes a cascade pre-training manner that can jointly enhance the retrieval stage through passage expansion with a pre-trained query generator and thus elevate the re-ranking stage with a pre-trained transformer encoder. The two stages can facilitate each other in a unified pre-training framework. H-ERNIE (Chuet al., 2022) proposes a multi-granularity PLM for web search.(2) By designing rational distillation procedure: LM Distill + Fine-Tuning (Gaoet al., 2020) explores a variety of distillation methods to equip a smaller re-ranker with both general-purpose language modeling knowledge learned in pre-training and search- specific relevance modeling knowledge learned in fine-tuning, and produces a faster re-ranker with better ranking performance. CAKD (Hofstätter et al., 2020) proposes a cross-architecture knowledge distillation procedure with a Margin-MSE loss, which can distill knowledge from multiple teachers at the same time. RocketQAv1 (Qu et al., 2021) trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful cross-encoder to empower the dual-encoder. RocketQAv2 (Ren et al., 2021) proposes a novel approach that jointly trains the dense passage retriever and passage re-ranker. The parameters of RocketQAv2 are inherited from RocketQAv1. Besides, RocketQAv2 utilizes a large PLM for data augmentation and denoising, which can also be regarded as a distillation procedure. Notably, these two types of studies anticipate more insightful information to be captured by the advanced ranking and training procedures, while neglecting the limitations of implicit knowledge extracted from noisy and heterogeneous data. Therefore, in this paper, we proposed the first knowledge-enhanced PLM based re-ranker, which thoughtfully leverages explicit external knowledge that improve the effectiveness of the model. We include several PLMs based re-rankers in our evaluation, including the state-of-the-art:•monoBERT (Nogueira and Cho, 2019): The first study that re-purposes BERT as a re-ranker and achieves state-of-the-art results.•duoBERT (Nogueiraet al., 2019a):This work proposes a pairwise classification approach using BERT, which obtains the ability to be more sensitive to semantics through greater computation.•UED (Yanet al., 2021): A unified pre-training framework that jointly refines re-ranker and query generator. For a fair comparison, we only use the re-ranker in UED without passage expansion.•LM Distill+Fine-Tuning (LDFT) (Gaoet al., 2020):A variety of distillation methods are compared in this paper. The experimental results indicate that a proper distillation procedure (i.e. first distill the language model, and then fine-tune on the ranking task) could produce a faster re-ranker with better ranking performance.•CAKD (Hofstätter et al., 2020): This work proposes a cross-architecture knowledge distillation procedure with Margin-MSE loss, which can distill knowledge from multiple teachers.•RocketQAv1 (Qu et al., 2021): This work mainly focuses on the training of PLM based retriever, where the re-ranker is an intermediate product of its training process.•RocketQAv2 (Ren et al., 2021): Based on RocketQAv1, this work proposes a novel approach that jointly trains the PLM based retriever and re-ranker.To compare the performance of different methods, we resort to two ranking metrics.For MSMARCO-DEV, We adopt Mean Reciprocal Rank (i.e., MRR@10).For TREC 2019 DL, we use Mean Average Precision, i.e., MAP@10 and MAP@30.For Ohsumed, both Mean Reciprocal Rank and Mean Average Precision (i.e., MRR@10 and MAP@10) are employed for comprehensive performance analysis in queries requiring in-depth domain knowledge.   \n",
      "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Passage Re-ranking is a crucial stage in modern information retrieval systems, which aims to reorder a small set of candidate passages to be presented to users. To put the most relevant passages on top of a ranking list, a re-ranker is usually designed with powerful capacity in modeling semantic relevance, which attracted a wealth of research studies in the past decade (Guo et al., 2020). Recently,large-scale pre-trained language models (PLMs), e.g. BERT (Devlinet al., 2018), ERNIE (Sun et al., 2019) and RoBERTa (Liu et al., 2019), have dominated many natural language processing tasks, and have also achieved remarkable success on passage re-ranking.For example, PLM based re-rankers (MacAvaney et al., 2019; Liet al., 2020; Dong and Niu, 2021; Donget al., 2022) have achieved state-of-the-art performance, which takes the concatenation of query-passage pair as input, and applies multi-layer full-attention to model their semantic relevance. Their superiority can be attributed to the expressive transformer structure and the pretrain-then-finetune paradigm, which allow the model to learn useful implicit knowledge (i.e., semantic relevance in the latent space) from massive textual corpus (Fan et al., 2021).   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    correct_answer  \\\n",
      "29  Using figure 2, \\nThese increasingly expressive architectures are in tension. While interaction-based models (i.e., Figure 2 (b) and (c)) tend to be superior for IR tasks (Guo et al., 2019; Mitraet al., 2018), a representation-focused model—by isolating the computations among q and d—makes it possible to pre-compute document representations offline (Zamani et al., 2018), greatly reducing the computational load per query. In this work, we observe that the fine-grained matching of interaction-based models and the pre-computation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query–document interaction. Figure 2 (d) illustrates an architecture that precisely does so. As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. This paradigm allows ColBERT to exploit deep LM-based representations while shifting the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., (Johnsonet al., 2017; Abuzaidet al., 2019)) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval.\\n\\n\\n\\n\\n\\n\\nThe distinction of proposed model : 1) a highly-effective model is proposed that employs novel BERT-based query and document encoders within the late interaction paradigm.(2) We show how to leverage ColBERT both for re-ranking on top of a term-based retrieval model (3) and for searching a full collection using vector similarity indexes.(4)We evaluate ColBERT on MS MARCO and TREC CAR, two recent passage search collections.   \n",
      "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            RocketQAv1 trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful cross-encoder to empower the dual-encoder. While it inherits the parameters from RocketQAv1, RocketQAv2 extends the first version through a novel approach that jointly trains the dense passage retriever and passage re-ranker, and by using a large PLM for data augmentation and denoising (i.e.,  a distillation procedure).   \n",
      "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Large-scale pre-trained language models (PLMs) have been found to be successful for passage re-ranking due to their ability to learn semantic relevance in the latent space from massive textual corpus. PLMs obtain this ability from their expressive transformer architecture and the pretrain-then-finetune paradigm.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   iris_answer  \\\n",
      "29  Pros of the interaction-based models (Figure 2(b) and (c)):\\n\\n* Superior for IR tasks (Guo et al., 2019; Mitra et al., 2018)\\n\\nCons of the interaction-based models:\\n\\n* Computationally expensive\\n* Limited by the number of documents that can be processed in a single batch\\n\\nPros of the representation-focused model (Figure 2(d)):\\n\\n* Pre-computes document representations offline, reducing computational load per query\\n* Allows for efficient vector-similarity search indexes (Johnson et al., 2017; Abuzaid et al., 2019)\\n\\nCons of the representation-focused model:\\n\\n* May not be as effective for IR tasks as interaction-based models\\n\\nDistinctions of the proposed model (ColBERT):\\n\\n* Combines the fine-grained matching of interaction-based models with the pre-computation of document representations of representation-based models\\n* Allows for efficient retrieval of top-k results directly from a large document collection, improving recall over models that only re-rank the output of term-based retrieval   \n",
      "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               RocketQAv2 is an improved version of RocketQAv1.\\n\\nRocketQAv1 trains dual-encoder and cross-encoder in a cascade manner, while RocketQAv2 jointly trains the dense passage retriever and passage re-ranker. Additionally, RocketQAv2 utilizes a large PLM for data augmentation and denoising, which can be regarded as a distillation procedure. This allows RocketQAv2 to improve the effectiveness of the model by leveraging explicit external knowledge   \n",
      "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The expressive transformer structure and the pretrain-then-finetune paradigm of large-scale pre-trained language models (PLMs) have contributed to their remarkable success on the passage re-ranking task   \n",
      "\n",
      "      Rouge1    Rouge2    RougeL       Bleu       Chrf   ChrfPlus    Meteor       Ter      Bert       WMS       SMS     Wisdm    Bleurt       BEM      Bart  Prometheus  Faithfullness  Relevancy  Correctness      RSim  Consistency  TSim   LLM        std  \n",
      "29  0.386861  0.231362  0.372263  22.049572  63.989216  58.613219  0.285440  0.006315  0.761908  0.772281  0.820886  0.638262 -0.017608       NaN  0.059788           4       0.714286   0.853037     0.358507  0.933812     0.846154   4.0  0.90  18.084584  \n",
      "78  0.526316  0.366667  0.526316  38.366105  60.737700  58.394368  0.561854  0.015021  0.928162  0.590332  0.794440  0.936918  0.099600  0.949727  0.158032           4       0.800000   0.979029     0.457012  0.970918     1.000000   4.0  0.70  18.155734  \n",
      "80  0.491803  0.303030  0.426230  20.499829  69.027956  63.641395  0.396444  0.007519  0.874722  0.664177  0.736484  0.637157  0.319817  0.961402  0.156724           4            NaN   0.972182     0.613946  0.955801     1.000000   5.0  0.95  19.431988  \n"
     ]
    }
   ],
   "source": [
    "print(\"Examples with Highest Standard Deviation:\")\n",
    "print(highest_std_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAL SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100['general_score'] = df_100.select_dtypes(include=['number']).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>iris_answer</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>RougeL</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Chrf</th>\n",
       "      <th>ChrfPlus</th>\n",
       "      <th>...</th>\n",
       "      <th>Bart</th>\n",
       "      <th>Prometheus</th>\n",
       "      <th>Faithfullness</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>Correctness</th>\n",
       "      <th>RSim</th>\n",
       "      <th>Consistency</th>\n",
       "      <th>TSim</th>\n",
       "      <th>LLM</th>\n",
       "      <th>general_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do these automated metrics for human prefe...</td>\n",
       "      <td>The ultimate aim of language technology is to ...</td>\n",
       "      <td>The automated metrics that are mentioned while...</td>\n",
       "      <td>These automated metrics for human preferences,...</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>2.073187</td>\n",
       "      <td>30.870571</td>\n",
       "      <td>26.756701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019156</td>\n",
       "      <td>3</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.965836</td>\n",
       "      <td>0.474554</td>\n",
       "      <td>0.898315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3.112030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does non-differentiable mean here? If the...</td>\n",
       "      <td>The ultimate aim of language technology is to ...</td>\n",
       "      <td>A formal definition of non-differentiability h...</td>\n",
       "      <td>Non-differentiable here refers to the fact tha...</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.306667</td>\n",
       "      <td>6.047905</td>\n",
       "      <td>46.334803</td>\n",
       "      <td>41.275842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050747</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871779</td>\n",
       "      <td>0.733083</td>\n",
       "      <td>0.932323</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>4.811819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why is the action space of language modeling p...</td>\n",
       "      <td>Language generation action spaces are orders o...</td>\n",
       "      <td>The action space for language modeling is equa...</td>\n",
       "      <td>No, the action space of language modeling is n...</td>\n",
       "      <td>0.300752</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.270677</td>\n",
       "      <td>7.904009</td>\n",
       "      <td>37.513400</td>\n",
       "      <td>34.184432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051670</td>\n",
       "      <td>4</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.930281</td>\n",
       "      <td>0.481069</td>\n",
       "      <td>0.924315</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>4.092154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What do the equations for Q-value and value re...</td>\n",
       "      <td>RL4LMs supports fine-tuning and training LMs f...</td>\n",
       "      <td>Q and V are mathematically expressed as: V_{t}...</td>\n",
       "      <td>The equations for Q-value and value represent ...</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>5.469678</td>\n",
       "      <td>18.034209</td>\n",
       "      <td>15.384644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031399</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.936193</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2.507687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why is it helpful to mask out less relevant to...</td>\n",
       "      <td>Specifically, NLPOmaintains a masking policy \\...</td>\n",
       "      <td>The authors hypothesize that their dynamic mas...</td>\n",
       "      <td>Masking out less relevant tokens helps the mod...</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.784980</td>\n",
       "      <td>27.699611</td>\n",
       "      <td>23.589224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005237</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931911</td>\n",
       "      <td>0.509556</td>\n",
       "      <td>0.838288</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.850837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Why did the authors use a mix of 1x1 and 3x3 f...</td>\n",
       "      <td>Strategy 2. Decrease the number of input chann...</td>\n",
       "      <td>Authors used a mix of 1x1 and 3x3 filters in t...</td>\n",
       "      <td>To maintain a small total number of parameters...</td>\n",
       "      <td>0.341880</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.324786</td>\n",
       "      <td>20.804895</td>\n",
       "      <td>52.975511</td>\n",
       "      <td>51.521053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113443</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992714</td>\n",
       "      <td>0.669791</td>\n",
       "      <td>0.964885</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>6.274042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>What is the total number of filters in squeeze...</td>\n",
       "      <td>We define the Fire module as follows.A Fire mo...</td>\n",
       "      <td>s1x1 is the number of filters in the squeeze l...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.747253</td>\n",
       "      <td>1.373626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734565</td>\n",
       "      <td>0.567839</td>\n",
       "      <td>0.771567</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.344774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>The Caffe framework does not natively support ...</td>\n",
       "      <td>\\bulletSo that the output activations from 1x1...</td>\n",
       "      <td>The additional cost of using 2 convolutional l...</td>\n",
       "      <td>Increased computational cost due to concatenat...</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.700323</td>\n",
       "      <td>31.773402</td>\n",
       "      <td>26.388293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028906</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.779767</td>\n",
       "      <td>0.522473</td>\n",
       "      <td>0.889844</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.196219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Did the authors use AlexNet for evaluation of ...</td>\n",
       "      <td>We now turn our attention to evaluating Squeez...</td>\n",
       "      <td>Yes, as told by authors that they used AlexNet...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.506189</td>\n",
       "      <td>7.672903</td>\n",
       "      <td>10.220869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010523</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.872393</td>\n",
       "      <td>0.696496</td>\n",
       "      <td>0.785894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.440345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>How would the effectiveness of SqueezeNet's mo...</td>\n",
       "      <td>In addition, these results demonstrate that De...</td>\n",
       "      <td>by combining CNN architectural innovation (Squ...</td>\n",
       "      <td>The effectiveness of SqueezeNet's model compre...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.493370</td>\n",
       "      <td>27.897980</td>\n",
       "      <td>22.989604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021403</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962381</td>\n",
       "      <td>0.727505</td>\n",
       "      <td>0.909493</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.794671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   How do these automated metrics for human prefe...   \n",
       "1   What does non-differentiable mean here? If the...   \n",
       "2   Why is the action space of language modeling p...   \n",
       "3   What do the equations for Q-value and value re...   \n",
       "4   Why is it helpful to mask out less relevant to...   \n",
       "..                                                ...   \n",
       "95  Why did the authors use a mix of 1x1 and 3x3 f...   \n",
       "96  What is the total number of filters in squeeze...   \n",
       "97  The Caffe framework does not natively support ...   \n",
       "98  Did the authors use AlexNet for evaluation of ...   \n",
       "99  How would the effectiveness of SqueezeNet's mo...   \n",
       "\n",
       "                                              context  \\\n",
       "0   The ultimate aim of language technology is to ...   \n",
       "1   The ultimate aim of language technology is to ...   \n",
       "2   Language generation action spaces are orders o...   \n",
       "3   RL4LMs supports fine-tuning and training LMs f...   \n",
       "4   Specifically, NLPOmaintains a masking policy \\...   \n",
       "..                                                ...   \n",
       "95  Strategy 2. Decrease the number of input chann...   \n",
       "96  We define the Fire module as follows.A Fire mo...   \n",
       "97  \\bulletSo that the output activations from 1x1...   \n",
       "98  We now turn our attention to evaluating Squeez...   \n",
       "99  In addition, these results demonstrate that De...   \n",
       "\n",
       "                                       correct_answer  \\\n",
       "0   The automated metrics that are mentioned while...   \n",
       "1   A formal definition of non-differentiability h...   \n",
       "2   The action space for language modeling is equa...   \n",
       "3   Q and V are mathematically expressed as: V_{t}...   \n",
       "4   The authors hypothesize that their dynamic mas...   \n",
       "..                                                ...   \n",
       "95  Authors used a mix of 1x1 and 3x3 filters in t...   \n",
       "96  s1x1 is the number of filters in the squeeze l...   \n",
       "97  The additional cost of using 2 convolutional l...   \n",
       "98  Yes, as told by authors that they used AlexNet...   \n",
       "99  by combining CNN architectural innovation (Squ...   \n",
       "\n",
       "                                          iris_answer    Rouge1    Rouge2  \\\n",
       "0   These automated metrics for human preferences,...  0.107527  0.018018   \n",
       "1   Non-differentiable here refers to the fact tha...  0.346667  0.063158   \n",
       "2   No, the action space of language modeling is n...  0.300752  0.117647   \n",
       "3   The equations for Q-value and value represent ...  0.129032  0.016807   \n",
       "4   Masking out less relevant tokens helps the mod...  0.187500  0.000000   \n",
       "..                                                ...       ...       ...   \n",
       "95  To maintain a small total number of parameters...  0.341880  0.282353   \n",
       "96                                                  1  0.000000  0.000000   \n",
       "97  Increased computational cost due to concatenat...  0.140845  0.000000   \n",
       "98                                                Yes  0.000000  0.000000   \n",
       "99  The effectiveness of SqueezeNet's model compre...  0.166667  0.000000   \n",
       "\n",
       "      RougeL       Bleu       Chrf   ChrfPlus  ...      Bart  Prometheus  \\\n",
       "0   0.107527   2.073187  30.870571  26.756701  ...  0.019156           3   \n",
       "1   0.306667   6.047905  46.334803  41.275842  ...  0.050747           4   \n",
       "2   0.270677   7.904009  37.513400  34.184432  ...  0.051670           4   \n",
       "3   0.129032   5.469678  18.034209  15.384644  ...  0.031399           4   \n",
       "4   0.125000   0.784980  27.699611  23.589224  ...  0.005237           3   \n",
       "..       ...        ...        ...        ...  ...       ...         ...   \n",
       "95  0.324786  20.804895  52.975511  51.521053  ...  0.113443           4   \n",
       "96  0.000000   0.000000   2.747253   1.373626  ...  0.001708           1   \n",
       "97  0.140845   0.700323  31.773402  26.388293  ...  0.028906           3   \n",
       "98  0.000000   1.506189   7.672903  10.220869  ...  0.010523           4   \n",
       "99  0.083333   1.493370  27.897980  22.989604  ...  0.021403           3   \n",
       "\n",
       "    Faithfullness  Relevancy  Correctness      RSim  Consistency  TSim   LLM  \\\n",
       "0        0.750000   0.965836     0.474554  0.898315     1.000000   2.0  0.80   \n",
       "1        1.000000   0.871779     0.733083  0.932323     0.750000   4.0  0.95   \n",
       "2        0.833333   0.930281     0.481069  0.924315     0.750000   2.0  0.70   \n",
       "3             NaN   1.000000          NaN  0.936193     0.750000   4.0  0.98   \n",
       "4        1.000000   0.931911     0.509556  0.838288     1.000000   2.0   NaN   \n",
       "..            ...        ...          ...       ...          ...   ...   ...   \n",
       "95       1.000000   0.992714     0.669791  0.964885     1.000000   4.0  0.95   \n",
       "96       0.000000   0.734565     0.567839  0.771567     0.166667   0.0  0.40   \n",
       "97       0.500000   0.779767     0.522473  0.889844     1.000000   2.0   NaN   \n",
       "98            NaN   0.872393     0.696496  0.785894     0.000000   2.0  1.00   \n",
       "99       0.000000   0.962381     0.727505  0.909493     0.500000   2.0  0.80   \n",
       "\n",
       "    general_score  \n",
       "0        3.112030  \n",
       "1        4.811819  \n",
       "2        4.092154  \n",
       "3        2.507687  \n",
       "4        2.850837  \n",
       "..            ...  \n",
       "95       6.274042  \n",
       "96       0.344774  \n",
       "97       3.196219  \n",
       "98       1.440345  \n",
       "99       2.794671  \n",
       "\n",
       "[100 rows x 28 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df_100.sort_values(by='general_score')\n",
    "\n",
    "lowest_score_examples = sorted_df.head(3)\n",
    "\n",
    "# Get the 3 examples with the highest std\n",
    "highest_score_examples = sorted_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Examples with Lowest Score:\n",
      "                                                                                                           question  \\\n",
      "33                        If both queries and documents are short, is still the fine-granular interaction required?   \n",
      "96                                                What is the total number of filters in squeeze convolution layer?   \n",
      "79  Would the performance be improved if the PLM model is pre-trained or fine-tuned on bio-medical domain datasets?   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  context  \\\n",
      "33  These increasingly expressive architectures are in tension. While interaction-based models (i.e., Figure 2 (b) and (c)) tend to be superior for IR tasks (Guo et al., 2019; Mitraet al., 2018), a representation-focused model—by isolating the computations among q and d—makes it possible to pre-compute document representations offline (Zamani et al., 2018), greatly reducing the computational load per query. In this work, we observe that the fine-grained matching of interaction-based models and the pre-computation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query–document interaction. Figure 2 (d) illustrates an architecture that precisely does so. As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. This paradigm allows ColBERT to exploit deep LM-based representations while shifting the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., (Johnsonet al., 2017; Abuzaidet al., 2019)) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval.   \n",
      "96                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We define the Fire module as follows.A Fire module is comprised of: a squeeze convolution layer (which has only 1x1 filters), feeding into an expand layer that has a mix of 1x1 and 3x3 convolution filters; we illustrate this in Figure 1.The liberal use of 1x1 filters in Fire modules is an application of Strategy 1 from Section 3.1.We expose three tunable dimensions (hyperparameters) in a Fire module: s_{1x1}, e_{1x1}, and e_{3x3}.In a Fire module, s_{1x1} is the number of filters in the squeeze layer (all 1x1), e_{1x1} is the number of 1x1 filters in the expand layer, and e_{3x3} is the number of 3x3 filters in the expand layer.When we use Fire modules we set s_{1x1} to be less than (e_{1x1} + e_{3x3}), so the squeeze layer helps to limit the number of input channels to the 3x3 filters, as per Strategy 2 from Section 3.1.   \n",
      "79                                                                                                                                                                                                                                                   However, implicit knowledge still has some inherent weaknesses, which limits the applicability of PLMs based re-rankers. First,queries and passages are usually created by different persons and have different expression ways (Nogueiraet al., 2019b), such as word usage and language style.Worse still, the data distributions of search queries and web contents are highly heterogeneous (Liuet al., 2021), where various specialized domains (e.g., bio-medical) may only have few training examples in a general corpus. Domain-specific knowledge can hardly be revealed and captured by the model, and thus the processing of domain-specific queries is often inaccurate. Results are obtained from Table 6. (1) Poor ranking performances of all models on bio-medical domain indicates that it is more challenging in the data scarcity scenario, where textual data is not covered widely in the PLMs’ pretraining datasets. (2) Compared with ERNIE, KERM has a higher relative improvement in bio-medical domain than general domain. This demonstrates that the incorporation of knowledge graph is more useful for a data scarcity domain. To verify this idea, we compare the size of knowledge meta graph used for different domains as follows.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                      correct_answer iris_answer  Rouge1  Rouge2  RougeL  Bleu      Chrf  ChrfPlus  Meteor       Ter      Bert       WMS       SMS  Wisdm    Bleurt       BEM      Bart  Prometheus  Faithfullness  Relevancy  Correctness      RSim  Consistency  TSim  LLM  general_score  \n",
      "33                                                                           During indexing, we use another server with the same CPU and system memory specifications but which has four Titan V GPUs attached, each with 12 GiBs of memory. Across all experiments, only one GPU is dedicated per query for retrieval (i.e., for methods with neural computations) but we use up to all four GPUs during indexing.          No     0.0     0.0     0.0   0.0  0.892857  0.595238     0.0  0.000179  0.097563  0.262268  0.086894    NaN -0.807949  0.069028  0.000596           1            NaN   0.000000     0.181251  0.725293     0.000000   0.0  NaN       0.155161  \n",
      "96                                                                                                                                                                                    s1x1 is the number of filters in the squeeze layer and it is set s1x1 to be less than (e1x1 + e3x3) -the total number of filters in expand layer of the fire module- to limit the number of input channels to the 3x3 filters.           1     0.0     0.0     0.0   0.0  2.747253  1.373626     0.0  0.000222  0.225376  0.191226  0.225376    NaN -0.902474  0.082079  0.001708           1            0.0   0.734565     0.567839  0.771567     0.166667   0.0  0.4       0.344774  \n",
      "79  In their experiments, the authors showed that all of the models performed poorly on the bio-medical domain due to the textual data of the domain not being covered widely in the PLMs’ pretraining dataset. This lack of data can cause the PLM to struggle to reveal and capture knowledge specific to that domain. These results suggest that further training on bio-medical data could increase performance.         Yes     0.0     0.0     0.0   0.0  1.445288  1.083966     0.0  0.000154  0.020217  0.280034  0.026476    NaN -0.952253  0.074972  0.000283           3            NaN   0.781284     0.481244  0.725221     0.000000   0.0  0.8       0.369852  \n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"3 Examples with Lowest Score:\")\n",
    "print(lowest_score_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Examples with Hihgest Score:\n",
      "                                                                                                                      question  \\\n",
      "29          What are pros and cons of these models illustrated in Figure 2, and what are distinctions of the proposed model?     \n",
      "78                                                                  What is the difference of RocketQAv1 and RocketQAv2 model?   \n",
      "80  What characteristics of large-scale pre-trained language models made it remarkable successful for passage re-ranking task?   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  context  \\\n",
      "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         These increasingly expressive architectures are in tension. While interaction-based models (i.e., Figure 2 (b) and (c)) tend to be superior for IR tasks (Guo et al., 2019; Mitraet al., 2018), a representation-focused model—by isolating the computations among q and d—makes it possible to pre-compute document representations offline (Zamani et al., 2018), greatly reducing the computational load per query. In this work, we observe that the fine-grained matching of interaction-based models and the pre-computation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query–document interaction. Figure 2 (d) illustrates an architecture that precisely does so. As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. This paradigm allows ColBERT to exploit deep LM-based representations while shifting the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., (Johnsonet al., 2017; Abuzaidet al., 2019)) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval. Our main contributions are as follows.(1)We propose late interaction (§3.1) as a paradigm for efficient and effective neural ranking.(2)We present ColBERT (§3.2 & 3.3), a highly-effective model that employs novel BERT-based query and document encoders within the late interaction paradigm.(3)We show how to leverage ColBERT both for re-ranking on top of a term-based retrieval model (§3.5) and for searching a full collection using vector similarity indexes (§3.6).(4)We evaluate ColBERT on MS MARCO and TREC CAR, two recent passage search collections.   \n",
      "78  Existing PLMs based re-rankers typically improve ranking performance from two aspects: (1) By optimizing the ranking procedure: monoBERT (Nogueira and Cho, 2019) is the first work that re-purposed BERT as a passage re-ranker and achieves state-of-the-art results. duoBERT (Nogueiraet al., 2019a) integrates monoBERT in a multistage ranking architecture and adopts a pairwise classification approach to passage relevance computation. UED (Yanet al., 2021) proposes a cascade pre-training manner that can jointly enhance the retrieval stage through passage expansion with a pre-trained query generator and thus elevate the re-ranking stage with a pre-trained transformer encoder. The two stages can facilitate each other in a unified pre-training framework. H-ERNIE (Chuet al., 2022) proposes a multi-granularity PLM for web search.(2) By designing rational distillation procedure: LM Distill + Fine-Tuning (Gaoet al., 2020) explores a variety of distillation methods to equip a smaller re-ranker with both general-purpose language modeling knowledge learned in pre-training and search- specific relevance modeling knowledge learned in fine-tuning, and produces a faster re-ranker with better ranking performance. CAKD (Hofstätter et al., 2020) proposes a cross-architecture knowledge distillation procedure with a Margin-MSE loss, which can distill knowledge from multiple teachers at the same time. RocketQAv1 (Qu et al., 2021) trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful cross-encoder to empower the dual-encoder. RocketQAv2 (Ren et al., 2021) proposes a novel approach that jointly trains the dense passage retriever and passage re-ranker. The parameters of RocketQAv2 are inherited from RocketQAv1. Besides, RocketQAv2 utilizes a large PLM for data augmentation and denoising, which can also be regarded as a distillation procedure. Notably, these two types of studies anticipate more insightful information to be captured by the advanced ranking and training procedures, while neglecting the limitations of implicit knowledge extracted from noisy and heterogeneous data. Therefore, in this paper, we proposed the first knowledge-enhanced PLM based re-ranker, which thoughtfully leverages explicit external knowledge that improve the effectiveness of the model. We include several PLMs based re-rankers in our evaluation, including the state-of-the-art:•monoBERT (Nogueira and Cho, 2019): The first study that re-purposes BERT as a re-ranker and achieves state-of-the-art results.•duoBERT (Nogueiraet al., 2019a):This work proposes a pairwise classification approach using BERT, which obtains the ability to be more sensitive to semantics through greater computation.•UED (Yanet al., 2021): A unified pre-training framework that jointly refines re-ranker and query generator. For a fair comparison, we only use the re-ranker in UED without passage expansion.•LM Distill+Fine-Tuning (LDFT) (Gaoet al., 2020):A variety of distillation methods are compared in this paper. The experimental results indicate that a proper distillation procedure (i.e. first distill the language model, and then fine-tune on the ranking task) could produce a faster re-ranker with better ranking performance.•CAKD (Hofstätter et al., 2020): This work proposes a cross-architecture knowledge distillation procedure with Margin-MSE loss, which can distill knowledge from multiple teachers.•RocketQAv1 (Qu et al., 2021): This work mainly focuses on the training of PLM based retriever, where the re-ranker is an intermediate product of its training process.•RocketQAv2 (Ren et al., 2021): Based on RocketQAv1, this work proposes a novel approach that jointly trains the PLM based retriever and re-ranker.To compare the performance of different methods, we resort to two ranking metrics.For MSMARCO-DEV, We adopt Mean Reciprocal Rank (i.e., MRR@10).For TREC 2019 DL, we use Mean Average Precision, i.e., MAP@10 and MAP@30.For Ohsumed, both Mean Reciprocal Rank and Mean Average Precision (i.e., MRR@10 and MAP@10) are employed for comprehensive performance analysis in queries requiring in-depth domain knowledge.   \n",
      "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Passage Re-ranking is a crucial stage in modern information retrieval systems, which aims to reorder a small set of candidate passages to be presented to users. To put the most relevant passages on top of a ranking list, a re-ranker is usually designed with powerful capacity in modeling semantic relevance, which attracted a wealth of research studies in the past decade (Guo et al., 2020). Recently,large-scale pre-trained language models (PLMs), e.g. BERT (Devlinet al., 2018), ERNIE (Sun et al., 2019) and RoBERTa (Liu et al., 2019), have dominated many natural language processing tasks, and have also achieved remarkable success on passage re-ranking.For example, PLM based re-rankers (MacAvaney et al., 2019; Liet al., 2020; Dong and Niu, 2021; Donget al., 2022) have achieved state-of-the-art performance, which takes the concatenation of query-passage pair as input, and applies multi-layer full-attention to model their semantic relevance. Their superiority can be attributed to the expressive transformer structure and the pretrain-then-finetune paradigm, which allow the model to learn useful implicit knowledge (i.e., semantic relevance in the latent space) from massive textual corpus (Fan et al., 2021).   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    correct_answer  \\\n",
      "29  Using figure 2, \\nThese increasingly expressive architectures are in tension. While interaction-based models (i.e., Figure 2 (b) and (c)) tend to be superior for IR tasks (Guo et al., 2019; Mitraet al., 2018), a representation-focused model—by isolating the computations among q and d—makes it possible to pre-compute document representations offline (Zamani et al., 2018), greatly reducing the computational load per query. In this work, we observe that the fine-grained matching of interaction-based models and the pre-computation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query–document interaction. Figure 2 (d) illustrates an architecture that precisely does so. As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. This paradigm allows ColBERT to exploit deep LM-based representations while shifting the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., (Johnsonet al., 2017; Abuzaidet al., 2019)) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval.\\n\\n\\n\\n\\n\\n\\nThe distinction of proposed model : 1) a highly-effective model is proposed that employs novel BERT-based query and document encoders within the late interaction paradigm.(2) We show how to leverage ColBERT both for re-ranking on top of a term-based retrieval model (3) and for searching a full collection using vector similarity indexes.(4)We evaluate ColBERT on MS MARCO and TREC CAR, two recent passage search collections.   \n",
      "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            RocketQAv1 trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful cross-encoder to empower the dual-encoder. While it inherits the parameters from RocketQAv1, RocketQAv2 extends the first version through a novel approach that jointly trains the dense passage retriever and passage re-ranker, and by using a large PLM for data augmentation and denoising (i.e.,  a distillation procedure).   \n",
      "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Large-scale pre-trained language models (PLMs) have been found to be successful for passage re-ranking due to their ability to learn semantic relevance in the latent space from massive textual corpus. PLMs obtain this ability from their expressive transformer architecture and the pretrain-then-finetune paradigm.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   iris_answer  \\\n",
      "29  Pros of the interaction-based models (Figure 2(b) and (c)):\\n\\n* Superior for IR tasks (Guo et al., 2019; Mitra et al., 2018)\\n\\nCons of the interaction-based models:\\n\\n* Computationally expensive\\n* Limited by the number of documents that can be processed in a single batch\\n\\nPros of the representation-focused model (Figure 2(d)):\\n\\n* Pre-computes document representations offline, reducing computational load per query\\n* Allows for efficient vector-similarity search indexes (Johnson et al., 2017; Abuzaid et al., 2019)\\n\\nCons of the representation-focused model:\\n\\n* May not be as effective for IR tasks as interaction-based models\\n\\nDistinctions of the proposed model (ColBERT):\\n\\n* Combines the fine-grained matching of interaction-based models with the pre-computation of document representations of representation-based models\\n* Allows for efficient retrieval of top-k results directly from a large document collection, improving recall over models that only re-rank the output of term-based retrieval   \n",
      "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               RocketQAv2 is an improved version of RocketQAv1.\\n\\nRocketQAv1 trains dual-encoder and cross-encoder in a cascade manner, while RocketQAv2 jointly trains the dense passage retriever and passage re-ranker. Additionally, RocketQAv2 utilizes a large PLM for data augmentation and denoising, which can be regarded as a distillation procedure. This allows RocketQAv2 to improve the effectiveness of the model by leveraging explicit external knowledge   \n",
      "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The expressive transformer structure and the pretrain-then-finetune paradigm of large-scale pre-trained language models (PLMs) have contributed to their remarkable success on the passage re-ranking task   \n",
      "\n",
      "      Rouge1    Rouge2    RougeL       Bleu       Chrf   ChrfPlus    Meteor       Ter      Bert       WMS       SMS     Wisdm    Bleurt       BEM      Bart  Prometheus  Faithfullness  Relevancy  Correctness      RSim  Consistency  TSim   LLM  general_score  \n",
      "29  0.386861  0.231362  0.372263  22.049572  63.989216  58.613219  0.285440  0.006315  0.761908  0.772281  0.820886  0.638262 -0.017608       NaN  0.059788           4       0.714286   0.853037     0.358507  0.933812     0.846154   4.0  0.90       7.344344  \n",
      "78  0.526316  0.366667  0.526316  38.366105  60.737700  58.394368  0.561854  0.015021  0.928162  0.590332  0.794440  0.936918  0.099600  0.949727  0.158032           4       0.800000   0.979029     0.457012  0.970918     1.000000   4.0  0.70       7.689501  \n",
      "80  0.491803  0.303030  0.426230  20.499829  69.027956  63.641395  0.396444  0.007519  0.874722  0.664177  0.736484  0.637157  0.319817  0.961402  0.156724           4            NaN   0.972182     0.613946  0.955801     1.000000   5.0  0.95       7.847119  \n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"3 Examples with Hihgest Score:\")\n",
    "print(highest_score_examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_eval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
