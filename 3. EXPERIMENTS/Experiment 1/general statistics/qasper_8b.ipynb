{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/computing scores/Qasper 8-70b/qasper_8b.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_100 = \"/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/computing scores/Qasper 8-70b/qasper_8b_100.csv\"\n",
    "df_100 = pd.read_csv(path_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'question', 'correct_answer', 'context', 'answer_8b',\n",
       "       'Rouge1', 'Rouge2', 'RougeL', 'Bleu', 'Chrf', 'Chrfplus', 'Meteor',\n",
       "       'Ter', 'Bert', 'WMS', 'SMS', 'Wisdm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'question', 'correct_answer', 'context',\n",
       "       'answer_8b', 'Rouge1', 'Rouge2', 'RougeL', 'Bleu', 'Chrf', 'Chrfplus',\n",
       "       'Meteor', 'Ter', 'Bert', 'WMS', 'SMS', 'Wisdm', 'Bart', 'BEM',\n",
       "       'Prometheus', 'Consistency', 'TSim', 'Faithfullness', 'Relevancy',\n",
       "       'Correctness', 'RSim', 'LLM', 'Bleurt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_100.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0          int64\n",
      "question           object\n",
      "correct_answer     object\n",
      "context            object\n",
      "answer_8b          object\n",
      "Rouge1            float64\n",
      "Rouge2            float64\n",
      "RougeL            float64\n",
      "Bleu              float64\n",
      "Chrf              float64\n",
      "Chrfplus          float64\n",
      "Meteor            float64\n",
      "Ter               float64\n",
      "Bert              float64\n",
      "WMS               float64\n",
      "SMS               float64\n",
      "Wisdm             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    705.000000\n",
      "mean       0.186498\n",
      "std        0.204828\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.125000\n",
      "75%        0.307692\n",
      "max        1.000000\n",
      "Name: Rouge1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_rouge1 = df['Rouge1'].describe()\n",
    "print(statistics_rouge1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    705.000000\n",
      "mean       0.078896\n",
      "std        0.147389\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        0.106667\n",
      "max        1.000000\n",
      "Name: Rouge2, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_rouge2 = df['Rouge2'].describe()\n",
    "print(statistics_rouge2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    705.000000\n",
      "mean       0.173394\n",
      "std        0.196460\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.117647\n",
      "75%        0.260870\n",
      "max        1.000000\n",
      "Name: RougeL, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_rougel = df['RougeL'].describe()\n",
    "print(statistics_rougel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    705.000000\n",
      "mean       7.206647\n",
      "std       11.487837\n",
      "min        0.000000\n",
      "25%        1.816085\n",
      "50%        3.100138\n",
      "75%        6.949044\n",
      "max      100.000000\n",
      "Name: Bleu, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_bleu = df['Bleu'].describe()\n",
    "print(statistics_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    705.000000\n",
      "mean       0.187387\n",
      "std        0.194677\n",
      "min        0.000000\n",
      "25%        0.041322\n",
      "50%        0.110294\n",
      "75%        0.279805\n",
      "max        0.999852\n",
      "Name: Meteor, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_meteor = df['Meteor'].describe()\n",
    "print(statistics_meteor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    705.000000\n",
      "mean      29.474206\n",
      "std       21.262520\n",
      "min        0.341530\n",
      "25%       11.006236\n",
      "50%       25.319258\n",
      "75%       43.691535\n",
      "max      100.000000\n",
      "Name: Chrf, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_chrf = df['Chrf'].describe()\n",
    "print(statistics_chrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    705.000000\n",
      "mean      27.529230\n",
      "std       20.119335\n",
      "min        0.452899\n",
      "25%       10.303648\n",
      "50%       22.425585\n",
      "75%       39.902326\n",
      "max      100.000000\n",
      "Name: Chrfplus, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_chrfplus = df['Chrfplus'].describe()\n",
    "print(statistics_chrfplus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    705.000000\n",
      "mean       0.007098\n",
      "std        0.038030\n",
      "min        0.000192\n",
      "25%        0.001664\n",
      "50%        0.003984\n",
      "75%        0.008264\n",
      "max        1.000000\n",
      "Name: Ter, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_ter = df['Ter'].describe()\n",
    "print(statistics_ter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    705.000000\n",
      "mean       0.437328\n",
      "std        0.267308\n",
      "min       -0.077841\n",
      "25%        0.189331\n",
      "50%        0.437431\n",
      "75%        0.662381\n",
      "max        1.000000\n",
      "Name: Bert, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_bert = df['Bert'].describe()\n",
    "print(statistics_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   705.000\n",
      "mean      0.497\n",
      "std       0.285\n",
      "min       0.000\n",
      "25%       0.264\n",
      "50%       0.482\n",
      "75%       0.717\n",
      "max       1.000\n",
      "Name: WMS, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_wms = df['WMS'].describe()\n",
    "print(statistics_wms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   705.000\n",
      "mean      0.444\n",
      "std       0.263\n",
      "min      -0.078\n",
      "25%       0.203\n",
      "50%       0.452\n",
      "75%       0.666\n",
      "max       1.000\n",
      "Name: SMS, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_sms = df['SMS'].describe()\n",
    "print(statistics_sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   528.000\n",
      "mean      0.489\n",
      "std       0.278\n",
      "min       0.000\n",
      "25%       0.253\n",
      "50%       0.508\n",
      "75%       0.720\n",
      "max       1.000\n",
      "Name: Wisdm, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_wisdm = df['Wisdm'].describe()\n",
    "print(statistics_wisdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      0.275\n",
      "std       0.295\n",
      "min       0.040\n",
      "25%       0.068\n",
      "50%       0.116\n",
      "75%       0.371\n",
      "max       0.992\n",
      "Name: Bleurt, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_bleurt = df_100['Bleurt'].describe()\n",
    "print(statistics_bleurt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      0.275\n",
      "std       0.295\n",
      "min       0.040\n",
      "25%       0.068\n",
      "50%       0.116\n",
      "75%       0.371\n",
      "max       0.992\n",
      "Name: BEM, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_bem = df_100['BEM'].describe()\n",
    "print(statistics_bem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      0.042\n",
      "std       0.077\n",
      "min       0.001\n",
      "25%       0.006\n",
      "50%       0.014\n",
      "75%       0.036\n",
      "max       0.502\n",
      "Name: Bart, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_bart = df_100['Bart'].describe()\n",
    "print(statistics_bart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      2.730\n",
      "std       1.188\n",
      "min       1.000\n",
      "25%       1.000\n",
      "50%       3.000\n",
      "75%       4.000\n",
      "max       5.000\n",
      "Name: Prometheus, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_prometheus = df_100['Prometheus'].describe()\n",
    "print(statistics_prometheus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      0.542\n",
      "std       0.424\n",
      "min       0.000\n",
      "25%       0.000\n",
      "50%       0.800\n",
      "75%       0.900\n",
      "max       1.000\n",
      "Name: LLM, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_llm = df_100['LLM'].describe()\n",
    "print(statistics_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   67.000\n",
      "mean     0.806\n",
      "std      0.336\n",
      "min      0.000\n",
      "25%      0.667\n",
      "50%      1.000\n",
      "75%      1.000\n",
      "max      1.000\n",
      "Name: Faithfullness, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_faithfulness = df_100['Faithfullness'].describe()\n",
    "print(statistics_faithfulness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      0.669\n",
      "std       0.350\n",
      "min       0.000\n",
      "25%       0.769\n",
      "50%       0.821\n",
      "75%       0.873\n",
      "max       0.988\n",
      "Name: Relevancy, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_rrel = df_100['Relevancy'].describe()\n",
    "print(statistics_rrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      0.528\n",
      "std       0.240\n",
      "min       0.170\n",
      "25%       0.210\n",
      "50%       0.583\n",
      "75%       0.718\n",
      "max       0.963\n",
      "Name: Correctness, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_correctness = df_100['Correctness'].describe()\n",
    "print(statistics_correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      0.819\n",
      "std       0.081\n",
      "min       0.679\n",
      "25%       0.753\n",
      "50%       0.807\n",
      "75%       0.889\n",
      "max       0.991\n",
      "Name: RSim, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_rsim = df_100['RSim'].describe()\n",
    "print(statistics_rsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      0.667\n",
      "std       0.435\n",
      "min       0.000\n",
      "25%       0.000\n",
      "50%       1.000\n",
      "75%       1.000\n",
      "max       1.000\n",
      "Name: Consistency, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_consistency = df_100['Consistency'].describe()\n",
    "print(statistics_consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   100.000\n",
      "mean      1.750\n",
      "std       1.789\n",
      "min       0.000\n",
      "25%       0.000\n",
      "50%       1.000\n",
      "75%       4.000\n",
      "max       5.000\n",
      "Name: TSim, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "statistics_tsim = df_100['TSim'].describe()\n",
    "print(statistics_tsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  Rouge1  Rouge2  RougeL    Bleu    Chrf  Chrfplus  Meteor  \\\n",
      "count     705.000 705.000 705.000 705.000 705.000 705.000   705.000 705.000   \n",
      "mean      352.000   0.186   0.079   0.173   7.207  29.474    27.529   0.187   \n",
      "std       203.660   0.205   0.147   0.196  11.488  21.263    20.119   0.195   \n",
      "min         0.000   0.000   0.000   0.000   0.000   0.342     0.453   0.000   \n",
      "25%       176.000   0.000   0.000   0.000   1.816  11.006    10.304   0.041   \n",
      "50%       352.000   0.125   0.000   0.118   3.100  25.319    22.426   0.110   \n",
      "75%       528.000   0.308   0.107   0.261   6.949  43.692    39.902   0.280   \n",
      "max       704.000   1.000   1.000   1.000 100.000 100.000   100.000   1.000   \n",
      "\n",
      "          Ter    Bert     WMS     SMS   Wisdm  \n",
      "count 705.000 705.000 705.000 705.000 528.000  \n",
      "mean    0.007   0.437   0.497   0.444   0.489  \n",
      "std     0.038   0.267   0.285   0.263   0.278  \n",
      "min     0.000  -0.078   0.000  -0.078   0.000  \n",
      "25%     0.002   0.189   0.264   0.203   0.253  \n",
      "50%     0.004   0.437   0.482   0.452   0.508  \n",
      "75%     0.008   0.662   0.717   0.666   0.720  \n",
      "max     1.000   1.000   1.000   1.000   1.000  \n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0.1  Unnamed: 0  Rouge1  Rouge2  RougeL    Bleu    Chrf  \\\n",
      "count       100.000     100.000 100.000 100.000 100.000 100.000 100.000   \n",
      "mean         49.500      49.500   0.168   0.072   0.157   6.533  26.946   \n",
      "std          29.011      29.011   0.208   0.129   0.197   9.727  21.495   \n",
      "min           0.000       0.000   0.000   0.000   0.000   0.000   0.342   \n",
      "25%          24.750      24.750   0.000   0.000   0.000   1.630   9.573   \n",
      "50%          49.500      49.500   0.093   0.000   0.093   2.840  20.265   \n",
      "75%          74.250      74.250   0.258   0.101   0.241   5.902  40.946   \n",
      "max          99.000      99.000   0.909   0.583   0.909  50.317  84.982   \n",
      "\n",
      "       Chrfplus  Meteor     Ter  ...     BEM  Prometheus  Consistency    TSim  \\\n",
      "count   100.000 100.000 100.000  ... 100.000     100.000      100.000 100.000   \n",
      "mean     25.262   0.170   0.005  ...   0.275       2.730        0.667   1.750   \n",
      "std      20.287   0.195   0.005  ...   0.295       1.188        0.435   1.789   \n",
      "min       1.655   0.000   0.000  ...   0.040       1.000        0.000   0.000   \n",
      "25%       9.463   0.039   0.001  ...   0.068       1.000        0.000   0.000   \n",
      "50%      19.215   0.071   0.003  ...   0.116       3.000        1.000   1.000   \n",
      "75%      38.830   0.240   0.006  ...   0.371       4.000        1.000   4.000   \n",
      "max      82.670   0.833   0.029  ...   0.992       5.000        1.000   5.000   \n",
      "\n",
      "       Faithfullness  Relevancy  Correctness    RSim     LLM  Bleurt  \n",
      "count         67.000    100.000      100.000 100.000 100.000 100.000  \n",
      "mean           0.806      0.669        0.528   0.819   0.542   0.275  \n",
      "std            0.336      0.350        0.240   0.081   0.424   0.295  \n",
      "min            0.000      0.000        0.170   0.679   0.000   0.040  \n",
      "25%            0.667      0.769        0.210   0.753   0.000   0.068  \n",
      "50%            1.000      0.821        0.583   0.807   0.800   0.116  \n",
      "75%            1.000      0.873        0.718   0.889   0.900   0.371  \n",
      "max            1.000      0.988        0.963   0.991   1.000   0.992  \n",
      "\n",
      "[8 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_100.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = pd.DataFrame(df.describe())\n",
    "statistics_100 = pd.DataFrame(df_100.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_statistics_100 = statistics_100[['Bart', 'BEM', 'Prometheus',\n",
    "       'Consistency', 'TSim', 'Faithfullness', 'Relevancy', 'Correctness',\n",
    "       'RSim', 'LLM']]\n",
    "final_df = pd.concat([statistics, sub_statistics_100], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>RougeL</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Chrf</th>\n",
       "      <th>Chrfplus</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Ter</th>\n",
       "      <th>Bert</th>\n",
       "      <th>...</th>\n",
       "      <th>Bart</th>\n",
       "      <th>BEM</th>\n",
       "      <th>Prometheus</th>\n",
       "      <th>Consistency</th>\n",
       "      <th>TSim</th>\n",
       "      <th>Faithfullness</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>Correctness</th>\n",
       "      <th>RSim</th>\n",
       "      <th>LLM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>705.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>67.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>352.000</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.173</td>\n",
       "      <td>7.207</td>\n",
       "      <td>29.474</td>\n",
       "      <td>27.529</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.275</td>\n",
       "      <td>2.730</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1.750</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>203.660</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.196</td>\n",
       "      <td>11.488</td>\n",
       "      <td>21.263</td>\n",
       "      <td>20.119</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.295</td>\n",
       "      <td>1.188</td>\n",
       "      <td>0.435</td>\n",
       "      <td>1.789</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.040</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>176.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.816</td>\n",
       "      <td>11.006</td>\n",
       "      <td>10.304</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.068</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>352.000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.118</td>\n",
       "      <td>3.100</td>\n",
       "      <td>25.319</td>\n",
       "      <td>22.426</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.116</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>528.000</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.261</td>\n",
       "      <td>6.949</td>\n",
       "      <td>43.692</td>\n",
       "      <td>39.902</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.371</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>704.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.992</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.991</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Rouge1  Rouge2  RougeL    Bleu    Chrf  Chrfplus  Meteor  \\\n",
       "count     705.000 705.000 705.000 705.000 705.000 705.000   705.000 705.000   \n",
       "mean      352.000   0.186   0.079   0.173   7.207  29.474    27.529   0.187   \n",
       "std       203.660   0.205   0.147   0.196  11.488  21.263    20.119   0.195   \n",
       "min         0.000   0.000   0.000   0.000   0.000   0.342     0.453   0.000   \n",
       "25%       176.000   0.000   0.000   0.000   1.816  11.006    10.304   0.041   \n",
       "50%       352.000   0.125   0.000   0.118   3.100  25.319    22.426   0.110   \n",
       "75%       528.000   0.308   0.107   0.261   6.949  43.692    39.902   0.280   \n",
       "max       704.000   1.000   1.000   1.000 100.000 100.000   100.000   1.000   \n",
       "\n",
       "          Ter    Bert  ...    Bart     BEM  Prometheus  Consistency    TSim  \\\n",
       "count 705.000 705.000  ... 100.000 100.000     100.000      100.000 100.000   \n",
       "mean    0.007   0.437  ...   0.042   0.275       2.730        0.667   1.750   \n",
       "std     0.038   0.267  ...   0.077   0.295       1.188        0.435   1.789   \n",
       "min     0.000  -0.078  ...   0.001   0.040       1.000        0.000   0.000   \n",
       "25%     0.002   0.189  ...   0.006   0.068       1.000        0.000   0.000   \n",
       "50%     0.004   0.437  ...   0.014   0.116       3.000        1.000   1.000   \n",
       "75%     0.008   0.662  ...   0.036   0.371       4.000        1.000   4.000   \n",
       "max     1.000   1.000  ...   0.502   0.992       5.000        1.000   5.000   \n",
       "\n",
       "       Faithfullness  Relevancy  Correctness    RSim     LLM  \n",
       "count         67.000    100.000      100.000 100.000 100.000  \n",
       "mean           0.806      0.669        0.528   0.819   0.542  \n",
       "std            0.336      0.350        0.240   0.081   0.424  \n",
       "min            0.000      0.000        0.170   0.679   0.000  \n",
       "25%            0.667      0.769        0.210   0.753   0.000  \n",
       "50%            1.000      0.821        0.583   0.807   0.800  \n",
       "75%            1.000      0.873        0.718   0.889   0.900  \n",
       "max            1.000      0.988        0.963   0.991   1.000  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/analyse results /statistics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfinal_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/analyse results /statistics/qasper_8b.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py:3964\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3953\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3955\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3956\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3957\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3961\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3962\u001b[0m )\n\u001b[0;32m-> 3964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/analyse results /statistics'"
     ]
    }
   ],
   "source": [
    "final_df.to_csv('/home/ubuntu/iris_repos/llm_evaluation_thesis/analysis/analyse results /statistics/qasper_8b.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAL STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100['std'] = df_100.select_dtypes(include=['number']).std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>context</th>\n",
       "      <th>answer_8b</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>RougeL</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Chrf</th>\n",
       "      <th>Chrfplus</th>\n",
       "      <th>...</th>\n",
       "      <th>BEM</th>\n",
       "      <th>Prometheus</th>\n",
       "      <th>Consistency</th>\n",
       "      <th>TSim</th>\n",
       "      <th>Faithfullness</th>\n",
       "      <th>Relevancy</th>\n",
       "      <th>Correctness</th>\n",
       "      <th>RSim</th>\n",
       "      <th>LLM</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the seed lexicon?</td>\n",
       "      <td>A vocabulary of positive and negative predicat...</td>\n",
       "      <td>The seed lexicon consists of positive and nega...</td>\n",
       "      <td>A lexicon of positive and negative predicates.</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.545</td>\n",
       "      <td>23.288</td>\n",
       "      <td>64.041</td>\n",
       "      <td>62.646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500</td>\n",
       "      <td>4.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.800</td>\n",
       "      <td>18.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the results?</td>\n",
       "      <td>Using all data to train: AL -- BiGRU achieved ...</td>\n",
       "      <td>FLOAT SELECTED: Table 3: Performance of variou...</td>\n",
       "      <td>Tables 3 and 4.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.259</td>\n",
       "      <td>2.192</td>\n",
       "      <td>2.099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How are relations used to propagate polarity?</td>\n",
       "      <td>Based on the relation between events, the sugg...</td>\n",
       "      <td>In this paper, we propose a simple and effecti...</td>\n",
       "      <td>Through discourse relations like Cause and Con...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.647</td>\n",
       "      <td>23.155</td>\n",
       "      <td>18.515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.900</td>\n",
       "      <td>5.937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How are relations used to propagate polarity?</td>\n",
       "      <td>Cause relation: both events in the relation sh...</td>\n",
       "      <td>In this paper, we propose a simple and effecti...</td>\n",
       "      <td>Through discourse relations of Cause and Conce...</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.095</td>\n",
       "      <td>1.958</td>\n",
       "      <td>35.611</td>\n",
       "      <td>28.988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.874</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.800</td>\n",
       "      <td>9.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How big is the Japanese data?</td>\n",
       "      <td>7000000 pairs of events were extracted from th...</td>\n",
       "      <td>As a raw corpus, we used a Japanese web corpus...</td>\n",
       "      <td>100 million sentences.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.647</td>\n",
       "      <td>11.711</td>\n",
       "      <td>10.387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.801</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>What is task success rate achieved?</td>\n",
       "      <td>96-97.6% using the objects color or shape and ...</td>\n",
       "      <td>To test our model, we generated 500 new scenar...</td>\n",
       "      <td>97.6% and 96.0%.</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>5.751</td>\n",
       "      <td>20.111</td>\n",
       "      <td>20.574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.655</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.800</td>\n",
       "      <td>5.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>How is performance of this system measured?</td>\n",
       "      <td>Using the BLEU score as a quantitative metric ...</td>\n",
       "      <td>We use the BLEU BIBREF30 metric on the validat...</td>\n",
       "      <td>Human evaluation and BLEU score.</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.222</td>\n",
       "      <td>7.142</td>\n",
       "      <td>57.003</td>\n",
       "      <td>52.880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.000</td>\n",
       "      <td>15.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>How big dataset is used for training this system?</td>\n",
       "      <td>For the question generation model 15,000 image...</td>\n",
       "      <td>We use MS COCO, Bing and Flickr datasets from ...</td>\n",
       "      <td>385,777 utterances and 15,000 images with 75,0...</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.462</td>\n",
       "      <td>21.084</td>\n",
       "      <td>60.450</td>\n",
       "      <td>58.581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.685</td>\n",
       "      <td>3</td>\n",
       "      <td>0.667</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.920</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>How do they obtain word lattices from words?</td>\n",
       "      <td>By considering words as vertices and generatin...</td>\n",
       "      <td>Word Lattice\\nAs shown in Figure FIGREF4 , a w...</td>\n",
       "      <td>They treat all possible substrings as vertexes.</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.095</td>\n",
       "      <td>2.908</td>\n",
       "      <td>21.844</td>\n",
       "      <td>18.997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>How better is proposed method than baselines p...</td>\n",
       "      <td>Perplexity of proposed MEED model is 19.795 vs...</td>\n",
       "      <td>Table TABREF34 gives the perplexity scores obt...</td>\n",
       "      <td>Significantly better (p-value &lt; 0.05).</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.287</td>\n",
       "      <td>10.919</td>\n",
       "      <td>9.584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.800</td>\n",
       "      <td>3.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0                           What is the seed lexicon?   \n",
       "1                               What are the results?   \n",
       "2       How are relations used to propagate polarity?   \n",
       "3       How are relations used to propagate polarity?   \n",
       "4                       How big is the Japanese data?   \n",
       "..                                                ...   \n",
       "95               What is task success rate achieved?    \n",
       "96        How is performance of this system measured?   \n",
       "97  How big dataset is used for training this system?   \n",
       "98       How do they obtain word lattices from words?   \n",
       "99  How better is proposed method than baselines p...   \n",
       "\n",
       "                                       correct_answer  \\\n",
       "0   A vocabulary of positive and negative predicat...   \n",
       "1   Using all data to train: AL -- BiGRU achieved ...   \n",
       "2   Based on the relation between events, the sugg...   \n",
       "3   Cause relation: both events in the relation sh...   \n",
       "4   7000000 pairs of events were extracted from th...   \n",
       "..                                                ...   \n",
       "95  96-97.6% using the objects color or shape and ...   \n",
       "96  Using the BLEU score as a quantitative metric ...   \n",
       "97  For the question generation model 15,000 image...   \n",
       "98  By considering words as vertices and generatin...   \n",
       "99  Perplexity of proposed MEED model is 19.795 vs...   \n",
       "\n",
       "                                              context  \\\n",
       "0   The seed lexicon consists of positive and nega...   \n",
       "1   FLOAT SELECTED: Table 3: Performance of variou...   \n",
       "2   In this paper, we propose a simple and effecti...   \n",
       "3   In this paper, we propose a simple and effecti...   \n",
       "4   As a raw corpus, we used a Japanese web corpus...   \n",
       "..                                                ...   \n",
       "95  To test our model, we generated 500 new scenar...   \n",
       "96  We use the BLEU BIBREF30 metric on the validat...   \n",
       "97  We use MS COCO, Bing and Flickr datasets from ...   \n",
       "98  Word Lattice\\nAs shown in Figure FIGREF4 , a w...   \n",
       "99  Table TABREF34 gives the perplexity scores obt...   \n",
       "\n",
       "                                            answer_8b  Rouge1  Rouge2  RougeL  \\\n",
       "0      A lexicon of positive and negative predicates.   0.545   0.381   0.545   \n",
       "1                                     Tables 3 and 4.   0.000   0.000   0.000   \n",
       "2   Through discourse relations like Cause and Con...   0.000   0.000   0.000   \n",
       "3   Through discourse relations of Cause and Conce...   0.095   0.000   0.095   \n",
       "4                              100 million sentences.   0.000   0.000   0.000   \n",
       "..                                                ...     ...     ...     ...   \n",
       "95                                   97.6% and 96.0%.   0.250   0.000   0.250   \n",
       "96                   Human evaluation and BLEU score.   0.444   0.125   0.222   \n",
       "97  385,777 utterances and 15,000 images with 75,0...   0.462   0.320   0.462   \n",
       "98    They treat all possible substrings as vertexes.   0.095   0.000   0.095   \n",
       "99             Significantly better (p-value < 0.05).   0.000   0.000   0.000   \n",
       "\n",
       "     Bleu   Chrf  Chrfplus  ...   BEM  Prometheus  Consistency  TSim  \\\n",
       "0  23.288 64.041    62.646  ... 0.591           3        0.500 4.000   \n",
       "1   0.259  2.192     2.099  ... 0.088           1        1.000 0.000   \n",
       "2   1.647 23.155    18.515  ... 0.068           4        1.000 2.000   \n",
       "3   1.958 35.611    28.988  ... 0.874           3        0.500 2.000   \n",
       "4   1.647 11.711    10.387  ... 0.051           1        1.000 1.000   \n",
       "..    ...    ...       ...  ...   ...         ...          ...   ...   \n",
       "95  5.751 20.111    20.574  ... 0.655           4        1.000 2.000   \n",
       "96  7.142 57.003    52.880  ... 0.522           4        1.000 5.000   \n",
       "97 21.084 60.450    58.581  ... 0.685           3        0.667 4.000   \n",
       "98  2.908 21.844    18.997  ... 0.078           3        1.000 1.000   \n",
       "99  2.287 10.919     9.584  ... 0.071           4        1.000 2.000   \n",
       "\n",
       "    Faithfullness  Relevancy  Correctness  RSim   LLM    std  \n",
       "0             NaN      0.967        0.732 0.929 0.800 18.691  \n",
       "1           1.000      0.801        0.554 0.718 0.800  0.698  \n",
       "2           1.000      0.922        0.701 0.804 0.900  5.937  \n",
       "3           0.000      0.922        0.813 0.852 0.800  9.201  \n",
       "4           0.000      0.759        0.200 0.801 1.000  3.190  \n",
       "..            ...        ...          ...   ...   ...    ...  \n",
       "95          1.000      0.842        0.717 0.868 0.800  5.863  \n",
       "96          0.500      0.842        0.611 0.945 0.000 15.588  \n",
       "97          0.000      0.866        0.605 0.920 1.000 17.183  \n",
       "98          1.000      0.891        0.578 0.812 0.000  5.788  \n",
       "99            NaN      0.790        0.570 0.779 0.800  3.011  \n",
       "\n",
       "[100 rows x 28 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df_100.sort_values(by='std')\n",
    "\n",
    "lowest_std_examples = sorted_df.head(3)\n",
    "\n",
    "# Get the 3 examples with the highest std\n",
    "highest_std_examples = sorted_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options to ensure full display\n",
    "pd.set_option('display.max_columns', None)      # Show all columns\n",
    "pd.set_option('display.max_rows', None)         # Show all rows\n",
    "pd.set_option('display.max_colwidth', None)     # No truncation of column content\n",
    "pd.set_option('display.width', 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Examples with Lowest Standard Deviation:\n",
      "                                                                     question                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        correct_answer  \\\n",
      "68  Do they train a different training method except from scheduled sampling?                                                                                                                                                                                                                                            Answer with content missing: (list missing) \\nScheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.\\n\\nYes.   \n",
      "1                                                       What are the results?  Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \\nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.   \n",
      "71                          What baselines did they compare their model with?                                                                                                                                                                                                                                                                                                                                                                                                                                       The baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                context                     answer_8b  Rouge1  Rouge2  \\\n",
      "68                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Since the official test set has not been released publicly, results reported on the official test set have been provided by the challenge organizers. For the prototype test set and for the ablation study presented in Table TABREF24 , we use the same code for evaluation metrics as used by BIBREF11 for fairness and comparability. We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below:                           No.   0.000   0.000   \n",
      "1   FLOAT SELECTED: Table 3: Performance of various models on the ACP test set.\\nFLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.\\nAs for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states.\\nWe trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$, $\\mathcal {L}_{\\rm ACP}$, and $\\mathcal {L}_{\\rm ACP} + \\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$.               Tables 3 and 4.   0.000   0.000   \n",
      "71                                                                                                                                                                                                                                                                                                                                                                 The baseline approach is based on BIBREF20 . It divides the task of interpreting commands for behavioral navigation into two steps: path generation, and path verification. For path generation, this baseline uses a standard sequence-to-sequence model augmented with an attention mechanism, similar to BIBREF23 , BIBREF6 . For path verification, the baseline uses depth-first search to find a route in the graph that matches the sequence of predicted behaviors. If no route matches perfectly, the baseline changes up to three behaviors in the predicted sequence to try to turn it into a valid path.  BIBREF20, BIBREF23, BIBREF6.   0.000   0.000   \n",
      "\n",
      "    RougeL  Bleu  Chrf  Chrfplus  Meteor   Ter  Bert   WMS   SMS  Wisdm  Bleurt  Bart   BEM  Prometheus  Consistency  TSim  Faithfullness  Relevancy  Correctness  RSim   LLM   std  \n",
      "68   0.000 0.575 1.104     2.112   0.009 0.000 0.097 0.256 0.730    NaN  -0.729 0.017 0.076           1        0.000 0.000            NaN      0.000        0.561 0.745 0.000 0.591  \n",
      "1    0.000 0.259 2.192     2.099   0.005 0.000 0.213 0.276 0.242    NaN  -0.801 0.020 0.088           1        1.000 0.000          1.000      0.801        0.554 0.718 0.800 0.698  \n",
      "71   0.000 1.816 0.342     1.655   0.026 0.001 0.206 0.000 0.206  0.000  -0.984 0.006 0.041           3        1.000 0.000            NaN      0.833        0.680 0.720 0.000 0.838  \n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"3 Examples with Lowest Standard Deviation:\")\n",
    "print(lowest_std_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples with Highest Standard Deviation:\n",
      "                                                                                                       question                                                                                                                                                                       correct_answer  \\\n",
      "5                                                             How does their model learn using mostly raw data?                                                                            By exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity.   \n",
      "53  What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?  Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text.   \n",
      "79                                                                                     what datasets were used?                                                                                                               IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      context  \\\n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                      In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.   \n",
      "53  Pre-processing. Tweets are subject to standard preprocessing steps for text such as tokenization, stemming, lemmatization, stop-word removal, and part-of-speech tagging. Moreover, due to their noisy nature, they are also processed using some Twitter-specific techniques such as substitution/removal of URLs, of user mentions, of hashtags, and of emoticons, spelling correction, elongation normalization, abbreviation lookup, punctuation removal, detection of amplifiers and diminishers, negation scope detection, etc. For this, one typically uses Twitter-specific NLP tools such as part-of-speech and named entity taggers, syntactic parsers, etc. BIBREF47 , BIBREF48 , BIBREF49 .\\nDespite all these opportunities, the rise of social media has also presented new challenges for natural language processing (NLP) applications, which had largely relied on NLP tools tuned for formal text genres such as newswire, and thus were not readily applicable to the informal language and style of social media. That language proved to be quite challenging with its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags. In addition to the genre difference, there is also a difference in length: social media messages are generally short, often length-limited by design as in Twitter, i.e., a sentence or a headline rather than a full document. How to handle such challenges has only recently been the subject of thorough research BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 .   \n",
      "79                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German.   \n",
      "\n",
      "                                                                                                                        answer_8b  Rouge1  Rouge2  RougeL   Bleu   Chrf  Chrfplus  Meteor   Ter  Bert   WMS   SMS  Wisdm  Bleurt  Bart   BEM  Prometheus  Consistency  TSim  Faithfullness  Relevancy  Correctness  RSim   LLM    std  \n",
      "5                                              By exploiting discourse relations to propagate polarity from a small seed lexicon.   0.750   0.583   0.750 50.317 75.255    72.859   0.560 0.020 0.841 0.806 0.841  0.839   0.438 0.078 0.641           4        1.000 4.000          1.000      0.776        0.741 0.962 0.800 22.826  \n",
      "53  Informal language, creative spelling, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations.   0.529   0.457   0.529 46.127 77.978    77.041   0.502 0.011 0.768 0.918 0.768  0.718   0.359 0.297 0.124           3        1.000 4.000          1.000      0.769        0.845 0.911 0.800 23.436  \n",
      "79                                                             IWSLT14 German-English, Turkish-English, and WMT14 English-German.   0.909   0.400   0.909 33.913 84.982    82.670   0.833 0.029 0.966 0.000 0.966  0.000   0.883 0.502 0.989           4        1.000 5.000          0.667      0.918        0.848 0.991 1.000 24.392  \n"
     ]
    }
   ],
   "source": [
    "print(\"Examples with Highest Standard Deviation:\")\n",
    "print(highest_std_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAL SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100['general_score'] = df_100.select_dtypes(include=['number']).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df_100.sort_values(by='general_score')\n",
    "\n",
    "lowest_score_examples = sorted_df.head(3)\n",
    "\n",
    "# Get the 3 examples with the highest std\n",
    "highest_score_examples = sorted_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Examples with Lowest Score:\n",
      "                                                                     question                                                                                                                                                                                                                                                                                                                                              correct_answer  \\\n",
      "68  Do they train a different training method except from scheduled sampling?  Answer with content missing: (list missing) \\nScheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.\\n\\nYes.   \n",
      "71                          What baselines did they compare their model with?                                                                                                                                                                                             The baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search.   \n",
      "19         By how much does their system outperform the lexicon-based models?                              Under the retrieval evaluation setting, their proposed model + IR2 had better MRR than NVDM by 0.3769, better MR by 4.6, and better Recall@10 by  20 . \\nUnder the generative evaluation setting the proposed model + IR2 had better BLEU by 0.044 , better CIDEr by 0.033, better ROUGE by 0.032, and better METEOR by 0.029.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             context  \\\n",
      "68                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Since the official test set has not been released publicly, results reported on the official test set have been provided by the challenge organizers. For the prototype test set and for the ablation study presented in Table TABREF24 , we use the same code for evaluation metrics as used by BIBREF11 for fairness and comparability. We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below:   \n",
      "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The baseline approach is based on BIBREF20 . It divides the task of interpreting commands for behavioral navigation into two steps: path generation, and path verification. For path generation, this baseline uses a standard sequence-to-sequence model augmented with an attention mechanism, similar to BIBREF23 , BIBREF6 . For path verification, the baseline uses depth-first search to find a route in the graph that matches the sequence of predicted behaviors. If no route matches perfectly, the baseline changes up to three behaviors in the predicted sequence to try to turn it into a valid path.   \n",
      "19  NVDM (Lexical, Neural) is a VAE-based approach for document modeling BIBREF10 . We compare our model with this baseline to demonstrate the effect of modeling topic.\\nTable TABREF31 shows the performance of our models and the baselines in retrieval evaluation. We first compare our proposed model with other popular unsupervised methods, including TF-IDF, LDA, and NVDM. TF-IDF retrieves the comments by similarity of words rather than the semantic meaning, so it achieves low scores on all the retrieval metrics. The neural variational document model is based on the neural VAE framework. It can capture the semantic information, so it has better performance than the TF-IDF model. LDA models the topic information, and captures the deeper relationship between the article and comments, so it achieves improvement in all relevance metrics. Finally, our proposed model outperforms all these unsupervised methods, mainly because the proposed model learns both the semantics and the topic information.\\nFLOAT SELECTED: Table 2: The performance of the unsupervised models and supervised models under the retrieval evaluation settings. (Recall@k, MRR: higher is better; MR: lower is better.)\\nTable TABREF32 shows the performance for our models and the baselines in generative evaluation. Similar to the retrieval evaluation, our proposed model outperforms the other unsupervised methods, which are TF-IDF, NVDM, and LDA, in generative evaluation. Still, the supervised IR achieves better scores than the seq2seq model. With the help of our proposed model, both IR and S2S achieve an improvement under the semi-supervised scenarios.\\nFLOAT SELECTED: Table 3: The performance of the unsupervised models and supervised models under the generative evaluation settings. (METEOR, ROUGE, CIDEr, BLEU: higher is better.)   \n",
      "\n",
      "                       answer_8b  Rouge1  Rouge2  RougeL      Bleu      Chrf  Chrfplus    Meteor       Ter      Bert       WMS       SMS  Wisdm    Bleurt      Bart       BEM  Prometheus  Consistency  TSim  Faithfullness  Relevancy  Correctness      RSim  LLM  general_score  \n",
      "68                           No.     0.0     0.0     0.0  0.574979  1.103753  2.112073  0.008787  0.000192  0.097487  0.255941  0.730304    NaN -0.728632  0.016909  0.076004           1          0.0   0.0            NaN   0.000000     0.561127  0.744508  0.0       0.312068  \n",
      "71  BIBREF20, BIBREF23, BIBREF6.     0.0     0.0     0.0  1.816085  0.341530  1.655153  0.025641  0.001498  0.205611  0.000000  0.205611    0.0 -0.983638  0.005503  0.041070           3          1.0   0.0            NaN   0.833043     0.680005  0.720103  0.0       0.433964  \n",
      "19                Not specified.     0.0     0.0     0.0  0.565776  4.019979  3.859605  0.008636  0.000357  0.084858  0.203294  0.092457    NaN -0.944062  0.009726  0.078579           1          0.0   0.0            NaN   0.000000     0.182176  0.728705  0.0       0.470956  \n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"3 Examples with Lowest Score:\")\n",
    "print(lowest_score_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Examples with Hihgest Score:\n",
      "                                                                                                       question                                                                                                                                                                       correct_answer  \\\n",
      "5                                                             How does their model learn using mostly raw data?                                                                            By exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity.   \n",
      "53  What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?  Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text.   \n",
      "79                                                                                     what datasets were used?                                                                                                               IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      context  \\\n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                      In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.   \n",
      "53  Pre-processing. Tweets are subject to standard preprocessing steps for text such as tokenization, stemming, lemmatization, stop-word removal, and part-of-speech tagging. Moreover, due to their noisy nature, they are also processed using some Twitter-specific techniques such as substitution/removal of URLs, of user mentions, of hashtags, and of emoticons, spelling correction, elongation normalization, abbreviation lookup, punctuation removal, detection of amplifiers and diminishers, negation scope detection, etc. For this, one typically uses Twitter-specific NLP tools such as part-of-speech and named entity taggers, syntactic parsers, etc. BIBREF47 , BIBREF48 , BIBREF49 .\\nDespite all these opportunities, the rise of social media has also presented new challenges for natural language processing (NLP) applications, which had largely relied on NLP tools tuned for formal text genres such as newswire, and thus were not readily applicable to the informal language and style of social media. That language proved to be quite challenging with its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags. In addition to the genre difference, there is also a difference in length: social media messages are generally short, often length-limited by design as in Twitter, i.e., a sentence or a headline rather than a full document. How to handle such challenges has only recently been the subject of thorough research BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 .   \n",
      "79                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German.   \n",
      "\n",
      "                                                                                                                        answer_8b    Rouge1    Rouge2    RougeL       Bleu       Chrf   Chrfplus    Meteor       Ter      Bert       WMS       SMS     Wisdm    Bleurt      Bart       BEM  Prometheus  Consistency  TSim  Faithfullness  Relevancy  Correctness      RSim  LLM  general_score  \n",
      "5                                              By exploiting discourse relations to propagate polarity from a small seed lexicon.  0.750000  0.583333  0.750000  50.317476  75.254881  72.859045  0.559797  0.019608  0.840717  0.805534  0.840717  0.838698  0.438479  0.078107  0.641318           4          1.0   4.0       1.000000   0.776334     0.740618  0.962471  0.8       9.515528  \n",
      "53  Informal language, creative spelling, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations.  0.529412  0.457143  0.529412  46.127047  77.978153  77.041078  0.501794  0.010654  0.767577  0.917855  0.767577  0.718430  0.359360  0.297275  0.123874           3          1.0   4.0       1.000000   0.769482     0.845463  0.911262  0.8       9.541428  \n",
      "79                                                             IWSLT14 German-English, Turkish-English, and WMT14 English-German.  0.909091  0.400000  0.909091  33.913261  84.981801  82.670462  0.833333  0.029126  0.966236  0.000000  0.966237  0.000000  0.882604  0.502202  0.988759           4          1.0   5.0       0.666667   0.917554     0.847838  0.991353  1.0       9.711983  \n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(\"3 Examples with Hihgest Score:\")\n",
    "print(highest_score_examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_eval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
