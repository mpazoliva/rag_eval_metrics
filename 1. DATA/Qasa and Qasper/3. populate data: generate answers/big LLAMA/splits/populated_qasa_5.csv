question,iris_answer,correct_answer,context
Do the authors use gradient accumulation while training the model?,"NO.

The authors do not use gradient accumulation while training the model. Instead, they use backpropagation through time (BPTT) to enable gradient propagation for large input sequences, and they divide the document into fixed-length batches of size b to make fine-tuning a classifier for large documents feasible",This work uses Backpropagation Through Time for Text Classification (BPT3C) where a document is divided into fixed-length batches and the gradients are back-propagated to batches. Whether backpropagation of gradients is the same as gradient accumulation or whether gradient accumulation was used in this work cannot be answered from this paper.,"Language models are trained with backpropagation through time (BPTT) to enable gradient propagation for large input sequences. In order to make fine-tuning a classifier for large documents feasible, we propose BPTT for Text Classification (BPT3C): We divide the document into fixed-length batches of size b. At the beginning of each batch, the model is initialized with the final state of the previous batch; we keep track of the hidden states for mean and max-pooling; gradients are back-propagated to the batches whose hidden states contributed to the final prediction. In practice, we use variable length backpropagation sequences Merity et al. (2017a)."
What is the difference in error rate on IMDb dataset with and without pretraining?,"Sure! Here's my answer:

With pretraining, the error rate on the IMDb dataset is significantly lower (1.4% vs 3.1%) compared to without pretraining","The authors compared using no pretraining with pretraining on WikiText-103 and showed that pretraining was useful in improving performance for small to larg-esized datasets. The results are found in Table 4, and the difference in error rate with and without pretraining on the IMDb dataset could be answered with this table.","We compare using no pretraining with pretraining on WikiText-103 Merity et al. (2017b) in Table 4. Pretraining is most useful for small and medium-sized datasets, which are most common in commercial applications. However, even for large datasets, pretraining improves performance."
"In language models, which method would be better for preventing overfitting from batch normalization and dropout?","Sure! Here's the answer to your question based on the provided context:

DROPOUT","According to this work, without dropout, a vanilla LM can run the risk of overfitting, which decreases performance. The effect that batch normalization has on preventing overfitting and, then, whether it is more effective than dropout cannot be answered in this paper.","In order to gauge the importance of choosing an appropriate LM, we compare a vanilla LM with the same hyperparameters without any dropout999To avoid overfitting, we only train the vanilla LM classifier for 5 epochs and keep dropout of 0.4 in the classifier. with the AWD-LSTM LM with tuned dropout parameters in Table 5. Using our fine-tuning techniques, even a regular LM reaches surprisingly good performance on the larger datasets. On the smaller TREC-6, a vanilla LM without dropout runs the risk of overfitting, which decreases performance."
Are (1) slanted triangular learning rate and (2) linear warmup followed by linear decay the same thing?,"No.

Slanted triangular learning rates (STLR) and linear warmup followed by linear decay are not the same thing, although they both involve adjusting the learning rate during training. STLR involves a short increase and a long decay period, while linear warmup followed by linear decay is a more gradual increase and decrease of the learning rate over time","Slanted triangular learning rate (SLTR) involves first linearly increasing the learning rate and then linearly decaying it according to a given update schedule. It modifies triangular learning rates by using a short increase and long decay period. The function of linear warmup and linear decay cannot be found in this paper. Thus, the difference between SLTR and these cannot be answered in this paper.","For adapting its parameters to task-specific features, we would like the model to quickly converge to a suitable region of the parameter space in the beginning of training and then refine its parameters. Using the same learning rate (LR) or an annealed learning rate throughout training is not the best way to achieve this behaviour.Instead, we propose slanted triangular learning rates (STLR), which first linearly increases the learning rate and then linearly decays it according to the following update schedule, which can be seen in Figure 2:\begin{split}cut&=\lfloor T\cdot cut\_frac\rfloor\\p&=\begin{cases}t/cut,&\text{if}\ t<cut\\1-\frac{t-cut}{cut\cdot(1/cut\_frac-1)},&\text{otherwise}\end{cases}\\\eta_{t}&=\eta_{max}\cdot\frac{1+p\cdot(ratio-1)}{ratio}\end{split}(3)where T is the number of training iterations555In other words, the number of epochs times the number of updates per epoch., cut\_frac is the fraction of iterations we increase the LR, cut is the iteration when we switch from increasing to decreasing the LR, p is the fraction of the number of iterations we have increased or will decrease the LR respectively, ratio specifies how much smaller the lowest LR is from the maximum LR \eta_{max}, and \eta_{t} is the learning rate at iteration t. We generally use cut\_frac=0.1, ratio=32 and \eta_{max}=0.01. STLR modifies triangular learning rates Smith (2017) with a short increase and a long decay period, which we found key for good performance.666We also credit personal communication with the author. In Section 5, we compare against aggressive cosine annealing, a similar schedule that has recently been used to achieve state-of-the-art performance in CV Loshchilov and Hutter (2017).777While Loshchilov and Hutter (2017) use multiple annealing cycles, we generally found one cycle to work best."
What are some other approaches for semantic similarity and how do they differ to Sentence Transformers in architecture and performance?,"Other approaches for semantic similarity include generate and retrieve (Mao et al., 2021) methods, which differ from Sentence Transformers in architecture and performance","The authors mention that they experiment with using a sentence transformer (Reimers and Gurevych, 2019) and a custom Seq2Seq model called GUD-IR for their retrieval function. The paper does not contain any information on any other models (apart from these two) that could be used for semantic similarity.","A retrieved past feedback that is incorrect might cause the model to make a mistake, thus necessitating a good retrieval function. We propose a two-stage method for effective retrieval involving: transforming \mathbf{x}, followed by a similarity lookup of the transformed \mathbf{x} in \mathcal{M}. When the task involves high surface-level similarity among past feedback, such as in lexical word tasks, then a simple heuristic-based transformation is sufficient.However, such simple transformations are insufficient for tasks that involves more complex retrieval e.g., when two lexically dissimilar situations can share the same understanding.For example, consider two situations from ert-nl: Filling a false time sheet at work and Being at a party, and telling parents I am studying.These situations look lexically dissimilar but correspond to the same underlying social principle lying to authority.In our experiments, off-the-shelf methods failed to address these challenges (see §4 later). After the transformation stage, the closest matching entry is then used as the corresponding \mathbf{fb}. Transformation reduces \mathcal{M}(\mathbf{x}) to a search over \mathbf{fb}_{1},\mathbf{fb}_{2},\ldots,\mathbf{fb}_{|\mathcal{M}|} with \hat{\mathbf{fb}} as the search query. We compute similarity based on a fine-tuned Sentence transformers (Reimers and Gurevych, 2019). To address these challenges with transformation in complex tasks, we have designed a novel SEQ 2 SEQ based transformation called GUD - IR. Given x, GUD - IR generates a transformed feedback fb for x using a generative SEQ 2 SEQ model. Our approach is inspired and supported by the recent success of generate and retrieve (Mao et al., 2021) methods. However, despite the similarity, the methods have different goals: Mao et al. (2021) leverage generative models for query expansion, whereas our goal is explainable input understanding. See Appendix B for more details on GUD - IR."
How accurate or correct was their few-shot approach to making GPT-3 verbalize its understanding?,"Partially accurate. The approach allowed users to detect and correct the model's understanding of their intent, but the model's actual answers were not always improved","The authors mention in multiple places how their iterative correction/feedback process depends on GPT verbalizing it's thinking process or understanding of the user's inputs or needs. They explain how they encourage this sort of behaviour through modifying the prompt, but this paper does not seem to quantifiably measure how ""accurate"" this verbalization would be.","We operationalize this idea by including task verbalization in the prompt (Fig. 3).Given a question What sounds like < sighted > ?, a vanilla prompting approach will generate the answer cited.In contrast, we include a \mathbf{u} the homophone for in the prompt.gpt-3 is adept at reasoning with just a handful of examples, and thus can be expected to mimic the prompt to generate task understanding and answer.gpt-3 generates such additional information in all our tasks.Given a test question What sounds similar to < sighted > ?, if the model generates the word that has the same meaning as \mathbf{u}, the user has a reason to believe that the answer is wrong. Figure 1 presents a sample interaction between a user and gpt-3 that our setup enables.The model was asked for a similar word. However, the model’s (incorrect) task understanding \mathbf{u} was “The homophone of good is”.The user can detect such discrepancy between the intended and interpreted task instruction, and can provide feedback \mathbf{fb} as ""similar to means with a similar meaning"", clarifying that they actually wanted a synonym.Crucially, note that such instructional correction is feasible even if the user does not knowthe correct answer to their question, as they are critiquing the model’s understanding of theirintent, rather than the answers themselves.Thus, our setup does not require the users to be experts at tasks being solved, another advantage of our approach. We present \ours, a novel, memory-enhanced gpt-3 that allows users to interact and improve the model without retraining. A key insight is to have the model articulate not just its answer but also its understanding of the user’s intent, providing an avenue for feedback.We show that deployed systems with fixed large-language models can still be improved by interacting with end-users, potentially improving their performance and broadening their utility. In Tandon et al. (2022), we show that using a memory of user feedback can be used to repair erroneous model in a supervised setting.In this work, we build upon the recent advances in few-shot prompting to modify gpt-3’s behavior by adding user feedback to the query (prompt).Like others, we use gpt-3 with few-shot prompting, where the prompt consistsof a prefix prefix containing a few input-output “training” examples of the task, followed by the input x, e.g., a question,to operate on. However, while prior work has focused on constructing better prefixes, e.g., dynamically selecting good “training” examplesbased on the question Le Scao and Rush (2021); Liu et al. (2021a), or even representing the prefix latently Li and Liang (2021),our work elaborates the input x itself to clarify the intended task, by adding user feedback fb from previous misunderstandings."
"Instead of editing the prompt based on previous cases, did they test a simpler approach where previous cases are concatenated to the prompt?",No,"Yes, they did. This is the GROW-PROMPT baseline mentioned in the paper - they add the latest few samples in memory to the prompt.","We compare \ours(memory-assisted prompt editing) with two baselines:\bulletno-mem This is the standard gpt-3 444We use gpt-3-175b (davinci) for all experiments. in few-shot prompting mode (hyper-parameters listed in Appendix §C). Input is \mathbf{p}\ \#\ \mathbf{x}_{i} (i.e., question \mathbf{x}_{i} appended to prompt \mathbf{p}).It generates answer \mathbf{y}_{i} and its understanding of the user’s intent \mathbf{u}_{i}.\bulletgrow-prompt: Similar to no-mem, but the \mathbf{p} is continuously grown with a subset of memory \mathcal{M} that can fit within the prompt (max. 2048 tokens).The most recent subset of \mathcal{M} of memory inserted is inserted in the prompt.The ethical reasoning tasks (ert) involve long examples, and the initial prompt itself takes close to the max allowed tokens.Thus, the grow-prompt setup is only provided for the lexical relations and word scrambling tasks."
Why does the approach need a gating mechanism when a good retrieval should be able to correctly filter out irrelevant feedback from the memory?,"The approach needs a gating mechanism because a good retrieval function alone may not be sufficient to filter out irrelevant feedback from the memory, especially in tasks with high surface-level similarity among past feedback. The gating mechanism helps to ignore irrelevant feedback and ensure that the model is not misled by incorrect or harmful feedback","A gating function is needed precisely because the retrieval function might not be able to filter out irrelevant feedback from memory. This is a challenging thing to implement since syntactically or lexically similar things might or might not refer to similar concepts. Another challenge is with adversarial feedback, made by users intending to mess with the system. It is true that a gating function in the combiner would not be required if the retrieval function was near perfect at eliminating irrelevant feedback. Regardless, some form of filtering has to be done at some stage - whether it's in the lookup/retrieval function or in the combiner function does not specifically matter, but the existence of a filtering mechanism is crucial for MemPrompt-like systems to work.",": A gating function allowing irrelevant, retrieved feedback to be ignored. A retrieved past feedback that is incorrect might cause the model to make a mistake, thus necessitating a good retrieval function. We propose a two-stage method for effective retrieval involving: transforming \mathbf{x}, followed by a similarity lookup of the transformed \mathbf{x} in \mathcal{M}. When the task involves high surface-level similarity among past feedback, such as in lexical word tasks, then a simple heuristic-based transformation is sufficient.However, such simple transformations are insufficient for tasks that involves more complex retrieval e.g., when two lexically dissimilar situations can share the same understanding.For example, consider two situations from ert-nl: Filling a false time sheet at work and Being at a party, and telling parents I am studying.These situations look lexically dissimilar but correspond to the same underlying social principle lying to authority.In our experiments, off-the-shelf methods failed to address these challenges (see §4 later). Extending the discussion on noise in feedback, our setting assumes that users will not provide any adversarial feedback. However, in real-world environments, this assumption is unlikely to hold. Additionally, there is a risk in the real-world deployment of our system, wherein an adversarial user might provide harmful feedback, thus maliciously controlling the systems (potentially a home-based robot) where our method is deployed. Thus, robust mechanisms such as gud-ir and memory adapters will be critical for successful real-world deployments."
"What are a couple of examples of ""lexical QA tasks""?","Sure! Here are a couple of examples of ""lexical QA tasks"" based on the context you provided:

Synonym and antonym prediction tasks","The five kinds of lexical question-and-answer tasks that the authors mention are: synonyms, antonyms, homophones, definitions and sentence usage generation. An example of a synonym task could be ""what is a word that has the same meaning as encumbrance"".","We apply our approach to four tasks: (1) lexical relations (e.g., antonyms, Figure 1),(2) word scrambling (e.g., anagrams), (3) ethics (with user feedback being the appropriate class of ethicalconsideration), and (4) ethics (with user feedback being natural language).For all five tasks, the dataset consists of (\mathbf{x},\mathbf{fb}\rightarrow\mathbf{u},\mathbf{y}) tuples, where \mathbf{fb} clarifies the task in \mathbf{x}.We have a simulated conversational setting, in which a user can ask the model \mathbf{x} (covering any of these five tasks). If the model gives a wrong answer to query \mathbf{x}, then \mathbf{fb} is used as the simulated corrective feedback.The sources for these datasets are listed in Appendix  §E. The lexical relation task is to predict a word with a given lexical relationship to an input word.We use five relationships: synonym (syn), antonym (ant), homophone (hom), definition (defn), and sentence usage generation (sent)."
Why does the approach not simply add all feedback examples in memory to the prompt if they will be adding examples anyways?,"Sure! Here's the answer to your question based on the provided context:

The approach does not simply add all feedback examples in memory to the prompt because it would not be effective and would not scale beyond the 2048 tokens limit. While adding all feedback examples to the prompt may seem like a straightforward solution, it would result in a large and unwieldy prompt that would be difficult to process and would exceed the maximum token limit. Additionally, the approach of using failure-driven retrieval and non-selective memory has been found to be more effective and efficient in improving performance","The proposed approach (MemPrompt) probably does not add all feedback examples in memory to the prompt since the size of the prompt is limited to 2048 tokens. Additionally, increasing the size of the prompt leads to higher cost (in terms of compute resources required to process the query).","Figure 8 reports the overall performance on the word reasoning tasks.The accuracy improves substantially within 300 examples when using memory (in yellow) vs. no memory (in blue).Note that we are operating in a few-shot prompting regime (i.e., there is no training data over which we train).The performance of grow-prompt (red) lies in between, showing that non-selective memory is partially helpful, although not as effective as failure-driven retrieval (our model).However, grow-prompt is \sim 3x more expensive (larger prompts) and cannot scale beyond the 2048 tokens limit.We also found that the retrieved feedback from memory was effective 97% of the time; only in \approx 3% of cases feedback had no positive effect."
"What if, instead of concatenating the feedback to the prompt, the prompt was automatically edited according to the feedback?","Our approach can also be seen as an instance of prompt engineering, where the prompt is edited based on user feedback","Strictly speaking, the proposed approach is editing the prompt - even additions or concatenations to a user prompt also count as ""editing"" the prompt. However, the current paper contains no information on what would happen if the prompt were somehow edited without concatenation.","Our approach, \ours, pairs gpt-3 with a growing memory of cases where the model misunderstood user’s intent and was provided with corrective feedback.This feedback is question dependent, and thus the prompt for each sample is edited to adapt to the input.In this sense, our work can be seen as an instance of prompt engineering Liu et al. (2021b) which involves editing the prompts. Our work adds interactivity to prompt engineering as it involves dynamically updating the prompt for every instance. Once the feedback is received from the user, can the model successfully utilize it? By adding a few examples of the form \mathbf{x},\mathbf{fb}\rightarrow\mathbf{u},\mathbf{y} in the prompt and setting \mathbf{fb}=\mathbf{u}, we force the model to use the task understanding present in the input when generating the output (Figure 4).Recently, it has been shown that such repetition plays a crucial role in the success of few-shot prompting models (Madaan and Yazdanbakhsh, 2022). Recent work such as Liu et al. (2021a) investigate using dynamic prompts for better generation. For a given input \mathbf{x}, their method( kate) relies on retrieving examples from the training set that are similar to \mathbf{x} for dynamically creating the prompt \mathbf{p}. Note that our method edits \mathbf{x} with a feedback \mathbf{fb}, and is thus complementary to kate.We verify this by experiments on ert-cat and ert-nl. Specifically, we create dynamic prompts using kate, whereas \oursis used like before to attach a \mathbf{fb} to the question. We observe a consistent 10% improvement by using kate across all baselines, showing that the improvements are complementary. In Tandon et al. (2022), we show that using a memory of user feedback can be used to repair erroneous model in a supervised setting.In this work, we build upon the recent advances in few-shot prompting to modify gpt-3’s behavior by adding user feedback to the query (prompt).Like others, we use gpt-3 with few-shot prompting, where the prompt consistsof a prefix prefix containing a few input-output “training” examples of the task, followed by the input x, e.g., a question,to operate on. However, while prior work has focused on constructing better prefixes, e.g., dynamically selecting good “training” examplesbased on the question Le Scao and Rush (2021); Liu et al. (2021a), or even representing the prefix latently Li and Liang (2021),our work elaborates the input x itself to clarify the intended task, by adding user feedback fb from previous misunderstandings."
How did they edit the prompt based on previous examples? Was it by editing the original text of the prompt or simply by concatenating the examples?,"By concatenating the examples. The authors edited the prompt by concatenating the input question with the feedback provided for similar questions in the past, forcing the model to use the task understanding present in the input when generating the output","They seem to be including memory in prompts by adding the natural language feedback (fb) that users provide on a prompt (x) and it's response (u) by including the tuple (x, u, fb) in a structured format. It does seem like they are merely concatenating multiple of these tuples and adding them to the prompt, but the exact format of the prompt itself is not fully explained in the paper.","In our setup, given an input \mathbf{x}, a model generates an output \mathbf{y} and a sentence \mathbf{u} expressing its understanding of the task, a skill learned through few-shot examples in theprompt (Appendix D).The user can then critique \mathbf{u} by providing natural language feedback \mathbf{fb}. This is feasible even if the user does not know the correctness of \mathbf{y} because they are critiquing the model’s understanding of their intent rather the answers themselves.  As mentioned, given an input \mathbf{x}, we prompt the model to generate an output \mathbf{y} and a sentence \mathbf{u} expressing its understanding of the task.Thus, the in-context examples for \oursare of the form \mathbf{x}\rightarrow\mathbf{u},\mathbf{y}.In addition to the input \mathbf{x}, \oursretrieves a \mathbf{fb} if a question similar to \mathbf{x} has been asked before.To enable the model to react to such feedback, we also include examples of the form (\mathbf{x},\mathbf{fb}\rightarrow\mathbf{u},\mathbf{y}) in the prompt, which are aimed to teach the model to react to \mathbf{fb} (Appendix D). Our approach, \ours, pairs gpt-3 with a growing memory of cases where the model misunderstood user’s intent and was provided with corrective feedback.This feedback is question dependent, and thus the prompt for each sample is edited to adapt to the input.In this sense, our work can be seen as an instance of prompt engineering Liu et al. (2021b) which involves editing the prompts. Our work adds interactivity to prompt engineering as it involves dynamically updating the prompt for every instance. Once the feedback is received from the user, can the model successfully utilize it? By adding a few examples of the form \mathbf{x},\mathbf{fb}\rightarrow\mathbf{u},\mathbf{y} in the prompt and setting \mathbf{fb}=\mathbf{u}, we force the model to use the task understanding present in the input when generating the output (Figure 4).Recently, it has been shown that such repetition plays a crucial role in the success of few-shot prompting models (Madaan and Yazdanbakhsh, 2022). Further, it is desirable to have a system that can leverage past feedback on new, unseen examples for prompt-editing. We maintain a memory \mathcal{M} of such feedback as a set of key-value pairs, where thekey is a misunderstood question, and the value is the user’s feedback to correct that misunderstanding. Given a new question, we check if the model has made a mistakeon a similar question earlier, by querying the memory for a similar question. If found,append the corresponding feedback to the question prompt. This mechanism aims toprevent the model from making the same type of mistake twice. This failure-driven remindingmechanism draws inspiration from the theory of recursive reminding in psychology Jacoby and Wahlheim (2013),which suggests humans index error corrections in the context in which those errors occurred. Recent work such as Liu et al. (2021a) investigate using dynamic prompts for better generation. For a given input \mathbf{x}, their method( kate) relies on retrieving examples from the training set that are similar to \mathbf{x} for dynamically creating the prompt \mathbf{p}. Note that our method edits \mathbf{x} with a feedback \mathbf{fb}, and is thus complementary to kate.We verify this by experiments on ert-cat and ert-nl. Specifically, we create dynamic prompts using kate, whereas \oursis used like before to attach a \mathbf{fb} to the question. We observe a consistent 10% improvement by using kate across all baselines, showing that the improvements are complementary."
Is this work focused only on solving cases where GPT-3 misunderstands the users' intents?,"No. While the work focuses on improving GPT-3's understanding of users' intents, it is not limited to solving cases where the model misunderstands the intent. The approach can also be used to improve the model's performance on tasks where it initially performs well, by providing feedback on the user's intended task","Yes, this work is primarily focused on solving cases when GPT-3 misunderstands user input. The authors' do discuss one specialized use case, on how memory assisted models such as these could be used to personalize models, but even this use-case could be seen as a subset of the broader use-case of users correcting a model's misunderstanding.","Finally, our work is a simple example of debugging and learning via dialog. While system debugging through dialogue has been explored in many contexts (Hixon et al., 2015; Wang et al., 2016; Davis, 1977), our contribution is a dialogue about the model’s understanding of the user’s intent. Our approach, \ours, pairs gpt-3 with a growing memory of cases where the model misunderstood user’s intent and was provided with corrective feedback.This feedback is question dependent, and thus the prompt for each sample is edited to adapt to the input.In this sense, our work can be seen as an instance of prompt engineering Liu et al. (2021b) which involves editing the prompts. Our work adds interactivity to prompt engineering as it involves dynamically updating the prompt for every instance. Figure 1 presents a sample interaction between a user and gpt-3 that our setup enables.The model was asked for a similar word. However, the model’s (incorrect) task understanding \mathbf{u} was “The homophone of good is”.The user can detect such discrepancy between the intended and interpreted task instruction, and can provide feedback \mathbf{fb} as ""similar to means with a similar meaning"", clarifying that they actually wanted a synonym.Crucially, note that such instructional correction is feasible even if the user does not knowthe correct answer to their question, as they are critiquing the model’s understanding of theirintent, rather than the answers themselves.Thus, our setup does not require the users to be experts at tasks being solved, another advantage of our approach. This paper sets out the general architecture and a simple implementation of its components.We then demonstrate the system on four tasks, using simulated user feedback:(1) lexical relations (e.g., antonyms, Figure 1),(2) word scrambling (e.g., anagrams), (3) ethics (with user feedback being the appropriate class of ethicalconsideration, e.g., “it is about cheating”, using a small set of categories), and (4) ethics (with user feedback beingnatural language). We find that in all cases, gpt-3’s accuracy significantly increases with time, without retraining,as our approach enables it to use corrective feedback from earlier examples to avoid similar misunderstandings on future examples. In summary, our contributions are:\bulletWe show that a large model like gpt-3 can be improved after deployment, without retraining, through a memory-assisted architecture.\bulletOur implementation, \ours, is the first demonstration that this is possible - this is an important step forward for real use of LMs, and the paper sets out a general architecture that others can build on, a specific implementation, and detailed evaluation on multiple tasks. We demonstrate an application of \oursfor personalization with a use-case where user language preferences can be folded in the memory. We simulate a user who does not speak fluent English and uses code-mixed language. The queries posed by the user contain words from two Indian languages: Hindi and Punjabi. gpt-3 predictably misunderstands the task. The user clarifies the meanings of their dialect/language phrases. While initial queries fail, subsequent queries that reuse similar words succeed because their clarifications are present in the memory (details in Appendix §G). We present \ours, a novel, memory-enhanced gpt-3 that allows users to interact and improve the model without retraining. A key insight is to have the model articulate not just its answer but also its understanding of the user’s intent, providing an avenue for feedback.We show that deployed systems with fixed large-language models can still be improved by interacting with end-users, potentially improving their performance and broadening their utility. In Tandon et al. (2022), we show that using a memory of user feedback can be used to repair erroneous model in a supervised setting.In this work, we build upon the recent advances in few-shot prompting to modify gpt-3’s behavior by adding user feedback to the query (prompt).Like others, we use gpt-3 with few-shot prompting, where the prompt consistsof a prefix prefix containing a few input-output “training” examples of the task, followed by the input x, e.g., a question,to operate on. However, while prior work has focused on constructing better prefixes, e.g., dynamically selecting good “training” examplesbased on the question Le Scao and Rush (2021); Liu et al. (2021a), or even representing the prefix latently Li and Liang (2021),our work elaborates the input x itself to clarify the intended task, by adding user feedback fb from previous misunderstandings."
"With ""memory of cases"" here, do they simply mean a prompt that contains all of these cases as examples?","No. The ""memory of cases"" refers to a set of key-value pairs where the keys are misunderstood questions and the values are the corresponding user feedback to correct those misunderstandings. The model is able to leverage this memory to correct its mistakes on new, unseen examples. The prompt itself does not contain all of these cases as examples, but rather the model is able to use the memory to inform its understanding of the task","Yes, ""memory of cases"" in this context does mean a prompt with all of these relevant cases as examples listed out. However, the MemPrompt model's input size is limited to 2048-tokens, so adding all possible matches to the prompt would not be possible, which is why the authors' proposed approach specifically focuses on selecting which prompts to include.","As mentioned, given an input \mathbf{x}, we prompt the model to generate an output \mathbf{y} and a sentence \mathbf{u} expressing its understanding of the task.Thus, the in-context examples for \oursare of the form \mathbf{x}\rightarrow\mathbf{u},\mathbf{y}.In addition to the input \mathbf{x}, \oursretrieves a \mathbf{fb} if a question similar to \mathbf{x} has been asked before.To enable the model to react to such feedback, we also include examples of the form (\mathbf{x},\mathbf{fb}\rightarrow\mathbf{u},\mathbf{y}) in the prompt, which are aimed to teach the model to react to \mathbf{fb} (Appendix D). Our approach, \ours, pairs gpt-3 with a growing memory of cases where the model misunderstood user’s intent and was provided with corrective feedback.This feedback is question dependent, and thus the prompt for each sample is edited to adapt to the input.In this sense, our work can be seen as an instance of prompt engineering Liu et al. (2021b) which involves editing the prompts. Our work adds interactivity to prompt engineering as it involves dynamically updating the prompt for every instance. Once the feedback is received from the user, can the model successfully utilize it? By adding a few examples of the form \mathbf{x},\mathbf{fb}\rightarrow\mathbf{u},\mathbf{y} in the prompt and setting \mathbf{fb}=\mathbf{u}, we force the model to use the task understanding present in the input when generating the output (Figure 4).Recently, it has been shown that such repetition plays a crucial role in the success of few-shot prompting models (Madaan and Yazdanbakhsh, 2022). Further, it is desirable to have a system that can leverage past feedback on new, unseen examples for prompt-editing. We maintain a memory \mathcal{M} of such feedback as a set of key-value pairs, where thekey is a misunderstood question, and the value is the user’s feedback to correct that misunderstanding. Given a new question, we check if the model has made a mistakeon a similar question earlier, by querying the memory for a similar question. If found,append the corresponding feedback to the question prompt. This mechanism aims toprevent the model from making the same type of mistake twice. This failure-driven remindingmechanism draws inspiration from the theory of recursive reminding in psychology Jacoby and Wahlheim (2013),which suggests humans index error corrections in the context in which those errors occurred. In Tandon et al. (2022), we show that using a memory of user feedback can be used to repair erroneous model in a supervised setting.In this work, we build upon the recent advances in few-shot prompting to modify gpt-3’s behavior by adding user feedback to the query (prompt).Like others, we use gpt-3 with few-shot prompting, where the prompt consistsof a prefix prefix containing a few input-output “training” examples of the task, followed by the input x, e.g., a question,to operate on. However, while prior work has focused on constructing better prefixes, e.g., dynamically selecting good “training” examplesbased on the question Le Scao and Rush (2021); Liu et al. (2021a), or even representing the prefix latently Li and Liang (2021),our work elaborates the input x itself to clarify the intended task, by adding user feedback fb from previous misunderstandings."
Where do the authors source their labelled dataset from? ,CNN and Daily Mail websites,"The source of the labelled dataset in the paper is two news websites, namely, CNN and Daily News. The authors created the dataset of approximately one million data points from ~93k CNN and ~220k Daily Mail online news articles.","In this work we seek to directly address the lack of real natural languagetraining data by introducing a novel approach to building a supervised readingcomprehension data set. We observe that summary and paraphrase sentences, withtheir associated documents, can be readily converted to context–query–answertriples using simple entity detection and anonymisation algorithms.Using this approach we have collected two new corpora of roughly a million newsstories with associated queries from the CNN and Daily Mail websites. Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document–query–answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets—and to apply this method to other sources—isavailable online333http://www.github.com/deepmind/rc-data/."
What is the ratio of the total number of articles collected from CNN and Daily News?,"3:4, or 75:25","Assuming “Daily News” here refers to “Daily Mail”, one of the websites the authors sourced the data from, the ratio of CNN:(Daily Mail) articles is approximately 93:220 or 1:2.36.","Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document–query–answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets—and to apply this method to other sources—isavailable online333http://www.github.com/deepmind/rc-data/."
In what ways does the authors' approach differ from how practitioners create datasets and pretrain models such as GPT?,"The authors' approach differs from traditional dataset creation and pre-training of models like GPT in several ways:

1. Supervised vs. Unsupervised: The authors propose a supervised approach to building a reading comprehension data set, while most previous work has relied on unsupervised methods.
2. Use of Attention Mechanism: The authors incorporate an attention mechanism in their model, which is not typically found in GPT-like models.
3. Large-Scale Corpora: The authors collect and use large corpora of document-query-answer triples for training, whereas GPT-like models are typically pre-trained on much smaller datasets","The authors mention that their approach, which involves creating a labelled dataset and using that for supervised learning objectives, differs from existing work in the field which focusses on unsupervised approaches. Authors claim that unsupervised approaches are explored more because of the difficulties and challenges associated with building labelled datasets. However, this paper does not contain information on GPT-style models specifically, so answering how the pretrianing process for GPT differs from the author’s approach is not possible from information in this paper.","In this work we seek to directly address the lack of real natural languagetraining data by introducing a novel approach to building a supervised readingcomprehension data set. We observe that summary and paraphrase sentences, withtheir associated documents, can be readily converted to context–query–answertriples using simple entity detection and anonymisation algorithms.Using this approach we have collected two new corpora of roughly a million newsstories with associated queries from the CNN and Daily Mail websites. The supervised paradigm for training machine reading and comprehension modelsprovides a promising avenue for making progress on the path to building fullnatural language understanding systems. We have demonstrated a methodology forobtaining a large number of document-query-answer triples and shown thatrecurrent and attention based neural networks provide an effective modellingframework for this task.Our analysis indicates that the Attentive and Impatient Readers are able topropagate and integrate semantic information over long distances. In particularwe believe that the incorporation of an attention mechanism is the keycontributor to these results. Such an approach requires a large training corpus of document–query–answertriples and until now such corpora have been limited to hundreds of examples andthus mostly of use only for testing [9]. This limitationhas meant that most work in this area has taken the form of unsupervisedapproaches which use templates or syntactic/semantic analysers to extractrelation tuples from the document to form a knowledge graph that can be queried."
"When defining the reading comprehension task, the authors explain that they wish to estimate p(a|c, q). What would a model trained on this task do if the context ""c"" itself had factually incorrect information? ",The model would likely produce incorrect answers,"The authors are training a reading comprehension model. Therefore, if the context “c” has incorrect information, the model is likely to answer based on the factually incorrect information itself. The authors clearly explain that the task their model is being built for and evaluated on is of identifying answers from a given text (i.e. comprehension) and not knowledge of global correctness.","In this work we seek to directly address the lack of real natural languagetraining data by introducing a novel approach to building a supervised readingcomprehension data set. We observe that summary and paraphrase sentences, withtheir associated documents, can be readily converted to context–query–answertriples using simple entity detection and anonymisation algorithms.Using this approach we have collected two new corpora of roughly a million newsstories with associated queries from the CNN and Daily Mail websites. The reading comprehension task naturally lends itself to a formulation as asupervised learning problem. Specifically we seek to estimate the conditionalprobability p(a|c,q), where c is a context document, q a query relating tothat document, and a the answer to that query.For a focused evaluation we wish to be able to exclude additional information,such as world knowledge gained from co-occurrence statistics, in order to test amodel’s core capability to detect and understand the linguistic relationshipsbetween entities in the context document. Note that the focus of this paper is to provide a corpus for evaluating a model’sability to read and comprehend a single document, not world knowledge orco-occurrence. To understand that distinction consider for instance thefollowing Cloze form queries (created from headlines in the Daily Mailvalidation set):a) The hi-tech bra that helps you beat breast X;b) Could Saccharin help beat X ?;c) Can fish oils help fight prostate X ?An ngram language model trained on the Daily Mail would easily correctly predictthat (X = cancer), regardless of the contents of the contextdocument, simply because this is a very frequently cured entity in the Daily Mailcorpus."
"The paper mentions using Daily News and CNN bullet-point summaries to generate queries. Would the authors' approach towards building this supervised dataset work effectively if these news sources created the summaries by merely extracting sentences from the whole article, instead of rephrasing and condensing text?","No.

The authors' approach relies on the summaries being abstractive and not simply copying sentences from the documents. If the summaries were created by extracting sentences, the resulting questions would be too easy and not provide enough challenge for the reading comprehension models being trained","The authors, in multiple places, emphasize that their approach relies on the fact that DailyMail and CNN both use abstractive summaries for their bullet points. This fact probably implies that the authors approach would not work on news sources that merely use excerpts or extracts for summaries.","In this work we seek to directly address the lack of real natural languagetraining data by introducing a novel approach to building a supervised readingcomprehension data set. We observe that summary and paraphrase sentences, withtheir associated documents, can be readily converted to context–query–answertriples using simple entity detection and anonymisation algorithms.Using this approach we have collected two new corpora of roughly a million newsstories with associated queries from the CNN and Daily Mail websites. Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document–query–answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets—and to apply this method to other sources—isavailable online333http://www.github.com/deepmind/rc-data/."
How are the bullet-point summaries converted to queries?,"The bullet-point summaries are converted to queries by replacing one entity at a time with a placeholder, using the Cloze style","Each article in the news websites they used (CNN, DailyMail) has a couple of bullet points containing an abstractive summary of the article. They convert each bullet point into a Cloze style question and answer using entity detection algorithms. More details on what Cloze-style questions are is not available in this paper.","In this work we seek to directly address the lack of real natural languagetraining data by introducing a novel approach to building a supervised readingcomprehension data set. We observe that summary and paraphrase sentences, withtheir associated documents, can be readily converted to context–query–answertriples using simple entity detection and anonymisation algorithms.Using this approach we have collected two new corpora of roughly a million newsstories with associated queries from the CNN and Daily Mail websites. Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document–query–answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets—and to apply this method to other sources—isavailable online333http://www.github.com/deepmind/rc-data/."
What weaknesses would a dataset that without entity replacement or anonymization have when training a reading comprehension model? Why is this a necessary step in the process?,"The dataset would be vulnerable to degenerate solutions and biased towards individuals with prior knowledge of the entities present in the text. This is because the model could simply memorize the entities and their context, rather than learning to comprehend the text as a whole. Anonymizing and randomizing the entities helps to mitigate these issues and ensure that the model is truly testing reading comprehension capabilities, rather than just memorization of specific entities","Since the authors are attempting to build a reading comprehension model, not anonymizing the entities before using the dataset might lead to a situation where models use external information, or statistics on the distribution/frequency of words themselves to guess answers. These steps are needed to ensure that models use the context to answer the questions.","To prevent such degenerate solutions and create a focused task we anonymise andrandomise our corpora with the following procedure,a) use a coreference system to establish coreferents in eachdata point;b) replace all entities with abstract entity markers according tocoreference;c) randomly permute these entity markers whenever a data point is loaded. Compare the original and anonymised version of the example in Table3. Clearly a human reader can answer both queries correctly.However in the anonymised setup the context document is required for answeringthe query, whereas the original version could also be answered by someone withthe requisite background knowledge.Therefore, following this procedure, the only remaining strategy for answeringquestions is to do so by exploiting the context presented with each question.Thus performance on our two corpora truly measures reading comprehensioncapability. Naturally a production system would benefit from using allavailable information sources, such as clues through language and co-occurrencestatistics."
"What is the main difference between the ""maximum frequency"" and ""exclusive frequency"" baselines? ","The main difference between the ""maximum frequency"" and ""exclusive frequency"" baselines is that the exclusive frequency baseline excludes the placeholder from the frequency count in the context document","The main difference between the “maximum frequency” and “exclusive frequency” baselines is that the latter eliminates all entities found in the query as potential answers. The maximum frequency baseline merely returns the most commonly seen entity in the context as the answer - the exclusive benchmark does the same, but returns the most commonly seen entity in context which is not in the query.","So far we have motivated the need for better datasets and tasks to evaluate thecapabilities of machine reading models. We proceed by describing a number ofbaselines, benchmarks and new models to evaluate against this paradigm. Wedefine two simple baselines, the majority baseline (maximum frequency)picks the entity most frequently observed in the context document, whereas theexclusive majority (exclusive frequency) chooses the entity mostfrequently observed in the context but not observed in the query. The ideabehind this exclusion is that the placeholder is unlikely to be mentioned twicein a single Cloze form query."
All the models proposed in this paper are sequence-to-sequence models. True or False?,False. The paper proposes both sequence-to-sequence models (Deep LSTM Reader) and attention-based models (Impatient and Attentive Readers),"The authors seem to be using LSTM models for performing their analysis and experiments. However, the term “sequence-to-sequence” models is not defined in this paper, so answering True or False for this question is not possible based on the contents of this paper alone.","Long short-term memory (LSTM, [18]) networks haverecently seen considerable success in tasks such as machine translation andlanguage modelling [17]. When used for translation, DeepLSTMs [19] have shown a remarkable ability to embed longsequences into a vector representation which contains enough information togenerate a full translation in another language. Our first neural model forreading comprehension tests the ability of Deep LSTM encoders to handlesignificantly longer sequences. We feed our documents one word at a time intoa Deep LSTM encoder, after a delimiter we then also feed the query into theencoder. Alternatively we also experiment with processing the query then thedocument. The result is that this model processes each document query pair as asingle long sequence. Given the embedded document and query the networkpredicts which token in the document answers the query. Within the group of neural models explored here, the results paint a clearpicture with the Impatient and the Attentive Readers outperforming all othermodels. This is consistent with our hypothesis that attention is a keyingredient for machine reading and question answering due to the need topropagate information over long distances. The Deep LSTM Readerperforms surprisingly well, once again demonstrating that this simple sequentialarchitecture can do a reasonable job of learning to abstract long sequences,even when they are up to two thousand tokens in length. However this model doesfail to match the performance of the attention based models, even though theseonly use single layer LSTMs.666Memory constraints prevented us fromexperimenting with deeper Attentive Readers."
What is the main mathematical difference between the attentive LSTM reader and the vanilla Deep LSTM?,"The attentive LSTM reader employs a finer-grained token-level attention mechanism, whereas the vanilla Deep LSTM does not",The main difference between the attention-based LSTM and the vanilla one is that the former addresses the limitation of vanilla LSTM’s fixed and limited context size by taking into account the entire context of every token via a token-level attention mechanism.,"The Attentive Reader can be viewed as a generalisation of the application ofMemory Networks to question answering [3]. That model employsan attention mechanism at the sentence level where each sentence is representedby a bag of embeddings. The Attentive Reader employs a finer grained tokenlevel attention mechanism where the tokens are embedded given their entirefuture and past context in the input document. We can visualise the attention mechanism as a heatmap over a context document togain further insight into the models’ performance. The highlighted words showwhich tokens in the document were attended to by the model. In addition we mustalso take into account that the vectors at each token integrate long rangecontextual information via the bidirectional LSTM encoders.Figure 3 depicts heat maps for two queries that were correctlyanswered by the Attentive Reader.777Note that these examples were chosenas they were short, the average CNN validation document contained 763 tokens and27 entities, thus most instances were significantly harder to answer than theseexamples. In both cases confidently arriving at the correct answer requires themodel to perform both significant lexical generalsiation, e.g. ‘killed’\rightarrow ‘deceased’, and co-reference or anaphora resolution, e.g. ‘ent119 was killed’ \rightarrow ‘he was identified.’ However it is alsoclear that the model is able to integrate these signals with rough heuristicindicators such as the proximity of query words to the candidate answer."
"Assuming the authors performed a brute force hyperparameter search on all permutations of the five hyperparameters - hidden layer sizes, depths, LR, batch size and dropout - how many total experiments would they have had to perform?","320.

To calculate this, we need to consider all possible permutations of the five hyperparameters:

* Hidden layer sizes: 3 (64, 128, 256)
* Depths: 3 (1, 2, 4)
* LR: 4 (1e-3, 5e-4, 1e-4, 5e-5)
* Batch size: 2 (16, 32)
* Dropout: 4 (0, 0.1, 0.2, 0.5)

Each permutation has 5 possible values, so the total number of experiments is:

3 x 3 x 4 x 2 x 4 = 320","For Deep LSTM readers 3 values of hidden layer sizes, 3 values of depths, 3 starting LRs, 2 batch sizes and 3 dropout fractions are considered. This is a total of (3 * 3 * 3 * 2 * 3) = 162 configurations. Similarly for attention models, they experimented with 3 hidden layer sizes, 4 possible learning rates, 3 batch sizes and 4 values of dropout (3*4*3*4) = 144 possible configurations. Thus, if the authors performed a brute force hyperparameter search, they would have had to done 162 experiments for the vanilla Deep LSTM model and 144 experiments for the attention based model, or a total of 162 + 144 = 306 experiments.","All model hyperparameters were tuned on the respective validation sets of thetwo corpora.555For the Deep LSTM Reader, we consider hidden layer sizes{[64,128,\underline{256}]}, depths {[1,\underline{2},4]}, initiallearning rates {[1\text{\sc{e}}{-}3,5\text{\sc{e}}{-}4,\underline{1\text{\sc{e}}{-}4},5\text{\sc{e}}{-}5]}, batchsizes {[16,\underline{32}]} and dropout [0.0,\underline{0.1},0.2]. Weevaluate two types of feeds. In the cqa setup we feed first thecontext document and subsequently the question into the encoder, while theqca model starts by feeding in the question followed by the contextdocument. We report results on the best model (underlined hyperparameters,qca setup). For the attention models we consider hidden layer sizes[64,128,256], single layer, initial learning rates[1\text{\sc{e}}{-}4,5\text{\sc{e}}{-}5,2.5\text{\sc{e}}{-}5,1\text{\sc{e}}{-}5], batch sizes [8,16,32] and dropout[0,0.1,0.2,0.5]. For all models we used asynchronous RmsProp[20] with a momentum of 0.9 and a decay of 0.95.See Appendix A for more details of the experimental setup.Our experimental results are in Table 5, with theAttentive and Impatient Readers performing best across both datasets."
Is hyperparameter optimization performed independently for the two dataset corpora?,"No, hyperparameter optimization is not performed independently for the two dataset corpora. The hyperparameters were tuned on the respective validation sets of the two corpora","Yes, it does appear that hyperparameter optimization for each dataset is performed independently.","All model hyperparameters were tuned on the respective validation sets of thetwo corpora.555For the Deep LSTM Reader, we consider hidden layer sizes{[64,128,\underline{256}]}, depths {[1,\underline{2},4]}, initiallearning rates {[1\text{\sc{e}}{-}3,5\text{\sc{e}}{-}4,\underline{1\text{\sc{e}}{-}4},5\text{\sc{e}}{-}5]}, batchsizes {[16,\underline{32}]} and dropout [0.0,\underline{0.1},0.2]. Weevaluate two types of feeds. In the cqa setup we feed first thecontext document and subsequently the question into the encoder, while theqca model starts by feeding in the question followed by the contextdocument. We report results on the best model (underlined hyperparameters,qca setup). For the attention models we consider hidden layer sizes[64,128,256], single layer, initial learning rates[1\text{\sc{e}}{-}4,5\text{\sc{e}}{-}5,2.5\text{\sc{e}}{-}5,1\text{\sc{e}}{-}5], batch sizes [8,16,32] and dropout[0,0.1,0.2,0.5]. For all models we used asynchronous RmsProp[20] with a momentum of 0.9 and a decay of 0.95.See Appendix A for more details of the experimental setup.Our experimental results are in Table 5, with theAttentive and Impatient Readers performing best across both datasets."
The deepest model that the authors experimented with had 8 layers in it. True or False? ,False. The deepest model the authors experimented with had 256 layers,"False. The deepest model the authors experimented with appears to be a four layer DeepLSTM Reader model. For attention models, the authors exclusively used only a single layer model.","All model hyperparameters were tuned on the respective validation sets of thetwo corpora.555For the Deep LSTM Reader, we consider hidden layer sizes{[64,128,\underline{256}]}, depths {[1,\underline{2},4]}, initiallearning rates {[1\text{\sc{e}}{-}3,5\text{\sc{e}}{-}4,\underline{1\text{\sc{e}}{-}4},5\text{\sc{e}}{-}5]}, batchsizes {[16,\underline{32}]} and dropout [0.0,\underline{0.1},0.2]. Weevaluate two types of feeds. In the cqa setup we feed first thecontext document and subsequently the question into the encoder, while theqca model starts by feeding in the question followed by the contextdocument. We report results on the best model (underlined hyperparameters,qca setup). For the attention models we consider hidden layer sizes[64,128,256], single layer, initial learning rates[1\text{\sc{e}}{-}4,5\text{\sc{e}}{-}5,2.5\text{\sc{e}}{-}5,1\text{\sc{e}}{-}5], batch sizes [8,16,32] and dropout[0,0.1,0.2,0.5]. For all models we used asynchronous RmsProp[20] with a momentum of 0.9 and a decay of 0.95.See Appendix A for more details of the experimental setup.Our experimental results are in Table 5, with theAttentive and Impatient Readers performing best across both datasets."
The Daily Mail part of the dataset is approximately 2x larger than the CNN section of the dataset. True or false?  ,True,"True. The ratio of number of articles from CNN and DailyMail is 1:2.36. Similarly, the ratio of queries from these datasets is given by 380,298:879,450 = 1:2.31. Since 2.31 and 2.36 both round down to 2, the statement is true, approximately.","Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document–query–answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets—and to apply this method to other sources—isavailable online333http://www.github.com/deepmind/rc-data/."
Do the authors claim that bigger datasets would improve the performance and expressiveness of reading comprehension models?,"No, the authors do not claim that bigger datasets would improve the performance and expressiveness of reading comprehension models. Instead, they focus on providing a corpus for evaluating a model's ability to read and comprehend a single document, rather than world knowledge or co-occurrence","Based on the information in this paper alone, it is unclear if a bigger dataset would improve the performance of reading comprehension models. While authors explain that a key contribution they make is the creation of a real-world, massive labelled reading comprehension dataset, it is unclear if such a dataset is essential to improve the performance of reading comprehension models - the authors pitch their dataset-building approach also as a way of evaluating performance of these models, which is different from the dataset itself leading to better performance.","While obtaining supervised natural language reading comprehension data hasproved difficult, some researchers have explored generating synthetic narrativesand queries [3, 4]. Such approaches allowthe generation of almost unlimited amounts of supervised data and enableresearchers to isolate the performance of their algorithms on individualsimulated phenomena. Work on such data has shown that neural network basedmodels hold promise for modelling reading comprehension, something that wewill build upon here. Historically, however, many similar approaches inComputational Linguistics have failed to manage the transition from syntheticdata to real environments, as such closed worlds inevitably fail tocapture the complexity, richness, and noise of natural language[5]. The supervised paradigm for training machine reading and comprehension modelsprovides a promising avenue for making progress on the path to building fullnatural language understanding systems. We have demonstrated a methodology forobtaining a large number of document-query-answer triples and shown thatrecurrent and attention based neural networks provide an effective modellingframework for this task.Our analysis indicates that the Attentive and Impatient Readers are able topropagate and integrate semantic information over long distances. In particularwe believe that the incorporation of an attention mechanism is the keycontributor to these results. Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document–query–answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets—and to apply this method to other sources—isavailable online333http://www.github.com/deepmind/rc-data/. Note that the focus of this paper is to provide a corpus for evaluating a model’sability to read and comprehend a single document, not world knowledge orco-occurrence. To understand that distinction consider for instance thefollowing Cloze form queries (created from headlines in the Daily Mailvalidation set):a) The hi-tech bra that helps you beat breast X;b) Could Saccharin help beat X ?;c) Can fish oils help fight prostate X ?An ngram language model trained on the Daily Mail would easily correctly predictthat (X = cancer), regardless of the contents of the contextdocument, simply because this is a very frequently cured entity in the Daily Mailcorpus."
How is the Uniform Reader model different from the base LSTM model?,"Sure! Here's the answer to your question based on the given context:

The Uniform Reader model is different from the base LSTM model in that it ignores the attention variables","Beyond some information that the uniform reader has poor performance, the paper does not explicitly define what this is.",The poor results of the Uniform Reader support our hypothesis ofthe significance of the attention mechanism in the Attentive model’sperformance as the only difference between these models is that the attentionvariables are ignored in the Uniform Reader. The precision@recall statistics inFigure 2 again highlight the strength of the attentive approach.
What is a BLEU score? ,"BLEU score is a measure of precision of word n-grams between generated and reference sentences, used to evaluate the quality of image descriptions","BLUE score measures the precision of n-grams between generated sentences and reference sentences, which has been shown to correlate well with human evaluation.","The rest of the metrics can be computed automatically assuming one has access togroundtruth, i.e. human generated descriptions. The most commonly used metricso far in the image description literature has been theBLEU score [25],which is a form of precision of word n-grams between generated and referencesentences 222In this literature, most previous work report BLEU-1, i.e., they only compute precision at the unigram level, whereas BLEU-n is a geometric average of precision over 1- to n-grams..Even though this metric has some obvious drawbacks, it has been shown to correlatewell with human evaluations. In this work, we corroborate this as well, aswe show in Section 4.3. An extensive evaluation protocol, as wellas the generated outputs of our system, can be found at \urlhttp://nic.droppages.com/."
What is batch normalization?,"Sure! Here's my answer:

Batch normalization is a technique used in deep learning to improve the stability and performance of neural networks, particularly in image classification tasks. It normalizes the inputs to each layer, helping the network learn more robust features and improve generalization","While P0 shows that the authors using batch normalization, it does not contains the definition. Therefore, this question cannot be answered in this paper and requires external knowledge.","For the representation of images, we use a Convolutional Neural Network(CNN). They have been widely used and studied for image tasks, and arecurrently state-of-the art for object recognition and detection. Our particularchoice of CNN uses a novel approach to batch normalization and yields thecurrent best performance on the ILSVRC 2014 classificationcompetition [12]. Furthermore, they have been shown togeneralize to other tasks such as scene classification by means oftransfer learning [4]. The words are represented with an embeddingmodel."
Could the authors have used a BiLSTM instead of an LSTM to improve the performance of their proposed model further?,"Yes.

The authors could have used a BiLSTM (Bidirectional LSTM) instead of an LSTM to improve the performance of their proposed model further. BiLSTMs are known to perform better than LSTMs in certain tasks, especially those that require both forward and backward information processing. By using a BiLSTM, the model could potentially capture more contextual information and produce better results","While the paper shows that LSTM has shown good performance on some sequence tasks, since there is no evidential information about BiLSTM in this paper this question cannot be answered in this paper. To answer the question, external knowledge about BiLSTM is required to compare how it would work compared to existing LSTM model.","It is natural to model p(S_{t}|I,S_{0},\ldots,S_{t-1}) with aRecurrent Neural Network (RNN), where the variable number ofwords we condition upon up to t-1 is expressed by a fixed lengthhidden state or memory h_{t}. This memory is updated after seeing anew input x_{t} by using a non-linear function f:h_{t+1}=f(h_{t},x_{t})\;.(3)To make the above RNN more concrete two crucial design choices are to be made: what isthe exact form of f and how are the images and words fed as inputs x_{t}. Forf we use a Long-Short Term Memory (LSTM) net, which has shown state-of-the artperformance on sequence tasks such as translation. This model is outlined in thenext section. The LSTM model is trained to predict each word of thesentence after it has seen the image as well as all preceding words as defined byp(S_{t}|I,S_{0},\ldots,S_{t-1}). For this purpose, it is instructive to thinkof the LSTM in unrolled form – a copy of the LSTM memory is created for theimage and each sentence word such that all LSTMs share the same parameters and theoutput m_{t-1} of the LSTM at time t-1 is fed to the LSTM at time t (seeFigure 3). All recurrent connections are transformed to feed-forward connections in theunrolled version. In more detail, if we denote by I the inputimage and by S=(S_{0},\ldots,S_{N}) a true sentence describing this image, theunrolling procedure reads:\displaystyle x_{-1}\displaystyle=\displaystyle\textrm{CNN}(I)(10)\displaystyle x_{t}\displaystyle=\displaystyle W_{e}S_{t},\quad t\in\{0\ldots N-1\}\quad(11)\displaystyle p_{t+1}\displaystyle=\displaystyle\textrm{LSTM}(x_{t}),\quad t\in\{0\ldots N-1\}\quad(12)where we represent each word as a one-hot vector S_{t} of dimension equal to thesize of the dictionary. Note that we denote by S_{0} a special start word and byS_{N} a special stop word which designates the start and end of the sentence.In particular by emitting the stop word the LSTM signals that a complete sentencehas been generated. Both the image and the words are mapped to the same space,the image by using a vision CNN, the words by using word embedding W_{e}. The imageI is only input once, at t=-1, to inform the LSTM about the image contents. Weempirically verified that feeding the image at each time step as an extra input yieldsinferior results, as the network can explicitly exploit noise in the image andoverfits more easily."
What metrics are used in the ILSVRC 2014 competition?,"The ILSVRC 2014 competition uses the following metrics:

* Top-5 accuracy
* Mean Average Precision (MAP)
* Correlation Coefficient (CC)

These metrics are used to evaluate the performance of the models on the image classification task","Since there is no information about what metrics is used for ILSVRC 2014, this question cannot be answered in this paper.","For the representation of images, we use a Convolutional Neural Network(CNN). They have been widely used and studied for image tasks, and arecurrently state-of-the art for object recognition and detection. Our particularchoice of CNN uses a novel approach to batch normalization and yields thecurrent best performance on the ILSVRC 2014 classificationcompetition [12]. Furthermore, they have been shown togeneralize to other tasks such as scene classification by means oftransfer learning [4]. The words are represented with an embeddingmodel."
What is the full form of ILSVRC? ,ImageNet Large Scale Visual Recognition Challenge (ILSVRC),"Since there are no information about the form of ILSVRC in this paper, this question cannot be answered and requires external knowledge.","For the representation of images, we use a Convolutional Neural Network(CNN). They have been widely used and studied for image tasks, and arecurrently state-of-the art for object recognition and detection. Our particularchoice of CNN uses a novel approach to batch normalization and yields thecurrent best performance on the ILSVRC 2014 classificationcompetition [12]. Furthermore, they have been shown togeneralize to other tasks such as scene classification by means oftransfer learning [4]. The words are represented with an embeddingmodel."
"Based on the authors' definition of the loss function used during training, will data points that contain longer sentences be likelier to have higher absolute value of loss and if so, why? ","Yes.

Longer sentences will have higher absolute value of loss because the negative log likelihood of the correct word at each step is summed over more words, resulting in a greater total loss","Since the loss function is defined as a sum of the negative log likelihood and not averaged, the value tends to be increased in proportion to the length of S (N in eq. (13)). Therefore, data points that contain longer sentences would be likely to have higher absolute value of loss.","Our loss is the sum of the negative log likelihood of the correct word at each step as follows:L(I,S)=-\sum_{t=1}^{N}\log p_{t}(S_{t})\;.(13)The above loss is minimized w.r.t. all the parameters of the LSTM, the top layer of theimage embedder CNN and word embeddings W_{e}."
How is the BLEU-1 score different from the BLEU-4 score?,"BLEU-1 and BLEU-4 differ in the level of precision they consider. BLEU-1 only considers unigrams (individual words), while BLEU-4 considers precision at the 4-gram level, which includes phrases of four words","While BLEU-4 compute precision at the 4-gram level, BLEU-1 compute precision at the unigram (1-gram) level.","The rest of the metrics can be computed automatically assuming one has access togroundtruth, i.e. human generated descriptions. The most commonly used metricso far in the image description literature has been theBLEU score [25],which is a form of precision of word n-grams between generated and referencesentences 222In this literature, most previous work report BLEU-1, i.e., they only compute precision at the unigram level, whereas BLEU-n is a geometric average of precision over 1- to n-grams..Even though this metric has some obvious drawbacks, it has been shown to correlatewell with human evaluations. In this work, we corroborate this as well, aswe show in Section 4.3. An extensive evaluation protocol, as wellas the generated outputs of our system, can be found at \urlhttp://nic.droppages.com/."
"Can the architecture the authors' proposed be replaced with newer model architectures such as attention-based models or transformers, or is their task incompatible with these newer architectures?","No, the authors' proposed architecture cannot be replaced with newer model architectures such as attention-based models or transformers. The task is incompatible with these newer architectures","Since the proposed method use RNN architecture for sequence modeling and not utilizing RNN-specific structures, other newer model like attention-based models or transformers also can be used instead of RNN.","In this work we combine deepconvolutional nets for image classification [12] withrecurrent networks for sequence modeling[10], to create a single networkthat generates descriptions of images. The RNN is trained in the context ofthis single “end-to-end” network. The model is inspiredby recent successes of sequence generation in machine translation[3, 2, 30], withthe difference that instead of starting with a sentence, we provide an imageprocessed by a convolutional net. The closest works are by Kiros et al. [15] whouse a neural net, but a feedforward one, to predict the next word given the imageand previous words. A recent work by Mao et al. [21] uses a recurrentNN for the same prediction task. This is very similar to the present proposal butthere are a number of important differences: we use a more powerful RNN model,and provide the visual input to the RNN model directly, which makes it possiblefor the RNN to keep track of the objects that have been explained by the text. Asa result of these seemingly insignificant differences, our system achievessubstantially better results on the established benchmarks. Lastly, Kiros et al. [14]propose to construct a joint multimodal embedding space by using a powerfulcomputer vision model and an LSTM that encodes text. In contrast to our approach,they use two separate pathways (one for images, one for text) to define a joint embedding,and, even though they can generate text, their approach is highly tuned for ranking."
"Based on the statement that the authors used a beam size of 20 during inference, how many total sentences would be generated till timestep t=10?","Sure! Here's the answer to your question based on the given context:

200.

Explanation: With a beam size of 20, the authors generated 20 sentences at each time step until timestep t=10, resulting in a total of 200 sentences","Since BeamSearch always keep only the resulting best k (=beam size) candidates in every time step, 20 sentences would be generated till timestep t=10.","There are multiple approaches that can be used to generate a sentence givenan image, with NIC. The first one is Sampling where we justsample the first word according to p_{1}, then provide the correspondingembedding as input and sample p_{2}, continuing like this until we sample thespecial end-of-sentence token or some maximum length.The second one is BeamSearch: iterativelyconsider the set of the k best sentences up to timet as candidates to generate sentences of size t+1, and keep only theresulting best k of them. This better approximatesS=\arg\max_{S^{\prime}}p(S^{\prime}|I).We used the BeamSearch approach in the following experiments, with abeam of size 20. Using a beam size of 1 (i.e., greedy search) did degrade ourresults by 2 BLEU points on average."
"Would more recent frameworks, such as JAX be considered imperative or declarative?","Sure! Here's my answer:

Declarative.

JAX is a more recent framework that is designed to be declarative, allowing users to specify the high-level computation they want to perform without worrying about the low-level implementation details. It uses a functional programming style to define the computation, which makes it easier to reason about and optimize the code","While some recent framework (Tensorflow) has aspect of both imperative and declarative paradigms, the question cannot be answered in this paper since there is no evidential information about the paradigm shift over time.","Most ML systems embed a domain-specific language (DSL) into a host language(e.g. Python, Lua, C++). Possible programming paradigms range from imperative,where the user specifies exactly “how” computation needs to beperformed, and declarative, where the user specification focuses on“what” to be done. Examples of imperative programming include numpy andMatlab, whereas packages such as Caffe, CXXNet program over layer definitionwhich abstracts away and hide the inner-working of actual implementation. The dividing line between the two can be muddy at times. Frameworkssuch as Theano and the more recent Tensorflow can also be viewed as a mixture of both, they declare a computational graph, yet the computation within the graph is imperatively specified."
"The authors mention that their framework, MXNet, uses ""lazy evaluation"". Define lazy evaluation.","Lazy evaluation is an evaluation strategy in which an expression is only evaluated when its value is actually needed, rather than when it is defined. In the context of MXNet, this means that the framework defers the actual computation of NDArrays until they are actually needed, allowing for more efficient use of resources and improved performance","Lazy evalution is the way of evalution of data like NDArray, which the actual data push and pull are scheduled by the backend engine so that the data dependency can be correctly resolved.","The above is as efficient as the implementation using a single but often much more complex symbolic expression.The reason is that MXNet uses lazy evaluation of NDArray and the backendengine can correctly resolve the data dependency between the two. The above mixed implementation has the same performance comparing to a singledeclarative program, because the actual data push and pull are executed by lazyevaluation, which are scheduled by the backend engine just like others."
Can collective communication primitives such as all_reduce or all_gather be implemented using MXNet?,"Sure! Here's my answer based on the given context:

Yes","Since MXNet provides distributed key-value store mechanism and user-defined updater logics, it is likely to be able to implement collective communication primitives using MXNet.",Paragraph 10
"Why do the authors use heuristics to estimate a variable's life span in the computational graph, instead of calculating it exactly?","The authors use heuristics to estimate a variable's life span in the computational graph instead of calculating it exactly because exact calculation of the life span would require O(n^2) time complexity, which is not feasible for large graphs","While calculating the variable's estimated life span costs quadratic time complexity, heuristics costs only linear time complexity which is much efficient. Also, experimental results show that heuristics can reduce the memory footprint effectively.","Memory Allocation.Note that each variable’s life time, namely the period between thecreation and the last time will be used, is known for a computation graph.So we can reuse memory for non-intersected variables.However, an ideal allocation strategy requires O(n^{2}) timecomplexity, where n is the number of variables. We proposed two heuristics strategies with linear time complexity.The first, called inplace, simulates the procedure of traversingthe graph, and keeps a reference counter of depended nodes that are not used sofar. If the counter reaches zero, the memory is recycled.The second, named co-share, allows two nodes to share a piece of memory if only ifthey cannot be run in parallel. Exploring co-share imposes one additionaldependency constraint.In particular, each time upon scheduling, among the pending paths in the graph, we find the longest path and perform needed memory allocations. Figure 7 shows the memory usages of the internal variables excepts for the outputs.As can be seen, both “inplace” and “co-share” can effective reduce the memoryfootprint. Combing them leads to a 2x reduction for all networks during modeltraining, and further improves to 4x for model prediction. For instance,even for the most expensive VGG net, training needs less than 16MB extra."
"In the reference counter approach for managed allocated memory, is it possible that an unused variable is not cleaned because of circular dependencies?","Yes.

In the reference counter approach, it is possible for an unused variable to not be cleaned due to circular dependencies. This can happen if two nodes are dependent on each other, and both nodes have a reference to each other in their memory allocation. In this case, the reference counter will not be decremented, and the memory will not be released, even if one of the nodes is not being used. This can lead to memory leaks and other issues","Although the paper mentions that the reference counter is used to traversing the computation graph, it does not contain the detail algorithm or not working cases. Therefore, this question cannot be answered in this paper and requires external knowledges.","We proposed two heuristics strategies with linear time complexity.The first, called inplace, simulates the procedure of traversingthe graph, and keeps a reference counter of depended nodes that are not used sofar. If the counter reaches zero, the memory is recycled.The second, named co-share, allows two nodes to share a piece of memory if only ifthey cannot be run in parallel. Exploring co-share imposes one additionaldependency constraint.In particular, each time upon scheduling, among the pending paths in the graph, we find the longest path and perform needed memory allocations."
"Would functional programming languages be considered to be a part of the imperative or the declarative paradigm, or neither?","Neither. Functional programming languages, such as Haskell and Lisp, do not fit neatly into the imperative/declarative paradigm distinction. They have their own programming paradigm, which emphasizes the evaluation of functions rather than the change of state","Although the paper describes the difference of imperative and declarative paradigm of the programming language, this question cannot be answered and requires external knowledges since there are no evidential information about the functional programming languages.","Most ML systems embed a domain-specific language (DSL) into a host language(e.g. Python, Lua, C++). Possible programming paradigms range from imperative,where the user specifies exactly “how” computation needs to beperformed, and declarative, where the user specification focuses on“what” to be done. Examples of imperative programming include numpy andMatlab, whereas packages such as Caffe, CXXNet program over layer definitionwhich abstracts away and hide the inner-working of actual implementation. The dividing line between the two can be muddy at times. Frameworkssuch as Theano and the more recent Tensorflow can also be viewed as a mixture of both, they declare a computational graph, yet the computation within the graph is imperatively specified."
Does the paper report empirical benchmarks for performance on non-GPU devices (eg. edge devices such as mobile phones)? ,"No, the paper does not report empirical benchmarks for performance on non-GPU devices such as edge devices like mobile phones","While the paper reports the experimental result on GPU device (Nvidia GTX980 card), the result on non-GPU devices is not included in the paper.","We fist compare MXNet with Torch7, Caffe, and TensorFlow on the popular“convnet-benchmarks” [2]. All these systems are compiled with CUDA 7.5 andCUDNN 3 except for TensorFlow, which only supports CUDA 7.0 and CUDNN 2. We usebatch size 32 for all networks and run the experiments on a single Nvidia GTX980 card. Results are shown in Figure 7. As expected thatMXNet has similar performance comparing to Torch7 and Caffe, because mostcomputations are spent on the CUDA/CUDNN kernels. TensorFlow is always 2xslower, which might be due its use of a lower CUDNN version."
What metrics are used to measure performance on segmentation tasks?,"Sure! Here's the answer to your question based on the provided context:

mIOU (mean Intersection over Union) is used to measure performance on segmentation tasks",mIOU is used to measure performance on segmentation tasks.,"In this section, we compare MobileNetV1 and MobileNetV2 models used as feature extractors with DeepLabv3 [39] for the task of mobile semantic segmentation.DeepLabv3 adopts atrous convolution [40, 41, 42], a powerful tool to explicitly control the resolution of computed feature maps, and builds five parallel heads including (a) Atrous Spatial Pyramid Pooling module (ASPP) [43] containing three 3\times 3 convolutions with different atrous rates, (b) 1\times 1 convolution head, and (c) Image-level features [44].We denote by output_stride the ratio of input image spatial resolution to final output resolution, which is controlled by applying the atrous convolution properly.For semantic segmentation, we usually employ \emph{output\_stride}=16 or 8 for denser feature maps.We conduct the experiments on the PASCAL VOC 2012 dataset [3], with extra annotated images from [45] and evaluation metric mIOU."
Define how a linear convolution layer functions.,"Sure! Here's my answer:

A linear convolution layer functions by applying a linear transformation to the input data, using a set of learnable weights to compute a dot product between the input and output features",Linear convolution layer projects the filtered high-dimensional representation to low-dimensional subspace.,Our main contribution is a novel layer module: the inverted residual with linear bottleneck. This module takes as an input a low-dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution. Features are subsequently projected back to a low-dimensional representation with a linear convolution. The official implementation is available as part of TensorFlow-Slim model library in [4].
Do the authors evaluate their architecture on non-mobile/cellphone type of edge devices such as FPGAs?,"Sure! Here's the answer to your question based on the provided context:

No, the authors do not evaluate their architecture on non-mobile/cellphone type of edge devices such as FPGAs",The authors only evaluated their architecture on mobile devices (Google Pixel 1) and did not evaluated on non-mobile type of devices.,"Table 4: Performance on ImageNet, comparison for different networks. As is common practice for ops, we count the total number of Multiply-Adds. In the last column we report running time in milliseconds (ms) for a single large core of the Google Pixel 1 phone (using TF-Lite). We do not report ShuffleNet numbers as efficient group convolutions and shuffling are not yet supported. Table 6: Performance comparison of MobileNetV2 + SSDLite and other realtime detectors on the COCO dataset object detection task. MobileNetV2 + SSDLite achieves competitive accuracy with significantly fewer parameters and smaller computational complexity. All models are trained on trainval35k and evaluated on test-dev. SSD/YOLOv2 numbers are from [35]. The running time is reported for the large core of the Google Pixel 1 phone, using an internal version of the TF-Lite engine."
"Given an input tensor of size (224, 224, 16), a convolution layer transforms the input to an output tensor of size (224, 224, 8), what would the computational cost of this operation  be according to this paper?",3072,"with hi = 224, wi = 224, di = 16 and dj = 8, the computation cost of this operation would be 224 × 224 × 16 × 8 × k × k = 6,422,528 × k^2.","Standard convolution takes an h_{i}\times w_{i}\times d_{i} input tensor L_{i}, and applies convolutional kernel K\in{\cal R}^{k\times k\times d_{i}\times d_{j}} to produce an h_{i}\times w_{i}\times d_{j} output tensor L_{j}.Standard convolutional layers have the computational cost of h_{i}\cdot w_{i}\cdot d_{i}\cdot d_{j}\cdot k\cdot k."
What differentiates a bottleneck block from a residual block?,"A bottleneck block has shortcuts directly between the bottlenecks, whereas a residual block does not","Inverted residuals differentiates a bottleneck block from a residual block, which shortcuts directly between the bottlenecks.","The bottleneck blocks appear similar to residual block where each block contains an input followed by several bottlenecks then followed by expansion [8].However, inspired by the intuition that the bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the tensor, we use shortcuts directly between the bottlenecks.Figure 3 provides a schematic visualization of the difference in the designs.The motivation for inserting shortcuts is similar to that of classical residual connections: we want to improve the ability of a gradient to propagate across multiplier layers.However, the inverted design is considerably more memory efficient (see Section 4 for details), as well as works slightly better in our experiments."
What is the key difference in model structure between Mobilenet style models and Shufflenet? ,"Sure! Here's the answer to your question based on the provided context:

ShuffleNet uses Group Convolutions and shuffling, whereas Mobilenet-style models do not","ShuffleNet introduces group convolutions and shuffling, while existing mobilenet style models do not have.","Figure 4: Comparison of convolutional blocks for different architectures. ShuffleNet uses Group Convolutions [20] and shuffling, it also uses conventional residual approach where inner blocks are narrower than output. ShuffleNet and NasNet illustrations are from respective papers."
"When discussing the information flow interpretation, the authors mention how expressiveness and capacity of their model can be independently analyse. What does ""capacity"" in this context mean?","Capacity refers to the ability of the network to model complex relationships between the input and output, which is determined by the number of parameters and the size of the bottleneck layers in the architecture","In this paper, capacity is the input/output domain of the bottleneck layers, which can be separated from the expressiveness part (layer transformation) for the proposed architecture.","One interesting property of our architecture is that it provides a natural separation between the input/output domains of the building blocks (bottleneck layers), and the layer transformation – that is a non-linear function that converts input to the output.The former can be seen as the capacity of the network at each layer, whereas the latter as the expressiveness.This is in contrast with traditional convolutional blocks, both regular and separable, where both expressiveness and capacity are tangled together and are functions of the output layer depth."
How does RELU6 differ from vanilla RELU?,RELU6 differs from vanilla RELU in its robustness when used with low-precision computation,RELU6 is more robust compared to vanilla RELU when used with low-precision computation. How RELU6 is calculated is not included in this paper and thus requires external knowledge to be answered.,"Now we describe our architecture in detail. As discussed in the previous section thebasic building block is a bottleneck depth-separable convolution with residuals.The detailed structureof this block is shown in Table 1. The architecture of MobileNetV2 contains the initial fully convolution layer with 32 filters, followed by 19 residual bottleneck layers described in the Table 2. We use {\operatorname{\mathop{ReLU6}\,}} as the non-linearity because of its robustness when used with low-precision computation [27]. We always use kernel size 3\times 3 as is standard for modern networks, and utilize dropout and batch normalization during training."
The authors mention that their primary network has a compute cost of 300 million multiply-adds. By how many orders of magnitude would this compute cost increase if the authors did not use bottleneck layers?,"Sure! Here's the answer to your question based on the provided context:

By approximately 3 orders of magnitude","The compute cost when using traditional layers are h · w · d' · d'' · k^2, so the cost would be increase by a factor of d'' · k^2/t(d' + k^2 + d'').","The basic implementation structure is illustrated in Table 1.For a block of size h\times w, expansion factor t and kernel size k with d^{\prime} input channels and d^{\prime\prime} output channels, the total number of multiply add required ish\cdot w\cdot d^{\prime}\cdot t(d^{\prime}+k^{2}+d^{\prime\prime}).Compared with (1) this expression has an extra term, as indeed we have an extra 1\times 1 convolution, however the nature of our networks allows us to utilize much smaller input and output dimensions.In Table 3 we compare the needed sizes for each resolution between MobileNetV1, MobileNetV2 and ShuffleNet."
Do the authors measure the quantify the impact on their model's performance when using RELU6 instead of RELU?,"Sure! Here's the answer to your question based on the provided context:

No, the authors do not measure the impact of using RELU6 instead of RELU on their model's performance","While the authors showed the effect of inverted residual connections and linear bottlenecks, they did not measure the impact of using RELU6 instead of RELU in the ablation study.","The importance of residual connection has been studied extensively [8, 30, 46]. The new result reported in this paper is that the shortcut connecting bottleneck perform better than shortcuts connecting the expanded layers (see Figure 6b for comparison). The linear bottleneck models are strictly less powerful than models with non-linearities, because the activations can always operate in linear regime with appropriate changes to biases and scaling. However our experiments shown in Figure 6a indicate that linear bottlenecks improve performance, providing support that non-linearity destroys information in low-dimensional space."
What are the likely problems authors would have encountered if they did not use batch normalization and dropout during training?,Overfitting and reduced generalization performance,"Since there is no evidential information about the effect of batch normalization and dropout, this question cannot be answered and requires external knowledges.","The architecture of MobileNetV2 contains the initial fully convolution layer with 32 filters, followed by 19 residual bottleneck layers described in the Table 2. We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation [27]. We always use kernel size 3 × 3 as is standard for modern networks, and utilize dropout and batch normalization during training."
Why is running information on resource-constrained environments an important concern when GPUs are becoming more commonplace today?,"Running information on resource-constrained environments is important because many mobile and embedded devices, despite the increasing prevalence of GPUs, still have limited computing resources, making it challenging to deploy state-of-the-art neural networks","Running information on resource-constrained environments is important concern, because many existing state-of-the-art models require high computational resources that makes them not usable in mobile and embedded applications.","Neural networks have revolutionized many areas of machine intelligence, enabling superhuman accuracy for challenging image recognition tasks. However, the drive to improve accuracy often comes at a cost: modern state of the art networks require high computational resources beyond the capabilities of many mobile and embedded applications."
What hyperparameters do each of the eight variants of LSTMs investigated by the authors of this paper have?,"The eight variants of LSTMs investigated by the authors of this paper have the following hyperparameters:

1. Number of LSTM blocks per hidden layer: log-uniform samples from [20, 200]
2. Learning rate: log-uniform samples from [10^-6, 10^-2]
3. Momentum: 1 - log-uniform samples from [0.01, 1.0]
4. Standard deviation of Gaussian input noise: uniform samples from [0, 1]",The authors investigated 1) number of LSTM blocks per hidden layer 2) learning rate 3) momentum 4) standard deviation of Gaussian input noise with random searches with uniform sampling.,"We performed 27 random searches (one for each combination of the nine variants and three datasets).Each random search encompasses 200 trials for a total of 5400 trials of randomly sampling the following hyperparameters:•number of LSTM blocks per hidden layer:log-uniform samples from [20,200];•learning rate:log-uniform samples from [10^{-6},10^{-2}];•momentum:1-\text{log-uniform samples from $[0.01,1.0]$};•standard deviation of Gaussian input noise:uniform samples from [0,1]."
"The authors mention that they measure performance of their models using the character error rate metric, which they calculate using best path encoding. What does ""best path"" here mean?","Sure! Here's the answer to your question based on the provided context:

The ""best path"" in this context refers to the most likely sequence of characters given the input sequence and the model's predictions. In other words, it's the sequence of characters that the model is most confident will come next, based on the input and its internal state","Since there is no evidential information about the detail of best path decoding, this question cannot be answered and requires external knowledge, specifically the reference [39].",The networks were trained using the Connectionist Temporal Classification (CTC) error function by Graves et al. [39] with 82 outputs (81 characters plus the special empty label). We measure performance in terms of the Character Error Rate (CER) after decoding using best-path decoding [39].
What does MIDI sequencing mean?,MIDI sequencing refers to the process of creating a musical composition using MIDI (Musical Instrument Digital Interface) technology,"Because ""MIDI sequence"" is not used as an action in this paper, MIDI sequencing does not make sense and thus cannot be answered.","JSB Chorales is a collection of 382 four-part harmonized chorales by J. S. Bach [40], consisting of 202 chorales in major keys and 180 chorals in minor keys.We used the preprocessed piano-rolls provided by Boulanger-Lewandowski et al. [41].555Available at http://www-etud.iro.umontreal.ca/~boulanni/icml2012 at the time of writing.These piano-rolls were generated by transposing each MIDI sequence in C major or C minor and sampling frames every quarter note. The networks where trained to do next-step prediction by minimizing the negative log-likelihood. The complete dataset consists of 229, 76, and 77 sequences (training, validation, and test sets respectively) with an average length of 61."
How is Nesterov-style momentum different from other momentum based optimizers such as Adam/AdamW?,"Nesterov-style momentum is different from other momentum-based optimizers like Adam/AdamW in that it uses a different update rule for the momentum vector. While Adam/AdamW use a running average of the gradient to update the momentum, Nesterov-style momentum uses a different update rule that takes into account the previous momentum value. This can lead to different convergence properties and potentially better performance in some cases","Since there is no evidential information about the detail of how Nesterov-style momentum and other optimizers works, this question cannot be answered and requires external knowledge.","In the case of the TIMIT dataset, two additional (boolean) hyperparameters were considered (not tuned for the other two datasets).The first one was the choice between traditional momentum and Nesterov-style momentum [42]. Our analysis showed that this had no measurable effect on performance so the latter was arbitrarily chosen for all further experiments.The second one was whether to clip the gradients to the range [-1,1]. This turned out to hurt overall performance,666Although this may very well be the result of the range having been chosen too tightly. therefore the gradients were never clipped in the case of the other two datasets."
The results section concludes that momentum affected neither the training compute cost nor the performance - why was this a surprising or unexpected result?,"The result that momentum did not affect training compute cost or performance was surprising because it was expected to have a positive impact, based on previous research and the authors' own hypotheses",It is not clear why was the result an unexpected result since there is no evidential information of what the authors expected when choosing the hyperparameters to assess their importance.,"One unexpected result of this study is that momentum affects neither performance nor training time in any significant way.This follows from the observation that for none of the datasets, momentum accounted for more than 1% of the variance of test set performance.It should be noted that for TIMIT the interaction between learning rate and momentum accounts for 2.5% of the total variance, but as with learning rate \times hidden size (cf. Interaction of Hyperparameters below) it does not reveal any interpretable structure.This may be the result of our choice to scale learning rates dependent on momentum (Section IV-B).These observations suggest that momentum does not offer substantial benefits when training LSTMs with online stochastic gradient descent. As expected, the learning rate is the most crucial hyperparameter, followed by the network size.Surprisingly though, the use of momentum was found to be unimportant in our setting of online gradient descent.Gaussian noise on the inputs was found to be moderately helpful for TIMIT, but harmful for the other datasets."
How did the authors ensure that fair comparison between the 9 variants of LSTMs they analysed?,The authors ensured fair comparison between the 9 variants of LSTMs they analyzed by individually tuning the hyperparameters for each variant and using random search to obtain good-performing hyperparameters for every combination of variant and dataset,"To ensure the fair comparison that needs to be similar for each variant, the authors tuned the hyperparameters individually for each variant, and use random search to 1) obtain good hyperparameters and 2) collect enough amount of samples for analyzing the general effect of each variant.","For fair comparison, the setup needs to be similar for each variant.Different variants might require different settings of hyperparameters to give good performance, and we are interested in the best performance that can be achieved with each variant.For this reason we chose to tune the hyperparameters like learning rate or amount of input noise individually for each variant.Since hyperparameter space is large and impossible to traverse completely, random search was used in order to obtain good-performing hyperparameters [18] for every combination of variant and dataset.Random search was also chosen for the added benefit of providing enough data for analyzing the general effect of various hyperparameters on the performance of each LSTM variant (Section V-B)."
What are the eight different LSTM variants that the authors experimented with?,"1. Vanilla LSTM (baseline)
2. Coupling the input and forget gates (CIFG)
3. Removing peephole connections (NP)
4. Removing the output activation function (NOAF)
5. Removing the forget gate (NFG)
6. Adding full gate recurrence (FGR)
7. Removing the input gate (NIG)
8. Removing the output gate (NOG)
9. Removing the input activation function (NIAF)","The authors conducted the experiment with these LSTM variants of the vanilla architecture to empirically compare different LSTM variants: No Input Gate (NIG), No Forget Gate (NFG), No Output Gate (NOG), No Input Activation Function (NIAF), No Output Activation Function (NOAF), Coupled Input and Forget Gate (CIFG), No Peepholes (NP), Full Gate Recurrence (FGR).","The focus of our study is to empirically compare different LSTM variants, and not to achieve state-of-the-art results.Therefore, our experiments are designed to keep the setup simple and the comparisons fair.The vanilla LSTM is used as a baseline and evaluated together with eight of its variants.Each variant adds, removes, or modifies the baseline in exactly one aspect, which allows to isolate their effect.They are evaluated on three different datasets from different domains to account for cross-domain variations. This paper reports the results of a large scale study on variants of the LSTM architecture. We conclude that the most commonly used LSTM architecture (vanilla LSTM) performs reasonably well on various datasets.None of the eight investigated modifications significantly improves performance.However, certain modifications such as coupling the input and forget gates (CIFG) or removing peephole connections (NP) simplified LSTMs in our experiments without significantly decreasing performance.These two variants are also attractive because they reduce the number of parameters and the computational cost of the LSTM. The first important observation based on Figure 3 is that removing the output activation function (NOAF) or the forget gate (NFG) significantly hurt performance on all three datasets. Apart from the CEC, the ability to forget old information and the squashing of the cell state appear to be critical for the LSTM architecture. Indeed, without the output activation function, the block output can in principle grow unbounded. Coupling the input and the forget gate avoids this problem and might render the use of an output non-linearity less important, which could explain why GRU performs well without it. Adding full gate recurrence (FGR) did not significantly change performance on TIMIT or IAM Online, but led to worse results on the JSB Chorales dataset. Given that this variant greatly increases the number of parameters, we generally advise against using it. Note that this feature was present in the original proposal of LSTM [14, 15], but has been absent in all following studies. Removing the input gate (NIG), the output gate (NOG), and the input activation function (NIAF) led to a significant reduction in performance on speech and handwriting recognition. However, there was no significant effect on music modeling performance. A small (but statistically insignificant) average performance improvement was observed for the NIG and NIAF architectures on music modeling. We hypothesize that these behaviors will generalize to similar problems such as language modeling. For supervised learning on continuous real-valued data (such as speech and handwriting recognition), the input gate, output gate, and input activation function are all crucial for obtaining good performance."
Do the authors use different ratios of test-train-validation split for each dataset? ,"No, the authors use the same ratio of test-train-validation split for all datasets","The authors use different ratios of test-train-validation split for each dataset. Speficially, the authors did not use the predefined ratio value when splitting the data into train-validation-test sets for the three datasets (TIMIT Speech corpus, IAM Online Handwriting Database, and JSB Chorales dataset) used in the experiment. Instead, they used the predefined data split for IAM Online Handwriting Database and JSB Chorales dataset. (5355:3859:2956 and 229:77:76) They also followed Halberstadt [37] in splitting the TIMIT dataset (3696:400:192).","The performance is measured as classification error percentage. The training, testing, and validation sets are split in line with Halberstadt [37] into 3696, 400, and 192 sequences, having 304 frames on average. The TIMIT Speech corpus [26] is large enough to be a reasonable acoustic modeling benchmark for speech recognition, yet it is small enough to keep a large study such as ours manageable. Our experiments focus on the frame-wise classification task for this dataset, where the objective is to classify each audio-frame as one of 61 phones.2 From the raw audio we extract 12 Mel Frequency Cepstrum Coefficients (MFCCs) [35] + energy over 25ms hamming-windows with stride of 10ms and a pre-emphasis coefficient of 0.97. This preprocessing is standard in speech recognition and was chosen in order to stay comparable with earlier LSTM-based results (e.g. [20, 36]). The 13 coefficients along with their first and second derivatives comprise the 39 inputs to the network and were normalized to have zero mean and unit variance. The IAM Online Handwriting Database [38] consists of English sentences as time series of pen movements that have to be mapped to characters. The IAM-OnDB dataset splits into one training set, two validation sets, and one test set, having 775, 192, 216, and 544 boards each. Each board, see Figure 2(a), contains multiple hand-written lines, which in turn consist of several strokes. We use one line per sequence, and joined the two validation sets together, so the final training, validation, and testing sets contain 5355, 2956 and 3859 sequences respectively. JSB Chorales: JSB Chorales is a collection of 382 four part harmonized chorales by J. S. Bach [40], consisting of 202 chorales in major keys and 180 chorals in minor keys. We used the preprocessed piano-rolls provided by Boulanger, Lewandowski et al. [41]. 5 These piano-rolls were generated by transposing each MIDI sequence in C major or C minor and sampling frames every quarter note. The networks where trained to do next-step prediction by minimizing the negative log-likelihood. The complete dataset consists of 229, 76, and 77 sequences (training, validation, and test sets respectively) with an average length of 61."
Define how fANOVA is performed?,"fANOVA is performed by marginalizing over dimensions efficiently in regression trees, allowing for predicting the marginal error for one hyperparameter while averaging over all the others, without requiring a full hyperparameter grid search",fANOVA marginalize over hyperparameter dimensions using regression trees to predict the marginal error for single parameter while averaging over all other parameters.,"The fANOVA framework for assessing hyperparameter importance by Hutter et al. [19] is based on the observation that marginalizing over dimensions can be done efficiently in regression trees.This allows predicting the marginal error for one hyperparameter while averaging over all the others.Traditionally this would require a full hyperparameter grid search, whereas here the hyperparameter space can be sampled at random."
"The authors say, ""a very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions"". What is this technique called?",Ensemble learning,"The authors define an “ensemble of models” as a set of separate models with the same architecture and training procedure, but different randomly initialized parameters whose predictions are then averaged to increase performance.","We trained 10 separate models to predict P(h_{t}|\mathbf{s}_{t};\boldsymbol{\theta}), using exactly the same architecture and trainingprocedure as the baseline. The models are randomly initialized with different initial parameter values and we find thatthis creates sufficient diversity in the trained models to allow the averaged predictions of the ensemble tosignificantly outperform the individual models. We have explored adding diversity to the models by varying the sets ofdata that each model sees, but we found this to not significantly change our results, so we opted for the simplerapproach. For the distillation we tried temperatures of [1,{\bf 2},5,10] and used a relative weight of 0.5 on thecross-entropy for the hard targets, where bold font indicates the best value that was used fortable 1 ."
"The authors proposed approach only works for classification models, and not for models that have other types of outputs. True or False?","False. The proposed approach can be applied to any type of model, not just classification models. The authors use a clustering algorithm on the predictions of a generalist model, which could be applied to any type of output, not just class labels","In this work, the approach assumes that there are classes that the models should be able to predict. The work focuses on classification models. Thus, whether the approach can work on models with other types of outputs cannot be answered from this paper.","In this section we give an example of such a dataset and we show howlearning specialist models that each focus on a different confusablesubset of the classes can reduce the total amount of computationrequired to learn an ensemble. The main problem with specialists thatfocus on making fine-grained distinctions is that they overfit veryeasily and we describe how this overfitting may be prevented by usingsoft targets. In order to derive groupings of object categories for the specialists, we decided to focus on categories that our fullnetwork often confuses. Even though we could have computed the confusion matrix and used it as a way to find suchclusters, we opted for a simpler approach that does not require the true labels to construct the clusters. In particular, we apply a clustering algorithm to the covariance matrix of the predictions of our generalist model, so that a set of classes S m that are often predicted together will be used as targets for one of our specialist models, m. We applied an on-line version of the K-means algorithm to the columns of the covariance matrix, and obtained reasonable clusters (shown in Table 2). We tried several clustering algorithms which produced similar results."
What is the numeric value of dropout fraction used during the training phase?,0.5,The neural net in this work was strongly regularized using dropout and weight constraints as described in [5]. The specific numeric value of the dropout fraction used for training cannot be answered from this paper.,"To see how well distillation works, we trained a single large neuralnet with two hidden layers of 1200 rectified linear hidden units onall 60,000 training cases. The net was strongly regularized usingdropout and weight-constraints as described in [5]. Dropoutcan be viewed as a way of training an exponentially large ensemble ofmodels that share weights. In addition, the input images were jitteredby up to two pixels in any direction. This net achieved 67 testerrors whereas a smaller net with two hidden layers of 800 rectifiedlinear hidden units and no regularization achieved 146 errors. But ifthe smaller net was regularized solely by adding the additional taskof matching the soft targets produced by the large net at atemperature of 20, it achieved 74 test errors. This shows that softtargets can transfer a great deal of knowledge to the distilled model,including the knowledge about how to generalize that is learned fromtranslated training data even though the transfer set does not containany translations."
"What does ""temperature"" mean in context of the authors' proposed model?","In the context of the authors' proposed model, ""temperature"" refers to a hyperparameter that controls the softness of the class probability distribution produced by the softmax output layer of the neural network. A higher temperature results in a softer distribution, while a lower temperature results in a more peaked distribution","Temperature is a value used in the softmax output layer. The softmax layer converts the logit computed for each class into a probability by comparing with other logits and increasing the temperature produces a softer probability distribution over classes. Specifically, in distillation, a high temperature is used in the cumbersome model to produce a soft target distribution for each case in the transfer set.","In the simplest form of distillation, knowledge is transferred to the distilled model by training it on a transfer setand using a soft target distribution for each case in the transfer set that is produced by using the cumbersome modelwith a high temperature in its softmax. The same high temperature is used when training the distilled model, but afterit has been trained it uses a temperature of 1. Neural networks typically produce class probabilities by using a “softmax” output layer that converts the logit, zi , computed for each class into a probability, qi , by comparing zi with the other logits. qi = exp(zi/T ) P j exp(zj/T ) (1) 2 where T is a temperature that is normally set to 1. Using a higher value for T produces a softer probability distribution over classes."
The authors use a different number of layers and rectified units for experiments on MNIST and speech recognition. What factors might the authors have considered while deciding on these numbers?,"The authors might have considered factors such as the complexity of the dataset, the size of the training set, and the desired level of accuracy when deciding on the number of layers and rectified units for their experiments on MNIST and speech recognition","For speech recognition, the architecture was based on the acoustic model used by Android voice search. For MNIST, the architecture was strongly regularized using dropout and weight constraints as described in prior work. This suggests that the authors considered prior work while deciding these numbers. However, all the factors that authors might have considered when deciding on the number of layers and rectified units cannot be answered from this paper.","To see how well distillation works, we trained a single large neuralnet with two hidden layers of 1200 rectified linear hidden units onall 60,000 training cases. The net was strongly regularized usingdropout and weight-constraints as described in [5]. Dropoutcan be viewed as a way of training an exponentially large ensemble ofmodels that share weights. In addition, the input images were jitteredby up to two pixels in any direction. This net achieved 67 testerrors whereas a smaller net with two hidden layers of 800 rectifiedlinear hidden units and no regularization achieved 146 errors. But ifthe smaller net was regularized solely by adding the additional taskof matching the soft targets produced by the large net at atemperature of 20, it achieved 74 test errors. This shows that softtargets can transfer a great deal of knowledge to the distilled model,including the knowledge about how to generalize that is learned fromtranslated training data even though the transfer set does not containany translations. We use an architecture with 8 hidden layers each containing 2560 rectified linear units and a final softmax layer with14,000 labels (HMM targets h_{t}). The input is 26 frames of 40 Mel-scaled filterbank coefficients with a 10ms advanceper frame and we predict the HMM state of 21{}^{st} frame. The total number of parameters is about 85M. This is aslightly outdated version of the acoustic model used by Android voice search, and should be considered as a very strongbaseline. To train the DNN acoustic model we use about 2000 hours of spoken English data, which yields about 700Mtraining examples. This system achieves a frame accuracy of 58.9%, and a Word Error Rate (WER) of 10.9% on our development set."
"For the speech recognition task, based on the information provided by the authors on the total number of samples in the dataset, how long (in seconds) is each training sample?","Sure! Based on the information provided, the answer is:

Each training sample is approximately 10.9 milliseconds long","P0: According to the authors, they used about 2000 hours of spoken English data which yielded about 700M training examples. Approximately, the length of the average training sample would be 0.01 seconds. The specific length of each training sample cannot be answered from this paper.","We use an architecture with 8 hidden layers each containing 2560 rectified linear units and a final softmax layer with14,000 labels (HMM targets h_{t}). The input is 26 frames of 40 Mel-scaled filterbank coefficients with a 10ms advanceper frame and we predict the HMM state of 21{}^{st} frame. The total number of parameters is about 85M. This is aslightly outdated version of the acoustic model used by Android voice search, and should be considered as a very strongbaseline. To train the DNN acoustic model we use about 2000 hours of spoken English data, which yields about 700Mtraining examples. This system achieves a frame accuracy of 58.9%, and a Word Error Rate (WER) of 10.9% on our development set."
What is the main difference between synchronous vs asynchronous SGD?,"Asynchronous SGD allows for faster training by computing gradients in parallel on different mini-batches and cores, while synchronous SGD waits for all replicas to finish computing gradients before updating parameters","In training of Google’s baseline model of JFT, asynchronous stochastic gradient descent (SGD) involved running replicas of the neural net different sets of cores to compute gradients on given mini-batches, which are then sent to a shared parameter server which returns new values for the parameters. The general definitions of asynchronous and synchronous SGD, and, thus, their differences cannot be answered from this paper.","JFT is an internal Google dataset that has 100 million labeled imageswith 15,000 labels. When we did this work, Google’s baseline model forJFT was a deep convolutional neural network [7] that had been trained forabout six months using asynchronous stochastic gradient descent on alarge number of cores. This training used two types ofparallelism [2]. First, there were many replicas of the neural net runningon different sets of cores and processing different mini-batches fromthe training set. Each replica computes the average gradient on itscurrent mini-batch and sends this gradient to a sharded parameter server whichsends back new values for the parameters. These new values reflect allof the gradients received by the parameter server since the last timeit sent parameters to the replica. Second, each replica is spread overmultiple cores by putting different subsets of the neurons on eachcore. Ensemble training is yet a third type of parallelism that can bewrapped around the other two types, but only if a lot more cores areavailable. Waiting for several years to train an ensemble of models wasnot an option, so we needed a much faster way to improve the baselinemodel."
What optimizer did the authors used for the distilled models?,The authors used a distributed stochastic gradient descent (SGD) approach for training the distilled models,The model was trained with a distributed stochastic gradient descent approach.,"Although it is possible (and desirable) to train the DNN in such a way that the decoder (and, thus, the language model) is taken into account by marginalizing over all possible paths, it is common to train the DNN to perform frame-by-frame classification by (locally) minimizing the cross entropy between the predictions made by the net and the labels given by a forced alignment with the ground truth sequence of states for each observation: θ = arg max θ ′ P(ht|st; θ ′ ) where θ are the parameters of our acoustic model P which maps acoustic observations at time t, st, to a probability, P(ht|st; θ ′ ) , of the “correct” HMM state ht, which is determined by a forced alignment with the correct sequence of words. The model is trained with a distributed stochastic gradient descent approach."
Would more recent approaches such as DECAF extreme classification (2021) serve as a stronger baseline than the specialized models discussed in the paper?,NO,"The specialist models were started from the baseline model which was Google’s deep convolutional network for JFT. The function and performance of DECAF, and how it compares to the JFT baseline model used in this work cannot be answered from this paper.","Starting from the trained baseline full network,the specialists train extremely fast (a few days instead of many weeks for JFT). Also, all the specialistsare trained completely independently. Table  3 shows the absolute test accuracy for thebaseline system and the baseline system combined with the specialistmodels. With 61 specialist models, there is a4.4% relative improvement in test accuracy overall. We also report conditional test accuracy, which is the accuracy by only considering examples belonging to the specialist classes, and restricting our predictions to that subset of classes. JFT is an internal Google dataset that has 100 million labeled images with 15,000 labels. When we did this work, Google’s baseline model for JFT was a deep convolutional neural network [7] that had been trained for about six months using asynchronous stochastic gradient descent on a large number of cores. This training used two types of parallelism [2]. First, there were many replicas of the neural net running on different sets of cores and processing different mini-batches from the training set. Each replica computes the average gradient on its current mini-batch and sends this gradient to a sharded parameter server which sends back new values for the parameters. These new values reflect all of the gradients received by the parameter server since the last time it sent parameters to the replica. Second, each replica is spread over multiple cores by putting different subsets of the neurons on each core. Ensemble training is yet a third type of parallelism that can be wrapped around the other two types, but only if a lot more cores are available. Waiting for several years to train an ensemble of models was not an option, so we needed a much faster way to improve the baseline model."
Is the KMeans algorithm discussed in the paper require a labelled dataset?,"No. The K-means algorithm used in the paper is applied to the covariance matrix of the predictions of the generalist model, and does not require labeled data","The K-means algorithm clusters the set of classes that the models often predict together. In this work, this clustering approach did not require true labels. However, the models themselves were trained using examples from a dataset, JFT, which contains labeled images. Thus, although the K-means algorithm does not require a labeled dataset, the models whose predictions are used in the algorithm required a labeled dataset.","JFT is an internal Google dataset that has 100 million labeled imageswith 15,000 labels. When we did this work, Google’s baseline model forJFT was a deep convolutional neural network [7] that had been trained forabout six months using asynchronous stochastic gradient descent on alarge number of cores. This training used two types ofparallelism [2]. First, there were many replicas of the neural net runningon different sets of cores and processing different mini-batches fromthe training set. Each replica computes the average gradient on itscurrent mini-batch and sends this gradient to a sharded parameter server whichsends back new values for the parameters. These new values reflect allof the gradients received by the parameter server since the last timeit sent parameters to the replica. Second, each replica is spread overmultiple cores by putting different subsets of the neurons on eachcore. Ensemble training is yet a third type of parallelism that can bewrapped around the other two types, but only if a lot more cores areavailable. Waiting for several years to train an ensemble of models wasnot an option, so we needed a much faster way to improve the baselinemodel. In order to derive groupings of object categories for the specialists, we decided to focus on categories that our fullnetwork often confuses. Even though we could have computed the confusion matrix and used it as a way to find suchclusters, we opted for a simpler approach that does not require the true labels to construct the clusters. In particular, we apply a clustering algorithm to the covariance matrix of the predictions of our generalist model, so thata set of classes S^{m} that are often predicted together will be used as targets for one of our specialist models, m. We applied an on-lineversion of the K-means algorithm to the columns of the covariance matrix, and obtained reasonable clusters (shown inTable 2). We tried several clustering algorithms which produced similar results. To reduce overfitting and share the work of learning lower level feature detectors, each specialist model is initialized with the weights of the generalist model. These weights are then slightly modified by training the specialist with half its examples coming from its special subset and half sampled at random from the remainder of the training set. After training, we can correct for the biased training set by incrementing the logit of the dustbin class by the log of the proportion by which the specialist class is oversampled."
Define KL divergence.,"KL divergence is a measure of the difference between two probability distributions. It is defined as the expected value of the logarithmic difference between the two distributions. Specifically, for two probability distributions P and Q, the KL divergence is defined as:

KL(P || Q) = ∑[P(x) log(P(x)/Q(x))]

where the sum is taken over all possible values x in the support of P. The KL divergence measures the amount of information that P provides beyond what is contained in Q. A lower KL divergence indicates that the two distributions are similar, while a higher KL divergence indicates that they are dissimilar","In this work, KL divergence is computed between the probability distribution of a specialist model or generalist full model and the full probability distribution over all classes. The definition of KL divergence cannot be answered from this paper.","Step 2: We then take all the specialist models, m, whose specialsubset of confusable classes,S^{m}, has a non-empty intersection with k and call this the activeset of specialists A_{k} (note that this set may be empty). We thenfind the full probability distribution \mathbf{q} over all the classesthat minimizes:KL(\mathbf{p}^{g},\mathbf{q})+\sum_{m\in A_{k}}KL(\mathbf{p}^{m},\mathbf{q})(5)where KL denotes the KL divergence, and \mathbf{p}^{m} \mathbf{p}^{g} denote theprobability distribution of a specialist model or the generalist fullmodel. The distribution \mathbf{p}^{m} is a distribution over all thespecialist classes of m plus a single dustbin class, so whencomputing its KL divergence from the full \mathbf{q} distribution we sumall of the probabilities that the full \mathbf{q} distribution assigns toall the classes in m’s dustbin."
"When discussing the JFT specialist training, the authors refer to a ""dustbin"" class. Give an example of a sample that might be assigned to this class.","A sample that might be assigned to the dustbin class could be an image that is difficult to classify into any of the other classes, such as an image of a mushroom that is not clearly one of the specific types of mushrooms that the specialist models are trained on",The dustbin class was the combination of all of the classes that a specialist does not care about. Examples of samples that might be assigned to the dustbin class cannot be provided from this paper.,"When the number of classes is very large, it makes sense for thecumbersome model to be an ensemble that contains one generalistmodel trained on all the data and many “specialist”models, each of which is trained on data that is highly enriched inexamples from a very confusable subset of the classes (like differenttypes of mushroom). The softmax of this type of specialist can be mademuch smaller by combining all of the classes it does not care about into asingle dustbin class. The specialists that we used in our experiments on the JFT datasetcollapsed all of their non-specialist classes into a single dustbinclass. If we allow specialists to have a full softmax over allclasses, there may be a much better way to prevent them overfitting than usingearly stopping. A specialist is trained on data that is highlyenriched in its special classes. This means that the effective sizeof its training set is much smaller and it has a strong tendency tooverfit on its special classes. This problem cannot be solved bymaking the specialist a lot smaller because then we lose the veryhelpful transfer effects we get from modeling all of thenon-specialist classes."
What factors could the authors have used while deciding the number of specialists to allocate for their task?,"The authors could have used factors such as the number of classes, the size of the dataset, and the desired level of accuracy improvement to decide the number of specialists to allocate for their task","P0: Through results shown in Table 4, the authors saw a general trend that accuracy improved when more specialists covered a particular class. This could have been a factor that authors considered in deciding on the number of specialists for their task.","For our JFT specialist experiments, we trained 61 specialist models, each with 300 classes (plus the dustbin class).Because the sets of classes for the specialists are not disjoint, we often had multiple specialists covering aparticular image class. Table  4 shows the number of test set examples, the change inthe number of examples correct at position 1 when using the specialist(s), and the relative percentage improvement intop1 accuracy for the JFT dataset broken down by the number of specialists covering the class. We are encouraged by thegeneral trend that accuracy improvements are larger when we have more specialists covering a particular class, sincetraining independent specialist models is very easy to parallelize."
How are mixture of expert gating functions designed?,Mixture of expert gating functions are designed using a gating network that computes the probability of assigning each example to each expert based on their relative discriminative performance,"In a mixture of experts, the gating function is a network that is learned to choose which experts to assign to each example through the relative discriminative performance of the experts on the sample.","The use of specialists that are trained on subsets of the data hassome resemblance to mixtures of experts [6] which use agating network to compute the probability of assigning each example toeach expert. At the same time as the experts are learning to deal withthe examples assigned to them, the gating network is learning tochoose which experts to assign each example to based on the relativediscriminative performance of the experts for that example. Using the discriminativeperformance of the experts to determine the learned assignments is muchbetter than simply clustering the input vectors and assigning anexpert to each cluster, but it makes the training hard to parallelize: First, theweighted training set for each expert keeps changing in a way thatdepends on all the other experts and second, the gating network needsto compare the performance of different experts on the same example toknow how to revise its assignment probabilities. These difficultieshave meant that mixtures of experts are rarely used in the regimewhere they might be most beneficial: tasks with huge datasets thatcontain distinctly different subsets."
Why did the authors use a different set of phonemes for decoding and scoring?,"To accommodate the extra ""end-of-sequence"" token and all-zero frame appended to each input sequence","There is no evidential information about using a different set of phonemes for decoding and scoring, this question cannot be answered in this paper. However, since the authors follow the settings from existing works so this could be found in those references.","We closely followed the procedure in [16]. All experimentswere performed on the TIMIT corpus [19]. We used the train-dev-testsplit from the Kaldi [20] TIMIT s5 recipe. We trained on thestandard 462 speaker set with all SA utterances removed and used the 50 speakerdev set for early stopping. We tested on the 24 speaker core test set. Allnetworks were trained on 40 mel-scale filter-bank features together with theenergy in each frame, and first and second temporal differences, yielding intotal 123 features per frame. Each feature was rescaled to have zero mean andunit variance over the training set. Networks were trained on the full 61-phoneset extended with an extra “end-of-sequence” token that was appended to eachtarget sequence. Similarly, we appended an all-zero frame at the end of eachinput sequence to indicate the end of the utterance. Decoding was performedusing the 61+1 phoneme set, while scoring was done on the 39 phoneme set."
"What considerations would authors need to take to extend this model to languages that have more phonemes than American English? (eg. Indian languages, Chinese, etc)","To extend this model to languages with more phonemes than American English, authors would need to consider the following:

1. Increase the number of phonemes in the phoneme set: This would require adding new phonemes to the set and possibly adjusting the rescaling of the features to ensure that the new phonemes are properly represented.
2. Increase the size of the training set: With more phonemes, the model will need more training data to learn the new phonemes and their distributions.
3. Adjust the feature extraction and rescaling: Depending on the language, different features may be more relevant, and the rescaling of the features may need to be adjusted to account for the different distributions of the new phonemes.
4. Modify the decoding and scoring mechanisms: The decoding and scoring mechanisms would need to be modified to account for the new phonemes and their distributions. This may involve changing the phoneme set used for decoding and scoring, or modifying the scoring algorithm to account for the new phonemes","When authors need to take to extend this model to other languages or other datasets than TIMIT corpus, they should extend the phonemes set for some phonemes that are not included in American English. This applies to all procedures including training/decoding/scoring.","We closely followed the procedure in [16]. All experiments were performed on the TIMIT corpus [19]. We used the train-dev-tests plit from the Kaldi [20] TIMIT s5 recipe. We trained on the standard 462 speaker set with all SA utterances removed and used the 50 speaker dev set for early stopping. We tested on the 24 speaker core test set. All networks were trained on 40 mel-scale filter-bank features together with the energy in each frame, and first and second temporal differences, yielding in total 123 features per frame. Each feature was rescaled to have zero mean and unit variance over the training set. Networks were trained on the full 61-phoneset extended with an extra “end-of-sequence” token that was appended to each target sequence. Similarly, we appended an all-zero frame at the end of each input sequence to indicate the end of the utterance. Decoding was performed using the 61+1 phoneme set, while scoring was done on the 39 phoneme set."
The authors mentioned that they used an all-zero frame of values to denote the end of sequence. Is this choice of all zero arbitrary (i.e. any unique set of values could be used to encode end of sequence) or is there some other benefit to choosing all-zeroes?,"The choice of all-zero frame to denote the end of sequence is not arbitrary.

In the context of speech recognition, using an all-zero frame to indicate the end of an utterance is a common convention, as it allows the model to distinguish between the end of the utterance and the beginning of a new one. This is particularly useful when dealing with overlapping speech or when there is a pause between utterances. By using an all-zero frame, the model can be trained to recognize the end of an utterance more accurately, and to avoid confusion with the beginning of a new one","Since there is no evidential information about the reason or the benefit of using the all-zero frame in this paper, this question cannot be answered and requires external knowledge.","We closely followed the procedure in [16]. All experimentswere performed on the TIMIT corpus [19]. We used the train-dev-testsplit from the Kaldi [20] TIMIT s5 recipe. We trained on thestandard 462 speaker set with all SA utterances removed and used the 50 speakerdev set for early stopping. We tested on the 24 speaker core test set. Allnetworks were trained on 40 mel-scale filter-bank features together with theenergy in each frame, and first and second temporal differences, yielding intotal 123 features per frame. Each feature was rescaled to have zero mean andunit variance over the training set. Networks were trained on the full 61-phoneset extended with an extra “end-of-sequence” token that was appended to eachtarget sequence. Similarly, we appended an all-zero frame at the end of eachinput sequence to indicate the end of the utterance. Decoding was performedusing the 61+1 phoneme set, while scoring was done on the 39 phoneme set."
"The proposed approach used a beam search with varying beam widths during the decoding stage. Define what ""beam search"" means in this context","Beam search in this context refers to a search algorithm used during decoding that explores a set of possible output sequences simultaneously, with the goal of finding the most likely sequence. The algorithm starts with a fixed number of sequences (the beam width) and iteratively expands the sequences that are most likely to be correct, based on the model's predictions and the input sequence. The beam width is gradually increased during decoding when the network fails to produce the ""end-of-sequence"" token with the narrower beam","Since there is no evidential information about the beam search other than the given paragraph, this question cannot be answered and requires external knowledge, specifically reference [24].","A left-to-right beam search over phoneme sequenceswas used during decoding [24]. Beam search was stoppedwhen the “end-of-sequence” token \left<\text{eos}\right> was emitted. Westarted with a beam width of 10, increasing it up to 40 when the network failedto produce \left<\text{eos}\right> with the narrower beam. As shown inFig. 2, decoding with a wider beam gives little-to-nonebenefit."
"The authors restrict batch size to 1 during training. Why did the authors do this, and what problems have might been encountered with higher batch size?","The authors restricted batch size to 1 during training to prevent overfitting due to the small size of the TIMIT dataset. With higher batch sizes, the model might have memorized the training data instead of learning generalizable patterns","Since there is no evidential information about how the authors decide the value of batch size and the paper lacks an ablation study about the batch size, this question cannot be answered. While the authors argue that proper regularization is required due to the small dataset size, the relation between regularization and batch size is not clear in this paper.","As TIMIT is a relatively small dataset, proper regularization is crucial. We usedthe adaptive weight noise as a main regularizer [22]. We firsttrained our models with a column norm constraint [23] with themaximum norm 1until the lowest development negative log-likelihood is achieved.333Applying the weight noise from the beginning of training caused severeunderfitting.During this time, \epsilon and \rho are set to 10^{-8} and 0.95,respectively. At this point, we began using the adaptive weight noise, andscaled down the model complexity cost L_{C} by a factor of 10, while disablingthe column norm constraints. Oncethe new lowest development log-likelihood was reached, we fine-tuned the modelwith a smaller \epsilon=10^{-10}, until we didnot observe the improvement in the development phoneme error rate (PER) for 100Kweight updates. Batch size 1 was used throughout thetraining."
"Did authors perform a hyperparameter search before deciding the values of batch size, epsilon and L_C during training?","Sure! Here's the answer to your question based on the provided context:

NO.

The authors did not perform a hyperparameter search before deciding the values of batch size, epsilon, and L_C during training. Instead, they used a fixed set of values that were determined through experimentation and trial and error","While the paper shows what value the author set for those hyperparameters, there is no evidential information about the hyperparameter search in this paper.","As TIMIT is a relatively small dataset, proper regularization is crucial. We usedthe adaptive weight noise as a main regularizer [22]. We firsttrained our models with a column norm constraint [23] with themaximum norm 1until the lowest development negative log-likelihood is achieved.333Applying the weight noise from the beginning of training caused severeunderfitting.During this time, \epsilon and \rho are set to 10^{-8} and 0.95,respectively. At this point, we began using the adaptive weight noise, andscaled down the model complexity cost L_{C} by a factor of 10, while disablingthe column norm constraints. Oncethe new lowest development log-likelihood was reached, we fine-tuned the modelwith a smaller \epsilon=10^{-10}, until we didnot observe the improvement in the development phoneme error rate (PER) for 100Kweight updates. Batch size 1 was used throughout thetraining."
How did the performance of the proposed model differ on shorter vs longer audio sequences?,"Sure! Here's the answer to your question based on the given context:

The proposed model performed better on shorter audio sequences than longer ones","While the proposed model showed good performance on shorter audio sequences (~200 phones), it failed to align most of phones on longer audio sequences.","The first column of Fig. 4 shows the number ofcorrectly aligned frames w.r.t. the utterance length (in frames) for some of theconsidered models. One can see that the baseline model was able to decodesequences up to about 120 phones when a single utterance was repeated, and up toabout 150 phones when different utterances were concatenated. Even when itfailed, it correctly aligned about 50 phones. On the other hand, the model withthe hybrid attention mechanism with convolutional features was able to alignsequences up to 200 phones long. However, once it began to fail, the model wasnot able to align almost all phones. The model with the smoothing behavedsimilarly to the one with convolutional features only."
"The authors mention that their baseline model was able to decode sequences upto 120 ""phones"" long, when processing an audio segment with repeated sounds or utterances. What does ""phones"" mean, in this context?","Phones refer to individual speech sounds or phoneemes in the TIMIT corpus, which is a dataset of speech recordings","While the paper does not include the definition of the term ""phone"", it is used as the unit of audio sequence, according to the experimental setup.","We closely followed the procedure in [16]. All experimentswere performed on the TIMIT corpus [19]. We used the train-dev-testsplit from the Kaldi [20] TIMIT s5 recipe. We trained on thestandard 462 speaker set with all SA utterances removed and used the 50 speakerdev set for early stopping. We tested on the 24 speaker core test set. Allnetworks were trained on 40 mel-scale filter-bank features together with theenergy in each frame, and first and second temporal differences, yielding intotal 123 features per frame. Each feature was rescaled to have zero mean andunit variance over the training set. Networks were trained on the full 61-phoneset extended with an extra “end-of-sequence” token that was appended to eachtarget sequence. Similarly, we appended an all-zero frame at the end of eachinput sequence to indicate the end of the utterance. Decoding was performedusing the 61+1 phoneme set, while scoring was done on the 39 phoneme set."
Do the authors use any datasets other than TIMIT to gauge the generalizability of their model?,"No, the authors do not use any datasets other than TIMIT to gauge the generalizability of their model",The authors used no dataset other than TIMIT for the evaluation of the model.,"In this paper, we evaluate attention-based models on aphoneme recognition task using the widely-used TIMITdataset. At each time step in generating an output sequence (phonemes),an attention mechanism selects or weighs the signals producedby a trained feature extraction mechanism at potentially all of the time stepsin the input sequence (speech frames). The weighted feature vector thenhelps to condition the generation of the next element of the output sequence.Since the utterances in this dataset are rathershort (mostly under 5 seconds), we measure theability of the considered models in recognizing much longerutterances which were created by artificially concatenatingthe existing utterances."
Authors explain the challenges AI models face when dealing with longer time sequences. Would human performance on this task also decline when analyzing longer sequnces?,"Sure! Here's the answer to your question based on the provided context:

Yes.

According to the authors, the AI model's performance degrades quickly with longer, concatenated utterances, indicating that human performance may also decline when analyzing longer sequences","While the authors explain the issue of dealing with longer time sequences, this question cannot be answered in this paper since there is no evidential information about human performance.","We start with a model proposed in[2] for the machine translation taskas the baseline. This model seems entirely vulnerable tothe issue of similar speech fragmentsbut despite ourexpectations it was competitive on the original test set, reaching 18.7% phonemeerror rate (PER). However, its performancedegraded quickly with longer, concatenated utterances. Weprovide evidence that this model adapted to track the absolutelocation in the input sequence of the content it isrecognizing, a strategy feasible for short utterances from theoriginal test set but inherently unscalable."
What are ASRGs?,Attention-based Recurrent Sequence Generators (ARSGs) are a type of Recurrent Neural Network (RNN) that use attention mechanisms to generate output sequences from input sequences,"ARSG is the sequence generator based on the RNN network, which utilizes the attention mechanism.","An attention-based recurrent sequence generator (ARSG) is a recurrent neuralnetwork that stochastically generates an output sequence (y_{1},\dots,y_{T})from an input x. In practice, x is often processed by an encoderwhich outputs a sequential input representation h=(h_{1},\ldots,h_{L}) moresuitable for the attention mechanism to work with."
What is CTC-training?,"CTC-training is a method of training speech recognizers using the connectionist temporal classification (CTC) algorithm, which is an extension of the RNNTransducer model","According to the related work section in the paper, CTC training is the deep-learning-based speech recognization model which performs MAP inference on the alignment as a latent random variable. There is no detailed information on how CTC training works in this paper and presumed to exist in [13].","Speech recognizers based on the connectionist temporal classification (CTC,[13]) and its extension, RNNTransducer [14], are the closest to the ARSG modelconsidered in this paper. They follow earlier work on end-to-end trainable deeplearning over sequences with gradient signals flowing through the alignmentprocess [15]. They have been shown to perform well on thephoneme recognition task [16]. Furthermore, the CTC wasrecently found to be able to directly transcribe text from speech without anyintermediate phonetic representation [17]. The considered ARSG is different from both the CTC and RNN Transducer in twoways. First, whereas the attention mechanism deterministically aligns the inputand the output sequences, the CTC and RNN Transducer treat the alignment as alatent random variable over which MAP (maximum a posteriori) inference isperformed. This deterministic nature of the ARSG’s alignment mechanism allowsbeam search procedure to be simpler. Furthermore, we empirically observe that amuch smaller beam width can be used with the deterministic mechanism, whichallows faster decoding (see Sec. 4.2 andFig. 2).Second, the alignment mechanism of both the CTC and RNN Transducer isconstrained to be “monotonic” to keep marginalization of the alignmenttractable. On the other hand, the proposed attention mechanism can result innon-monotonic alignment, which makes it suitable for a larger variety of tasksother than speech recognition."
"Similar to some of the related work, do the authors also use CTC-training for their proposed model?",NO. The authors do not use CTC-training for their proposed model,"The authors used attention-based recurrent sequence generator (ARSG) in this paper, which is different from CTC. While CTC based models are close to the proposed ARSG model, it has different characteristics from those models such as how the alignment is treated and whether the model can result in non-monotonic alignment.","Speech recognizers based on the connectionist temporal classification (CTC,[13]) and its extension, RNNTransducer [14], are the closest to the ARSG modelconsidered in this paper. They follow earlier work on end-to-end trainable deeplearning over sequences with gradient signals flowing through the alignmentprocess [15]. They have been shown to perform well on thephoneme recognition task [16]. Furthermore, the CTC wasrecently found to be able to directly transcribe text from speech without anyintermediate phonetic representation [17]. The considered ARSG is different from both the CTC and RNN Transducer in two ways. First,"
Explain how phoneme error rate (PER) is calculated?,"PER is calculated by counting the number of incorrect phonemes in a transcribed utterance and dividing that number by the total number of phonemes in the utterance.

In other words, PER is a measure of the proportion of incorrect phonemes in a transcribed utterance, and it is calculated as follows:

PER = (number of incorrect phonemes) / (total number of phonemes)

For example, if an utterance contains 100 phonemes, and 5 of those phonemes are incorrect, the PER would be 5%",While the phoneme error rate PER is used for the evaluation metric in this paper since there is no evidential information on how this metric can be calculated this question cannot be answered and requires external knowledge.,"As TIMIT is a relatively small dataset, proper regularization is crucial. We usedthe adaptive weight noise as a main regularizer [22]. We firsttrained our models with a column norm constraint [23] with themaximum norm 1until the lowest development negative log-likelihood is achieved.333Applying the weight noise from the beginning of training caused severeunderfitting.During this time, \epsilon and \rho are set to 10^{-8} and 0.95,respectively. At this point, we began using the adaptive weight noise, andscaled down the model complexity cost L_{C} by a factor of 10, while disablingthe column norm constraints. Oncethe new lowest development log-likelihood was reached, we fine-tuned the modelwith a smaller \epsilon=10^{-10}, until we didnot observe the improvement in the development phoneme error rate (PER) for 100Kweight updates. Batch size 1 was used throughout thetraining. All the models achieved competitivePERs (see Table 1).With the convolutional features, we see 3.7% relative improvement over thebaseline and further 5.9% with the smoothing. We start with a model proposed in[2] for the machine translation taskas the baseline. This model seems entirely vulnerable tothe issue of similar speech fragmentsbut despite ourexpectations it was competitive on the original test set, reaching 18.7% phonemeerror rate (PER). However, its performancedegraded quickly with longer, concatenated utterances. Weprovide evidence that this model adapted to track the absolutelocation in the input sequence of the content it isrecognizing, a strategy feasible for short utterances from theoriginal test set but inherently unscalable. Therefore, the contribution of this work is three-fold. For one, we present anovel purely neural speech recognition architecture based on an attentionmechanism, whose performance is comparable to that of the conventionalapproaches on the TIMIT dataset. Moreover, we propose a generic method ofadding location awareness to the attention mechanism. Finally, we introduce amodification of the attention mechanism to avoid concentrating the attention ona single frame, and thus avoid obtaining less “effective training examples”,bringing the PER down to 17.6%."
What point in StarGAN is valuable compared to Conditional GAN?,Flexibility in target domain steering,"paper's model can process multiple different domains. According to the parer, several different paragraph compares previous researches and their model. However, there is no evidence that, that models are related to conditional GAN.","Conditional GANs. GAN-based conditional image generation has also been actively studied. Prior studies have provided both the discriminator and generator with class information in order to generate samples conditioned on the class  [20, 21, 22]. Other recent approaches focused on generating particular images highly relevant to a given text description  [25, 30]. The idea of conditional image generation has also been successfully applied to domain transfer [9, 28], super-resolution imaging[14], and photo editing [2, 27].In this paper, we propose a scalable GAN framework that can flexibly steer the image translation to various target domains, by providing conditional domain information."
What term do they add in loss function to guarantee that generated images are not differentiable from real images?,adversarial loss,Adversarial loss. Paper noted that adversarial loss was adopted in order to make indistinguishable image,"Adversarial Loss. To make the generated images indistinguishable from real images, we adopt an adversarial loss\begin{split}\mathcal{L}_{adv}=&\thinspace{\mathbb{E}}_{x}\left[\log{{D}_{src}(x)}\right]\>\>+\\&\thinspace{\mathbb{E}}_{x,c}[\log{(1-{D}_{src}(G(x,c)))}],\end{split}(1)where G generates an image G(x,c) conditioned on both the input image x and the target domain label c, while D tries to distinguish between real and fake images. In this paper, we refer to the term {D}_{src}(x) as a probability distribution over sources given by D. The generator G tries to minimize this objective, while the discriminator D tries to maximize it. Generative Adversarial Networks. Generative adversarial networks (GANs) [3] have shown remarkable results in various computer vision tasks such as image generation [6, 24, 32, 8], image translation [7, 9, 33], super-resolution imaging [14], and face image synthesis [10, 16, 26, 31]. A typical GAN model consists of two modules: a discriminator and a generator. The discriminator learns to distinguish between real and fake samples, while the generator learns to generate fake samples that are indistinguishable from real samples. Our approach also leverages the adversarial loss to make the generated images as realistic as possible."
What value is assigned to unknown labels?,Zero,"zeros. According to paper, authors wrote that they simply assign zero values on unknown labels.","Mask Vector. To alleviate this problem, we introduce a mask vector m that allows StarGAN to ignore unspecified labels and focus on the explicitly known label provided by a particular dataset. In StarGAN, we use an n-dimensional one-hot vector to represent m, with n being the number of datasets. In addition, we define a unified version of the label as a vector\tilde{c}=[{c}_{1},...,{c}_{n},m],(7)where [\cdot] refers to concatenation, and {c}_{i} represents a vector for the labels of the i-th dataset. The vector of the known label {c}_{i} can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes. For the remaining n\mathbb{-}1 unknown labels we simply assign zero values. In our experiments, we utilize the CelebA and RaFD datasets, where n is two."
Model architecture of StarGAN is based on two other GAN models. What is it?,CycleGAN and PatchGAN,"The CycleGAN and the PatchGAN. According to the paper, StarGAN was basically generated from CycleGAN. In addition, paper note that discriminator was made by PatchGAN.","Network Architecture.Adapted from CycleGAN [33], StarGAN has the generator network composed of two convolutional layers with the stride size of two for downsampling, six residual blocks [5], and two transposed convolutional layers with the stride size of two for upsampling. We use instance normalization [29] for the generator but no normalization for the discriminator. We leverage PatchGANs [7, 15, 33] for the discriminator network, which classifies whether local image patches are real or fake. See the appendix (Section 7.2) for more details about the network architecture."
They perform only a qualitative analysis of the proposed model. Is it true?,"NO. While the authors do present qualitative evaluations of their proposed model through visual comparisons and human evaluations, they also perform quantitative evaluations using Amazon Mechanical Turk (AMT) and a facial expression classifier",False. They provided not only qualitative analysis but also quantitive analysis for their model.,"Qualitative evaluation. Fig. 4 shows the facial attribute transfer results on CelebA. We observed that our method provides a higher visual quality of translation results on test data compared to the cross-domain models. One possible reason is the regularization effect of StarGAN through a multi-task learning framework. In other words, rather than training a model to perform a fixed translation (e.g., brown-to-blond hair), which is prone to overfitting, we train our model to flexibly translate images according to the labels of the target domain. This allows our model to learn reliable features universally applicable to multiple domains of images with different facial attribute values. Quantitative evaluation protocol. For quantitative evaluations, we performed two user studies in a survey format using Amazon Mechanical Turk (AMT) to assess single and multiple attribute transfer tasks. Given an input image, the Turkers were instructed to choose the best generated image based on perceptual realism, quality of transfer in attribute(s), and preservation of a figure’s original identity. The options were four randomly shuffled images generated from four different methods. The generated images in one study have a single attribute transfer in either hair color (black, blond, brown), gender, or age. In another study, the generated images involve a combination of attribute transfers. Each Turker was asked 30 to 40 questions with a few simple yet logical questions for validating human effort. The number of validated Turkers in each user study is 146 and 100 in single and multiple transfer tasks, respectively. Quantitative results. Tables 1 and 2 show the results of our AMT experiment on single- and multi-attribute transfer tasks, respectively. StarGAN obtained the majority of votes for best transferring attributes in all cases.In the case of gender changes in Table 1, the voting difference between our model and other models was marginal, e.g., 39.1% for StarGAN vs. 31.4% for DIAT. However, in multi-attribute changes, e.g., the ‘G+A’ case in Table 2, the performance difference becomes significant, e.g., 49.8% for StarGAN vs. 20.3% for IcGAN), clearly showing the advantages of StarGAN in more complicated, multi-attribute transfer tasks. This is because unlike the other methods, StarGAN can handle image translation involving multiple attribute changes by randomly generating a target domain label in the training phase. Qualitative evaluation. As seen in Fig. 5, StarGAN clearly generates the most natural-looking expressions while properly maintaining the personal identity and facial features of the input. While DIAT and CycleGAN mostly preserve the identity of the input, many of their results are shown blurry and do not maintain the degree of sharpness as seen in the input. IcGAN even fails to preserve the personal identity in the image by generating male images. Quantitative evaluation. For a quantitative evaluation, we compute the classification error of a facial expression on synthesized images. We trained a facial expression classifier on the RaFD dataset (90%/10% splitting for training and test sets) using a ResNet-18 architecture [5], resulting in a near-perfect accuracy of 99.55%. We then trained each of image translation models using the same training set and performed image translation on the same, unseen test set. Finally, we classified the expression of these translated images using the above-mentioned classifier. As can be seen in Table 3, our model achieves the lowest classification error, indicating that our model produces the most realistic facial expressions among all the methods compared."
How many generators have to be trained if you train the cross-domain model for a 5-domain image translation task using the previous approach?,5 generators,"it takes 20 generators in order to do 5-domain image translation task. According to the paper, for k-domain task, k(k-1) generators are necessary.","However, existing models are both inefficient and ineffective in such multi-domain image translation tasks. Their inefficiency results from the fact that in order to learn all mappings among k domains, k(k\mathbb{-}1) generators have to be trained. Fig. 2 (a) illustrates how twelve distinct generator networks have to be trained to translate images among four different domains. Meanwhile, they are ineffective that even though there exist global features that can be learned from images of all domains such as face shapes, each generator cannot fully utilize the entire training data and only can learn from two domains out of k. Failure to fully utilize training data is likely to limit the quality of generated images. Furthermore, they are incapable of jointly training domains from different datasets because each dataset is partially labeled, which we further discuss in Section 3.2."
How many terms are used for the loss function of a generator?,3 terms,"it takes three. According to the equation, we can think that G means generator.","Full Objective. Finally, the objective functions to optimize G and D are written, respectively, as\mathcal{L}_{D}=-\mathcal{L}_{adv}+{\lambda}_{cls}\thinspace\mathcal{L}_{cls}^{r},(5)\mathcal{L}_{G}=\mathcal{L}_{adv}+{\lambda}_{cls}\thinspace\mathcal{L}_{cls}^{f}+{\lambda}_{rec}\thinspace\mathcal{L}_{rec},(6)where {\lambda}_{cls} and {\lambda}_{rec} are hyper-parameters that control the relative importance of domain classification and reconstruction losses, respectively, compared to the adversarial loss. We use {\lambda}_{cls}=1 and {\lambda}_{rec}=10 in all of our experiments."
What does “AMT” mean?,"Sure! Here's my answer:

Amazon Mechanical Turk (AMT)",AMT means Amazon Mechanical Turk.,"Quantitative evaluation protocol. For quantitative evaluations, we performed two user studies in a survey format using Amazon Mechanical Turk (AMT) to assess single and multiple attribute transfer tasks. Given an input image, the Turkers were instructed to choose the best generated image based on perceptual realism, quality of transfer in attribute(s), and preservation of a figure’s original identity. The options were four randomly shuffled images generated from four different methods. The generated images in one study have a single attribute transfer in either hair color (black, blond, brown), gender, or age. In another study, the generated images involve a combination of attribute transfers. Each Turker was asked 30 to 40 questions with a few simple yet logical questions for validating human effort. The number of validated Turkers in each user study is 146 and 100 in single and multiple transfer tasks, respectively."
What is an example attribute used in experiment of joint training?,facial expressions,We can't know from given paragraphs.,"Several image datasets come with a number of labeled attributes. For instance, the CelebA[19] dataset contains 40 labels related to facial attributes such as hair color, gender, and age, and the RaFD [13] dataset has 8 labels for facial expressions such as ‘happy’, ‘angry’ and ‘sad’. These settings enable us to perform more interesting tasks, namely multi-domain image-to-image translation, where we change images according to attributes from multiple domains. The first five columns in Fig. StarGAN: Unified Generative Adversarial Networks  for Multi-Domain Image-to-Image Translation show how a CelebA image can be translated according to any of the four domains, ‘blond hair’, ‘gender’, ‘aged’, and ‘pale skin’. We can further extend to training multiple domains from different datasets, such as jointly training CelebA and RaFD images to change a CelebA image’s facial expression using features learned by training on RaFD, as in the rightmost columns of Fig. StarGAN: Unified Generative Adversarial Networks  for Multi-Domain Image-to-Image Translation.  Effects of joint training. Fig. 6 shows qualitative comparisons between StarGAN-SNG and StarGAN-JNT, where the task is to synthesize facial expressions of images in CelebA. StarGAN-JNT exhibits emotional expressions with high visual quality, while StarGAN-SNG generates reasonable but blurry images with gray backgrounds. This difference is due to the fact that StarGAN-JNT learns to translate CelebA images during training but not StarGAN-SNG. In other words, StarGAN-JNT can leverage both datasets to improve shared low-level tasks such facial keypoint detection and segmentation. By utilizing both CelebA and RaFD, StarGAN-JNT can improve these low-level tasks, which is beneficial to learning facial expression synthesis."
How do they train a single-generator to learn a variety in translating images?,"By training the single generator G to translate input images into output images conditioned on the target domain label c, and using an auxiliary classifier to allow a single discriminator to control multiple domains","By randomly generated domain label, model can practice multiple domain.","Our goal is to train a single generator G that learns mappings among multiple domains. To achieve this, we train G to translate an input image x into an output image y conditioned on the target domain label c, G(x,c)\rightarrow y.We randomly generate the target domain label c so that G learns to flexibly translate the input image. We also introduce an auxiliary classifier [22] that allows a single discriminator to control multiple domains. That is, our discriminator produces probability distributions over both sources and domain labels, D:x\rightarrow\{{D}_{src}(x),{D}_{cls}(x)\}. Fig. 3 illustrates the training process of our proposed approach."
What problem can occur when you train a generator only with an adversarial loss and a domain classification loss?,"Sure! Here's the answer to your question based on the provided context:

The generator may not preserve the content of the input image","When loss function does not contains reconstruction loss, minimizing the losses does not guarantee that translated images preserve the content of its input images while changing only the domain-related part of the inputs.","Reconstruction Loss. By minimizing the adversarial and classification losses, G is trained to generate images that are realistic and classified to its correct target domain. However, minimizing the losses (Eqs. (1) and (3)) does not guarantee that translated images preserve the content of its input images while changing only the domain-related part of the inputs. To alleviate this problem, we apply a cycle consistency loss [9, 33] to the generator, defined as\mathcal{L}_{rec}={\mathbb{E}}_{x,c,c^{\prime}}[{||x-G(G(x,c),c^{\prime})||}_{1}],(4)where G takes in the translated image G(x,c) and the original domain label c^{\prime} as input and tries to reconstruct the original image x. We adopt the L1 norm as our reconstruction loss. Note that we use a single generator twice, first to translate an original image into an image in the target domain and then to reconstruct the original image from the translated image."
"Why do they divide domain classification loss in two terms for real image and fake image, respectively?",To optimize the domain classification of both real and fake images separately,"to translate x into an output image y, which is properly classified to the target domain c, authors divided classification loss in two terms.","Domain Classification Loss. For a given input image x and a target domain label c, our goal is to translate x into an output image y, which is properly classified to the target domain c. To achieve this condition, we add an auxiliary classifier on top of D and impose the domain classification loss when optimizing both D and G. That is, we decompose the objective into two terms: a domain classification loss of real images used to optimize D, and a domain classification loss of fake images used to optimize G. In detail, the former is defined as\mathcal{L}_{cls}^{r}={\mathbb{E}}_{x,c^{\prime}}[-\log{{D}_{cls}(c^{\prime}|x)}],(2)where the term {D}_{cls}(c^{\prime}|x) represents a probability distribution over domain labels computed by D. By minimizing this objective, D learns to classify a real image x to its corresponding original domain c^{\prime}. We assume that the input image and domain label pair (x,c^{\prime}) is given by the training data. On the other hand, the loss function for the domain classification of fake images is defined as\mathcal{L}_{cls}^{f}={\mathbb{E}}_{x,c}[-\log{{D}_{cls}(c|G(x,c))}].(3)In other words, G tries to minimize this objective to generate images that can be classified as the target domain c. "
How do they make StarGAN to ignore unspecified labels?,They use a mask vector m to ignore unspecified labels,"By using mask vector, StarGAN was allowed to ignore unspecified labels.","Mask Vector. To alleviate this problem, we introduce a mask vector m that allows StarGAN to ignore unspecified labels and focus on the explicitly known label provided by a particular dataset. In StarGAN, we use an n-dimensional one-hot vector to represent m, with n being the number of datasets. In addition, we define a unified version of the label as a vector\tilde{c}=[{c}_{1},...,{c}_{n},m],(7)where [\cdot] refers to concatenation, and {c}_{i} represents a vector for the labels of the i-th dataset. The vector of the known label {c}_{i} can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes. For the remaining n\mathbb{-}1 unknown labels we simply assign zero values. In our experiments, we utilize the CelebA and RaFD datasets, where n is two. Training Strategy. When training StarGAN with multiple datasets, we use the domain label \tilde{c} defined in Eq. (7) as input to the generator. By doing so, the generator learns to ignore the unspecified labels, which are zero vectors, and focus on the explicitly given label. The structure of the generator is exactly the same as in training with a single dataset, except for the dimension of the input label \tilde{c}. On the other hand, we extend the auxiliary classifier of the discriminator to generate probability distributions over labels for all datasets. Then, we train the model in a multi-task learning setting, where the discriminator tries to minimize only the classification error associated to the known label. For example, when training with images in CelebA, the discriminator minimizes only classification errors for labels related to CelebA attributes, and not facial expressions related to RaFD. Under these settings, by alternating between CelebA and RaFD the discriminator learns all of the discriminative features for both datasets, and the generator learns to control all the labels in both datasets."
How do they show the importance of mask vectors?,"By demonstrating the proper learning of the mask vector's intended role in image-to-image translations, specifically in synthesizing facial expressions of CelebA images using features learned from RaFD, and showing the failure of the model when a wrong mask vector is used","In order to ignore unspecified vectors and link different domains, mask vectors are necessray.","Mask Vector. To alleviate this problem, we introduce a mask vector m that allows StarGAN to ignore unspecified labels and focus on the explicitly known label provided by a particular dataset. In StarGAN, we use an n-dimensional one-hot vector to represent m, with n being the number of datasets. In addition, we define a unified version of the label as a vector\tilde{c}=[{c}_{1},...,{c}_{n},m],(7)where [\cdot] refers to concatenation, and {c}_{i} represents a vector for the labels of the i-th dataset. The vector of the known label {c}_{i} can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes. For the remaining n\mathbb{-}1 unknown labels we simply assign zero values. In our experiments, we utilize the CelebA and RaFD datasets, where n is two. Learned role of mask vector. In this experiment, we gave a one-hot vector c by setting the dimension of a particular facial expression (available from the second dataset, RaFD) to one. In this case, since the label associated with the second data set is explicitly given, the proper mask vector would be [0,1]. Fig. 7 shows the case where this proper mask vector was given and the opposite case where a wrong mask vector of [1,0] was given.When the wrong mask vector was used, StarGAN-JNT fails to synthesize facial expressions, and it manipulates the age of the input image. This is because the model ignores the facial expression label as unknown and treats the facial attribute label as valid by the mask vector. Note that since one of the facial attributes is ‘young’, the model translates the image from young to old when it takes in a zero vector as input. From this behavior, we can confirm that StarGAN properly learned the intended role of a mask vector in image-to-image translations when involving all the labels from multiple datasets altogether. In this paper, we proposed StarGAN, a scalable image-to-image translation model among multiple domains using a single generator and a discriminator. Besides the advantages in scalability, StarGAN generated images of higher visual quality compared to existing methods [16, 23, 33], owing to the generalization capability behind the multi-task learning setting. In addition, the use of the proposed simple mask vector enables StarGAN to utilize multiple datasets with different sets of domain labels, thus handling all available labels from them. We hope our work to enable users to develop interesting image translation applications across multiple domains.  We also introduce a simple but effective approach that enables joint training between domains of different datasets by adding a mask vector to the domain label. Our proposed method ensures that the model can ignore unknown labels and focus on the label provided by a particular dataset. In this manner, our model can perform well on tasks such as synthesizing facial expressions of CelebA images using features learned from RaFD, as shown in the rightmost columns of Fig. StarGAN: Unified Generative Adversarial Networks  for Multi-Domain Image-to-Image Translation. As far as our knowledge goes, our work is the first to successfully perform multi-domain image translation across different datasets. Overall, our contributions are as follows:\bulletWe propose StarGAN, a novel generative adversarial network that learns the mappings among multiple domains using only a single generator and a discriminator, training effectively from images of all domains.\bulletWe demonstrate how we can successfully learn multi-domain image translation between multiple datasets by utilizing a mask vector method that enables StarGAN to control all available domain labels.\bulletWe provide both qualitative and quantitative results on facial attribute transfer and facial expression synthesis tasks using StarGAN, showing its superiority over baseline models."
They propose StarGAN to overcome a limitation of high computational complexity of current image-to-image translation models. Is it true?,Yes,"It is false. There is no evidence that the motivation of StarGAN was made in order to overcome a limitation of high computational complexity, even it achieved that.","Another important advantage of our model is the scalability in terms of the number of parameters required. The last column in Table 3 shows that the number of parameters required to learn all translations by StarGAN is seven times smaller than that of DIAT and fourteen times smaller than that of CycleGAN. This is because StarGAN requires only a single generator and discriminator pair, regardless of the number of domains, while in the case of cross-domain models such as CycleGAN, a completely different model should be trained for each source-target domain pair."
How many benchmark dataset are used to evaluate the proposed model?,3,Three benchmark datasets are used.,"We utilize three standard citation network benchmark datasets—Cora, Citeseer and Pubmed (Sen et al., 2008)—and closely follow the transductive experimental setup of Yang et al. (2016). In all of these datasets, nodes correspond to documents and edges to (undirected) citations. Node features correspond to elements of a bag-of-words representation of a document. Each node has a class label. We allow for only 20 nodes per class to be used for training—however, honoring the transductive setup, the training algorithm has access to all of the nodes’ feature vectors. The predictive power of the trained models is evaluated on 1000 test nodes, and we use 500 additional nodes for validation purposes (the same ones as used by Kipf & Welling (2017)). The Cora dataset contains 2708 nodes, 5429 edges, 7 classes and 1433 features per node. The Citeseer dataset contains 3327 nodes, 4732 edges, 6 classes and 3703 features per node. The Pubmed dataset contains 19717 nodes, 44338 edges, 3 classes and 500 features per node."
It is not necessary to know the graph structure when using the proposed method. Is it true?,Yes,It is true.,"We have presented graph attention networks (GATs), novel convolution-style neural networks that operate on graph-structured data, leveraging masked self-attentional layers. The graph attentional layer utilized throughout these networks is computationally efficient (does not require costly matrix operations, and is parallelizable across all nodes in the graph), allows for (implicitly) assigning different importances to different nodes within a neighborhood while dealing with different sized neighborhoods, and does not depend on knowing the entire graph structure upfront—thus addressing many of the theoretical issues with previous spectral-based approaches. Our models leveraging attention have successfully achieved or matched state-of-the-art performance across four well-established node classification benchmarks, both transductive and inductive (especially, with completely unseen graphs used for testing)."
What does “attention coefficient” mean?,"Sure! Here's my answer:

The attention coefficient is a weighting factor that determines the importance of each feature in the input data","Normalized attention coefficient is a coefficient that is used for compute linear combination of the futures. However, there is no explanation about simple ""attention coefficient"".","Once obtained, the normalized attention coefficients are used to compute a linear combination of the features corresponding to them, to serve as the final output features for every node (after potentially applying a nonlinearity, \sigma):\vec{h}^{\prime}_{i}=\sigma\left(\sum_{j\in\mathcal{N}_{i}}\alpha_{ij}{\bf W}\vec{h}_{j}\right).(4)"
What point is different in GATs in terms of assigning weight compared to GCN?,"GATs allow for (implicitly) assigning different importances to nodes of a same neighborhood, whereas GCNs assume all neighbors have equal importance",GATs uses implicit weight assigning while GCN doesn't.,"The graph attentional layer described in subsection 2.1 directly addresses several issues that were present in prior approaches to modelling graph-structured data with neural networks:•Computationally, it is highly efficient: the operation of the self-attentional layer can be parallelized across all edges, and the computation of output features can be parallelized across all nodes. No eigendecompositions or similar costly matrix operations are required. The time complexity of a single GAT attention head computing F^{\prime} features may be expressed as O(|V|FF^{\prime}+|E|F^{\prime}), where F is the number of input features, and |V| and |E| are the numbers of nodes and edges in the graph, respectively. This complexity is on par with the baseline methods such as Graph Convolutional Networks (GCNs) (Kipf & Welling, 2017). Applying multi-head attention multiplies the storage and parameter requirements by a factor of K, while the individual heads’ computations are fully independent and can be parallelized.•As opposed to GCNs, our model allows for (implicitly) assigning different importances to nodes of a same neighborhood, enabling a leap in model capacity. Furthermore, analyzing the learned attentional weights may lead to benefits in interpretability, as was the case in the machine translation domain (e.g. the qualitative analysis of Bahdanau et al. (2015)).•The attention mechanism is applied in a shared manner to all edges in the graph, and therefore it does not depend on upfront access to the global graph structure or (features of) all of its nodes (a limitation of many prior techniques). This has several desirable implications:–The graph is not required to be undirected (we may simply leave out computing \alpha_{ij} if edge j\rightarrow i is not present).–It makes our technique directly applicable to inductive learning—including tasks where the model is evaluated on graphs that are completely unseen during training.•The recently published inductive method of Hamilton et al. (2017) samples a fixed-size neighborhood of each node, in order to keep its computational footprint consistent; this does not allow it access to the entirety of the neighborhood while performing inference. Moreover, this technique achieved some of its strongest results when an LSTM (Hochreiter & Schmidhuber, 1997)-based neighborhood aggregator is used. This assumes the existence of a consistent sequential node ordering across neighborhoods, and the authors have rectified it by consistently feeding randomly-ordered sequences to the LSTM. Our technique does not suffer from either of these issues—it works with the entirety of the neighborhood (at the expense of a variable computational footprint, which is still on-par with methods like the GCN), and does not assume any ordering within it.•As mentioned in Section 1, GAT can be reformulated as a particular instance of MoNet (Monti et al., 2016). More specifically, setting the pseudo-coordinate function to be u(x,y)=f(x)\|f(y), where f(x) represent (potentially MLP-transformed) features of node x and \| is concatenation; and the weight function to be w_{j}(u)=\mathrm{softmax}(\mathrm{MLP}(u)) (with the softmax performed over the entire neighborhood of a node) would make MoNet’s patch operator similar to ours. Nevertheless, one should note that, in comparison to previously considered MoNet instances, our model uses node features for similarity computations, rather than the node’s structural properties (which would assume knowing the graph structure upfront). We were able to produce a version of the GAT layer that leverages sparse matrix operations, reducing the storage complexity to linear in the number of nodes and edges and enabling the execution of GAT models on larger graph datasets. However, the tensor manipulation framework we used only supports sparse matrix multiplication for rank-2 tensors, which limits the batching capabilities of the layer as it is currently implemented (especially for datasets with multiple graphs). Appropriately addressing this constraint is an important direction for future work. Depending on the regularity of the graph structure in place, GPUs may not be able to offer major performance benefits compared to CPUs in these sparse scenarios. It should also be noted that the size of the “receptive field” of our model is upper-bounded by the depth of the network (similarly as for GCN and similar models). Techniques such as skip connections (He et al., 2016) could be readily applied for appropriately extending the depth, however. Lastly, parallelization across all the graph edges, especially in a distributed manner, may involve a lot of redundant computation, as the neighborhoods will often highly overlap in graphs of interest. For the inductive learning task, we apply a three-layer GAT model. Both of the first two layers consist of K=4 attention heads computing F^{\prime}=256 features (for a total of 1024 features), followed by an ELU nonlinearity. The final layer is used for (multi-label) classification: K=6 attention heads computing 121 features each, that are averaged and followed by a logistic sigmoid activation. The training sets for this task are sufficiently large and we found no need to apply L_{2} regularization or dropout—we have, however, successfully employed skip connections (He et al., 2016) across the intermediate attentional layer. We utilize a batch size of 2 graphs during training. To strictly evaluate the benefits of applying an attention mechanism in this setting (i.e. comparing with a near GCN-equivalent model), we also provide the results when a constant attention mechanism, a(x,y)=1, is used, with the same architecture—this will assign the same weight to every neighbor."
Which benchmark dataset used in this paper has the largest data size?,Pubmed,"It is hard to say a single benchmark set has largest size, because size definition is not given. If we can say the node size can represent dataset's size, then the Pubmed's dataset is the largest one.","We utilize three standard citation network benchmark datasets—Cora, Citeseer and Pubmed (Sen et al., 2008)—and closely follow the transductive experimental setup of Yang et al. (2016). In all of these datasets, nodes correspond to documents and edges to (undirected) citations. Node features correspond to elements of a bag-of-words representation of a document. Each node has a class label. We allow for only 20 nodes per class to be used for training—however, honoring the transductive setup, the training algorithm has access to all of the nodes’ feature vectors. The predictive power of the trained models is evaluated on 1000 test nodes, and we use 500 additional nodes for validation purposes (the same ones as used by Kipf & Welling (2017)). The Cora dataset contains 2708 nodes, 5429 edges, 7 classes and 1433 features per node. The Citeseer dataset contains 3327 nodes, 4732 edges, 6 classes and 3703 features per node. The Pubmed dataset contains 19717 nodes, 44338 edges, 3 classes and 500 features per node."
How do they solve the problem that the risk of the training process of self-attention could be unstable using multi-head attention?,By normalizing the attention coefficients using a softmax function,"to make self-attention stable, they used different mechanism which uses multi-head attention, similar to Vaswani et al.'s one.","To stabilize the learning process of self-attention, we have found extending our mechanism to employ multi-head attention to be beneficial, similarly to Vaswani et al. (2017). Specifically, K independent attention mechanisms execute the transformation of Equation 4, and then their features are concatenated, resulting in the following output feature representation:\vec{h}^{\prime}_{i}=\operatorname*{\scalebox{1.0}[1.5]{$\parallel$}}_{k=1}^{K}\sigma\left(\sum_{j\in\mathcal{N}_{i}}\alpha_{ij}^{k}{\bf W}^{k}\vec{h}_{j}\right)(5)where \parallel represents concatenation, \alpha_{ij}^{k} are normalized attention coefficients computed by the k-th attention mechanism (a^{k}), and {\bf W}^{k} is the corresponding input linear transformation’s weight matrix. Note that, in this setting, the final returned output, {\bf h}^{\prime}, will consist of KF^{\prime} features (rather than F^{\prime}) for each node."
How can it actually benefit that GAT can learn regardless of the graph structure?,"GAT can learn regardless of the graph structure because it does not require knowing the entire graph upfront, allowing for efficient and parallelizable computation on large graphs",There are no clue to find what graph can GAT process by its regardless of the graph structure.,"We have presented graph attention networks (GATs), novel convolution-style neural networks that operate on graph-structured data, leveraging masked self-attentional layers. The graph attentional layer utilized throughout these networks is computationally efficient (does not require costly matrix operations, and is parallelizable across all nodes in the graph), allows for (implicitly) assigning different importances to different nodes within a neighborhood while dealing with different sized neighborhoods, and does not depend on knowing the entire graph structure upfront—thus addressing many of the theoretical issues with previous spectral-based approaches. Our models leveraging attention have successfully achieved or matched state-of-the-art performance across four well-established node classification benchmarks, both transductive and inductive (especially, with completely unseen graphs used for testing)."
How can the GATs work well without any assumption of node order?,"The GATs work well without any assumption of node order because the attention mechanism is applied in a shared manner to all edges in the graph, and therefore it does not depend on upfront access to the global graph structure or (features of) all of its nodes","GATs work with the entirety of the neighborhood, like GCN did. This is the main difference with LSTM method and the reason why GATs work well without any assumption of node order.","The graph attentional layer described in subsection 2.1 directly addresses several issues that were present in prior approaches to modelling graph-structured data with neural networks:•Computationally, it is highly efficient: the operation of the self-attentional layer can be parallelized across all edges, and the computation of output features can be parallelized across all nodes. No eigendecompositions or similar costly matrix operations are required. The time complexity of a single GAT attention head computing F^{\prime} features may be expressed as O(|V|FF^{\prime}+|E|F^{\prime}), where F is the number of input features, and |V| and |E| are the numbers of nodes and edges in the graph, respectively. This complexity is on par with the baseline methods such as Graph Convolutional Networks (GCNs) (Kipf & Welling, 2017). Applying multi-head attention multiplies the storage and parameter requirements by a factor of K, while the individual heads’ computations are fully independent and can be parallelized.•As opposed to GCNs, our model allows for (implicitly) assigning different importances to nodes of a same neighborhood, enabling a leap in model capacity. Furthermore, analyzing the learned attentional weights may lead to benefits in interpretability, as was the case in the machine translation domain (e.g. the qualitative analysis of Bahdanau et al. (2015)).•The attention mechanism is applied in a shared manner to all edges in the graph, and therefore it does not depend on upfront access to the global graph structure or (features of) all of its nodes (a limitation of many prior techniques). This has several desirable implications:–The graph is not required to be undirected (we may simply leave out computing \alpha_{ij} if edge j\rightarrow i is not present).–It makes our technique directly applicable to inductive learning—including tasks where the model is evaluated on graphs that are completely unseen during training.•The recently published inductive method of Hamilton et al. (2017) samples a fixed-size neighborhood of each node, in order to keep its computational footprint consistent; this does not allow it access to the entirety of the neighborhood while performing inference. Moreover, this technique achieved some of its strongest results when an LSTM (Hochreiter & Schmidhuber, 1997)-based neighborhood aggregator is used. This assumes the existence of a consistent sequential node ordering across neighborhoods, and the authors have rectified it by consistently feeding randomly-ordered sequences to the LSTM. Our technique does not suffer from either of these issues—it works with the entirety of the neighborhood (at the expense of a variable computational footprint, which is still on-par with methods like the GCN), and does not assume any ordering within it.•As mentioned in Section 1, GAT can be reformulated as a particular instance of MoNet (Monti et al., 2016). More specifically, setting the pseudo-coordinate function to be u(x,y)=f(x)\|f(y), where f(x) represent (potentially MLP-transformed) features of node x and \| is concatenation; and the weight function to be w_{j}(u)=\mathrm{softmax}(\mathrm{MLP}(u)) (with the softmax performed over the entire neighborhood of a node) would make MoNet’s patch operator similar to ours. Nevertheless, one should note that, in comparison to previously considered MoNet instances, our model uses node features for similarity computations, rather than the node’s structural properties (which would assume knowing the graph structure upfront). We were able to produce a version of the GAT layer that leverages sparse matrix operations, reducing the storage complexity to linear in the number of nodes and edges and enabling the execution of GAT models on larger graph datasets. However, the tensor manipulation framework we used only supports sparse matrix multiplication for rank-2 tensors, which limits the batching capabilities of the layer as it is currently implemented (especially for datasets with multiple graphs). Appropriately addressing this constraint is an important direction for future work. Depending on the regularity of the graph structure in place, GPUs may not be able to offer major performance benefits compared to CPUs in these sparse scenarios. It should also be noted that the size of the “receptive field” of our model is upper-bounded by the depth of the network (similarly as for GCN and similar models). Techniques such as skip connections (He et al., 2016) could be readily applied for appropriately extending the depth, however. Lastly, parallelization across all the graph edges, especially in a distributed manner, may involve a lot of redundant computation, as the neighborhoods will often highly overlap in graphs of interest."
How performing a self-attention mechanism to graph can be useful in node classification?,"Performing a self-attention mechanism on a graph can be useful in node classification by allowing the model to focus on the most relevant neighbors of each node, capture complex contextual relationships between nodes, and generalize to unseen graphs","By using self-attention mechanism, model can find hidden meanings in the graph and it helps to do node classification.","Inspired by this recent work, we introduce an attention-based architecture to perform node classification of graph-structured data. The idea is to compute the hidden representations of each node in the graph, by attending over its neighbors, following a self-attention strategy. The attention architecture has several interesting properties: (1) the operation is efficient, since it is parallelizable across node-neighbor pairs; (2) it can be applied to graph nodes having different degrees by specifying arbitrary weights to the neighbors; and (3) the model is directly applicable to inductive learning problems, including tasks where the model has to generalize to completely unseen graphs. We validate the proposed approach on four challenging benchmarks: Cora, Citeseer and Pubmed citation networks as well as an inductive protein-protein interaction dataset, achieving or matching state-of-the-art results that highlight the potential of attention-based models when dealing with arbitrarily structured graphs."
Why was Graph Neural Networks(GNNs) proposed before even if Convolutional Neural Networks(CNNs) have been successful in many tasks?,"GNNs were proposed to address the limitations of CNNs in handling graph-structured data, which cannot be easily represented as a grid-like structure","Because CNN can process only grid-like structure, GNN, which can process general graph structure proposed.","Convolutional Neural Networks (CNNs) have been successfully applied to tackle problems such as image classification (He et al., 2016), semantic segmentation (Jégou et al., 2017) or machine translation (Gehring et al., 2016), where the underlying data representation has a grid-like structure. These architectures efficiently reuse their local filters, with learnable parameters, by applying them to all the input positions. There have been several attempts in the literature to extend neural networks to deal with arbitrarily structured graphs. Early work used recursive neural networks to process data represented in graph domains as directed acyclic graphs (Frasconi et al., 1998; Sperduti & Starita, 1997). Graph Neural Networks (GNNs) were introduced in Gori et al. (2005) and Scarselli et al. (2009) as a generalization of recursive neural networks that can directly deal with a more general class of graphs, e.g. cyclic, directed and undirected graphs. GNNs consist of an iterative process, which propagates the node states until equilibrium; followed by a neural network, which produces an output for each node based on its state. This idea was adopted and improved by Li et al. (2016), which propose to use gated recurrent units (Cho et al., 2014) in the propagation step."
What does “patchwise training” mean?,"Patchwise training refers to training a deep learning model on a subset of the input data, specifically on individual patches or regions of the image, rather than the entire image",there is no clue to define what does patchwise train mean.,"This method is efficient, both asymptotically and absolutely, and precludes the need for the complications in other works.Patchwise training is common [27, 2, 8, 28, 11], but lacks the efficiency of fully convolutional training.Our approach does not make use of pre- and post-processing complications, including superpixels [8, 16], proposals [16, 14], or post-hoc refinement by random fields or local classifiers [8, 16].Our model transfers recent success in classification [19, 31, 32] to dense prediction by reinterpreting classification nets as fully convolutional and fine-tuning from their learned representations.In contrast, previous works have applied small convnets without supervised pre-training [8, 28, 27]. In stochastic optimization, gradient computation is driven by the training distribution.Both patchwise training and fully-convolutional training can be made to produce any distribution, although their relative computational efficiency depends on overlap and minibatch size.Whole image fully convolutional training is identical to patchwise training where each batch consists of all the receptive fields of the units below the loss for an image (or collection of images).While this is more efficient than uniform sampling of patches, it reduces the number of possible batches.However, random selection of patches within an image may be recovered simply.Restricting the loss to a randomly sampled subset of its spatial terms (or, equivalently applying a DropConnect mask [36] between the output and the loss) excludes patches from the gradient computation."
How fast is the proposed method compared to the naive approach?,The proposed method is approximately 5 times faster than the naive approach,Proposed method is five times faster than the naive approach.,"Furthermore, while the resulting maps are equivalent to the evaluation of the original net on particular input patches, the computation is highly amortized over the overlapping regions of those patches.For example, while AlexNet takes 1.2 ms (on a typical GPU) to produce the classification scores of a 227\times 227 image, the fully convolutional version takes 22 ms to produce a 10\times 10 grid of outputs from a 500\times 500 image, which is more than 5 times faster than the naïve approach111Assuming efficient batching of single image inputs.The classification scores for a single image by itself take 5.4 ms to produce, which is nearly 25 times slower than the fully convolutional version.."
What does “receptive fields” mean?,"Sure! Here's my answer:

Receptive fields refer to the regions in the input image that a particular neuron in a higher layer of a convnet is sensitive to, and can respond to changes in those regions",receptive fields mean the higher layer which is connected to the original layer.,"Each layer of data in a convnet is a three-dimensional array of size h\times w\times d, where h and w are spatial dimensions, and d is the feature or channel dimension.The first layer is the image, with pixel size h\times w, and d color channels.Locations in higher layers correspond to the locations in the image they are path-connected to, which are called their receptive fields."
What is the expectation of training FCNs end-to-end for pixelwise prediction and from supervised pre-training?,To exceed the state-of-the-art in semantic segmentation without requiring additional machinery,They expect that FCN exceeds the state-of-the-art without further machinery,"We show that a fully convolutional network (FCN), trained end-to-end, pixels-to-pixels on semantic segmentation exceeds the state-of-the-art without further machinery.To our knowledge, this is the first work to train FCNs end-to-end (1) for pixelwise prediction and (2) from supervised pre-training.Fully convolutional versions of existing networks predict dense outputs from arbitrary-sized inputs.Both learning and inference are performed whole-image-at-a-time by dense feedforward computation and backpropagation.In-network upsampling layers enable pixelwise prediction and learning in nets with subsampled pooling."
How do they utilize fully-connected layers on dense prediction of image?,They utilize fully-connected layers for in-network upsampling to learn dense prediction of images,they used in-network upsampling.,"Dense prediction with convnets Several recent works have applied convnets to dense prediction problems,including semantic segmentation by Ning et al. [27], Farabet et al. [8], and Pinheiro and Collobert [28];boundary prediction for electron microscopy by Ciresan et al. [2] and for natural images by a hybrid neural net/nearest neighbor model by Ganin and Lempitsky [11];and image restoration and depth estimation by Eigen et al. [5, 6].Common elements of these approaches include•small models restricting capacity and receptive fields;•patchwise training [27, 2, 8, 28, 11];•post-processing by superpixel projection, random field regularization, filtering, or local classification [8, 2, 11];•input shifting and output interlacing for dense output [28, 11] as introduced by OverFeat [29];•multi-scale pyramid processing [8, 28, 11];•saturating \tanh nonlinearities [8, 5, 28]; and•ensembles [2, 11],whereas our method does without this machinery. However, we do study patchwise training 3.4 and “shift-and-stitch” dense output 3.2 from the perspective of FCNs.We also discuss in-network upsampling 3.3, of which the fully connected prediction by Eigen et al. [6] is a special case. In our experiments, we find that in-network upsampling is fast and effective for learning dense prediction.Our best segmentation architecture uses these layers to learn to upsample for refined prediction in Section 4.2."
Using filters allows us to see finer information. Is there any risk of using filters? Why?,"Yes, there is a risk of using filters.

Using filters can lead to a tradeoff where the filters see finer information, but have smaller receptive fields and take longer to compute, which can result in slower performance and increased computational cost. Additionally, the shift-and-stitch trick can prohibit the filters from accessing information at a finer scale than their original design, limiting their ability to capture detailed information",Might get smaller receptive fields and take longer time to compute.,"Simply decreasing subsampling within a net is a tradeoff: the filters see finer information, but have smaller receptive fields and take longer to compute.We have seen that the shift-and-stitch trick is another kind of tradeoff: the output is made denser without decreasing the receptive field sizes of the filters, but the filters are prohibited from accessing information at a finer scale than their original design."
They achieved state-of-the-art performance on several benchmark datasets. Is it true?,Yes,It is true.,"Fine-tuning from classification to segmentation gave reasonable predictions for each net.Even the worst model achieved \sim 75\% of state-of-the-art performance.The segmentation-equippped VGG net (FCN-VGG16) already appears to be state-of-the-art at 56.0 mean IU on val, compared to 52.6 on test [16].Training on extra data raises performance to 59.4 mean IU on a subset of val77footnotemark: 7.Training details are given in Section 4.3. SIFT Flow is a dataset of 2,688 images with pixel labels for 33 semantic categories (“bridge”, “mountain”, “sun”), as well as three geometric categories (“horizontal”, “vertical”, and “sky”).An FCN can naturally learn a joint representation that simultaneously predicts both types of labels.We learn a two-headed version of FCN-16s with semantic and geometric prediction layers and losses.The learned model performs as well on both tasks as two independently trained models, while learning and inference are essentially as fast as each independent model by itself.The results in Table 5, computed on the standard split into 2,488 training and 200 test images,101010Three of the SIFT Flow categories are not present in the test set.We made predictions across all 33 categories, but only included categories actually present in the test set in our evaluation.(An earlier version of this paper reported a lower mean IU, which included all categories either present or predicted in the evaluation.)show state-of-the-art performance on both tasks."
What is the evaluation metric to evaluate a semantic segmentation model?,Pixel accuracy,"Pixel accuracy, mean accuracy, mean IU and frequency weighted IU are answers.","Metrics We report four metrics from common semantic segmentation and scene parsing evaluations that are variations on pixel accuracy and region intersection over union (IU).Let n_{ij} be the number of pixels of class i predicted to belong to class j, where there are n_{\text{cl}} different classes, and let t_{i}=\sum_{j}n_{ij} be the total number of pixels of class i. We compute:•pixel accuracy: \sum_{i}n_{ii}/\sum_{i}t_{i} •mean accuraccy: (1/n_{\text{cl}})\sum_{i}n_{ii}/t_{i} •mean IU: (1/n_{\text{cl}})\sum_{i}n_{ii}/\left(t_{i}+\sum_{j}n_{ji}-n_{ii}\right) •frequency weighted IU: \left(\sum_{k}t_{k}\right)^{-1}\sum_{i}t_{i}n_{ii}/\left(t_{i}+\sum_{j}n_{ji}-n_{ii}\right) "
How do Projected Attention Layers work?,"Sure! Here's my answer:

Projected Attention Layers (PALs) work by injecting patterns back into the pre-trained transformer encoder through additional attention heads. These attention heads are projected onto the input sequence, allowing the model to focus on specific parts of the input when computing the attention weights. This allows the model to selectively focus on the patterns it has learned during training, and to use them to inform its attention when processing new input","Projected Attention Layers (PAL) takes the hidden state from the previous layer and runs parallel to the self-attention layer. In each PAL, the hidden size of the pretrained layer is linearly projected and then passed through its own self-attention layer before undergoing transformation back to the original hidden state size.","We then experiment with injecting the patterns back into the pre-trained transformer encoder.In particular, we injectthem through additional attention heads in the form of a Projected Attention Layer (PAL) (Stickland and Murray, 2019), along with the parameters of the original model. Details regarding PALs are described in Appendix A."
"What does ""injecting"" information into attention layers mean?",Injecting information into attention layers means adding predefined patterns or constraints to the attention weights in the transformer-based neural models to reduce run-time complexity while maintaining competitive accuracy,"Pattern injection means pre-determining the weights of the transformer layer's scaled dot product attention values, such that run-time complexity can be lowered while maintaining the interpretability of the model.","Meanwhile, a parallel line of research has exploredinjecting predefined patterns into attention matrices of transformers in an attemptto reduce the run-time complexity of self-attention while maintaining competitive accuracy.This can be done by either replacing theattention weights with a fixed matrix (Raganato et al., 2020; Tay et al., 2021; Xiao et al., 2020); or alternatively by guiding the attention weights through more flexible masking strategies (Mihaylov and Frank, 2019; Child et al., 2019; Guo et al., 2019; Li et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020; Bai et al., 2021). Once the important patterns are identified, there are two common approaches(i.e. fixing and masking)to injectthem as constraints to the attention matrices in the transformer-based neural models (see §3.2). The pipeline alsoenables two scenarios,in which injectingthe patterns can be beneficial:the first one is to train a new model with the patterns injected, while the second one is to enhance the original model. Overall, with the discovered patterns injected,our models are arguably more interpretable thanplain transformers on both tasks, as we know with certaintythe information encoded in each masked/fixed attention heads. To further justify our claim of interpretability, the attention headswith patterns injectedtend to have higher importance scores than the other heads666An illustrative example is shown in Appendix C.1, suggesting that such patterns are effectively leveraged by the model."
"Beyond simple addition of a constant to the attention values, how can the patterns applied back to the original model?","Modifying the attention weights through fixed or masked attention allows for injecting patterns into the model, but beyond simple addition, more sophisticated techniques can be used to apply patterns back to the original model. These include:

1. Pattern-aware attention: Instead of fixing or masking the attention weights, the patterns can be incorporated into the attention mechanism itself. For example, using pattern-aware attention weights that depend on the input sequence and the predicted pattern.
2. Pattern-based regularization: Regularizing the model with a pattern-based loss function that encourages the model to produce outputs consistent with the predicted patterns.
3. Pattern-aware input embedding: Injecting patterns into the input embedding layer, so that the model is aware of the patterns from the very beginning.
4. Pattern-based output transformation: Applying patterns to the output of the model, for example, by using pattern-based post-processing techniques to modify the output of the model",Patterns can be applied to a pretrained transformer either by adding a set of pre-computed constants or by adding an input-dependent weight matrix.,"In this work, we injectthe discovered patternsby either fixing or masking the attention weights prior to the softmax function.For fixed attention weights, the attention logits in the scaled-dot-product attention is replaced with a fixed (possibly input dependent) matrix such that:\textrm{FixAttn}(V,X)=\sigma(F^{(P)}(X))V(2)where \sigma is the softmax operation, V is the value vectors, and F(X)\in[0,1] computes a binary matrix from the input sequence X based on the predicated P for the specific pattern. Similarly, a pattern can also be injectedby casting a mask over the attention weights computed from the key and query vectors, as:\textrm{MaskAttn}(Q,K,V,X)=\sigma(M^{(P)}(X)+QK^{T})V(3)where M(X)\in[0,-\infty) computes the desired behaviour in the same fashion as F(X), and is added to the attention logits to approximate the multiplication of the attention distribution by a weight."
How can attention patterns from larger models be applied to smaller models if the models might differ in the size and number of attention layers?,"By fine-tuning the smaller models with the pre-trained attention patterns from the larger models, the smaller models can learn to adapt the patterns to their own architecture and task","BERTSum is the summarization model, and Cross-Segment BERT is the topic segmentation model. The paper does not discuss transferring patterns onto pretrained transformers of different architecture.","After verifying on the validation set, we discoverthree patternsconsistently existing in both tasks (over 50% of important heads).This suggests that important patterns are generalizable across multiple NLP tasks, whichis consistent with the findings in Bian et al. (2021). Further analysis also shows that the attention patterns are consistent after fine-tuning, where we report an average Jensen-Shannon Divergenceof 0.01 between the attention distributions of BERTSum across 3 random seeds.We hope our findings provide motivation for the in-depth study of pattern importance in different NLP tasks. Lastly, while it may be argued that this step of the pipeline can be automated by directly evaluating the importance and relevance of predefined patterns (e.g. syntax, discourse) based on intuitions, as indicated below, our interactive approach allows the discovery of interpretable patterns which otherwise would be hard to define due to the infinite search space of possible patterns. Next, we describe the three discovered patterns in detail."
What is the BERTSum model and how does it differ from just the BERT model?,"BERTSum is a specific extractive summarization model that leverages the contextualized representations from BERT to predict the most representative sentences for a given document. It differs from just using BERT as a feature extractor, as it includes a binary classifier to select the most relevant sentences for the summary","BERTSum is a specialized variant of BERT on the task of extractive summarization, picking out the sentences from a text to constitute its summary.","We adopt the popular BERTSum (Liu and Lapata, 2019) for extractive summarization. With the contextualized representation from BERT, the model uses a binary classifier to predict whether each sentence belongs in the summary. We train the model on the CNN/DM dataset (See et al., 2017), and use ROUGE (Lin, 2004) as the evaluation metric. Extractive summarization is the taskof picking the most representative sentences as the summary for the given document(s). Current state-of-the-art models, which are mostly based on large-scale pretrained language models Liu and Lapata (2019); Zhong et al. (2020); Jia et al. (2020); Ruan et al. (2022), can deliver good performance, but why and how such models work so well still remain an open question. In our case study, we adoptthe popular BERTSum(Liu and Lapata, 2019)."
How was performance measured and was the performance of the human-guided knowledge distilled model significantly higher?,"The performance was measured using ROUGE and F1 scores, and the human-guided knowledge distilled model significantly outperformed the baseline models for both summarization and topic segmentation tasks, with an average 15% improvement for summarization and 12% improvement for topic segmentation",Interpretability is measured with the PDR framework. Summarization performance measured in ROUGE is 15% better. Topic segmentation performance measured in F1 is 12% better.,"In both scenarios, we argue the interpretability of the resulting model is improved. We provide a justification of our claim based on the Predictive, Descriptive, and Relevant (PDR) framework proposed by Murdoch et al. (2019). Specifically, by injecting human-interpretable patterns into the model, we increase the model’s descriptive accuracy by explicitly encoding useful relationships between input tokens in the attention weights while simultaneously improving the predictive accuracy in task performance. Further, the patterns are relevant for the problem since they are discovered in the human-in-the-loop process and are verified to be important for the task. As shown in Table 1, our pattern-infused models outperform the plain transformer models for both the CNN/DM and NYT-50 datasets under all three settings(6 Layer 8 Heads, 6 Layer 12 Heads, and 6 Layer 12 Heads with BERT embeddings).Similarly for topic segmentation,results also show that the pattern-injection approach substantially outperforms the vanilla transformer across all metrics.It is worthemphasizing that the performance gain is slightly higher for summarization models. When normalized by the ROUGE scores of extractive oracle summaries555As reported by Liu and Lapata (2019), the ROUGE scores (R-1/R-2/R-L) of the oracle upper bound for CNN/DM and NYT-50 are respectively, 52.59/31.24/48.87 and 49.18/33.24/46.02., the pattern-infused summarization models achieve an average 15\% improvement over the baselines, while the topic-segmentation models achieve a 12\% improvement over the baselines.In-line with prior work (McCoy et al., 2020), we also find that the performance is consistent across random seeds, where we report an extremely low standard deviation of0.03 (ROUGE) and 0.002 (F1) for extractive summarization and topic segmentation, respectively. Overall, the results from our experiments convincingly demonstrates the benefits of our approach and the generalizability of the patterns discovered by our pipeline. In summary, the key aim of our experiments was to verify consistent improvements over our own baselines under the same settings in order to probethe benefits (effectiveness and efficiency) of the discovered patterns for the task. Therefore, we do not perform extensive tuning to achieve the same results reported by Liu and Lapata (2019)."
What was the highest performing estimation method for the authors' experiments,"The highest performing estimation method for the authors' experiments was the 6 Layer 12 Heads with BERT embeddings setting, which achieved the best results for both summarization and topic segmentation tasks","There is no """"highest"""" performer by any single measure.","As shown in Table 1, our pattern-infused models outperform the plain transformer models for both the CNN/DM and NYT-50 datasets under all three settings(6 Layer 8 Heads, 6 Layer 12 Heads, and 6 Layer 12 Heads with BERT embeddings).Similarly for topic segmentation,results also show that the pattern-injection approach substantially outperforms the vanilla transformer across all metrics.It is worthemphasizing that the performance gain is slightly higher for summarization models. When normalized by the ROUGE scores of extractive oracle summaries555As reported by Liu and Lapata (2019), the ROUGE scores (R-1/R-2/R-L) of the oracle upper bound for CNN/DM and NYT-50 are respectively, 52.59/31.24/48.87 and 49.18/33.24/46.02., the pattern-infused summarization models achieve an average 15\% improvement over the baselines, while the topic-segmentation models achieve a 12\% improvement over the baselines.In-line with prior work (McCoy et al., 2020), we also find that the performance is consistent across random seeds, where we report an extremely low standard deviation of0.03 (ROUGE) and 0.002 (F1) for extractive summarization and topic segmentation, respectively. Overall, the results from our experiments convincingly demonstrates the benefits of our approach and the generalizability of the patterns discovered by our pipeline."
What is the difference between calculating the Taylor expansion and the Hessian?,The Taylor expansion is a first-order approximation of the Hessian,"Hessian is the second-order partial derivative matrix itself, and Taylor expansion is the method used to approximate it.","We adapt the Taylor expansion method (Molchanov et al., 2019) as a proxy score for the head importance estimation.Following Li et al. (2021), we use the first-order expansion to avoid the overhead from computing the Hessian, where the gradient w.r.t. the validation loss is summed over all parameters of an attention head to estimate its importance."
"Instead of automatically identifying important attention patterns, why should a human be involved in this process?",Human involvement is necessary to ensure task-specific patterns are identified and to avoid overfitting the model to the validation set,Human involvement can provide interpretable results from the identified patterns and the performance enhancement from the pattern injection.,"Once the the most important heads are identified, their attention distributions are inspected to look for patterns. In this work, we propose and test a novel human-in-the-loop pipeline that to the best of our knowledge is the first attemptto combine research on analyzing self-attention with work on injecting patterns into attention matrices.To start, human users visually explore the attention matrices of transformers to identify task-specific patterns that could be formalized as a predicate. After quantitatively evaluating the patterns on the validation set, they can be injected into attention heads of transformer models to simultaneously improve task accuracy and make the model more efficient by sparsifying the attention matrices111The implementation of our work is publicly available at: https://github.com/raymondzmc/Attention-Pattern-Exploitation. This is in contrast to previous work that mostly focuses on the trade-off between two metrics. To discovertask-specific patterns,we analyze the top-3 most important heads of each layer, and look forhuman-interpretable relationships encoded in the attention weights. In practice, we use the instance-level interactions provided by the visual framework (Li et al., 2021), and randomly select 5 validation examples per task for our analysis.The entire process takesless than one hour to complete for each task, where we manually examinethe attention weights for less than half of the tokens for each example.It is worth noting that detailed analysis regarding the trade-off between human costandpattern recallwould require extensive user studies beyond the scope of this work. As future work, we plan to apply our pipeline to other NLP tasks (e.g. language modeling, abstractive summarization) and explore and verify whether the important patterns from one task can be transferable to another task. Similarly, we also plan to apply our pipeline to different model variants to examine and compare the patterns encoded in the attention weights.In the long term, our pipeline could be naturally automated by replacing the pattern discovery step with evaluating predefined linguistic patterns. However, assessing the efficiency gains from injecting such patterns (requiring ground-truth annotations) would require more in-depth studies beyond the scope of this paper.Finally, since human factors are an important aspect of interpretability, we plan to conduct extensive user studies across different NLP tasks and model sizes to examine the trade-off between human-cost and the coverage of discovered patterns."
"Why are the patterns only defined between pairs of tokens instead of other possible options (e.g., trios, sequence, sets)?","The patterns are defined between pairs of tokens because the authors are interested in modeling the ""attending to matching tokens"" behavior, where the attention value between two tokens is high when they are the same. This is more easily captured by considering pairs of tokens rather than larger groups",The structural definition of a pattern in this paper follows only naturally from the design of the attention mechanism.,"We define an attention pattern to be interpretable iff it can be modeled as a predicate P between any pair of input tokens (x_{i},x_{j}).For instance, the positional pattern ‘preceding token’ would be true if x_{i} appears before x_{j}. Candidate patterns can be discovered following two criteria:1) they are beneficial for the downstream task;2) they occur consistently among relevant tokens. This pattern describes the “attending to matching tokens” behaviour, wherethe attention value \alpha_{i,j}^{h} between input tokens x_{i} and x_{j} on the head h is high whenever x_{i}=x_{j}. For example, as shown in footnote 3 (i), the token ""photo"" mostly attends to other appearances of the token ""photo"" in the input sequence. To evaluate whether this pattern has a large global relevance for any head, we only consider tokens that appear at least twice within a single documents, and compute GR (Eq. 1), in which P(x_{i},x_{j}) holds if and only if x_{i}=x_{j}, i.e. \mathbbm{1}_{P(x_{i},x_{j})}=(\mathbbm{1}_{\textrm{freq}(x_{i})>1})\times(\mathbbm{1}_{x_{i}=x_{j}}). This pattern describes the behaviour of only attending to tokens within a text span.For summarization, these heads will focus on attending tokens within the same sentence (footnote 3 (ii)). Similarly, the same heads in topic segmentation models will focus on attending tokens within the same context (left or right).To evaluate this pattern, GR is computed with P(x_{i},x_{j}) holding iff x_{i} and x_{j} occur within the same text span. footnote 3 (C) reveals that this pattern appears more frequently in the mid to upper layers of the transformer encoder."
"Then, overall, is it true that a pattern is determined to have global relevance on an attention head if the pattern has a significantly higher attention value across all samples in a dataset (compared to other heads)?",Yes,"Yes, it is.","With a pattern discoveredin §3.1.2, this step confirms the pattern’s global relevance by empirically measuring the proportion of attention values aligning with the pattern. For each attention head, the associated predicate is evaluated over the entire validation set to ensure the pattern is not appearing by chance on the certain data that the user happen to look at. Specifically, we define the global relevance (GR) of a pattern P for a head h as follows:\textrm{GR}(P,h)=\frac{1}{|X|}\sum_{x\in X}\frac{\sum_{i}^{|x|}\sum_{j}^{|x|}\alpha_{i,j}^{(x,h)}\cdot\mathbbm{1}_{P(x_{i},x_{j})}}{|x|}(1)where the attention value from the token x_{i} to x_{j} on the head h for an input sample x, denoted as \alpha_{i,j}^{(x,h)}, is aggregated if and only if P(x_{i},x_{j}) holds. To validate a pattern’s generality, the relevance is averaged over the validation set X. This pattern describes the “attending to matching tokens” behaviour, wherethe attention value \alpha_{i,j}^{h} between input tokens x_{i} and x_{j} on the head h is high whenever x_{i}=x_{j}. For example, as shown in footnote 3 (i), the token ""photo"" mostly attends to other appearances of the token ""photo"" in the input sequence. To evaluate whether this pattern has a large global relevance for any head, we only consider tokens that appear at least twice within a single documents, and compute GR (Eq. 1), in which P(x_{i},x_{j}) holds if and only if x_{i}=x_{j}, i.e. \mathbbm{1}_{P(x_{i},x_{j})}=(\mathbbm{1}_{\textrm{freq}(x_{i})>1})\times(\mathbbm{1}_{x_{i}=x_{j}})."
How does using the fixed attention approach affect performance differently when compared to the masked attention approach?,"Using the fixed attention approach can lead to better performance when the number of applicable tokens is small, while the masked attention approach is more flexible and performs better when the number of applicable tokens is large",There is no discussion on how the performance difference is brought about.,"In this work, we injectthe discovered patternsby either fixing or masking the attention weights prior to the softmax function.For fixed attention weights, the attention logits in the scaled-dot-product attention is replaced with a fixed (possibly input dependent) matrix such that:\textrm{FixAttn}(V,X)=\sigma(F^{(P)}(X))V(2)where \sigma is the softmax operation, V is the value vectors, and F(X)\in[0,1] computes a binary matrix from the input sequence X based on the predicated P for the specific pattern. Similarly, a pattern can also be injectedby casting a mask over the attention weights computed from the key and query vectors, as:\textrm{MaskAttn}(Q,K,V,X)=\sigma(M^{(P)}(X)+QK^{T})V(3)where M(X)\in[0,-\infty) computes the desired behaviour in the same fashion as F(X), and is added to the attention logits to approximate the multiplication of the attention distribution by a weight. Although the two methods are very similar with respect to the improvement they contribute (see §4), masking allows more flexibility and is generally used for patterns with a large number of applicable tokens, while fixing is more rigid and better suited for a small number of applicable tokens."
"How did the authors measure ""interpretability""?","The authors measured interpretability using the Predictive, Descriptive, and Relevant (PDR) framework proposed by Murdoch et al. (2019), which considers the model's ability to accurately predict task performance, describe the input data, and provide relevant information for the problem at hand",The authors' definition of interpretability is measured in terms of higher importance scores in the attention heads.,"In both scenarios, we argue the interpretability of the resulting model is improved. We provide a justification of our claim based on the Predictive, Descriptive, and Relevant (PDR) framework proposed by Murdoch et al. (2019). Specifically, by injecting human-interpretable patterns into the model, we increase the model’s descriptive accuracy by explicitly encoding useful relationships between input tokens in the attention weights while simultaneously improving the predictive accuracy in task performance. Further, the patterns are relevant for the problem since they are discovered in the human-in-the-loop process and are verified to be important for the task. Overall, with the discovered patterns injected,our models are arguably more interpretable thanplain transformers on both tasks, as we know with certaintythe information encoded in each masked/fixed attention heads. To further justify our claim of interpretability, the attention headswith patterns injectedtend to have higher importance scores than the other heads666An illustrative example is shown in Appendix C.1, suggesting that such patterns are effectively leveraged by the model. In the context of Machine Learning, interpretability can be defined as the description of the internals of a model in a way that is understandable to humans (Gilpin et al., 2018).With the rise of deep learning, various techniques have been proposed to interpret the inner workings of neural NLP models. For example, probing classifiers are often used for finding linguistic or knowledge information learned by neural networks (Conneau et al., 2018; Tenney et al., 2019; Pimentel et al., 2020; Voita and Titov, 2020; Hou and Sachan, 2021; Aghazadeh et al., 2022), while behaviour testing aims at understanding how models behave through inferences under different controlled settings (McCoy et al., 2019; Ross and Pavlick, 2019; Ribeiro et al., 2020; Koh et al., 2021; Goel et al., 2021). In contrast, our work is an example of making interpretability an inherent attribute of the neural models (e.g. Chen and Ji (2020); Hu et al. (2021)),with human-distinguishable patterns revealing insights regarding a subset of parameters in the model."
Fast R-CNN is more than six times faster than some of the baseline models. Is it true?,"Yes. Fast R-CNN is more than six times faster than some of the baseline models, as shown in the comparison of training time and testing rate in Table 4","Probably true. Despite there is no provided baseline model, (in evidences) Fast R-CNN is more than six times faster than R-CNN is true.","Fast training and testing times are our second main result.Table 4 compares training time (hours), testing rate (seconds per image), and mAP on VOC07 between Fast R-CNN, R-CNN, and SPPnet.For VGG16, Fast R-CNN processes images 146\times faster than R-CNN without truncated SVD and 213\times faster with it.Training time is reduced by 9\times, from 84 hours to 9.5.Compared to SPPnet, Fast R-CNN trains VGG16 2.7\times faster (in 9.5 vs. 25.5 hours) and tests 7\times faster without truncated SVD or 10\times faster with it.Fast R-CNN also eliminates hundreds of gigabytes of disk storage, because it does not cache features."
"What was the problem of previous work, R-CNN in terms of the efficiency?",Training time and test time efficiency,R-CNN needs multi-stage pipeline training and time consumes when evaluate them.,"The Region-based Convolutional Network method (R-CNN) [9] achieves excellent object detection accuracy by using a deep ConvNet to classify object proposals.R-CNN, however, has notable drawbacks:1.Training is a multi-stage pipeline.R-CNN first fine-tunes a ConvNet on object proposals using log loss.Then, it fits SVMs to ConvNet features.These SVMs act as object detectors, replacing the softmax classifier learnt by fine-tuning.In the third training stage, bounding-box regressors are learned.2.Training is expensive in space and time.For SVM and bounding-box regressor training, features are extracted from each object proposal in each image and written to disk.With very deep networks, such as VGG16, this process takes 2.5 GPU-days for the 5k images of the VOC07 trainval set.These features require hundreds of gigabytes of storage.3.Object detection is slow.At test-time, features are extracted from each object proposal in each test image.Detection with VGG16 takes 47s / image (on a GPU). R-CNN is slow because it performs a ConvNet forward pass for each object proposal, without sharing computation.Spatial pyramid pooling networks (SPPnets) [11] were proposed to speed up R-CNN by sharing computation.The SPPnet method computes a convolutional feature map for the entire input image and then classifies each object proposal using a feature vector extracted from the shared feature map.Features are extracted for a proposal by max-pooling the portion of the feature map inside the proposal into a fixed-size output (e.g., 6\times 6).Multiple output sizes are pooled and then concatenated as in spatial pyramid pooling [15].SPPnet accelerates R-CNN by 10 to 100\times at test time.Training time is also reduced by 3\times due to faster proposal feature extraction."
What strategy is used to reduce time spent on detection?,Truncated SVD,Truncated SVD,Truncated SVD can reduce detection time by more than 30% with only a small (0.3 percentage point) drop in mAP and without needing to perform additional fine-tuning after model compression.Fig. 2 illustrates how using the top 1024 singular values from the 25088\times 4096 matrix in VGG16’s fc6 layer and the top 256 singular values from the 4096\times 4096 fc7 layer reduces runtime with little loss in mAP.Further speed-ups are possible with smaller drops in mAP if one fine-tunes again after compression.
What does “RoI” mean?,Region of Interest,The RoI is a rectangular window into a conv feature map.,"The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of H\times W (e.g., 7\times 7), where H and W are layer hyper-parameters that are independent of any particular RoI.In this paper, an RoI is a rectangular window into a conv feature map.Each RoI is defined by a four-tuple (r,c,h,w) that specifies its top-left corner (r,c) and its height and width (h,w)."
How was the ratio of extracted RoIs from object proposals?,"Sure! Here's the answer to your question based on the provided context:

The ratio of extracted RoIs from object proposals was 25% for foreground objects (IoU overlap with ground truth ≥ 0.5) and 75% for background objects (IoU overlap with ground truth in [0.1, 0.5)",25%,"During fine-tuning, each SGD mini-batch is constructed from N=2 images, chosen uniformly at random (as is common practice, we actually iterate over permutations of the dataset).We use mini-batches of size R=128, sampling 64 RoIs from each image.As in [9], we take 25% of the RoIs from object proposals that have intersection over union (IoU) overlap with a ground-truth bounding box of at least 0.5.These RoIs comprise the examples labeled with a foreground object class, i.e. u\geq 1.The remaining RoIs are sampled from object proposals that have a maximum IoU with ground truth in the interval [0.1,0.5), following [11].These are the background examples and are labeled with u=0.The lower threshold of 0.1 appears to act as a heuristic for hard example mining [8].During training, images are horizontally flipped with probability 0.5.No other data augmentation is used."
What does “scale invariance” mean?,"Sure! Here's my answer:

Scale invariance refers to the ability of a network to perform well on images of different sizes, without being affected by the scale of the image",scale invariance means there are different scales on the table for model,"The multi-scale approach, in contrast, provides approximate scale-invariance to the network through an image pyramid.At test-time, the image pyramid is used to approximately scale-normalize each object proposal.During multi-scale training, we randomly sample a pyramid scale each time an image is sampled, following [11], as a form of data augmentation.We experiment with multi-scale training for smaller networks only, due to GPU memory limits. We compare two strategies for achieving scale-invariant object detection: brute-force learning (single scale) and image pyramids (multi-scale).In either case, we define the scale s of an image to be the length of its shortest side."
How SPPnet address the drawback of R-CNN?,"SPPnet addresses the drawback of R-CNN by sharing computation, reducing the number of ConvNet forward passes required for each object proposal, resulting in 10 to 100 times faster test time and 3 times faster training time","SPPnet computes a convolutional feature map for the entire input image and then classifies each object proposal using a feature vector extracted from the shared feature map. By this method, SPPnet can solve time consuming issue which is occurred on R-CNN.","R-CNN is slow because it performs a ConvNet forward pass for each object proposal, without sharing computation.Spatial pyramid pooling networks (SPPnets) [11] were proposed to speed up R-CNN by sharing computation.The SPPnet method computes a convolutional feature map for the entire input image and then classifies each object proposal using a feature vector extracted from the shared feature map.Features are extracted for a proposal by max-pooling the portion of the feature map inside the proposal into a fixed-size output (e.g., 6\times 6).Multiple output sizes are pooled and then concatenated as in spatial pyramid pooling [15].SPPnet accelerates R-CNN by 10 to 100\times at test time.Training time is also reduced by 3\times due to faster proposal feature extraction."
Why does the author use a rectangular shaped RoI?,"The author uses a rectangular shaped RoI because it allows for a fixed spatial extent of the feature map, which is useful for computing the maximum value within a region of interest",in order to make a fixed feature map.,"The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of H\times W (e.g., 7\times 7), where H and W are layer hyper-parameters that are independent of any particular RoI.In this paper, an RoI is a rectangular window into a conv feature map.Each RoI is defined by a four-tuple (r,c,h,w) that specifies its top-left corner (r,c) and its height and width (h,w)."
Why does the network receive not only input of images but also a list of RoI as the input value?,"The network receives a list of RoIs as input because it needs to focus on specific regions of the image when detecting objects, rather than processing the entire image at once",RoI extracts a fixed-length feature vector from the feature map in the model.,"Third, the network is modified to take two data inputs: a list of images and a list of RoIs in those images. Fig. 1 illustrates the Fast R-CNN architecture.A Fast R-CNN network takes as input an entire image and a set of object proposals.The network first processes the whole image with several convolutional (conv) and max pooling layers to produce a conv feature map.Then, for each object proposal a region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map.Each feature vector is fed into a sequence of fully connected (fc) layers that finally branch into two sibling output layers: one that produces softmax probability estimates over K object classes plus a catch-all “background” class and another layer that outputs four real-valued numbers for each of the K object classes.Each set of 4 values encodes refined bounding-box positions for one of the K classes."
Perplexity is used to evaluate what performance?,"Sure! Here's my answer:

Model performance",Perplexity is used to evaluate the performance of the model.,"As is standard in language modeling, we use perplexity ( PPL ) to evaluate the performance of our models. Perplex- ity of a model over a sequence [ w 1 , . . . , w T ] is given by"
Does large model always shows better performance than small model?,"Sure! Here's the answer based on the given context:

NO",No.,"As can be seen from Table 3, our large model is on par with the existing state-of-the-art (Zaremba et al. 2014), despite having approximately 60% fewer parameters. Our small model signiﬁcantly outperforms other NLMs of sim- ilar size, even though it is penalized by the fact that the dataset already has OOV words replaced with < unk > (other models are purely word-level models). While lower perplex- ities have been reported with model ensembles (Mikolov and Zweig 2012), we do not include them here as they are not comparable to the current work."
Author shows that the batch size can affect the performance of the model. Is it true?,"Sure! Here's my answer:

Yes",No.,The models are trained by truncated backpropagation through time (Werbos 1990; Graves 2013). We backprop- agate for 35 time steps using stochastic gradient descent where the learning rate is initially set to 1 . 0 and halved if the perplexity does not decrease by more than 1 . 0 on the validation set after an epoch. On D ATA - S we use a batch size of 20 and on D ATA - L we use a batch size of 100 (for
How many times better performance is the model than the baseline?,"Sure! Here's the answer based on the given context:

The model outperforms the baseline by approximately 60% fewer parameters",almost same,"As can be seen from Table 3, our large model is on par with the existing state-of-the-art (Zaremba et al. 2014), despite having approximately 60% fewer parameters. Our small model signiﬁcantly outperforms other NLMs of sim- ilar size, even though it is penalized by the fact that the dataset already has OOV words replaced with < unk > (other models are purely word-level models). While lower perplex- ities have been reported with model ensembles (Mikolov and Zweig 2012), we do not include them here as they are not comparable to the current work."
What does “OOV” mean?,"Sure! Here's my answer:

OOV stands for ""Out-of-Vocabulary.""","Out-of-Vocabulary
composition: True",In Vocabulary Out-of-Vocabulary
Why did author try to use hierarchical softmax in optimization process?,To speed up training on D ATA - L,They use hierachical softmax in order to increase learning speed.,"Finally, in order to speed up training on D ATA - L we em- ploy a hierarchical softmax (Morin and Bengio 2005)—a common strategy for training language models with very large |V| —instead of the usual softmax. We pick the number of clusters c = (cid:100) (cid:112) |V|(cid:101) and randomly split V into mutually exclusive and collectively exhaustive subsets V 1 , . . . , V c of (approximately) equal size. 10 Then Pr ( w t +1 = j | w 1: t ) be- comes,"
Why did they feed word embedding with the sum of morpheme matrix as the input of the LSTM?,To capture subword information through morpheme embeddings that are summed at the input and output layers,in order to compare their model with MLBL model,"We compare our results against the morphological log- bilinear (MLBL) model from Botha and Blunsom (2014), whose model also takes into account subword information through morpheme embeddings that are summed at the input and output layers. As comparison against the MLBL mod- els is confounded by our use of LSTMs—widely known to outperform their feed-forward/log-bilinear cousins—we also train an LSTM version of the morphological NLM, where the input representation of a word given to the LSTM is a summation of the word’s morpheme embeddings. Con- cretely, suppose that M is the set of morphemes in a lan- guage, M ∈ R n ×|M| is the matrix of morpheme embed- dings, and m j is the j -th column of M (i.e. a morpheme embedding). Given the input word k , we feed the following representation to the LSTM:"
What is the advantage of using character-level input on language modeling?,"Sure! Here's the answer to your question based on the provided context:

Novel word generation",Several pre-processing progress is not necessary for character-level inputs. That is why character-level inputs are powerful.,"Another direction of work has involved purely character- level NLMs, wherein both input and output are charac- ters (Sutskever, Martens, and Hinton 2011; Graves 2013). Character-level models obviate the need for morphological tagging or manual feature engineering, and have the attrac- tive property of being able to generate novel words. How- ever they are generally outperformed by word-level models (Mikolov et al. 2012)."
What are the metrics used to evaluate the model performance in question answering experiments?,EM and F1 score,Exact Match and F1 score,"SQuAD is a machine comprehension dataset on a large set of Wikipedia articles, with more than 100,000 questions. The answer to each question is always a span in the context.The model is given a credit if its answer matches one of the human written answers.Two metrics are used to evaluate models: Exact Match (EM) and a softer metric, F1 score, which measures the weighted average of the precision and recall rate at character level.The dataset consists of 90k/10k train/dev question-context tuples with a large hidden test set.It is one of the largest available MC datasets with human-written questions and serves as a great test bed for our model."
Does BiDAF show state-of-the-art performance on benchmark dataset?,Yes,It is true.,"Our BiDAF model111Our code and interactive demo are available at: allenai.github.io/bi-att-flow/ outperforms all previous approaches on the highly-competitive Stanford Question Answering Dataset (SQuAD) test set leaderboard at the time of submission.With a modification to only the output layer, BiDAF achieves the state-of-the-art results on the CNN/DailyMail cloze test.We also provide an in-depth ablation study of our model on the SQuAD development set, visualize the intermediate feature spaces in our model, and analyse its performance as compared to a more traditional language model for machine comprehension (Rajpurkar et al., 2016)."
Author mentioned char-level embedding. Did they show the experimental result with char-level embedding?,Yes. The author did show experimental results with char-level embedding in Table 1(b),They showed that.,"Table 1(b) shows the performance of our model and its ablations on the SQuAD dev set. Both char-level and word-level embeddings contribute towards the model’s performance. We conjecture that word-level embedding is better at representing the semantics of each word as a whole, while char-level embedding can better handle out-of-vocab (OOV) or rare words. To evaluate bi-directional attention, we remove C2Q and Q2C attentions. For ablating C2Q attention, we replace the attended question vector \tilde{\bf U} with the average of the output vectors of the question’s contextual embedding layer (LSTM). C2Q attention proves to be critical with a drop of more than 10 points on both metrics. For ablating Q2C attention, the output of the attention layer, {\bf G}, does not include terms that have the attended Q2C vectors, \tilde{\bf H}. To evaluate the attention flow, we study a dynamic attention model, where the attention is dynamically computed within the modeling layer’s LSTM, following previous work (Bahdanau et al., 2015; Wang & Jiang, 2016). This is in contrast with our approach, where the attention is pre-computed before flowing to the modeling layer. Despite being a simpler attention mechanism, our proposed static attention outperforms the dynamically computed attention by more than 3 points. We conjecture that separating out the attention layer results in a richer set of features computed in the first 4 layers which are then incorporated by the modeling layer.We also show the performance of BiDAF with several different definitions of \alpha and {\bm{\beta}} functions (Equation 1 and 2) in Appendix B."
What does “cloze-style” mean?,"Cloze-style refers to a type of reading comprehension task where the reader is asked to fill in words that have been removed from a passage, to measure their ability to comprehend text",cloze-style indicates unfilled context.,"In this section, we evaluate our model on the task of question answering using the recently released SQuAD (Rajpurkar et al., 2016), which has gained a huge attention over a few months. In the next section, we evaluate our model on the task of cloze-style reading comprehension. In a cloze test, the reader is asked to fill in words that have been removed from a passage, for measuring one’s ability to comprehend text. Hermann et al. (2015) have recently compiled a massive Cloze-style comprehension dataset, consisting of 300k/4k/3k and 879k/65k/53k (train/dev/test) examples from CNN and DailyMail news articles, respectively. Each example has a news article and an incomplete sentence extracted from the human-written summary of the article. To distinguish this task from language modeling and force one to refer to the article to predict the correct missing word, the missing word is always a named entity, anonymized with a random ID.Also, the IDs must be shuffled constantly during test, which is also critical for full anonymization."
How many question-context tuple is used to train a model in question answering experiment?,"90,000","90,000 tuples are used to train.","SQuAD is a machine comprehension dataset on a large set of Wikipedia articles, with more than 100,000 questions. The answer to each question is always a span in the context.The model is given a credit if its answer matches one of the human written answers.Two metrics are used to evaluate models: Exact Match (EM) and a softer metric, F1 score, which measures the weighted average of the precision and recall rate at character level.The dataset consists of 90k/10k train/dev question-context tuples with a large hidden test set.It is one of the largest available MC datasets with human-written questions and serves as a great test bed for our model."
What is the difference between C2Q and Q2C?,"C2Q (Context-to-Query) attention focuses on which query words are most relevant to each context word, while Q2C (Query-to-Context) attention focuses on which context words have the closest similarity to one of the query words","C2Q deals about which query words are most relevant to each context word. However, Q2C deals about which context words have the closest similarity to one of the query words.","Context-to-query Attention.Context-to-query (C2Q) attention signifies which query words are most relevant to each context word.Let {\bf a}_{t}\in\mathbb{R}^{J} represent the attention weights on the query words by t-th context word, \sum{\bf a}_{tj}=1 for all t. The attention weight is computed by {\bf a}_{t}=\mathrm{softmax}({\bf S}_{t:})\in\mathbb{R}^{J},and subsequently each attended query vector is \tilde{{\bf U}}_{:t}=\sum_{j}{\bf a}_{tj}{\bf U}_{:j}.Hence \tilde{{\bf U}} is a 2d-by-T matrix containing the attended query vectors for the entire context. Query-to-context Attention.Query-to-context (Q2C) attention signifies which context words have the closest similarity to one of the query words and are hence critical for answering the query. We obtain the attention weights on the context words by {\bf b}=\mathrm{softmax}(\max_{col}({\bf S}))\in\mathbb{R}^{T}, where the maximum function (\max_{col}) is performed across the column. Then the attended context vector is \tilde{\bf h}=\sum_{t}{\bf b}_{t}{\bf H}_{:t}\in\mathbb{R}^{2d}. This vector indicates the weighted sum of the most important words in the context with respect to the query.\tilde{\bf h} is tiled T times across the column, thus giving \tilde{\bf H}\in\mathbb{R}^{2d\times T}."
Why did the author add one more direction in attention flow?,"To provide complimentary information. The author added both query-to-context and context-to-query attention mechanisms to capture both the contextual information and the query-specific information, leading to better performance","In order to obtain a query-aware context representation, author used bi-directional attention flow.","In this paper, we introduce the Bi-Directional Attention Flow  (BiDAF) network, a hierarchical multi-stage architecture for modeling the representations of the context paragraph at different levels of granularity (Figure 1).BiDAF includes character-level, word-level, and contextual embeddings, and uses bi-directional attention flow to obtain a query-aware context representation.Our attention mechanism offers following improvements to the previously popular attention paradigms.First, our attention layer is not used to summarize the context paragraph into a fixed-size vector.Instead, the attention is computed for every time step, and the attended vector at each time step, along with the representations from previous layers, is allowed to flow through to the subsequent modeling layer.This reduces the information loss caused by early summarization.Second, we use a memory-less attention mechanism.That is, while we iteratively compute attention through time as in Bahdanau et al. (2015), the attention at each time step is a function of only the query and the context paragraph at the current time step and does not directly depend on the attention at the previous time step.We hypothesize that this simplification leads to the division of labor between the attention layer and the modeling layer.It forces the attention layer to focus on learning the attention between the query and the context, and enables the modeling layer to focus on learning the interaction within the query-aware context representation (the output of the attention layer).It also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps.Our experiments show that memory-less attention gives a clear advantage over dynamic attention.Third, we use attention mechanisms in both directions, query-to-context and context-to-query, which provide complimentary information to each other. In this paper, we introduce BiDAF, a multi-stage hierarchical process that represents the context at different levels of granularity and uses a bi-directional attention flow mechanism to achieve a query-aware context representation without early summarization. The experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test. The ablation analyses demonstrate the importance of each component in our model. The visualizations and discussions show that our model is learning a suitable representation for MC and is capable of answering complex questions by attending to correct locations in the given paragraph. Future work involves extending our approach to incorporate multiple hops of the attention layer."
Why did the author adopt an attention mechanism as a base architecture of model?,"The author adopted an attention mechanism as a base architecture of the model because it enables the system to focus on a targeted area within a context paragraph or image that is most relevant to answer the question, improving the accuracy and efficiency of the model",attention mechanism is the finest model that author can used to their model.,"The tasks of machine comprehension (MC) and question answering (QA) have gained significant popularity over the past few years within the natural language processing and computer vision communities. Systems trained end-to-end now achieve promising results on a variety of tasks in the text and image domains.One of the key factors to the advancement has been the use of neural attention mechanism, which enables the system to focus on a targeted area within a context paragraph (for MC) or within an image (for Visual QA), that is most relevant to answer the question (Weston et al., 2015; Antol et al., 2015; Xiong et al., 2016a).Attention mechanisms in previous works typically have one or more of the following characteristics.First, the computed attention weights are often used to extract the most relevant information from the context for answering the question by summarizing the context into a fixed-size vector.Second, in the text domain, they are often temporally dynamic, whereby the attention weights at the current time step are a function of the attended vector at the previous time step.Third, they are usually uni-directional, wherein the query attends on the context paragraph or the image."
"According to Figure 2-(a), ‘May’ is far from other months in visualized word embed space. Why did this result happen?","May has multiple meanings in the English language, which causes it to be separated from other months in the visualized word embed space","Because ""May"" has several different meanings in English, ""May"" is far from other months.","We also visualize these two feature spaces using t-SNE in Figure 2. t-SNE is performed on a large fraction of dev data but we only plot data points corresponding to the months of the year.An interesting pattern emerges in the Word space, where May is separated from the rest of the months because May has multiple meanings in the English language.The contextual embedding layer uses contextual cues from surrounding words and is able to separate the usages of the word May. Finally we visualize the attention matrices for some question-context tuples in the dev data in Figure 3. In the first example, Where matches locations and in the second example, many matches quantities and numerical symbols. Also, entities in the question typically attend to the same entities in the context, thus providing a feature for the model to localize possible answers."
How does the author design the model to receive unfixed-size input?,"The author uses a regular-expression-based word tokenizer (PTB Tokenizer) to tokenize each paragraph and question, allowing the model to handle input of varying lengths","paper's model can process multiple different domains. According to the parer, several different paragraph compares previous researches and their model. However, there is no evidence that, that models are related to conditional GAN.","The model architecture used for this task is depicted in Figure 1. Each paragraph and question are tokenized by a regular-expression-based word tokenizer (PTB Tokenizer) and fed into the model. We use 100 1D filters for CNN char embedding, each with a width of 5.The hidden state size (d) of the model is 100.The model has about 2.6 million parameters.We use the AdaDelta (Zeiler, 2012) optimizer, with a minibatch size of 60 and an initial learning rate of 0.5, for 12 epochs.A dropout (Srivastava et al., 2014) rate of 0.2 is used for the CNN, all LSTM layers, and the linear transformation before the softmax for the answers.During training, the moving averages of all weights of the model are maintained with the exponential decay rate of 0.999.At test time, the moving averages instead of the raw weights are used.The training process takes roughly 20 hours on a single Titan X GPU. We also train an ensemble model consisting of 12 training runs with the identical architecture and hyper-parameters.At test time, we choose the answer with the highest sum of confidence scores amongst the 12 runs for each question."
How can author claim that only using absolute positional encoding with Transformer can show the relaxed structural inductive bias?,"The author can claim that only using absolute positional encoding with the Transformer can exhibit a too relaxed structural inductive bias because distance or Laplacian-based positional representations do not provide a measure of structural similarity between nodes, especially in the inductive case where two nodes are from different graphs",Author claims that Transformer only using absolute positional encoding often generates dissimilar representations for nodes with similar local structures.  It shows the relaxed structural inductive bias. The reason is that structural similarity between nodes is not contained  in absolute positional encoding.,"We further argue that only using absolute positional encoding with the Transformer would exhibit a too relaxed structural inductive bias which is not guaranteed to generate similar node representations even if two nodes have similar local structures. This is due to the fact that distance or Laplacian-based positional representations generally serve as structural or positional signatures but do not provide a measure of structural similarity between nodes, especially in the inductive case where two nodes are from different graphs. This is also empirically affirmed in Section 5 by their relatively worse performance without using our structural encoding. In contrast, the subgraph representations used in the structure-aware attention can be tailored to measure the structural similarity between nodes, and thus generate similar node-level representations if they possess similar attributes and surrounding structures. We can formally state this in the following theorem:"
How the author extract the subgraph of each node?,The author uses the k-hop neighborhood of each node to extract the subgraph of each node,Author extract entire k-hop subgraphs for each node.,"A more expressive extractor is to use a GNN to directly compute the representation of the entire k-hop subgraph centered at u rather than just the node representation u. Recent work has explored the idea of using subgraphs rather than subtrees around a node in GNNs, with positive experimental results (Zhang & Li, 2021; Wijesinghe & Wang, 2022), as well as being strictly more powerful than the 1-WL test (Zhang & Li, 2021). We follow the same setup as is done in Zhang & Li (2021), and adapt our GNN extractor to utilize the entire k-hop subgraph.The k-subgraph GNN extractor aggregates the updated node representations of all nodes within the k-hop neighborhood using a pooling function such as summation. Formally, if we denote by {\mathcal{N}}_{k}(u) the k-hop neighborhood of node u including itself, the representation of a node u is:\varphi(u,G)=\sum_{v\in{\mathcal{N}}_{k}(u)}\text{GNN}^{(k)}_{G}(v).(8)"
what is limitations of gnns?,"Limited expressiveness, over-smoothing, and over-squashing","There are two most widely adopted limitations of GNNs : over-smoothing and over-squashing.
Over-smoothing is a phenomenon that indicates representations of GNNs get similar to each others as the number of layers increases.
Over-squashing is a difficulty of node representations to contain messages that come from distant neighbors.
(False : The answer can be found only in P0)","While many different message-passing strategies have been proposed, some critical limitations have been uncovered in this class of GNNs. These include the limited expressiveness of GNNs (Xu et al., 2019; Morris et al., 2019), as well as known problems such as over-smoothing (Li et al., 2018, 2019; Chen et al., 2020; Oono & Suzuki, 2020) and over-squashing (Alon & Yahav, 2021).Over-smoothing manifests as all node representations converging to a constant after sufficiently many layers, while over-squashing occurs when messages from distant nodes are not effectively propagated through certain “bottlenecks” in a graph, since too many messages get compressed into a single fixed-length vector. Designing new architectures beyond neighborhood aggregation is thus essential to solve these problems. Message passing graph neural networks have recently been one of the leading methods for graph representation learning. An early seminal example is the GCN (Kipf & Welling, 2017), which was based on performing convolutions on the graph. Gilmer et al. (2017) reformulated the early GNNs into a framework of message passing GNNs, which has since then become the predominant framework of GNNs in use today, with extensive examples (Hamilton et al., 2017; Xu et al., 2019; Corso et al., 2020; Hu et al., 2020b; Veličković et al., 2018; Li et al., 2020a; Yang et al., 2022). However, as mentioned above, they suffer from problems of limited expressiveness, over-smoothing, and over-squashing."
how the structure information of graphs different from the positional information of graphs?,"The structure information of graphs differs from the positional information of graphs in that the former captures the relationships between nodes, while the latter only captures the spatial arrangement of nodes","Structural information of graphs serves a measure of structural similarity between nodes.
The reason is that most existing approaches fail to identify structural similarities between nodes, compared to SAIT that tries to capture structural similarities among nodes by encoding structural information.","Transformers (Vaswani et al., 2017), which have proved to be successful in natural language understanding (Vaswani et al., 2017), computer vision (Dosovitskiy et al., 2020), and biological sequence modeling (Rives et al., 2021), offer the potential to address these issues. Rather than only aggregating local neighborhood information in the message-passing mechanism, the Transformer architecture is able to capture interaction information between any node pair via a single self-attention layer.Moreover, in contrast to GNNs, the Transformer avoids introducing any structural inductive bias at intermediate layers, addressing the expressivity limitation of GNNs. Instead, it encodes structural or positional information about nodes only into input node features, albeit limiting how much information it can learn from the graph structure. Integrating information about the graph structure into the Transformer architecture has thus gained growing attention in the graph representation learning field. However, most existing approaches only encode positional relationships between nodes, rather than explicitly encoding the structural relationships. As a result, they may not identify structural similarities between nodes and could fail to model the structural interaction between nodes (see Figure 1). This could explain why their performance was dominated by sparse GNNs in several tasks (Dwivedi et al., 2022). We further argue that only using absolute positional encoding with the Transformer would exhibit a too relaxed structural inductive bias which is not guaranteed to generate similar node representations even if two nodes have similar local structures. This is due to the fact that distance or Laplacian-based positional representations generally serve as structural or positional signatures but do not provide a measure of structural similarity between nodes, especially in the inductive case where two nodes are from different graphs. This is also empirically affirmed in Section 5 by their relatively worse performance without using our structural encoding. In contrast, the subgraph representations used in the structure-aware attention can be tailored to measure the structural similarity between nodes, and thus generate similar node-level representations if they possess similar attributes and surrounding structures. We can formally state this in the following theorem:"
"What are the advantages of using relative encoding compared to absolute encoding, which performs well?","Using relative encoding compared to absolute encoding can provide several advantages, such as:

* Incorporating positional information into the self-attention mechanism, allowing the model to capture relative relationships between nodes on the graph.
* Distinguishing between true and created edges, as in the case of SAN (Kreuzer et al., 2021).
* Allowing for the incorporation of structural information into the graph Transformer, as in the case of centrality measures and shortest path distances (Ying et al., 2021).

These advantages can lead to improved performance on graph-related tasks, as relative encoding can provide a more nuanced understanding of the graph structure and node relationships","The advantage of relative encoding compared to absolute encoding is the flexibility of using representations of position or distances into the self-attention mechanism directly.
The reason is that self-attentions using absolute encoding only use node features, but self-attention mechanisms with relative encoding are able to utilize representations.","While the absolute encoding methods listed above can be used with message passing GNNs, they also play a crucial role in the (graph) Transformer architecture. Graph Transformer (Dwivedi & Bresson, 2021) provided an early example of how to generalize the Transformer architecture to graphs, using Laplacian eigenvectors as an absolute encoding and computing attention on the immediate neighborhood of each node, rather than on the full graph. SAN (Kreuzer et al., 2021) also used the Laplacian eigenvectors for computing an absolute encoding, but computed attention on the full graph, while distinguishing between true and created edges. Many graph Transformer methods also use a relative encoding (Shaw et al., 2018) in addition to absolute encoding. This strategy incorporates representations of the relative position or distances between nodes on the graph directly into the self-attention mechanism, as opposed to the absolute encoding which is only applied once to the input node features. Mialon et al. (2021) propose a relative encoding by means of kernels on graphs to bias the self-attention calculation, which is then able to incorporate positional information into Transformers via the choice of kernel function. Other recent work seeks to incorporate structural information into the graph Transformer, for example by encoding some carefully selected graph theoretic properties such as centrality measures and shortest path distances as positional representations (Ying et al., 2021) or by using GNNs to integrate the graph structure (Rong et al., 2020; Jain et al., 2021; Mialon et al., 2021; Shi et al., 2021)."
"What is the meaning of ""using graph structures explicitly""?",Using graph structures explicitly means incorporating information about the topological structure of the graph into the self-attention mechanism of the Transformer architecture,"The meaning of using graph structures explicitly is to explicity incorporate structural information into the self-attention.
The reason is that both P3 and P7 state the main contribution of SAT with paraphrasing.
P3 indicates that to consider graph structure explicitly is a main idea of SAT, and P7 emphasizes it as to incorporate structural information in the self-attention.","In this work, we address the critical question of how to encode structural information into a Transformer architecture. Our principal contribution is to introduce a flexible structure-aware self-attention mechanism that explicitly considers the graph structure and thus captures structural interaction between nodes. The resulting class of Transformers, which we call the Structure-Aware Transformer (SAT), can provide structure-aware representations of graphs, in contrast to most existing position-aware Transformers for graph-structured data. Specifically:•We reformulate the self-attention mechanism in Vaswani et al. (2017) as a kernel smoother andextend the original exponential kernel on node features to also account for local structures, by extracting a subgraph representation centered around each node.•We propose several methods for automatically generating the subgraph representations, enabling the resulting kernel smoother to simultaneously capture structural and attributed similarities between nodes. The resulting representations are theoretically guaranteed to be at least as expressive as the subgraph representations.•We demonstrate the effectiveness of SAT models on five graph and node property prediction benchmarks by showing it achieves better performance than state-of-the-art GNNs and Transformers. Furthermore, we show how SAT can easily leverage any GNN to compute the node representations which incorporate subgraph information and outperform the base GNN, making it an effortless enhancer of any existing GNN.•Finally, we show that we can attribute the performance gains to the structure-aware aspect of our architecture, and showcase how SAT is more interpretable than the classic Transformer with an absolute encoding. The key contribution of SAT is its ability to explicitly incorporate structural information in the self-attention. Here, we seek to demonstrate that this information provides crucial predictive information, and study how the choice of k affects the results. Figure 2(a) shows how the test MAE is impacted by varying k for k-subtree and k-subgraph extractors using PNA on the ZINC dataset. All models use the RWPE. k=0 corresponds to the vanilla Transformer only using absolute positional encoding, i.e. not using structure. We find that incorporating structural information leads to substantial improvement in performance, with optimal performance around k=3 for both k-subtree and k-subgraph extractors. As k increases beyond k=4, the performance in k-subtree extractors deteriorated, which is consistent with the observed phenomenon that GNNs work best in shallower networks (Kipf & Welling, 2017). We observe that k-subgraph does not suffer as much from this issue, underscoring a new aspect of its usefulness. On the other hand, k-subtree extractors are more computationally efficient and scalable to larger OGB datasets."
Can kernel functions other than the exponential kernel be applied?,"Yes, kernel functions other than the exponential kernel can be applied.

In the context of the paper, the authors propose using a more general kernel function, \kappa_graph, that can capture both attributed and structural similarity between nodes. This kernel function can be any function that compares a pair of subgraphs and returns a scalar value. The authors propose using the exponential kernel as a specific example of such a kernel function, but they note that other kernel functions could also be used","Any kernel that compares a pair of subgraphs can replace the exponential kernel.
The reason is that \kappa_{\text{graph}} is defined as \kappa_{\exp} in P6, but P2 states that \kappa_{\text{graph}} can be changed as other kernel that is able to compare a pair of subgraphs.","As presented above, self-attention in the Transformer can be rewritten as a kernel smoother where the kernel is a trainable exponential kernel defined on node features, and which only captures attributed similarity between a pair of nodes. The problem with this kernel smoother is that it cannot filter out nodes that are structurally different from the node of interest when they have the same or similar node features. In order to also incorporate the structural similarity between nodes, we consider a more generalized kernel that additionally accounts for the local substructures around each node. By introducing a set of subgraphs centered at each node, we define our structure-aware attention as:\text{SA-attn}(v):=\sum_{u\in V}\frac{\kappa_{\text{graph}}(S_{G}(v),S_{G}(u))}{\sum_{w\in V}\kappa_{\text{graph}}(S_{G}(v),S_{G}(w))}f(x_{u}),(5)where S_{G}(v) denotes a subgraph in G centered at a node v associated with node features \mathbf{X} and \kappa_{\text{graph}} can be any kernel that compares a pair of subgraphs. This new self-attention function not only takes the attributed similarity into account but also the structural similarity between subgraphs. It thus generates more expressive node representations than the original self-attention, as we will show in Section 4.4. Moreover, this self-attention is no longer equivariant to any permutation of nodes but only to nodes whose features and subgraphs coincide, which is a desirable property. In the rest of the paper, we will consider the following form of \kappa_{\text{graph}} that already includes a large class of expressive and computationally tractable models:\kappa_{\text{graph}}(S_{G}(v),S_{G}(u))=\kappa_{\exp}(\varphi(v,G),\varphi(u,G)),(6)where \varphi(u,G) is a structure extractor that extracts vector representations of some subgraph centered at u with node features \mathbf{X}. We provide several alternatives of the structure extractor below. It is worth noting that our structure-aware self-attention is flexible enough to be combined with any model that generates representations of subgraphs, including GNNs and (differentiable) graph kernels. For notational simplicity, we assume there are no edge attributes, but our method can easily incorporate edge attributes as long as the structure extractor can accommodate them. The edge attributes are consequently not considered in the self-attention computation, but are incorporated into the structure-aware node representations. In the structure extractors presented in this paper, this means that edge attributes were included whenever the base GNN was able to handle edge attributes."
Does graph property prediction task create one representation corresponding to the graph?,"Sure! Here's the answer to your question based on the provided context:

No.

In the context of graph property prediction, there is no single representation corresponding to the graph as a whole. Instead, different methods can be used to aggregate node-level representations into a graph representation, such as taking the average or sum, or using the embedding of a virtual [CLS] node","Yes, in general, a graph representation is created by aggregating node representations to predict graph property. The reason is that many existing works for graph property prediction aggregate node representations into a graph representation.","Finally, for graph property prediction, there are various ways to aggregate node-level representations into a graph representation, such as by taking the average or sum. Alternatively, one can use the embedding of a virtual [CLS] node (Jain et al., 2021) that is attached to the input graph without any connectivity to other nodes. We compare these approaches in Section 5."
What are the results we can get after going through the proposed Transformer-based model?,"Sure! Here's the answer to your question based on the provided context:

SAT models consistently outperform SOTA methods on various datasets, demonstrating their ability to combine the benefits of GNNs and Transformers, and providing better interpretability than Transformers","SAT predicts class of nodes and graphs better than other SOTA models. Also, SAT is more explainable compared to other transformer-based models.
The reason is that performance comparision shows SATs performs better than others,  and we can also explain the best range of substructure to consider with minimal hyperparameter tuning.","We show the performance of SATs compared to other GNNs and Transformers in Table 1 and 2. SAT models consistently outperform SOTA methods on these datasets, showing its ability to combine the benefits of both GNNs and Transformers. In particular, for the CODE2 dataset, our SAT models outperform SOTA methods by a large margin despite a relatively small number of parameters and minimal hyperparameter tuning, which will put it at the first place on the OGB leaderboard. We introduced the SAT model, which successfully incorporates structural information into the Transformer architecture and overcomes the limitations of the absolute encoding. In addition to SOTA empirical performance with minimal hyperparameter tuning, SAT also provides better interpretability than the Transformer."
What drives SAT possible to study the expressiveness of the output representation?,"The unique design of SAT, which includes a subgraph structure extractor, allows for a formal study of the expressiveness of the output representations, demonstrating that the node representation from a structure-aware attention layer is at least as expressive as the subgraph representation given by the structure extractor","Thanks to the unique design of our SAT, which relies on a subgraph structure extractor, it becomes possible to study the expressiveness of the output representations. More specifically, we formally show that the node representation from a structure-aware attention layer is at least as expressive as its subgraph representation given by the structure extractor.","The expressive power of graph Transformers compared to classic GNNs has hardly been studied, since the soft structural inductive bias introduced in absolute encoding is generally hard to characterize. Thanks to the unique design of our SAT, which relies on a subgraph structure extractor, it becomes possible to study the expressiveness of the output representations. More specifically, we formally show that the node representation from a structure-aware attention layer is at least as expressive as its subgraph representation given by the structure extractor, following the injectivity of the attention function with respect to the query:"
What is the effective value of small value k?,k=3,"We find that optimal performance around k=3 for both k-subtree and k-subgraph extractors. The reason is that Figure 2(a) shows how the test MAE is impacted by varying k for k-subtree and k-subgraph extractors using PNA on the ZINC dataset. 
As k increases till k=3, the performance of k-subtree and k-subgraph extractors gets better.
However, as k increases beyond k=4, the performance in k-subtree extractors deteriorated.","The key contribution of SAT is its ability to explicitly incorporate structural information in the self-attention. Here, we seek to demonstrate that this information provides crucial predictive information, and study how the choice of k affects the results. Figure 2(a) shows how the test MAE is impacted by varying k for k-subtree and k-subgraph extractors using PNA on the ZINC dataset. All models use the RWPE. k=0 corresponds to the vanilla Transformer only using absolute positional encoding, i.e. not using structure. We find that incorporating structural information leads to substantial improvement in performance, with optimal performance around k=3 for both k-subtree and k-subgraph extractors. As k increases beyond k=4, the performance in k-subtree extractors deteriorated, which is consistent with the observed phenomenon that GNNs work best in shallower networks (Kipf & Welling, 2017). We observe that k-subgraph does not suffer as much from this issue, underscoring a new aspect of its usefulness. On the other hand, k-subtree extractors are more computationally efficient and scalable to larger OGB datasets."
Why did author choose RWPE model to compare the effective k value?,"The author chose the RWPE model to compare the effective k value because it is a widely used and effective absolute positional representation that can be easily combined with the self-attention mechanism, and because it is learnable and can capture complex positional information","Author chooses RWPE for absolute positional encoding to show the outperformance of SAT is due to its structure-awareness.
The reason is that SAT is equivalent to a vanilla Transformer using RWPE that isn't structure-aware if k=0.
Hence, the performance improvement with the k growth shows the effectiveness of structure-aware encoding.","While the self-attention in Eq. (5) is structure-aware, most absolute encoding techniques are only position-aware and could therefore provide complementary information. Indeed, we find that the combination leads to further performance improvements, which we show in Section 5. We choose to use the RWPE (Dwivedi et al., 2022), though any other absolute positional representations, including learnable ones, can also be used. The key contribution of SAT is its ability to explicitly incorporate structural information in the self-attention. Here, we seek to demonstrate that this information provides crucial predictive information, and study how the choice of k affects the results. Figure 2(a) shows how the test MAE is impacted by varying k for k-subtree and k-subgraph extractors using PNA on the ZINC dataset. All models use the RWPE. k=0 corresponds to the vanilla Transformer only using absolute positional encoding, i.e. not using structure. We find that incorporating structural information leads to substantial improvement in performance, with optimal performance around k=3 for both k-subtree and k-subgraph extractors. As k increases beyond k=4, the performance in k-subtree extractors deteriorated, which is consistent with the observed phenomenon that GNNs work best in shallower networks (Kipf & Welling, 2017). We observe that k-subgraph does not suffer as much from this issue, underscoring a new aspect of its usefulness. On the other hand, k-subtree extractors are more computationally efficient and scalable to larger OGB datasets."
Did author experiment only with Mutagenicity dataset to show the interpretability of proposed model?,No,"No. Author conducted additional experiments for model interpretability in appendix.
The reason is that they mentioned it in section 5.5.","In addition to performance improvement, we show that SAT offers better model interpretability compared to the classic Transformer with only absolute positional encoding. We respectively train a SAT model and a Transformer with a CLS readout on the Mutagenicity dataset, and visualize the attention scores between the [CLS] node and other nodes learned by SAT and the Transformer in Figure 4. The salient difference between the two models is that SAT has structure-aware node embeddings, and thus we can attribute the following interpretability gains to that. While both models manage to identify some chemical motifs known for mutagenicity, such as NO{}_{2} and NH{}_{2}, the attention scores learned by SAT are sparser and more informative, meaning that SAT puts more attention weights on these known mutagenic motifs than the Transformer with RWPE. The vanilla Transformer even fails to put attention on some important atoms such as the H atoms in the NH{}_{2} group. The only H atoms highlighted by SAT are those in the NH{}_{2} group, suggesting that our SAT indeed takes the structure into account. More focus on these discriminative motifs makes the SAT model less influenced by other chemical patterns that commonly exist in the dataset, such as benzene, and thus leads to overall improved performance. More results are provided in the Appendix."
"what does ""kernel smoother"" mean?","a kernel smoother is a way to modify the self-attention mechanism in the Transformer architecture to also capture structural similarity between nodes, in addition to attributed similarity",Kernel-smoother is a kernel defined on node features to capture local structure of nodes by calculating similarity between node pairs.,"As presented above, self-attention in the Transformer can be rewritten as a kernel smoother where the kernel is a trainable exponential kernel defined on node features, and which only captures attributed similarity between a pair of nodes. The problem with this kernel smoother is that it cannot filter out nodes that are structurally different from the node of interest when they have the same or similar node features. In order to also incorporate the structural similarity between nodes, we consider a more generalized kernel that additionally accounts for the local substructures around each node. By introducing a set of subgraphs centered at each node, we define our structure-aware attention as:\text{SA-attn}(v):=\sum_{u\in V}\frac{\kappa_{\text{graph}}(S_{G}(v),S_{G}(u))}{\sum_{w\in V}\kappa_{\text{graph}}(S_{G}(v),S_{G}(w))}f(x_{u}),(5)where S_{G}(v) denotes a subgraph in G centered at a node v associated with node features \mathbf{X} and \kappa_{\text{graph}} can be any kernel that compares a pair of subgraphs. This new self-attention function not only takes the attributed similarity into account but also the structural similarity between subgraphs. It thus generates more expressive node representations than the original self-attention, as we will show in Section 4.4. Moreover, this self-attention is no longer equivariant to any permutation of nodes but only to nodes whose features and subgraphs coincide, which is a desirable property. In this work, we address the critical question of how to encode structural information into a Transformer architecture. Our principal contribution is to introduce a flexible structure-aware self-attention mechanism that explicitly considers the graph structure and thus captures structural interaction between nodes. The resulting class of Transformers, which we call the Structure-Aware Transformer (SAT), can provide structure-aware representations of graphs, in contrast to most existing position-aware Transformers for graph-structured data. Specifically:•We reformulate the self-attention mechanism in Vaswani et al. (2017) as a kernel smoother andextend the original exponential kernel on node features to also account for local structures, by extracting a subgraph representation centered around each node.•We propose several methods for automatically generating the subgraph representations, enabling the resulting kernel smoother to simultaneously capture structural and attributed similarities between nodes. The resulting representations are theoretically guaranteed to be at least as expressive as the subgraph representations.•We demonstrate the effectiveness of SAT models on five graph and node property prediction benchmarks by showing it achieves better performance than state-of-the-art GNNs and Transformers. Furthermore, we show how SAT can easily leverage any GNN to compute the node representations which incorporate subgraph information and outperform the base GNN, making it an effortless enhancer of any existing GNN.•Finally, we show that we can attribute the performance gains to the structure-aware aspect of our architecture, and showcase how SAT is more interpretable than the classic Transformer with an absolute encoding."
"What does ""overfitting"" mean?","Sure! Here's my answer:

Overfitting refers to a model that is too complex and learns the noise in the training data, rather than the underlying patterns",Over-fitting means that a large size model  has a difficulty to analyze data effectively due to its large number of parameters.,A central issue with applying (2) to highly multi-relational data is the rapid growth in number of parameters with the number of relations in the graph. In practice this can easily lead to overfitting on rare relations and to models of very large size.
What kind of problems will occur if the inverse triple exists?,"Sure! Here's the answer to your question based on the provided context:

Memorization of affected triplet pairs",The existence of inverse triples reduces a large part of the prediction task to memorization of affected triplet pairs.,"Link prediction algorithms are commonly evaluated on FB15k, a subset of the relational database Freebase, and WN18, a subset of WordNet containing lexical relations between words. In ? (?), a serious flaw was observed in both datasets: The presence of inverse triplet pairs t=(e_{1},r,e_{2}) and t^{\prime}=(e_{2},r^{-1},e_{1}) with t in the training set and t^{\prime} in the test set. This reduces a large part of the prediction task to memorization of affected triplet pairs. A simple baseline LinkFeat employing a linear classifier on top of sparse feature vectors of observed training relations was shown to outperform existing systems by a large margin. To address this issue, Toutanova and Chen proposed a reduced dataset FB15k-237 with all such inverse triplet pairs removed. We therefore choose FB15k-237 as our primary evaluation dataset. Since FB15k and WN18 are still widely used, we also include results on these datasets using the splits introduced by ? (?)."
"The proposed model accumulates the feature vector of neighboring nodes together regardless of type and direction of an edge, based on message-passing framework. Is this true?","True. The proposed model accumulates the feature vector of neighboring nodes together regardless of type and direction of an edge, based on a message-passing framework","No. RGCN considers the direction of edges. The reason is that RGCN updates of an entity representation using multi-graphs with directional relations. Also, R contains relations both in canonical direction and in inverse direction.""","Motivated by these architectures, we define the following simple propagation model for calculating the forward-pass update of an entity or node denoted by v_{i} in a relational (directed and labeled) multi-graph:h_{i}^{(l+1)}=\sigma\left(\sum_{r\in\mathcal{R}}\sum_{j\in\mathcal{N}^{r}_{i}}\frac{1}{c_{i,r}}W_{r}^{(l)}h_{j}^{(l)}+W_{0}^{(l)}h_{i}^{(l)}\right),(2)where \mathcal{N}^{r}_{i} denotes the set of neighbor indices of node i under relation r\in\mathcal{R}. c_{i,r} is a problem-specific normalization constant that can either be learned or chosen in advance (such as c_{i,r}=|\mathcal{N}^{r}_{i}|). Paragraph 10 : 1R contains relations both in canonical direction (e.g. born in) and in inverse direction (e.g. born in inv). 2Note that this represents a simplification of the message passing neural network proposed in (Gilmer et al. 2017) that suffices to include the aforementioned models as special cases."
Why author introduce two regularization methods in this model?,"Sure! Here's the answer to your question based on the provided context:

To address the issue of overfitting on rare relations and to reduce the size of the model","Authors use two regularization methods to handle overfitting issue.
The reason is that RGCN has to solve the issue that the number of parameters grows rapidly as the number of relations, and two regularization methods are used to learn the issue.","A central issue with applying (2) to highly multi-relational data is the rapid growth in number of parameters with the number of relations in the graph. In practice this can easily lead to overfitting on rare relations and to models of very large size. Paragraph 10 : To address this issue, we introduce two separate methods for regularizing the weights of R-GCN-layers: basisand block-diagonal-decomposition. With the basis decomposition, each W(l) r is defined as follows:"
How many basis components did author use?,"Sure! Based on the context you provided, the answer is:

2 basis components","The number of basis components is 100. The reason is that the number of basis components is the dimension of embeddings over the block size. We can observe that 500 is the dimension of embedding and 5 is the block size. Hence, the number of basis is 500/5 = 100.""","We evaluate hyperparameter choices on the respective validation splits. We found a normalization constant defined as c_{i,r}=c_{i}=\sum_{r}|\mathcal{N}^{r}_{i}| — in other words, applied across relation types – to work best. For FB15k and WN18, we report results using basis decomposition (Eq. 3) with two basis functions, and a single encoding layer with 200-dimensional embeddings. For FB15k-237, we found block decomposition (Eq. 4) to perform best, using two layers with block dimension 5\times 5 and 500-dimensional embeddings. We regularize the encoder through edge dropout applied before normalization, with dropout rate 0.2 for self-loops and 0.4 for other edges. Using edge droupout makes our training objective similar to that of denoising autoencoders (?). We apply l2 regularization to the decoder with a penalty of 0.01. Paragraph 10 : To address this issue, we introduce two separate methods for regularizing the weights of R-GCN-layers: basisand block-diagonal-decomposition. With the basis decomposition, each W(l)r is defined as follows:"
"What is the structural difference when applying RGCN to two tasks(entity classification, link prediction)?","The main structural difference between applying R-GCN to entity classification and link prediction is the use of a tensor factorization model in the latter task. In entity classification, the R-GCN outputs a set of node representations that are directly used for classification, while in link prediction, the R-GCN outputs a set of node representations that are then used as input to a tensor factorization model to predict the missing links","There is a structural difference between RGCN according to its tasks.
The reason is that RGCN use softmax classifier to classify entities, supported by P3, whereas it requires a tensor factorization decoder when it comes to predict links as P7.","Our entity classification model, similarly to ? (?), uses softmax classifiers at each node in the graph. The classifiers take node representations supplied by a relational graph convolutional network (R-GCN) and predict the labels. The model, including R-GCN parameters, is learned by optimizing the cross-entropy loss. Our link prediction model can be regarded as an autoencoder consisting of (1) an encoder: an R-GCN producing latent feature representations of entities, and (2) a decoder: a tensor factorization model exploiting these representations to predict labeled edges. Though in principle the decoder can rely on any type of factorization (or generally any scoring function), we use one of the simplest and most effective factorization methods: DistMult (?). We observe that our method achieves competitive results on standard benchmarks, outperforming, among other baselines, direct optimization of the factorization (i.e. vanilla DistMult). This improvement is especially large when we consider the more challenging FB15k-237 dataset (?). This result demonstrates that explicit modeling of neighborhoods in R-GCNs is beneficial for recovering missing facts in knowledge bases."
"In link prediction task, what method did author use to predict the edges on decoder part?",DistMult,"Authors mainly use vertex representations and a DistMult function to predict links with RGCN on decorder part.
The reason is that P0 states that RGCN uses DisMult to predict links, and P1 describes detailed process of RGCN how to predict links with results of encoder using its own scoring function.","In order to tackle this problem, we introduce a graph auto-encoder model, comprised of an entity encoder and a scoring function (decoder). The encoder maps each entity v_{i}\in\mathcal{V} to a real-valued vector e_{i}\in\mathbb{R}^{d}. The decoder reconstructs edges of the graph relying on the vertex representations; in other words, it scores (subject, relation, object)-triples through a function s:\mathbb{R}^{d}\times\mathcal{R}\times\mathbb{R}^{d}\to\mathbb{R}. Most existing approaches to link prediction (for example, tensor and neural factorization methods (?; ?; ?; ?; ?)) can be interpreted under this framework. The crucial distinguishing characteristic of our work is the reliance on an encoder. Whereas most previous approaches use a single, real-valued vector e_{i} for every v_{i}\in\mathcal{V} optimized directly in training, we compute representations through an R-GCN encoder with e_{i}=h_{i}^{(L)}, similar to the graph auto-encoder model introduced in ? (?) for unlabeled undirected graphs.Our full link prediction model is schematically depicted in Figure 2(b). Our link prediction model can be regarded as an autoencoder consisting of (1) an encoder: an R-GCN producing latent feature representations of entities, and (2) a decoder: a tensor factorization model exploiting these representations to predict labeled edges. Though in principle the decoder can rely on any type of factorization (or generally any scoring function), we use one of the simplest and most effective factorization methods: DistMult (?). We observe that our method achieves competitive results on standard benchmarks, outperforming, among other baselines, direct optimization of the factorization (i.e. vanilla DistMult). This improvement is especially large when we consider the more challenging FB15k-237 dataset (?). This result demonstrates that explicit modeling of neighborhoods in R-GCNs is beneficial for recovering missing facts in knowledge bases."
"What does ""semi-supervised"" mean?","Semi-supervised refers to the fact that only a portion of the nodes (entities) have labeled outputs, while the rest are unlabeled","Semi-supervised is to learn information from ground truth labels.
The reason is that, for semi-supervised task, RGCN minimizes loss on all labeled nodes ignoring unlabeled nodes.","For (semi-)supervised classification of nodes (entities), we simply stack R-GCN layers of the form (2), with a \mathrm{softmax}(\cdot) activation (per node) on the output of the last layer. We minimize the following cross-entropy loss on all labeled nodes (while ignoring unlabeled nodes):\mathcal{L}=-\sum_{i\in\mathcal{Y}}\sum_{k=1}^{K}t_{ik}\ln h_{ik}^{(L)}\,,(5)where \mathcal{Y} is the set of node indices that have labels and h_{ik}^{(L)} is the k-th entry of the network output for the i-th labeled node. t_{ik} denotes its respective ground truth label. In practice, we train the model using (full-batch) gradient descent techniques. A schematic depiction of our entity classification model is given in Figure 2(a)."
Why does a link prediction task require decoder modules unlike a entity classification task?,"The link prediction task requires decoder modules because it involves predicting new facts (triples) between entities, which requires generating new edges in the graph. In contrast, entity classification only involves assigning pre-defined labels to existing entities","A decoder is required for RGCN to predict links because it is impossible to model the possibility of edges without decoder.
The reason is that the possibility of edges is determined by the scores of corresponding triplets.
As RGCN itself does not calculates scores of triples, decoder modules are required to assign scores over them.","Link prediction deals with prediction of new facts (i.e. triples (subject, relation, object)). Formally, the knowledge base is represented by a directed, labeled graph G=(\mathcal{V},\mathcal{E},\mathcal{R}). Rather than the full set of edges \mathcal{E}, we are given only an incomplete subset \hat{\mathcal{E}}. The task is to assign scores f(s,r,o) to possible edges (s,r,o) in order to determine how likely those edges are to belong to \mathcal{E}. In order to tackle this problem, we introduce a graph auto-encoder model, comprised of an entity encoder and a scoring function (decoder). The encoder maps each entity v_{i}\in\mathcal{V} to a real-valued vector e_{i}\in\mathbb{R}^{d}. The decoder reconstructs edges of the graph relying on the vertex representations; in other words, it scores (subject, relation, object)-triples through a function s:\mathbb{R}^{d}\times\mathcal{R}\times\mathbb{R}^{d}\to\mathbb{R}. Most existing approaches to link prediction (for example, tensor and neural factorization methods (?; ?; ?; ?; ?)) can be interpreted under this framework. The crucial distinguishing characteristic of our work is the reliance on an encoder. Whereas most previous approaches use a single, real-valued vector e_{i} for every v_{i}\in\mathcal{V} optimized directly in training, we compute representations through an R-GCN encoder with e_{i}=h_{i}^{(L)}, similar to the graph auto-encoder model introduced in ? (?) for unlabeled undirected graphs.Our full link prediction model is schematically depicted in Figure 2(b)."
How did author made negative samples at decoder module in link prediction task?,The author used random corruption of either the subject or object of each positive example to create negative samples for the decoder module in the link prediction task,Authors sample \omega negative ones for each positive sample. They sample by randomly corrupting either the subject or the object of each positive example.,"As in previous work on factorization (?; ?), we train the model with negative sampling. For each observed example we sample \omega negative ones. We sample by randomly corrupting either the subject or the object of each positive example.We optimize for cross-entropy loss to push the model to score observable triples higher than the negative ones:\begin{split}\mathcal{L}=-\frac{1}{(1+\omega)|\mathcal{\hat{E}}|}\sum\limits_{(s,r,o,y)\in\mathcal{T}}y\log l\bigl{(}f(s,r,o)\bigr{)}+\\(1-y)\log\bigl{(}1-l\bigl{(}f(s,r,o)\bigr{)}\bigr{)}\,,\end{split}(7)where \mathcal{T} is the total set of real and corrupted triples, l is the logistic sigmoid function, and y is an indicator set to y=1 for positive triples and y=0 for negative ones."
Did datasets used in entity classification different with datasets used in link prediction?,"No. The datasets used in entity classification (AIFB, MUTAG, BGS, and AM) are the same as the datasets used in link prediction (FB15k and WN18)","There is no dataset that is commonly used to entity classification and link prediction.
The reason is that link prediction use FB15k-237, FB15k, and WN18, while entity classification use AIFB, MUTAG, BGS, and AM.","We evaluate our model on four datasets333http://dws.informatik.uni-mannheim.de/en/research/a-collection-of-benchmark-datasets-for-ml in Resource Description Framework (RDF) format (?): AIFB, MUTAG, BGS, and AM. Relations in these datasets need not necessarily encode directed subject-object relations, but are also used to encode the presence, or absence, of a specific feature for a given entity. In each dataset, the targets to be classified are properties of a group of entities represented as nodes. The exact statistics of the datasets can be found in Table 1. For a more detailed description of the datasets the reader is referred to ? (?). We remove relations that were used to create entity labels: employs and affiliation for AIFB, isMutagenic for MUTAG, hasLithogenesis for BGS, and objectCategory and material for AM. Link prediction algorithms are commonly evaluated on FB15k, a subset of the relational database Freebase, and WN18, a subset of WordNet containing lexical relations between words. In ? (?), a serious flaw was observed in both datasets: The presence of inverse triplet pairs t=(e_{1},r,e_{2}) and t^{\prime}=(e_{2},r^{-1},e_{1}) with t in the training set and t^{\prime} in the test set. This reduces a large part of the prediction task to memorization of affected triplet pairs. A simple baseline LinkFeat employing a linear classifier on top of sparse feature vectors of observed training relations was shown to outperform existing systems by a large margin. To address this issue, Toutanova and Chen proposed a reduced dataset FB15k-237 with all such inverse triplet pairs removed. We therefore choose FB15k-237 as our primary evaluation dataset. Since FB15k and WN18 are still widely used, we also include results on these datasets using the splits introduced by ? (?)."
"What characteristics of MUTAG and BGS datasets made the difference in performance, compare with other datasets(AIFB, AM)?","The unique characteristics of MUTAG and BGS datasets, such as the high-degree hub nodes and the presence of RDF format, contribute to the difference in performance compared to other datasets like AIFB and AM","Compared to AIFB and AM, labeled nodes in MUTAG and BGS only connect to high-degree nodes. The reason is that labeled entities in MUTAG and BGS are only connected via high-degree hub nodes.","Our model achieves state-of-the-art results on AIFB and AM. To explain the gap in performance on MUTAG and BGS it is important to understand the nature of these datasets. MUTAG is a dataset of molecular graphs, which was later converted to RDF format, where relations either indicate atomic bonds or merely the presence of a certain feature. BGS is a dataset of rock types with hierarchical feature descriptions which was similarly converted to RDF format, where relations encode the presence of a certain feature or feature hierarchy. Labeled entities in MUTAG and BGS are only connected via high-degree hub nodes that encode a certain feature."
"What does ""filtered setting"" mean?","Sure! Here's my answer:

The filtered setting refers to the evaluation metrics that have been adjusted to account for the relevance of the retrieved documents, rather than just the number of hits","Filtered setting means filtered MRR and filtered Hits at 1,3, and 10","We provide results using two commonly used evaluation metrics: mean reciprocal rank (MRR) and Hits at n (H@n).Following ? (?), both metrics can be computed in a raw and a filtered setting. We report both filtered and raw MRR (with filtered MRR typically considered more reliable), and filtered Hits at 1, 3, and 10."
"Why did author said that RGCN can be ""under a differentiable message passing interpretation""?","The author states that R-GCNs can be understood under a differentiable message passing interpretation because the model can be viewed as a special case of a more general message passing framework, which allows for differentiable message passing between nodes in the graph","Since RGCN is a kind of graph neural network, it can be interpreted as a differentiable message passing interpretation.
The reason is that RGCN is a sub-class of graph neural networks which are special cases of differentiable message-passing framework.","R-GCNs can further be seen as a sub-class of message passing neural networks (?), which encompass a number of previous neural models for graphs, including GCNs, under a differentiable message passing interpretation. Our model is primarily motivated as an extension of GCNs that operate on local graph neighborhoods (?; ?) to large-scale relational data. These and related methods such as graph neural networks (?) can be understood as special cases of a simple differentiable message-passing framework (?):h_{i}^{(l+1)}=\sigma\left(\sum_{m\in\mathcal{M}_{i}}g_{m}(h_{i}^{(l)},h_{j}^{(l)})\right),(1)where h_{i}^{(l)}\in\mathbb{R}^{d^{(l)}} is the hidden state of node v_{i} in the l-th layer of the neural network, with d^{(l)} being the dimensionality of this layer’s representations. Incoming messages of the form g_{m}(\cdot,\cdot) are accumulated and passed through an element-wise activation function \sigma(\cdot), such as the \mathrm{ReLU}(\cdot)=\max(0,\cdot).222Note that this represents a simplification of the message passing neural network proposed in (?) that suffices to include the aforementioned models as special cases. \mathcal{M}_{i} denotes the set of incoming messages for node v_{i} and is often chosen to be identical to the set of incoming edges. g_{m}(\cdot,\cdot) is typically chosen to be a (message-specific) neural network-like function or simply a linear transformation g_{m}(h_{i},h_{j})=Wh_{j} with a weight matrix W such as in ? (?)."
In what ways can it be said that the concatenation acts as a skip connection?,"The concatenation of the aggregated embeddings from different layers can be seen as a skip connection, as it allows the model to retain information from previous layers and combine it with new information from the current layer. This helps the model to capture longer-range dependencies and improve the overall performance of the embedding generation","Skip connection is to consider information from different search depths or layers simultaneously.
GraphSAGE use a set of weight matrices and concatenation to consider information from diverse search depths. 
It can be interpreted as a skip connection.
The reason is that a set of weight matrices are used to propagate information 
 between different layers of the model or search depths, while considering different search depth is a kind of skip-connection.","In this section, we describe the embedding generation, or forward propagation algorithm (Algorithm 1), which assumes that the model has already been trained and that the parameters are fixed.In particular, we assume that we have learned the parameters of K aggregator functions (denoted \textsc{aggregate}_{k},\forall k\in\{1,...,K\}), which aggregate information from node neighbors, as well as a set of weight matrices \mathbf{W}^{k},\forall k\in\{1,...,K\}, which are used to propagate information between different layers of the model or “search depths”.Section 3.2 describes how we train these parameters. Instead of training a distinct embedding vector for each node, we train a set of aggregator functions that learn to aggregate feature information from a node’s local neighborhood (Figure 1).Each aggregator function aggregates information from a different number of hops, or search depth, away from a given node.At test, or inference time, we use our trained system to generate embeddings for entirely unseen nodes by applying the learned aggregation functions.Following previous work on generating node embeddings, we design an unsupervised loss function that allows GraphSAGE to be trained without task-specific supervision.We also show that GraphSAGE can be trained in a fully supervised manner. Paragraph 10 :"
How did authro decide the size of the neighborhood?,"Sure! Here's my answer:

The author did not explicitly state how they decided the size of the neighborhood",The size of neighborhoods is set as 25 for 1-hop and 10 for 2-hop. The reason is that authors state that neighborhood sample sizes S1 = 25 and S2 = 10.,Paragraph 10 :
"Does ""previously unseen data"" mean node that did not appear on training data?",Yes,"Yes. Unseen data indicates node that is not contained in training data. 
The reason is that the purpose of this paper is to generate embeddings quickly for the systems which constantly encounter entirely new nodes and graphs.
Also, authors train algorithms with 2000-2004 data, while test is conducted on 2005 data.","However, previous works have focused on embedding nodes from a single fixed graph, and many real-world applications require embeddings to be quickly generated for unseen nodes, or entirely new (sub)graphs.This inductive capability is essential for high-throughput, production machine learning systems, which operate on evolving graphs and constantly encounter unseen nodes (e.g., posts on Reddit, users and videos on Youtube).An inductive approach to generating node embeddings also facilitates generalization across graphs with the same form of features:for example, one could train an embedding generator on protein-protein interaction graphs derived from a model organism, and then easily produce node embeddings for data collected on new organisms using the trained model. Our first two experiments are on classifying nodes in evolving information graphs, a task that is especially relevant to high-throughput production systems, which constantly encounter unseen data. Citation data.Our first task is predicting paper subject categories on a large citation dataset.We use an undirected citation graph dataset derived from the Thomson Reuters Web of Science Core Collection, corresponding to all papers in six biology-related fields for the years 2000-2005.The node labels for this dataset correspond to the six different field labels.In total, this is dataset contains 302,424 nodes with an average degree of 9.15.We train all the algorithms on the 2000-2004 data and use the 2005 data for testing (with 30% used for validation).For features, we used node degrees and processed the paper abstracts according Arora et al.’s [2] sentence embedding approach, with 300-dimensional word vectors trained using the GenSim word2vec implementation [30]."
How did author sampling a node's local neighborhood features to generate the embeddings?,"Sure! Here's the answer to your question based on the provided context:

The author sampled a node's local neighborhood features to generate the embeddings by forward sampling the required neighborhood sets (up to depth K) and then computing only the representations that are necessary to satisfy the recursion at each depth",Authors sample the required neighborhood sets (up to depth K).,"To extend Algorithm 1 to the minibatch setting, given a set of input nodes, we first forward sample the required neighborhood sets (up to depth K) and then we run the inner loop (line 3 in Algorithm 1), but instead of iterating over all nodes, we compute only the representations that are necessary to satisfy the recursion at each depth (Appendix A contains complete minibatch pseudocode)."
What are some examples of unseen nodes in the real world?,"Sure! Here's my answer:

Examples of unseen nodes in the real world include new users on social media platforms, new posts on Reddit, new videos on YouTube, and new organisms in biological research","New posts on Reddit, new users and videos on Youtube  are examples of unseen data.","However, previous works have focused on embedding nodes from a single fixed graph, and many real-world applications require embeddings to be quickly generated for unseen nodes, or entirely new (sub)graphs.This inductive capability is essential for high-throughput, production machine learning systems, which operate on evolving graphs and constantly encounter unseen nodes (e.g., posts on Reddit, users and videos on Youtube).An inductive approach to generating node embeddings also facilitates generalization across graphs with the same form of features:for example, one could train an embedding generator on protein-protein interaction graphs derived from a model organism, and then easily produce node embeddings for data collected on new organisms using the trained model."
How can LSTM that are not symmetric deal with neighborhoods that have no order?,"Sure! Here's the answer to your question based on the provided context:

LSTMs can deal with neighborhoods that have no order by applying the LSTMs to a random permutation of the node's neighbors",Authors permute neighbors of nodes to operate LSTMs to deal with unordered neighbor set.,"LSTM aggregator. We also examined a more complex aggregator based on an LSTM architecture [14].Compared to the mean aggregator, LSTMs have the advantage of larger expressive capability. However, it is important to note that LSTMs are not inherently symmetric (i.e., they are not permutation invariant), since they process their inputs in a sequential manner.We adapt LSTMs to operate on an unordered set by simply applying the LSTMs to a random permutation of the node’s neighbors."
Is 'perform entirely unseen graphs in PPI' means test the proposed model on new PPI graphs?,"Sure! Here's my answer:

Yes",Yes it is. The reason is that authors test on new graphs that is not shown in training.,"We test the performance of GraphSAGE on three benchmark tasks: (i) classifying academic papers into different subjects using the Web of Sciencecitation dataset, (ii) classifying Reddit posts as belonging to different communities, and (iii) classifying protein functions across various biological protein-protein interaction (PPI) graphs. Sections 4.1 and 4.2 summarize the datasets, and the supplementary material contains additional information.In all these experiments, we perform predictions on nodes that are not seen during training, and, in the case of the PPI dataset, we test on entirely unseen graphs."
How author create embeddings for each post in Reddit data? ,The author uses off-the-shelf 300-dimensional GloVe CommonCrawl word vectors to create embeddings for each post in the Reddit data,"For Reddit data, authors encode Glove word vectors with GraphSAGE.  They concatenated (i) the average embedding of the post title, (ii) the average embedding of all the post’s comments (iii) the post’s score, and (iv) the number of comments made on the post.","Reddit data.In our second task, we predict which community different Reddit posts belong to.Reddit is a large online discussion forum where users post and comment on content in different topical communities.We constructed a graph dataset from Reddit posts made in the month of September, 2014.The node label in this case is the community, or “subreddit”, that a post belongs to.We sampled 50 large communities and built a post-to-post graph, connecting posts if the same user comments on both.In total this dataset contains 232,965 posts with an average degree of 492.We use the first 20 days for training and the remaining days for testing (with 30% used for validation).For features, we use off-the-shelf 300-dimensional GloVe CommonCrawl word vectors [27]; for each post, we concatenated (i) the average embedding of the post title, (ii) the average embedding of all the post’s comments (iii) the post’s score, and (iv) the number of comments made on the post."
"The paper test classifying nodes on evolving graphs with 2 datasets(Citation and Reddit). 
What is the difference between two experiments with each dataset?","For the citation dataset, the experiment uses a static graph, while for the Reddit dataset, the experiment uses an evolving graph","Citation and Reddit data differ in the semantic of their edges and word vectors.
The reason is that an edge in citation indicates a paper cite others, whereas an edge in Reddit indicates that they're written by a common writer.
Also, Citation and Reddit data use GenSim and Glove word vectors as their feature, respectively.","Citation data.Our first task is predicting paper subject categories on a large citation dataset.We use an undirected citation graph dataset derived from the Thomson Reuters Web of Science Core Collection, corresponding to all papers in six biology-related fields for the years 2000-2005.The node labels for this dataset correspond to the six different field labels.In total, this is dataset contains 302,424 nodes with an average degree of 9.15.We train all the algorithms on the 2000-2004 data and use the 2005 data for testing (with 30% used for validation).For features, we used node degrees and processed the paper abstracts according Arora et al.’s [2] sentence embedding approach, with 300-dimensional word vectors trained using the GenSim word2vec implementation [30]. Reddit data.In our second task, we predict which community different Reddit posts belong to.Reddit is a large online discussion forum where users post and comment on content in different topical communities.We constructed a graph dataset from Reddit posts made in the month of September, 2014.The node label in this case is the community, or “subreddit”, that a post belongs to.We sampled 50 large communities and built a post-to-post graph, connecting posts if the same user comments on both.In total this dataset contains 232,965 posts with an average degree of 492.We use the first 20 days for training and the remaining days for testing (with 30% used for validation).For features, we use off-the-shelf 300-dimensional GloVe CommonCrawl word vectors [27]; for each post, we concatenated (i) the average embedding of the post title, (ii) the average embedding of all the post’s comments (iii) the post’s score, and (iv) the number of comments made on the post."
What are some examples of roles of node?,"Protein roles in various human tissues, such as cellular functions from gene ontology","The roles of nodes can be their protein function or categories.
The reason is this paper classifies the function of proteins for PPI network, while classifies categories of nodes in Reddit network and Citation network.","We now consider the task of generalizing across graphs, which requires learning about node roles rather than community structure.We classify protein roles—in terms of their cellular functions from gene ontology—in variousprotein-protein interaction (PPI) graphs, with each graph corresponding to a different human tissue [41].We use positional gene sets, motif gene sets and immunological signatures as features and geneontology sets as labels (121 in total), collected from the Molecular Signatures Database [34].The average graph contains 2373 nodes, with an average degree of 28.8.We train all algorithms on 20 graphs and then average prediction F1 scores on two test graphs (with two other graphs used for validation). We evaluate our algorithm on three node-classification benchmarks, which test GraphSAGE’s ability to generate useful embeddings on unseen data.We use two evolving document graphs based on citation data and Reddit post data (predicting paper and post categories, respectively), and a multi-graph generalization experiment based on a dataset of protein-protein interactions (predicting protein functions).Using these benchmarks, we show that our approach is able to effectively generate representations for unseen nodes and outperform relevant baselines by a significant margin: across domains, our supervised approach improves classification F1-scores by an average of 51% compared to using node features alone and GraphSAGE consistently outperforms a strong, transductive baseline [28], despite this baseline taking ∼100×{\sim}100\times∼ 100 × longer to run on unseen nodes.We also show that the new aggregator architectures we propose provide significant gains (7.4% on average) compared to an aggregator inspired by graph convolutional networks [17].Lastly, we probe the expressive capability of our approach and show, through theoretical analysis, that GraphSAGE is capable of learning structural information about a node’s role in a graph, despite the fact that it is inherently based on features (Section 5)."
How many experimental setting factors have been considered in experiments?,"6.

The experimental setting factors considered in the experiments are:

1. Aggregator architecture (4 variants of GraphSAGE)
2. Hyperparameter sweep (different sets of hyperparameters for each variant)
3. Baseline comparisons (4 baselines: random classifier, logistic regression, DeepWalk, and concatenation of raw features and DeepWalk embeddings)
4. Training objective (inductive/unsupervised vs. supervised)
5. Non-linearity (rectified linear units)
6. Neighborhood sample sizes (S_1=25 and S_2=10)","As far as my knowledge, seven factors are considered in experiments.
The reason is that authors state rectified linear units, K, two sample sizes, identical implementation of minibatch iterators, loss function, and neighborhood sampler.","Experimental set-up.To contextualize the empirical results on our inductive benchmarks, we compare against four baselines:a random classifer, a logistic regression feature-based classifier (that ignores graph structure), the DeepWalk algorithm [28] as a representative factorization-based approach, and aconcatenation of the raw features and DeepWalk embeddings.We also compare four variants of GraphSAGE that use the different aggregator functions (Section 3.3).Since, the “convolutional” variant of GraphSAGE is an extended, inductive version of Kipf et al’s semi-supervised GCN [17], we term this variant GraphSAGE-GCN.We test unsupervised variants of GraphSAGE  trained according to the loss in Equation (1), as well as supervised variants that are trained directly on classification cross-entropy loss.For all the GraphSAGE variants we used rectified linear units as the non-linearity and set K=2 with neighborhood sample sizes S_{1}=25 and S_{2}=10 (see Section 4.4 for sensitivity analyses). All models were implemented in TensorFlow [1] with the Adam optimizer [16] (except DeepWalk, which performed better with the vanilla gradient descent optimizer).We designed our experiments with the goals of (i) verifying the improvement of GraphSAGE over the baseline approaches (i.e., raw features and DeepWalk) and (ii) providing a rigorous comparison of the different GraphSAGE aggregator architectures.In order to provide a fair comparison, all models share an identical implementation of their minibatch iterators, loss function and neighborhood sampler (when applicable).Moreover, in order to guard against unintentional “hyperparameter hacking” in the comparisons between GraphSAGE aggregators, we sweep over the same set of hyperparameters for all GraphSAGE variants (choosing the best setting for each variant according to performance on a validation set).The set of possible hyperparameter values was determined on early validation tests using subsets of the citation and Reddit data that we then discarded from our analyses.The appendix contains further implementation details.555Code and links to the datasets: http://snap.stanford.edu/graphsage/"
Does the graph structure include the proportion of triangles and clustering coefficient of a node ?,No,"Yes it is. The proportion of triangles and clustering coefficient of a node implies information about graphs and serves as a building block for many more complicated structural motifs. 
The reason is that  clustering coefficient, the proportion of triangles that are closed within the node’s 1-hop neighborhood, is used as a popular measure of how clustered a node’s local neighborhood is.","In this section, we probe the expressive capabilities of GraphSAGE in order toprovide insight into how GraphSAGE can learn about graph structure, even though it is inherently based on features.As a case-study, we consider whether GraphSAGE can learn to predict the clustering coefficient of a node, i.e., the proportion of triangles that are closed within the node’s 1-hop neighborhood [38]. The clustering coefficient is a popular measure of how clustered a node’s local neighborhood is, and it serves as a building block for many more complicated structural motifs [3]. We can show that Algorithm 1 is capable of approximating clustering coefficients to an arbitrary degree of precision:"
"Instead of concatenate the three parts, what can be more effective way to consider these three behaviors collectively?","Sure! Here's my answer:

Considering these three behaviors collectively can be more effective by using a multi-modal approach, combining the three sequences into a single representation that captures the richness of the user's behavior","Retention indicates a state that whether a set of apps is installed now. However, other behavior data is operation composed of apps and corresponding dates. The result is that retention is a set of apps installed on one’s phone at present, whereas installation and uninstallation operations are composed of installed apps and corresponding dates.","As stated in Section 3.1, behaviors of each user are preprocessed into one’s “retention” and four sequences defined as follows."
Did author consider only three user behaviors on mobile usage?,"No. The author did not consider only three user behaviors on mobile usage. The context mentions several user behaviors, including app install advertisements, mobile app recommendations, and news recommendations, which are all based on rich preference information from user behaviors on mobile apps","Authors only consider three behaviors: retention, installation, and un-installation.
The reason is that authors note the three behaviors, and also model behavior type embeddings of those 3 behaviors.","User behaviors on mobile apps usage contain rich preference information and have been used in a variety of applications (Lu et al., 2014). The most significant of which is app install advertisements (Gogel, 2018; Lee and Shin, 2016) and mobile app recommendations (Zhuet al., 2014). Yahoo posted a large scale prediction engine for app install advertising based on a two-step logistic regression model considering user features generated from behaviors on apps (Bhamidipatiet al., 2017). For reducing sparseness, Yahoo also classifies apps into predefined interest taxonomies when understanding app usage patterns (Radosavljevicet al., 2016). Usage patterns of apps are learned for app purchase recommendations with a Deep Memory Network(Gligorijevic et al., 2018). Beyond app install advertising, users’ app-installation behaviors are also used for news recommendations (Liuet al., 2017), where the knowledge of the neighborhood of the cold-start users is transferred from an APP domain to a new domain.A large survey on mobile app user behaviors across main app markets around the world was conducted to instruct cross-country app competitions and analyze the challenges for software engineering (Lim et al., 2014). Paragraph 10 : The transformer encoder part receives the user retention, shared app embeddings, date embeddings, and behavior type embeddings (retention, installation, and  ninstallation) as input. Thus, the inputs altogether include complete information on when and whether users install or uninstall what apps as well as their current status of app usage. The date embeddings make the transformer suitable for modeling user behaviors that are low-frequency and distribute unevenly over time. Besides, we also introduce a masked app prediction task like BERT [9] to help extract information more productively."
How do author categorize each apps in this paper?,"Sure! Here's the answer to your question based on the provided context:

The author categorizes each app into one of four typical niche categories that need app advertising to enlarge their user base",Authors select four typical categories in the next week's prediction.,"Paragraph 10 : The four categories we selected in the next week’s prediction is four typical niche ones that need app advertising to enlarge their user base. Apps from these categories are also the long-tailed ones suffer from serious sparsity. The average installation rates for these four categories are approximately 600, 400, 25, and 300 per million people, respectively."
"What does ""long-tailed app"" mean?","A long-tailed app refers to an app that has a small number of installations among a large number of users, but the users who do install it tend to have strong personalized interests in it","Long-tailed application is a type of software or service that is not installed in a number of users. As it is installed in a few users, it can easily represent the properties and behaviors of users.","•Retention, installation and uninstallation need to be modeled collectively. They represent the preference of users from different aspects, and building representations for the three parts separately and then concatenating them may limit the performance. For example, for users who have installed multiple games, uninstalling a game app may only indicate that she has finished the game and wants to start a new one. While for a user who has not installed other games, immediately uninstalling after installation may suggest that she does not like this kind of game at all. Modeling such complex relationships using traditional recurrent neural networks (RNNs) is challenging.•Actions of (un)installing apps are low-frequency and unevenly distributed over time. Figure 1 presents a demo of app installation and uninstallation records of a user. As excitement over the new phone fades, most users only install or uninstall apps when they need to. Moreover, users usually do not operate for even a month but may suddenly install or uninstall several apps in a single day. In this case, various intervals between every two behaviors are not omittable. Although RNN-based models have succeeded in analyzing user activities (Hidasi et al., 2016; Liet al., 2017), the behaviors in those scenarios are usually with notably higher-frequency and nearly even distribution over time. Therefore, traditional RNNs may not perform well for this task.•Many long-tailed apps suffer from serious sparsity. Popular apps like Wechat and Alipay have been installed on almost all the smartphones in China, while long-tailed apps may only have a few hundreds of installations among one million users. However, user’s behaviors over the long-tailed apps often reflect one’s personalized interests better. Building effective user representations need to utilize the information from long-tailed apps without suffering from severe sparsity."
What is the difference between an online experiment and an offline experiment?,"An online experiment is conducted on a live system, where the results are observed in real-time, while an offline experiment is conducted on a static dataset, where the results are observed after the experiment is completed","Authors conduct online A/B test, whereas coduct three downstream applications for offline experiments.
The reason is that they conduct online feed recommendation A/B testing from 2020-02-01 to 2020-02-10, in the “Good Morning” tab of Tencent Mobile Manager and the “Find” tab of Tencent Wi-Fi Manager. 
For offline experiments, they compare the baseline with four different versions of AETN in three typical downstream offline experiments.","In this section, we demonstrate the offline performance of AETN in generating general-purpose user embeddings. We compare the baseline with four different versions of AETN in three typical downstream offline experiments. Then we show that the auxiliary retention reconstruction task for the autoencoder part can help the convergency of the transformer parts. Finally, we compare the user embeddings generated by the baseline and AETN intuitively. We conduct our offline experiments on three typical downstream applications, including applications from both related domains and a different domain. The evaluation tasks are as follows: To further verify the effectiveness of the output user embeddings, we conduct online feed recommendation A/B testing from 2020-02-01 to 2020-02-10, in the “Good Morning” tab of Tencent Mobile Manager and the “Find” tab of Tencent Wi-Fi Manager. We split online A/B test traffic by userIDs evenly for the tested models. We evaluate the base models, models with DAE embeddings, and models with AETN embeddings. The improvement results compared with the base models are reported in Table 2."
How did author reduce the noise in user behavior data?,By keeping only the most recent 10 installation or uninstallation operations in a week for each user,"For each user, authors keep the most recent 10 installation or uninstallation operations in a week.","We need to preprocess the user data into a format suitable for subsequent models to handle and also reduce the noise in data. After data preprocessing, each user is represented with one’s “retention” and four sequences. “Retention” is a set of apps installed on one’s phone at present. Two of the sequences, representing recent “installation” operations, are composed of installed apps and corresponding dates. The rest two sequences represent recent “uninstallation” operations. To reduce the noise in user behaviors, we keep the most recent 10 installation or uninstallation operations in a week for each user."
What are the two roles of autoencoder in proposed model?,"The two roles of the autoencoder in the proposed model are:

1. Learning high-quality app embeddings from co-occurrence relationships.
2. Providing effective representations of user retention for the transformer part","Two roles of autoencoder in proposed models are to help to learn co-occurunce relationship among applications and to help transformer encoder to learn effective user retention representations.
The reason is that autoencoder helps to learn high-quality app embeddings from the co-occurrence relationship.
Meanwhile, the autoencoder helps transformers to encode effective low-dimensional representations.","The role of this autoencoder is two-folds. Firstly, it helps to learn high-quality app embeddings from the co-occurrence relationship of apps. The weight matrix of the first hidden layer \mathbf{W}^{(1)} acts as the shared app embedding matrix \mathbf{W}^{a} for the whole network, i.e., we have(2)\mathbf{W}^{a}=\mathbf{W}^{(1)}\in\mathbb{R}^{M\times{d_{model}}}.To further alleviate the problem of sparsity, the shared app embedding matrix is carefully designed and tied with some other weight matrices. More details are provided in Section 4.3. Secondly, this autoencoder provides effective representations of user retention for the transformer part. The transformer encoder part needs to be fed with the retention for compressing long-term interests into user embeddings. However, retention is originally in the form of high-dimensional sparse features. This autoencoder encodes retention into the first hidden layer \bm{x}^{(1)}\in\mathbb{R}^{d_{model}}. As a low-dimensional dense encoding, \bm{x}^{(1)} plays an important role in the transformer encoder part."
Which part of the proposed model helps to solve the sparsity problem?,"The retention autoencoder part helps to solve the sparsity problem by encoding retention into a low-dimensional dense encoding, \bm{x}^{(1)}, which plays an important role in the transformer encoder part","Both transformers and retention autoencoder parts try to solve the sparsity problem by using tied weight matrices.
The reason of this is that \mathbf{W}^{\Omega}=\mathbf{W}^{\Theta}=\mathbf{W}^{\Phi}=\mathbf{W}^{(4)}={\mathbf{W}^{a}}^{\mathrm{T}}. 
Since \mathbf{W}^{\Omega} is used in transformer part and \mathbf{W}^{(4)} is used in autoencoder part, both of them try to solve the sparsity problem.","Secondly, this autoencoder provides effective representations of user retention for the transformer part. The transformer encoder part needs to be fed with the retention for compressing long-term interests into user embeddings. However, retention is originally in the form of high-dimensional sparse features. This autoencoder encodes retention into the first hidden layer \bm{x}^{(1)}\in\mathbb{R}^{d_{model}}. As a low-dimensional dense encoding, \bm{x}^{(1)} plays an important role in the transformer encoder part. We carefully design our weight matrices for several parts of the model, which helps to solve the sparsity problem and tightly couple the autoencoder part and the transformer parts. As shown in Figure 5, the app embeddings are built based on both the app ID and its corresponding category ID. Even if the usage of some app is gravely sparse, its category can still provide valid information. This setting helps to overcome the problem of sparsity. As introduced previously, we repeatedly use the M\times d_{model} embedding matrix for apps, i.e., at the input and output of the retention autoencoder, the input of the transformer encoder, the output for the masked app prediction, the output of the transformer decoder, as well as the reconstruction part for retention from the user embeddings (bottleneck). We tie the weight matrices of all these parts together, i.e.,(3)\displaystyle\mathbf{W}^{\Omega}=\mathbf{W}^{\Theta}=\mathbf{W}^{\Phi}=\mathbf{W}^{(4)}={\mathbf{W}^{a}}^{\mathrm{T}}.We reduce the total number of parameters by tying weight matrices of the above layers, which benefits of overcoming the problem of sparsity. Moreover, weight tying benefits the backpropagation of the gradient and speeds the convergence."
"Where did author apply ""mask apps in installation and uninstallation""?",In the installation and uninstallation of apps,"To calculate mask loss in masked app prediction stage, authors randomly mask apps in installation and uninstallation but keep the corresponding date and behavior type.
The reason is that transformer encoders mask apps randomly in masked app prediction.","Task #3: Masked App Prediction. This task is similar to the “Masked LM” task in BERT (Devlinet al., 2019). We randomly mask apps in installation and uninstallation but keep the corresponding date and behavior type. The transformer encoder is trained only to predict the masked apps. For simplicity, we just follow the masking rate in BERT and abandon the “random replacement or keep”. We calculate the loss of this task, denoted as \mathcal{L}_{mask}, by averaging the softmax cross-entropy loss of every masked app."
Are the three stages sequentially conducted in the model?,"Yes, the three stages are sequentially conducted in the model","No. It is hard to see the three stages are conducted sequentially. As the basic-level masking and phrase-level masking both masks basic language units as input, those two stages are conducted distinctly.","The first learning stage is to use basic level masking, It treat a sentence as a sequence of basic Language unit, for English, the basic language unit is word, and for Chinese, the basic language unit is Chinese Character. In the training process, We randomly mask 15 percents of basic language units, and using other basic units in the sentence as inputs, and train a transformer to predict the mask units.Based on basic level mask, we can obtain a basic word representation. Because it is trained on a random mask of basic semantic units, high level semantic knowledge is hard to be fully modeled. The second stage is to employ phrase-level masking. Phrase is a small group of words or characters together acting as a conceptual unit. For English, we use lexical analysis and chunking tools to get the boundary of phrases in the sentences, and use some language dependent segmentation tools to get the word/phrase information in other language such as Chinese. In phrase-level mask stage, we also use basic language units as training input, unlike random basic units mask, this time we randomly select a few phrases in the sentence, mask and predict all the basic units in the same phrase. At this stage, phrase information is encoded into the word embedding."
How did ERINE model incorporate knowledge into the language model?,"ERINE incorporates knowledge into the language model using knowledge masking strategies, including phrase-level and entity-level strategies, which implicitly learn prior knowledge of phrases and entities during training, guiding word embedding learning and improving generalization and adaptability","ERNIE use multi-level masking to incorporate knowledge into language model, which includes entity-level masking and phrase-level masking.
The reason is that to learn enhanced language representation by entity-level masking and
phrase-level masking is a main purpose of ERNIE.","In this paper, we propose a model called ERNIE (enhanced representation through knowledge integration) by using knowledge masking strategies. In addition to basic masking strategy, we use two kinds of knowledge strategies: phrase-level strategy and entity-level strategy. We take a phrase or a entity as one unit, which is usually composed of several words. All of the words in the same unit are masked during word representation training, instead of only one word or character being masked. In this way, the prior knowledge of phrases and entities are implicitly learned during the training procedure.Instead of adding the knowledge embedding directly, ERNIE implicitly learned the information about knowledge and longer semantic dependency, such as the relationship between entities, the property of a entity and the type of a event, to guide word embedding learning. This can make the model have better generalization and adaptability. Paragraph 10 : We present a novel language representation"
How can author claim that language models perform better if they use prior knowledge as well as context?,"By using prior knowledge, language models can better understand the relationships between entities and contexts, leading to more accurate predictions and more reliable language representations","People share a lot of prior knowledge when they talk each other.
Also, they assume others also know the prior knowledge such as ”Harry Potter is a series of fantasy novels written by J. K. Rowling”.
Hence, to make language model similar to human dialogue, it is important to use prior knowledge as well as context.
The reason is that  they metioned it is intuitive that if the model learns more about prior knowledge,  with using the example of Harry Potter.","The vast majority of these studies model the representations by predicting the missing word only through the contexts. These works do not consider the prior knowledge in the sentence.For example, In the sentence ” Harry Potter is a series of fantasy novels written by J. K. Rowling”. Harry Potter is a novel name and J. K. Rowling is the writer. It is easy for the model to predict the missing word of the entity  Harry Potter by word collocations inside this entity without the help of long contexts.The model cannot predict  Harry Potter according to the relationship between  Harry Potter and J. K. Rowling.It is intuitive that if the model learns more about prior knowledge, the model can obtain more reliable language representation."
How did the knowledge masking strategies of proposed model different with the basic making strategy?,"The proposed model's knowledge masking strategies differ from basic masking strategies by using phrase-level and entity-level strategies, where entire units of words (phrases or entities) are masked during training, rather than just one word or character","Compared to basic masking strategy, ERNIE use knowledge masking strategies.
It takes a phrase or a entity as one unit and masks words in a same unit at once.","In this paper, we propose a model called ERNIE (enhanced representation through knowledge integration) by using knowledge masking strategies. In addition to basic masking strategy, we use two kinds of knowledge strategies: phrase-level strategy and entity-level strategy. We take a phrase or a entity as one unit, which is usually composed of several words. All of the words in the same unit are masked during word representation training, instead of only one word or character being masked. In this way, the prior knowledge of phrases and entities are implicitly learned during the training procedure.Instead of adding the knowledge embedding directly, ERNIE implicitly learned the information about knowledge and longer semantic dependency, such as the relationship between entities, the property of a entity and the type of a event, to guide word embedding learning. This can make the model have better generalization and adaptability."
"Did author also test the Erine to not chinese data(eg, English data)?","Sure! Here's my answer:

No","No. It only test on chinese data.
The reason is that ERNIE is applied to only 5 Chinese NLP tasks.","ERNIE is applied to 5 Chinese NLP tasks, including natural language inference, semantic similarity, named entity recognition, sentiment analysis, and question answering."
How did previous models incorporate the knowledge embedding directly to language model?,Previous models incorporated knowledge embedding directly into language models by using word embeddings as the initialization for the word vectors in other deep learning models,"Existing models incorporate the knowledge embedding by pre-training process.
They use these knowledge embedding as their initialized representation.
After the process, self-attention model is also used to capture the contextual information.
The reason is that GPT-2 and XLM add information by pre-training.","Language representation pre-training Mikolov et al. (2013); Devlin et al. (2018) has been shown effective for improving many natural language processing tasks such as named entity recognition, sentiment analysis, and question answering.In order to get reliable word representation, neural language models are designed to learn word co-occurrence and then obtain word embedding with unsupervised learning.The methods in Word2Vec Mikolov et al. (2013) and Glove Pennington et al. (2014) represent words as vectors, where similar words have similar word representations. These word representations provide an initialization for the word vectors in other deep learning models. Recently, lots of works such as Cove McCann et al. (2017), Elmo Peters et al. (2018), GPT Radford et al. (2018) and BERT Devlin et al. (2018) improved word representation via different strategies, which has been shown to be more effective for down-stream natural language processing tasks. Some other researchers try to add more information based on these models. MT-DNN Liu et al. (2019) combine pre-training learning and multi-task learning to improve the performances over several different tasks in GLUE Wang et al. (2018). GPT-2 Radford et al. (2019) adds task information into the pre-training process and adapt their model to zero-shot tasks. XLM Lample and Conneau (2019) adds language embedding to the pre-training process which achieved better results in cross-lingual tasks. ERNIE use multi-layer Transformer Vaswani et al. (2017) as basic encoder like previous pre-traning model such as GPT, BERT and XLM.The Transformer can capture the contextual information for each token in the sentence via self-attention, and generates a sequence of contextual embeddings."
"There will be differences in basic units depending on the language. As a result, what should be changed in the model, and did the author consider that part?","The model should be adapted to handle different basic units for different languages.

The author considered this aspect by using language-dependent segmentation tools to identify phrases in other languages, such as Chinese. However, the model may still require further adjustments to account for differences in basic units between languages","To consider differences in basic language unit between english and chinese, authors set the basic units per language differently.
Then, they use some language dependent segmentation tools to get the word/phrase information.","The first learning stage is to use basic level masking, It treat a sentence as a sequence of basic Language unit, for English, the basic language unit is word, and for Chinese, the basic language unit is Chinese Character. In the training process, We randomly mask 15 percents of basic language units, and using other basic units in the sentence as inputs, and train a transformer to predict the mask units.Based on basic level mask, we can obtain a basic word representation. Because it is trained on a random mask of basic semantic units, high level semantic knowledge is hard to be fully modeled. The second stage is to employ phrase-level masking. Phrase is a small group of words or characters together acting as a conceptual unit. For English, we use lexical analysis and chunking tools to get the boundary of phrases in the sentences, and use some language dependent segmentation tools to get the word/phrase information in other language such as Chinese. In phrase-level mask stage, we also use basic language units as training input, unlike random basic units mask, this time we randomly select a few phrases in the sentence, mask and predict all the basic units in the same phrase. At this stage, phrase information is encoded into the word embedding."
"Compared with phrase-level masking, what are the advantages of entity-level phrase?","Entity-level masking has several advantages over phrase-level masking, including:

* Better capture of long-range dependencies: By masking entire entities, the model can better capture the relationships between entities and their properties, leading to improved generalization and adaptability.
* More effective knowledge transfer: Entity-level masking allows the model to learn more explicit knowledge about entities and their relationships, which can be more effective in transferring knowledge to downstream tasks","Compared to entity-level masking, phrase-level masking make models have better generalization and adaptability.
Because it can infer longer semantic dependency by masking words in the same unit at once.","In this paper, we propose a model called ERNIE (enhanced representation through knowledge integration) by using knowledge masking strategies. In addition to basic masking strategy, we use two kinds of knowledge strategies: phrase-level strategy and entity-level strategy. We take a phrase or a entity as one unit, which is usually composed of several words. All of the words in the same unit are masked during word representation training, instead of only one word or character being masked. In this way, the prior knowledge of phrases and entities are implicitly learned during the training procedure.Instead of adding the knowledge embedding directly, ERNIE implicitly learned the information about knowledge and longer semantic dependency, such as the relationship between entities, the property of a entity and the type of a event, to guide word embedding learning. This can make the model have better generalization and adaptability."
What is the instance of 'paired examples'?,"Image-to-image translation tasks, such as converting grayscale images to color images, semantic segmentation, and artistic stylization","[Paired training data consists of training examples ({xi, yi}^N i=1), where the correspondence between xi
and yi exists. An instance of 'paired examples' is labels↔photos from the CMP Facade Database.]","This problem can be more broadly described as image-to-image translation [22], converting an image from one representation of a given scene, x, to another, y, e.g., grayscale to color, image to semantic labels, edge-map to photograph.Years of research in computer vision, image processing, computational photography, and graphics have produced powerful translation systems in the supervised setting, where example image pairs \{x_{i},y_{i}\}_{i=1}^{N} are available (Figure 2, left), e.g.,[11, 19, 22, 23, 28, 33, 45, 56, 58, 62].However, obtaining paired training data can be difficult and expensive. For example, only a couple of datasets exist for tasks like semantic segmentation (e.g., [4]), and they are relatively small.Obtaining input-output pairs for graphics tasks like artistic stylization can be even more difficult since the desired output is highly complex, typically requiring artistic authoring. For many tasks, like object transfiguration (e.g., zebra\leftrightarrowhorse,  Figure 1 top-middle), the desired output is not even well-defined. Figure 8 shows some example results on other paired datasets used in “pix2pix” [22], such as architectural labels\leftrightarrowphotos from the CMP Facade Database [40], and edges\leftrightarrowshoes from the UT Zappos50Kdataset [60]. The image quality of our results is close to those produced by the fully supervised pix2pix while our method learns the mapping without paired supervision."
What is the definition of 'cycle consistency loss'?,"The cycle consistency loss is a loss function that encourages the translation to be ""cycle consistent"", meaning that the translation should be able to bring the input back to the original input, and the inverse translation should also be able to bring the output back to the original output. It is defined as:

$$\mathcal{L}_{\text{cyc}}(G,F) = \mathbb{E}_{x\sim p_{\text{data}}(x)}[\lVert F(G(x))-x\rVert_{1}] + \mathbb{E}_{y\sim p_{\text{data}}(y)}[\lVert G(F(y))-y\rVert_{1}]$$

where $G$ and $F$ are the two translation networks, and $p_{\text{data}}(x)$ is the data distribution","[Cyclic consistency implies that for each image x from domain X, the image translation cycle should be able to bring x back to the original image. The data can be of any nature other than an image as well.]","Cycle ConsistencyThe idea of using transitivity as a way to regularize structured data has a long history. In visual tracking, enforcing simple forward-backward consistency has been a standard trick for decades [24, 48]. In the language domain, verifying and improving translations via “back translation and reconciliation” is a technique used by human translators [3] (including, humorously, by Mark Twain [51]), as well as by machines [17].More recently, higher-order cycle consistency has been used instructure from motion [61],3D shape matching [21], co-segmentation [55], dense semantic alignment [65, 64], and depth estimation [14]. Of these, Zhou et al. [64] and Godard et al. [14] are most similar to our work, as they use a cycle consistency loss as a way of using transitivity to supervise CNN training.In this work, we are introducing a similar loss to push G and F to be consistent with each other. Concurrent with our work, in these same proceedings, Yi et al. [59] independently use a similar objective for unpaired image-to-image translation, inspired by dual learning in machine translation [17]. Adversarial training can, in theory, learn mappings G and F that produce outputs identically distributed as target domains Y and X respectively (strictly speaking, this requires G and F to be stochastic functions) [15]. However, with large enough capacity, a network can map the same set of input images to any random permutation of images in the target domain, where any of the learned mappings can induce an output distribution that matches the target distribution. Thus, adversarial losses alone cannot guarantee that the learned function can map an individual input x_{i} to a desired output y_{i}. To further reduce the space of possible mapping functions, we argue that the learned mapping functions should be cycle-consistent: as shown in Figure 3 (b), for each image x from domain X, the image translation cycle should be able to bring x back to the original image, i.e., x\rightarrow G(x)\rightarrow F(G(x))\approx x. We call this forward cycle consistency. Similarly, as illustrated in Figure 3 (c), for each image y from domain Y, G and F should also satisfy backward cycle consistency: y\rightarrow F(y)\rightarrow G(F(y))\approx y.We incentivize this behavior using a cycle consistency loss:\displaystyle\mathcal{L}_{\text{cyc}}(G,F)=\displaystyle\ \mathbb{E}_{x\sim p_{\text{data}}(x)}[\lVert F(G(x))-x\rVert_{1}]\displaystyle+\displaystyle\ \mathbb{E}_{y\sim p_{\text{data}}(y)}[\lVert G(F(y))-y\rVert_{1}].(2)In preliminary experiments, we also tried replacing the L1 norm in this loss with an adversarial loss between F(G(x)) and x, and between G(F(y)) and y, but did not observe improved performance. These issues call for adding more structure to our objective. Therefore, we exploit the property that translation should be “cycle consistent”, in the sense that if we translate, e.g., a sentence from English to French, and then translate it back from French to English, we should arrive back at the original sentence [3]. Mathematically, if we have a translator G:X\rightarrow Y and another translator F:Y\rightarrow X, then G and F should be inverses of each other, and both mappings should be bijections. We apply this structural assumption by training both the mapping G and F simultaneously, and adding a cycle consistency loss [64] that encourages F(G(x))\approx x and G(F(y))\approx y. Combining this loss with adversarial losses on domains X and Y yields our full objective for unpaired image-to-image translation."
Why does this method use patchGAN? Are there any benefits using this model?,The method uses PatchGAN for its discriminator network because it has fewer parameters than a full-image discriminator and can work on arbitrarily-sized images in a fully convolutional fashion. This allows for more efficient training and better performance,"[The paper uses 70 × 70 PatchGANs for the discriminator networks, which aim to classify whether 70 × 70 overlapping image patches are real or fake. The advantage of using such a patch-level discriminator architecture is that it has fewer parameters than a full-image discriminator and can work on arbitrarily sized images in a fully convolutional fashion.]","We adopt the architecture for our generative networks from Johnson et al. [23] who have shown impressive results for neural style transfer and super-resolution. This network contains three convolutions, several residual blocks [18], two fractionally-strided convolutions with stride \frac{1}{2}, and one convolution that maps features to RGB. We use 6 blocks for 128\times 128 images and 9 blocks for 256\times 256 and higher-resolution training images. Similar to Johnson et al. [23], we use instance normalization [53]. For the discriminator networks we use 70\times 70 PatchGANs [22, 30, 29], which aim to classify whether 70\times 70 overlapping image patches are real or fake. Such a patch-level discriminator architecture has fewer parameters than a full-image discriminator and can work on arbitrarily-sized images in a fully convolutional fashion [22]."
What's the difference between pix2pix and CycleGAN?,"Pix2pix and CycleGAN are both image-to-image translation methods, but they differ in their training approach. Pix2pix uses paired training examples, while CycleGAN uses unpaired examples","[CycleGAN builds on the “pix2pix” framework, which uses a conditional generative adversarial network to learn a mapping from input to output images. However, unlike pix2pix, the paper learns the mapping without paired training examples. The paper trains CycleGAN and pix2pix at 512 by 512 resolution and observes the comparable performance: maps\rightarrowaerial photos: CycleGAN: 37.5% +/- 3.6% and pix2pix: 33.9% +/-  3.1%; aerial photos\rightarrowmaps: CycleGAN: 16.5% +/- 4.1% and pix2pix: 8.5% +/- 2.6%.]","Table 1 reports performance regarding the AMT perceptual realism task. Here, we see that our method can fool participants on around a quarter of trials, in both the maps\rightarrowaerial photos direction and the aerial photos\rightarrowmaps direction at 256\times 256 resolution333We also train CycleGAN and pix2pix at 512\times 512 resolution, and observe the comparable performance: maps\rightarrowaerial photos: CycleGAN: 37.5\%\pm 3.6\% and pix2pix: 33.9\%\pm 3.1\%; aerial photos\rightarrowmaps: CycleGAN: 16.5\%\pm 4.1\% and pix2pix: 8.5\%\pm 2.6\%. All the baselines almost never fooled participants. Image-to-Image TranslationThe idea of image-to-image translation goes back at least to Hertzmann et al.’s Image Analogies [19], who employ a non-parametric texture model [10] on a single input-output training image pair.More recent approaches use a dataset of input-output examples to learn a parametric translation function using CNNs (e.g., [33]). Our approach builds on the “pix2pix” framework of Isola et al. [22], which uses a conditional generative adversarial network [16] to learn a mapping from input to output images. Similar ideas have been applied to various tasks such as generating photographs from sketches [44] or from attribute and semantic layouts [25]. However, unlike the above prior work, we learn the mapping without paired training examples."
"Why is this method unable to achieve compelling results with any of the baselines in Figure 5, 6?","The method is unable to achieve compelling results with any of the baselines in Figure 5 and 6 because it is not well-suited for tasks that require geometric changes, and the distribution characteristics of the training datasets can cause confusion","[The paper’s generator architectures which are tailored for good performance on the appearance changes might cause failures. For example, on the task of dog→cat transfiguration, the learned translation degenerates into making minimal changes to the input. Some failure cases are caused by the distribution characteristics of the training datasets. For example, the paper’s method has got confused in the horse→zebra example, because the paper’s model was trained on the wild horse and zebra synsets of ImageNet, which does not contain images of a person riding a horse or zebra. The paper also observes a lingering gap between the results achievable with paired training data and those achieved by the paper’s unpaired method. In some cases, this gap may be hard or even impossible to close: for example, the paper’s method sometimes permutes the labels for tree and building in the output of the photos→labels task. Resolving this ambiguity may require some form of weak semantic supervision. Integrating weak or semi-supervised data may lead to substantially more powerful translators, still at a fraction of the annotation cost of the fully-supervised systems.]","Although our method can achieve compelling results in many cases, the results are far from uniformly positive. Figure 17 shows several typical failure cases. On translation tasks that involve color and texture changes, as many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. For example, on the task of dog\rightarrowcat transfiguration, the learned translation degenerates into making minimal changes to the input (Figure 17). This failure might be caused by our generator architectures which are tailored for good performance on the appearance changes. Handling more varied and extreme transformations, especially geometric changes, is an important problem for future work. Some failure cases are caused by the distribution characteristics of the training datasets. For example, our method has got confused in the horse \rightarrow zebra example (Figure 17, right), because our model was trained on the wild horse and zebra synsets of ImageNet, which does not contain images of a person riding a horse or zebra. [We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method. In some cases, this gap may be very hard – or even impossible – to close: for example, our method sometimes permutes the labels for tree and building in the output of the photos→labels task. Resolving this ambiguity may require some form of weak semantic supervision. Integrating weak or semi-supervised data may lead to substantially more powerful translators, still at a fraction of the annotation cost of the fully-supervised systems.]"
How can we check if the model suffers from mode collapse?,"Check for mode collapse by visualizing the output distribution over yˆ and verifying that it matches the empirical distribution pdata(y). Additionally, monitor the optimization process and look for signs of mode collapse, such as all input images mapping to the same output image","[If all input images map to the same output image and the optimization fails to make progress, then the model is suffering from “mode collapse”. For example, the paper evaluates its method with the cycle loss in only one direction: GAN + forward cycle loss, or GAN + backward cycle loss (in Equation 2) and finds that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed.]","In Table 4 and Table 5, we compare against ablations of our full loss. Removing the GAN loss substantially degrades results, as does removing the cycle-consistency loss. We therefore conclude that both terms are critical to our results. We also evaluate our method with the cycle loss in only one direction: GAN + forward cycle loss \mathbb{E}_{x\sim p_{\text{data}}(x)}[\lVert F(G(x))-x\rVert_{1}], or GAN + backward cycle loss \mathbb{E}_{y\sim p_{\text{data}}(y)}[\lVert G(F(y))-y\rVert_{1}] (Equation 2) and find that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed. Figure 7 shows several qualitative examples. [We therefore seek an algorithm that can learn to translate between domains without paired input-output examples (Figure 2, right). We assume there is some underlying relationship between the domains – for example, that they are two different renderings of the same underlying scene – and seek to learn that relationship. Although we lack supervision in the form of paired examples, we can exploit supervision at the level of sets: we are given one set of images in domain X and a different set in domain Y . We may train a mapping G : X → Y such that the output yˆ = G(x), x ∈ X, is indistinguishable from images y ∈ Y by an adversary trained to classify yˆ apart from y. In theory, this objective can induce an output distribution over yˆ that matches the empirical distribution pdata(y) (in general, this requires G to be stochastic). The optimal G thereby translates the domain X to a domain Yˆ distributed identically to Y . However, such a translation does not guarantee that an individual input x and output y are paired up in a meaningful way – there are infinitely many mappings G that will induce the same distribution over yˆ. Moreover, in practice, we have found it difficult to optimize the adversarial objective in isolation: standard procedures often lead to the wellknown problem of mode collapse, where all input images map to the same output image and the optimization fails to make progress.]"
"For other than the painting -> photo case (e.g. photo -> painting), does L_identity still work well?","Sure! Here's my answer:

No","[Without L_identitiy,  the generator G and F are free to change the tint of input images when there is no need to. For example, when learning the mapping between Monet’s paintings and Flickr photographs, the generator often maps paintings of daytime to photographs taken during sunset, because such a mapping may be equally valid under the adversarial loss and cycle consistency loss. The same could happen for the case between Flickr photographs and Monet’s paintings, the generator may map photographs taken during sunset to paintings of daytime. Therefore, L_identity might work well in this case as well, but it is not explicitly stated in the paper or proven through experiments. ]","Without \mathcal{L}_{\text{identity}}, the generator G and F are free to change the tint of input images when there is no need to. For example, when learning the mapping between Monet’s paintings and Flickr photographs, the generator often maps paintings of daytime to photographs taken during sunset, because such a mapping may be equally valid under the adversarial loss and cycle consistency loss.The effect of this identity mapping loss are shown in Figure 9."
What is the instance of the 'geometric changes'?,Dog → cat transfiguration,"[The paper has explored tasks that require geometric changes, with little success. For example, on the task of dog→cat transfiguration, the learned translation degenerates into making minimal changes to the input. This failure might be caused by the paper’s generator architectures which are tailored for good performance on the appearance changes. This implies that a geometric change doesn’t include a change in appearance, but the paper doesn't explicitly mention an instance of ""geometric change"".]","Although our method can achieve compelling results in many cases, the results are far from uniformly positive. Figure 17 shows several typical failure cases. On translation tasks that involve color and texture changes, as many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. For example, on the task of dog\rightarrowcat transfiguration, the learned translation degenerates into making minimal changes to the input (Figure 17). This failure might be caused by our generator architectures which are tailored for good performance on the appearance changes. Handling more varied and extreme transformations, especially geometric changes, is an important problem for future work."
How could we handle more varied and extreme transformations in the unsupervised setting?,"Sure! Here's the answer to your question based on the provided context:

One possible approach to handle more varied and extreme transformations in the unsupervised setting is to explore different generator architectures that are better suited for handling geometric changes, such as using generative adversarial networks (GANs) or other types of diffusion models that can capture more complex transformations","[The paper mentions that the handling of more varied and extreme transformations, especially geometric changes, is an important problem for future work.]","Although our method can achieve compelling results in many cases, the results are far from uniformly positive. Figure 17 shows several typical failure cases. On translation tasks that involve color and texture changes, as many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. For example, on the task of dog\rightarrowcat transfiguration, the learned translation degenerates into making minimal changes to the input (Figure 17). This failure might be caused by our generator architectures which are tailored for good performance on the appearance changes. Handling more varied and extreme transformations, especially geometric changes, is an important problem for future work."
How could we define whether two domains are distributed identically?,Two domains are distributed identically if their output distributions are the same,"[Given one set of images in domain X and a different set in domain Y, it is possible to train a mapping G:X→ Y such that the output \hat{y}=G(x), x\in X, is indistinguishable from images y\in Y by an adversary trained to classify \hat{y} apart from y. In theory, this objective can induce an output distribution over \hat{y} that matches the empirical distribution p_{data}(y), which requires G to be stochastic. The optimal G thereby translates the domain X to a domain \hat{Y} distributed identically to Y.]","We therefore seek an algorithm that can learn to translate between domains without paired input-output examples (Figure 2, right). We assume there is some underlying relationship between the domains – for example, that they are two different renderings of the same underlying scene – and seek to learn that relationship. Although we lack supervision in the form of paired examples, we can exploit supervision at the level of sets: we are given one set of images in domain X and a different set in domain Y. We may train a mapping G:X\rightarrow Y such that the output \hat{y}=G(x), x\in X, is indistinguishable from images y\in Y by an adversary trained to classify \hat{y} apart from y. In theory, this objective can induce an output distribution over \hat{y} that matches the empirical distribution p_{data}(y) (in general, this requires G to be stochastic) [16]. The optimal G thereby translates the domain X to a domain \hat{Y} distributed identically to Y. However, such a translation does not guarantee that an individual input x and output y are paired up in a meaningful way – there are infinitely many mappings G that will induce the same distribution over \hat{y}. Moreover, in practice, we have found it difficult to optimize the adversarial objective in isolation: standard procedures often lead to the well-known problem of mode collapse, where all input images map to the same output image and the optimization fails to make progress [15]."
Would 'bijection' always be guaranteed in this case?,"No.

In this case, the property of ""cycle consistency"" is used to ensure that the translator G and its inverse F are bijections, but it is not guaranteed that they will always be bijections. The cycle consistency loss only encourages the model to learn a translation that preserves the original input, but it does not enforce it strictly","If there is a translator G:X\rightarrow Y and another translator F:Y\rightarrow X, then G and F should be inverses of each other, and both mappings should be bijections. However, the question doesn't specify any specific case where the """"bijections"""" should be examined, therefore it cannot be answered.","These issues call for adding more structure to our objective. Therefore, we exploit the property that translation should be “cycle consistent”, in the sense that if we translate, e.g., a sentence from English to French, and then translate it back from French to English, we should arrive back at the original sentence [3]. Mathematically, if we have a translator G:X\rightarrow Y and another translator F:Y\rightarrow X, then G and F should be inverses of each other, and both mappings should be bijections. We apply this structural assumption by training both the mapping G and F simultaneously, and adding a cycle consistency loss [64] that encourages F(G(x))\approx x and G(F(y))\approx y. Combining this loss with adversarial losses on domains X and Y yields our full objective for unpaired image-to-image translation."
What is the definition of 'translate'?,"'translate' refers to the process of converting an image from one representation of a scene to another, such as grayscale to color, image to semantic labels, or edge-map to photograph","[To ""translate"" an image means  to convert an image from one representation of a given scene to another, e.g., grayscale to color, image to semantic labels, etc. ]","This problem can be more broadly described as image-to-image translation [22], converting an image from one representation of a given scene, x, to another, y, e.g., grayscale to color, image to semantic labels, edge-map to photograph.Years of research in computer vision, image processing, computational photography, and graphics have produced powerful translation systems in the supervised setting, where example image pairs \{x_{i},y_{i}\}_{i=1}^{N} are available (Figure 2, left), e.g.,[11, 19, 22, 23, 28, 33, 45, 56, 58, 62].However, obtaining paired training data can be difficult and expensive. For example, only a couple of datasets exist for tasks like semantic segmentation (e.g., [4]), and they are relatively small.Obtaining input-output pairs for graphics tasks like artistic stylization can be even more difficult since the desired output is highly complex, typically requiring artistic authoring. For many tasks, like object transfiguration (e.g., zebra\leftrightarrowhorse,  Figure 1 top-middle), the desired output is not even well-defined."
Could we guess more about what is the instance of 'underlying relationship'?,The underlying relationship is assumed to be a rendering of the same underlying scene,"[For translation between domains, the underlying relationship could be the relationship between those domains – for instance, that they are two different renderings of the same underlying scene.]","We therefore seek an algorithm that can learn to translate between domains without paired input-output examples (Figure 2, right). We assume there is some underlying relationship between the domains – for example, that they are two different renderings of the same underlying scene – and seek to learn that relationship. Although we lack supervision in the form of paired examples, we can exploit supervision at the level of sets: we are given one set of images in domain X and a different set in domain Y. We may train a mapping G:X\rightarrow Y such that the output \hat{y}=G(x), x\in X, is indistinguishable from images y\in Y by an adversary trained to classify \hat{y} apart from y. In theory, this objective can induce an output distribution over \hat{y} that matches the empirical distribution p_{data}(y) (in general, this requires G to be stochastic) [16]. The optimal G thereby translates the domain X to a domain \hat{Y} distributed identically to Y. However, such a translation does not guarantee that an individual input x and output y are paired up in a meaningful way – there are infinitely many mappings G that will induce the same distribution over \hat{y}. Moreover, in practice, we have found it difficult to optimize the adversarial objective in isolation: standard procedures often lead to the well-known problem of mode collapse, where all input images map to the same output image and the optimization fails to make progress [15]."
What if there is no underlying relationship?,The algorithm may not be effective in the absence of an underlying relationship between the domains,"[The translation between domains, with no underlying relationship, will have input x and output y paired up in an unmeaningful way ]","We therefore seek an algorithm that can learn to translate between domains without paired input-output examples (Figure 2, right). We assume there is some underlying relationship between the domains – for example, that they are two different renderings of the same underlying scene – and seek to learn that relationship. Although we lack supervision in the form of paired examples, we can exploit supervision at the level of sets: we are given one set of images in domain X and a different set in domain Y. We may train a mapping G:X\rightarrow Y such that the output \hat{y}=G(x), x\in X, is indistinguishable from images y\in Y by an adversary trained to classify \hat{y} apart from y. In theory, this objective can induce an output distribution over \hat{y} that matches the empirical distribution p_{data}(y) (in general, this requires G to be stochastic) [16]. The optimal G thereby translates the domain X to a domain \hat{Y} distributed identically to Y. However, such a translation does not guarantee that an individual input x and output y are paired up in a meaningful way – there are infinitely many mappings G that will induce the same distribution over \hat{y}. Moreover, in practice, we have found it difficult to optimize the adversarial objective in isolation: standard procedures often lead to the well-known problem of mode collapse, where all input images map to the same output image and the optimization fails to make progress [15]."
"In order to determine whether two domains are similar or not, how could we define 'similarity'?","Sure! Here's the answer to your question based on the provided context:

We do not rely on any predefined similarity function between the input and output. Instead, we directly compare against several prior and contemporary approaches in Section 5.1 to determine similarity between domains","[""Similarity"" may be defined as a function between input and output, however, it may vary from one particular task or formulation to another. Therefore, the definition of similarity is task-specific.]","Unlike the above approaches, our formulation does not rely on any task-specific, predefined similarity function between the input and output, nor do we assume that the input and output have to lie in the same low-dimensional embedding space. This makes our method a general-purpose solution for many vision and graphics tasks. We directly compare against several prior and contemporary approaches in Section 5.1."
"Normally, GAN training is unstable. Does this framework help to make the model stable?","Yes, this framework helps to make the model stable. By replacing the negative log likelihood objective with a least-squares loss and using a history of generated images to update the discriminators, the authors were able to reduce model oscillation and improve the stability of the training procedure","[The paper applies two techniques to stabilize its model training procedure. First, for \mathcal{L}_{\text{GAN}} (Equation 1), the paper replaces the negative log likelihood objective by a least-squares loss. Secondly, to reduce model oscillation, the paper follows Shrivastava et al.’s strategy and updates the discriminators using a history of generated images rather than the ones produced by the latest generators. The paper keeps an image buffer that stores the 50 previously created images. The paper also evaluates its method with the cycle loss in only one direction and finds that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed.]","We apply two techniques from recent works to stabilize our model training procedure. First, for \mathcal{L}_{\text{GAN}} (Equation 1), we replace the negative log likelihood objective by a least-squares loss [35]. This loss is more stable during training and generates higher quality results. In particular, for a GAN loss \mathcal{L}_{\text{GAN}}(G,D,X,Y), we train the G to minimize \mathbb{E}_{x\sim p_{\text{data}}(x)}[(D(G(x))-1)^{2}] and train the D to minimize \mathbb{E}_{y\sim p_{\text{data}}(y)}[(D(y)-1)^{2}]+\mathbb{E}_{x\sim p_{\text{data}}(x)}[D(G(x))^{2}]. Second, to reduce model oscillation [15], we follow Shrivastava et al.’s strategy [46] and update the discriminators using a history of generated images rather than the ones produced by thelatest generators. We keep an image buffer that stores the 50 previously created images. In Table 4 and Table 5, we compare against ablations of our full loss. Removing the GAN loss substantially degrades results, as does removing the cycle-consistency loss. We therefore conclude that both terms are critical to our results. We also evaluate our method with the cycle loss in only one direction: GAN + forward cycle loss \mathbb{E}_{x\sim p_{\text{data}}(x)}[\lVert F(G(x))-x\rVert_{1}], or GAN + backward cycle loss \mathbb{E}_{y\sim p_{\text{data}}(y)}[\lVert G(F(y))-y\rVert_{1}] (Equation 2) and find that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed. Figure 7 shows several qualitative examples."
What's the value of the λ scale?,"Sure! Based on the context you provided, the answer is:

The value of the λ scale is 10","For all the experiments, the paper set lambda=10.  For flower photo enhancement and Monet’s paintings\rightarrowphotos, the identity mapping loss of weight 0.5*lambda was used, while lambda=10 was kept throughout.","For all the experiments, we set \lambda=10 in Equation 3.We use the Adam solver [26] with a batch size of 1. All networks were trained from scratch with a learning rate of 0.0002. We keep the same learning rate for the first 100 epochs and linearly decay the rate to zero over the next 100 epochs.Please see the appendix (Section 7) for more details about the datasets, architectures, and training procedures. Monet’s paintings\rightarrowphotos To achieve high resolution while conserving memory, we used random square crops of the original images for training. To generate results, we passed images of width 512 pixels with correct aspect ratio to the generator network as input. The weight for the identity mapping loss was 0.5\lambda where \lambda was the weight for cycle consistency loss. We set \lambda=10. Flower photo enhancement Flower images taken on smartphones were downloaded from Flickr by searching for the photos taken byApple iPhone 5, 5s, or 6, with search text flower. DSLR images with shallow DoF were also downloaded from Flickr by search tag flower, dof. The images were scaled to 360 pixels by width. The identity mapping loss of weight 0.5\lambda was used. The training set size of the smartphone and DSLR dataset were 1813 and 3326, respectively. We set \lambda=10."
Is the paper's method also could be interpreted as a weighted nearest-neighbor classifier applied within an embedding space?,Yes,"[The paper’s approach, prototypical networks, is based on the idea that there exists an embedding in which points cluster around a single prototype representation for each class. Prototypical networks differ from matching networks in the few-shot case with equivalence in the one-shot scenario. Matching networks produce a weighted nearest neighbor classifier given the support set, while prototypical networks produce a linear classifier when squared Euclidean distance is used. In the case of one-shot learning, ck = xk since there is only one support point per class, and matching networks and prototypical networks become equivalent. When they become equivalent, then the paper’s approach and matching networks (which produce a weighted nearest neighbor classifier), can be interpreted the same.]","We attack the problem of few-shot learning by addressing the key issue of overfitting. Since data is severely limited, we work under the assumption that a classifier should have a very simple inductive bias. Our approach, prototypical networks, is based on the idea that there exists an embedding in which points cluster around a single prototype representation for each class. In order to do this, we learn a non-linear mapping of the input into an embedding space using a neural network and take a class’s prototype to be the mean of its support set in the embedding space. Classification is then performed for an embedded query point by simply finding the nearest class prototype. We follow the same approach to tackle zero-shot learning; here each class comes with meta-data giving a high-level description of the class rather than a small number of labeled examples. We therefore learn an embedding of the meta-data into a shared space to serve as the prototype for each class. Classification is performed, as in the few-shot scenario, by finding the nearest class prototype for an embedded query point. [Prototypical networks differ from matching networks in the few-shot case with equivalence in the one-shot scenario. Matching networks [29] produce a weighted nearest neighbor classifier given the support set, while prototypical networks produce a linear classifier when squared Euclidean distance is used. In the case of one-shot learning, ck = xk since there is only one support point per class, and matching networks and prototypical networks become equivalent.]"
What is the definition of 'episodes'?,"Episodes are randomly selected subsets of classes from the training set, used in few-shot learning to mimic the test environment and improve generalization","[Matching networks utilize sampled mini-batches called episodes during training, where each episode is designed to mimic the few-shot task by subsampling classes as well as data points. The use of episodes makes the training problem more faithful to the test environment and thereby improves generalization. Training episodes are formed by randomly selecting a subset of classes from the training set, then choosing a subset of examples within each class to act as the support set and a subset of the remainder to serve as query points.]","Two recent approaches have made significant progress in few-shot learning.Vinyals et al. (2016) proposed matching networks, which uses an attention mechanism over a learned embedding of the labeled set of examples (the support set) to predict classes for the unlabeled points (the query set). Matching networks can be interpreted as a weighted nearest-neighbor classifier applied within an embedding space. Notably, this model utilizes sampled mini-batches called episodes during training, where each episode is designed to mimic the few-shot task by subsampling classes as well as data points. The use of episodes makes the training problem more faithful to the test environment and thereby improves generalization.Ravi and Larochelle (2017) take the episodic training idea further and propose a meta-learning approach to few-shot learning. Their approach involves training an LSTM Hochreiter and Schmidhuber (1997) to produce the updates to a classifier, given an episode, such that it will generalize well to a test-set. Here, rather than training a single model over multiple episodes, the LSTM meta-learner learns to train a custom model for each episode. Prototypical networks compute an M-dimensional representation \mathbf{c}_{k}\in\mathbb{R}^{M}, or prototype, of each class through an embedding function f_{\bm{\phi}}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M} with learnable parameters \bm{\phi}. Each prototype is the mean vector of the embedded support points belonging to its class:\mathbf{c}_{k}=\frac{1}{|S_{k}|}\sum_{(\mathbf{x}_{i},y_{i})\in S_{k}}f_{\bm{\phi}}(\mathbf{x}_{i})(1)Given a distance function d:\mathbb{R}^{M}\times\mathbb{R}^{M}\rightarrow[0,+\infty), prototypical networks produce a distribution over classes for a query point \mathbf{x} based on a softmax over distances to the prototypes in the embedding space:p_{\bm{\phi}}(y=k\,|\,\mathbf{x})=\frac{\exp(-d(f_{\bm{\phi}}(\mathbf{x}),\mathbf{c}_{k}))}{\sum_{k^{\prime}}\exp(-d(f_{\bm{\phi}}(\mathbf{x}),\mathbf{c}_{k^{\prime}}))}(2)Learning proceeds by minimizing the negative log-probability J(\bm{\phi})=-\log p_{\bm{\phi}}(y=k\,|\,\mathbf{x}) of the true class k via SGD. Training episodes are formed by randomly selecting a subset of classes from the training set, then choosing a subset of examples within each class to act as the support set and a subset of the remainder to serve as query points. Pseudocode to compute the loss J(\bm{\phi}) for a training episode is provided in Algorithm 1."
How this paper define a prototype?,The paper defines a prototype as the mean vector of the embedded support points belonging to a class,"[The paper learns a non-linear mapping of the input into an embedding space using a neural network and takes a class’s prototype to be the mean of its support set in the embedding space. It learns the embedding of the meta-data into a shared space to serve as the prototype for each class. Classification is performed, as in the few-shot scenario, by finding the nearest class prototype for an embedded query point. Each prototype is the mean vector of the embedded support points belonging to its class:\mathbf{c}_{k}=\frac{1}{|S_{k}|}\sum_{(\mathbf{x}_{i},y_{i})\in S_{k}}f_{\bm{\phi}}(\mathbf{x}_{i}).]","We attack the problem of few-shot learning by addressing the key issue of overfitting. Since data is severely limited, we work under the assumption that a classifier should have a very simple inductive bias. Our approach, prototypical networks, is based on the idea that there exists an embedding in which points cluster around a single prototype representation for each class. In order to do this, we learn a non-linear mapping of the input into an embedding space using a neural network and take a class’s prototype to be the mean of its support set in the embedding space. Classification is then performed for an embedded query point by simply finding the nearest class prototype. We follow the same approach to tackle zero-shot learning; here each class comes with meta-data giving a high-level description of the class rather than a small number of labeled examples. We therefore learn an embedding of the meta-data into a shared space to serve as the prototype for each class. Classification is performed, as in the few-shot scenario, by finding the nearest class prototype for an embedded query point. Prototypical networks compute an M-dimensional representation \mathbf{c}_{k}\in\mathbb{R}^{M}, or prototype, of each class through an embedding function f_{\bm{\phi}}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M} with learnable parameters \bm{\phi}. Each prototype is the mean vector of the embedded support points belonging to its class:\mathbf{c}_{k}=\frac{1}{|S_{k}|}\sum_{(\mathbf{x}_{i},y_{i})\in S_{k}}f_{\bm{\phi}}(\mathbf{x}_{i})(1)Given a distance function d:\mathbb{R}^{M}\times\mathbb{R}^{M}\rightarrow[0,+\infty), prototypical networks produce a distribution over classes for a query point \mathbf{x} based on a softmax over distances to the prototypes in the embedding space:p_{\bm{\phi}}(y=k\,|\,\mathbf{x})=\frac{\exp(-d(f_{\bm{\phi}}(\mathbf{x}),\mathbf{c}_{k}))}{\sum_{k^{\prime}}\exp(-d(f_{\bm{\phi}}(\mathbf{x}),\mathbf{c}_{k^{\prime}}))}(2)Learning proceeds by minimizing the negative log-probability J(\bm{\phi})=-\log p_{\bm{\phi}}(y=k\,|\,\mathbf{x}) of the true class k via SGD. Training episodes are formed by randomly selecting a subset of classes from the training set, then choosing a subset of examples within each class to act as the support set and a subset of the remainder to serve as query points. Pseudocode to compute the loss J(\bm{\phi}) for a training episode is provided in Algorithm 1."
What is the value of M?,M = D,"[Prototypical networks compute an M-dimensional representation \mathbf{c}_{k}\in\mathbb{R}^{M}, or prototype, of each class through an embedding function f_{\bm{\phi}}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M} with learnable parameters \bm{\phi}. Each prototype is the mean vector of the embedded support points belonging to its class:\mathbf{c}_{k}=\frac{1}{|S_{k}|}\sum_{(\mathbf{x}_{i},y_{i})\in S_{k}}f_{\bm{\phi}}(\mathbf{x}_{i}). Hence M=Dimension of the prototype.]","Prototypical networks compute an M-dimensional representation \mathbf{c}_{k}\in\mathbb{R}^{M}, or prototype, of each class through an embedding function f_{\bm{\phi}}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M} with learnable parameters \bm{\phi}. Each prototype is the mean vector of the embedded support points belonging to its class:\mathbf{c}_{k}=\frac{1}{|S_{k}|}\sum_{(\mathbf{x}_{i},y_{i})\in S_{k}}f_{\bm{\phi}}(\mathbf{x}_{i})(1)Given a distance function d:\mathbb{R}^{M}\times\mathbb{R}^{M}\rightarrow[0,+\infty), prototypical networks produce a distribution over classes for a query point \mathbf{x} based on a softmax over distances to the prototypes in the embedding space:p_{\bm{\phi}}(y=k\,|\,\mathbf{x})=\frac{\exp(-d(f_{\bm{\phi}}(\mathbf{x}),\mathbf{c}_{k}))}{\sum_{k^{\prime}}\exp(-d(f_{\bm{\phi}}(\mathbf{x}),\mathbf{c}_{k^{\prime}}))}(2)Learning proceeds by minimizing the negative log-probability J(\bm{\phi})=-\log p_{\bm{\phi}}(y=k\,|\,\mathbf{x}) of the true class k via SGD. Training episodes are formed by randomly selecting a subset of classes from the training set, then choosing a subset of examples within each class to act as the support set and a subset of the remainder to serve as query points. Pseudocode to compute the loss J(\bm{\phi}) for a training episode is provided in Algorithm 1."
Is it true that prototype computation can be viewed in terms of hard clustering on the support set?,Yes,"[Yes, prototype computation can be viewed in terms of hard clustering on the support set, with one cluster per class and each support point assigned to its corresponding class cluster.]","Prototype computation can be viewed in terms of hard clustering on the support set, with one cluster per class and each support point assigned to its corresponding class cluster. It has been shown Banerjee et al. (2005) for Bregman divergences that the cluster representative achieving minimal distance to its assigned points is the cluster mean. Thus the prototype computation in Equation (1) yields optimal cluster representatives given the support set labels when a Bregman divergence is used."
Is it true that any regular exponential family distribution pψ(z|θ) with parameters θ and cumulant function ψ can be written in terms of a uniquely determined regular Bregman divergence?,"Yes.

According to Banerjee et al. (2005), any regular exponential family distribution pψ(z|θ) can be written in terms of a uniquely determined regular Bregman divergence. This means that the exponential family distribution can be expressed as a Bregman divergence, which is a special type of distance measure","[Yes, any regular exponential family distribution p_{\psi}(\mathbf{z}|\bm{\theta}) with parameters \bm{\theta} and cumulant function \psi can be written in terms of a uniquely determined regular Bregman divergence as: p_{\psi}(\mathbf{z}|\bm{\theta})=\exp\{\mathbf{z}^{T}\bm{\theta}-\psi(\bm{\theta})-g_{\psi}(\mathbf{z})\}=\exp\{-d_{\varphi}(\mathbf{z},\bm{\mu}(\bm{\theta}))-g_{\varphi}(\mathbf{z})\}]","Moreover, any regular exponential family distribution p_{\psi}(\mathbf{z}|\bm{\theta}) with parameters \bm{\theta} and cumulant function \psi can be written in terms of a uniquely determined regular Bregman divergence Banerjee et al. (2005):p_{\psi}(\mathbf{z}|\bm{\theta})=\exp\{\mathbf{z}^{T}\bm{\theta}-\psi(\bm{\theta})-g_{\psi}(\mathbf{z})\}=\exp\{-d_{\varphi}(\mathbf{z},\bm{\mu}(\bm{\theta}))-g_{\varphi}(\mathbf{z})\}(4)Consider now a regular exponential family mixture model with parameters \bm{\Gamma}=\{\bm{\theta}_{k},\pi_{k}\}_{k=1}^{K}:p(\mathbf{z}|\bm{\Gamma})=\sum_{k=1}^{K}\pi_{k}p_{\psi}(\mathbf{z}|\bm{\theta}_{k})=\sum_{k=1}^{K}\pi_{k}\exp(-d_{\varphi}(\mathbf{z},\bm{\mu}(\bm{\theta}_{k}))-g_{\varphi}(\mathbf{z}))(5)Given \bm{\Gamma}, inference of the cluster assignment y for an unlabeled point \mathbf{z} becomes:p(y=k|\mathbf{z})=\frac{\pi_{k}\exp(-d_{\varphi}(\mathbf{z},\bm{\mu}(\bm{\theta}_{k})))}{\sum_{k^{\prime}}\pi_{k^{\prime}}\exp(-d_{\varphi}(\mathbf{z},\bm{\mu}(\bm{\theta}_{k})))}(6)For an equally-weighted mixture model with one cluster per class, cluster assignment inference (6) is equivalent to query class prediction (2) with f_{\phi}(\mathbf{x})=\mathbf{z} and \mathbf{c}_{k}=\bm{\mu}(\bm{\theta}_{k}).In this case, prototypical networks are effectively performing mixture density estimation with an exponential family distribution determined by d_{\varphi}. The choice of distance therefore specifies modeling assumptions about the class-conditional data distribution in the embedding space."
Would it be better to use 1 prototype per class rather than multiple prototypes?,"Sure! Here's the answer to your question based on the provided context:

No","[If the number of prototypes per class is fixed and greater than 1, then this would require a partitioning scheme to further cluster the support points within a class. This has been proposed in previous works which require a separate partitioning phase that is decoupled from the weight updates, while the paper’s approach is simple to learn with ordinary gradient descent methods.]","A natural question is whether it makes sense to use multiple prototypes per class instead of just one.If the number of prototypes per class is fixed and greater than 1, then this would require a partitioning scheme to further cluster the support points within a class. This has been proposed in Mensink et al. (2013) and Rippel et al. (2016); however both methods require a separate partitioning phase that is decoupled from the weight updates, while our approach is simple to learn with ordinary gradient descent methods."
Why is it beneficial to fix the prototype embedding g to have unit length?,To avoid the prototype embeddings from dominating the query embeddings,"[In Zero-shot learning, since the meta-data vector and query point come from different input domains, the paper found it empirically beneficial to fix the prototype embedding g to have unit length, however the paper did not constrain the query embedding f.]","[Zero-shot learning differs from few-shot learning in that instead of being given a support set of training points, we are given a class meta-data vector vk for each class. These could be determined in advance, or they could be learned from e.g., raw text [7]. Modifying prototypical networks to deal with the zero-shot case is straightforward: we simply define ck = gϑ(vk) to be a separate embedding of the meta-data vector. An illustration of the zero-shot procedure for prototypical networks as it relates to the few-shot procedure is shown in Figure 1. Since the meta-data vector and query point come from different input domains, we found it was helpful empirically to fix the prototype embedding g to have unit length, however we do not constrain the query embedding f. ]"
How many episodes are needed for training the model with the omniglot dataset?,600 episodes,[The paper computed classification accuracy for its models averaged over 1000 randomly generated episodes from the test set. However it does not mention the number of episodes required for training on omniglot dataset.],"We trained prototypical networks using Euclidean distance in the 1-shot and 5-shot scenarios with training episodes containing 60 classes and 5 query points per class. We found that it is advantageous to match the training-shot with the test-shot, and to use more classes (higher “way”) per training episode rather than fewer. We compare against various baselines, including the neural statistician (Edwards and Storkey, 2017) and both the fine-tuned and non-fine-tuned versions of matching networks (Vinyals et al., 2016). We computed classification accuracy for our models averaged over 1000 randomly generated episodes from the test set. The results are shown in Table 1 and to our knowledge they represent the state-of-the-art on this dataset."
Is it fair to use the same embedding architecture as the prototypical network in the matching networks setting?,"Yes, it is fair to use the same embedding architecture as the prototypical network in the matching networks setting","[Yes. The paper conducted further analysis, to determine the effect of distance metric and the number of training classes per episode on the performance of prototypical networks and matching networks. To make the methods comparable, the paper used its own implementation of matching networks that utilizes the same embedding architecture as the paper’s prototypical networks.]","We conducted further analysis, to determine the effect of distance metric and the number of training classes per episode on the performance of prototypical networks and matching networks. To make the methods comparable, we use our own implementation of matching networks that utilizes the same embedding architecture as our prototypical networks.In Figure 2 we compare cosine vs. Euclidean distance and 5-way vs. 20-way training episodes in the 1-shot and 5-shot scenarios, with 15 query points per class per episode. We note that 20-way achieves higher accuracy than 5-way and conjecture that the increased difficulty of 20-way classification helps the network to generalize better, because it forces the model to make more fine-grained decisions in the embedding space. Also, using Euclidean distance improves performance substantially over cosine distance. This effect is even more pronounced for prototypical networks, in which computing the class prototype as the mean of embedded support points is more naturally suited to Euclidean distances since cosine distance is not a Bregman divergence."
Is it true that it is because cosine distance is not a Bregman divergence?,No,"[The question seems incomplete. However, using Euclidean distance improves performance substantially over cosine distance. This effect is even more pronounced for prototypical networks, in which computing the class prototype as the mean of embedded support points is more naturally suited to Euclidean distances since cosine distance is not a Bregman divergence.]","We conducted further analysis, to determine the effect of distance metric and the number of training classes per episode on the performance of prototypical networks and matching networks. To make the methods comparable, we use our own implementation of matching networks that utilizes the same embedding architecture as our prototypical networks.In Figure 2 we compare cosine vs. Euclidean distance and 5-way vs. 20-way training episodes in the 1-shot and 5-shot scenarios, with 15 query points per class per episode. We note that 20-way achieves higher accuracy than 5-way and conjecture that the increased difficulty of 20-way classification helps the network to generalize better, because it forces the model to make more fine-grained decisions in the embedding space. Also, using Euclidean distance improves performance substantially over cosine distance. This effect is even more pronounced for prototypical networks, in which computing the class prototype as the mean of embedded support points is more naturally suited to Euclidean distances since cosine distance is not a Bregman divergence."
What is the resolution of the CUB image?,1024x1024 pixels,"[The CUB dataset contains 11,788 images of 200 bird species. The CUB image resolution is not mentioned in the paper, however, for images the paper uses 1,024-dimensional features extracted by applying GoogLeNet to middle, upper left, upper right, lower left, and lower right crops of the original and horizontally-flipped image.]","In order to assess the suitability of our approach for zero-shot learning, we also run experiments on the Caltech-UCSD Birds (CUB) 200-2011 dataset (Welinder et al., 2010). The CUB dataset contains 11,788 images of 200 bird species. We closely follow the procedure of Reed et al. (2016) in preparing the data. We use their splits to divide the classes into 100 training, 50 validation, and 50 test. For images we use 1,024-dimensional features extracted by applying GoogLeNet (Szegedy et al., 2015) to middle, upper left, upper right, lower left, and lower right crops of the original and horizontally-flipped image222Features downloaded from https://github.com/reedscot/cvpr2016.. At test time we use only the middle crop of the original image. For class meta-data we use the 312-dimensional continuous attribute vectors provided with the CUB dataset. These attributes encode various characteristics of the bird species such as their color, shape, and feather patterns."
"In related work, what is the most relevant method to this paper?","Non-linear extension of NCA (Salakhutdinov and Hinton, 2007)","[The paper’s method is most similar to the non-linear extension of NCA because the paper uses a neural network to perform the embedding and optimizes a softmax based on Euclidean distances in the transformed space, as opposed to a margin loss.]","The literature on metric learning is vast (Kulis, 2012; Bellet et al., 2013); we summarize here the work most relevant to our proposed method. Neighborhood Components Analysis (NCA) (Goldberger et al., 2004) learns a Mahalanobis distance to maximize K-nearest-neighbor’s (KNN) leave-one-out accuracy in the transformed space.Salakhutdinov and Hinton (2007) extend NCA by using a neural network to perform the transformation. Large margin nearest neighbor (LMNN) classification (Weinberger et al., 2005) also attempts to optimize KNN accuracy but does so using a hinge loss that encourages the local neighborhood of a point to contain other points with the same label. The DNet-KNN (Min et al., 2009) is another margin-based method that improves upon LMNN by utilizing a neural network to perform the embedding instead of a simple linear transformation. Of these, our method is most similar to the non-linear extension of NCA (Salakhutdinov and Hinton, 2007) because we use a neural network to perform the embedding and we optimize a softmax based on Euclidean distances in the transformed space, as opposed to a margin loss. A key distinction between our approach and non-linear NCA is that we form a softmax directly over classes, rather than individual points, computed from distances to each class’s prototype representation. This allows each class to have a concise representation independent of the number of data points and obviates the need to store the entire support set to make predictions."
What is the unit of local regions (the meaning of 'local'),Manhattan distance,"[A local region of a pixel consists of pixels with array indices within certain Manhattan distance (kernel size) of the pixel. In a point set sampled from a metric space, the neighborhood of a point is defined by metric distance. The distance metric defines local neighborhoods that may exhibit different properties. For example, the density and other attributes of points may not be uniform across different locations — in 3D scanning the density variability can come from perspective effects, radial density variations, motion, etc.]","We are interested in analyzing geometric point sets which are collections of points in a Euclidean space. A particularly important type of geometric point set is point cloud captured by 3D scanners, e.g., from appropriately equipped autonomous vehicles. As a set, such data has to be invariant to permutations of its members. In addition, the distance metric defines local neighborhoods that may exhibit different properties. For example, the density and other attributes of points may not be uniform across different locations — in 3D scanning the density variability can come from perspective effects, radial density variations, motion, etc. In convolutional neural networks, a local region of a pixel consists of pixels with array indices within certain Manhattan distance (kernel size) of the pixel. In a point set sampled from a metric space, the neighborhood of a point is defined by metric distance."
What distance metric this paper uses,Euclidean distance,"[In the paper, in a point set sampled from a metric space, the neighborhood of a point is defined by metric distance. For instance, suppose \mathcal{X}=(M,d) is a discrete metric space whose metric is inherited from a Euclidean space \mathbb{R}^{n}, where M\subseteq\mathbb{R}^{n} is the set of points and d is the distance metric. The paper targets at points sampled from a metric space and explicitly considers the underlying distance metric in its design.]","In convolutional neural networks, a local region of a pixel consists of pixels with array indices within certain Manhattan distance (kernel size) of the pixel. In a point set sampled from a metric space, the neighborhood of a point is defined by metric distance. A few very recent works [20, 28] have studied how to apply deep learning to unordered sets. They ignore the underlying distance metric even if the point set does possess one. As a result, they are unable to capture local context of points and are sensitive to global set translation and normalization. In this work, we target at points sampled from a metric space and tackle these issues by explicitly considering the underlying distance metric in our design. Suppose that \mathcal{X}=(M,d) is a discrete metric space whose metric is inherited from a Euclidean space \mathbb{R}^{n}, where M\subseteq\mathbb{R}^{n} is the set of points and d is the distance metric. In addition, the density of M in the ambient Euclidean space may not be uniform everywhere. We are interested in learning set functions f that take such \mathcal{X} as the input (along with additional features for each point) and produce information of semantic interest regrading \mathcal{X}. In practice, such f can be classification function that assigns a label to \mathcal{X} or a segmentation function that assigns a per point label to each member of M."
Is it right to define partition in Euclidean space?,No,"[The paper shows the generalizability of its approach to non-Euclidean space as well. For example, shapes in SHREC15 are 2D surfaces embedded in 3D space. Geodesic distances along the surfaces naturally induce a metric space. The paper shows through experiments that adopting PointNet++ in this metric space is an effective way to capture intrinsic structure of the underlying point set. Hence defining partitions in Euclidean space is not a wrong choice as it doesn't affect the generalizability of the paper's approach.]","In this section, we show generalizability of our approach to non-Euclidean space. In non-rigid shape classification (Fig. 7), a good classifier should be able to classify (a) and (c) in Fig. 7 correctly as the same category even given their difference in pose, which requires knowledge of intrinsic structure.Shapes in SHREC15 are 2D surfaces embedded in 3D space. Geodesic distances along the surfaces naturally induce a metric space. We show through experiments that adopting PointNet++ in this metric space is an effective way to capture intrinsic structure of the underlying point set."
Is it true that FPS could cover the whole set?,Yes,"[For evenly covering the whole set, the paper selects centroids among the input point set by a farthest point sampling (FPS) algorithm. Given input points \{x_{1},x_{2},...,x_{n}\}, the paper uses iterative FPS to choose a subset of points \{x_{i_{1}},x_{i_{2}},...,x_{i_{m}}\}, such that x_{i_{j}} is the most distant point (in metric distance) from the set \{x_{i_{1}},x_{i_{2}},...,x_{i_{j-1}}\} with regard to the rest points. Compared with random sampling, it has better coverage of the entire point set given the same number of centroids.]","Given input points \{x_{1},x_{2},...,x_{n}\}, we use iterative farthest point sampling (FPS) to choose a subset of points \{x_{i_{1}},x_{i_{2}},...,x_{i_{m}}\}, such that x_{i_{j}} is the most distant point (in metric distance) from the set \{x_{i_{1}},x_{i_{2}},...,x_{i_{j-1}}\} with regard to the rest points. Compared with random sampling, it has better coverage of the entire point set given the same number of centroids. In contrast to CNNs that scan the vector space agnostic of data distribution, our sampling strategy generates receptive fields in a data dependent manner.  One issue that still remains is how to generate overlapping partitioning of a point set. Each partition is defined as a neighborhood ball in the underlying Euclidean space, whose parameters include centroid location and scale. To evenly cover the whole set, the centroids are selected among input point set by a farthest point sampling (FPS) algorithm. Compared with volumetric CNNs that scan the space with fixed strides, our local receptive fields are dependent on both the input data and the metric, and thus more efficient and effective."
What are the benefits of being dependent on both the input data and the metric?,Greater efficiency and effectiveness,"[In comparison with volumetric CNNs that scan the space with fixed strides, the paper's local receptive fields are dependent on both the input data and the metric. The dependency on both input data and metric makes them more efficient and effective.]","One issue that still remains is how to generate overlapping partitioning of a point set. Each partition is defined as a neighborhood ball in the underlying Euclidean space, whose parameters include centroid location and scale. To evenly cover the whole set, the centroids are selected among input point set by a farthest point sampling (FPS) algorithm. Compared with volumetric CNNs that scan the space with fixed strides, our local receptive fields are dependent on both the input data and the metric, and thus more efficient and effective."
What are the benefits of hierarchical features for capturing local context?,"Sure! Here's the answer to your question based on the provided context:

Hierarchical features benefit scene understanding by capturing local context at multiple scales, allowing for the labeling of objects with varying sizes and the comprehension of scenes at multiple levels",[PointNet lacks the ability to capture local context at different scales. The paper introduces a hierarchical feature learning framework to resolve this limitation. The idea of hierarchical feature learning has been very successful and convolutional neural network is one of the most prominent examples. The paper's approach introduces hierarchical feature learning and captures geometry features at different scales. This is very important for understanding scenes at multiple levels and labeling objects with various sizes.],"PointNet achieved impressive performance on a few benchmarks. However, it lacks the ability to capture local context at different scales. We will introduce a hierarchical feature learning framework in the next section to resolve the limitation. Our approach outperforms all the baseline methods by a large margin. In comparison with [5], which learns on voxelized scans, we directly learn on point clouds to avoid additional quantization error, and conduct data dependent sampling to allow more effective learning. Compared with [20], our approach introduces hierarchical feature learning and captures geometry features at different scales. This is very important for understanding scenes at multiple levels and labeling objects with various sizes.We visualize example scene labeling results in Fig. 6. The idea of hierarchical feature learning has been very successful. Among all the learning models, convolutional neural network [10, 25, 8] is one of the most prominent ones.However, convolution does not apply to unordered point sets with distance metrics, which is the focus of our work."
Is it true that ball query's local neighborhood could make the local region feature more generalizable across space?,Yes,"[It is true. Ball query finds all points that are within a radius to the query point (an upper limit of K is set in implementation). Ball query’s local neighborhood guarantees a fixed region scale thus making local region feature more generalizable across space, which is preferred for tasks requiring local pattern recognition (e.g. semantic point labeling).]","Ball query finds all points that are within a radius to the query point (an upper limit of K is set in implementation). An alternative range query is K nearest neighbor (kNN) search which finds a fixed number of neighboring points. Compared with kNN, ball query’s local neighborhood guarantees a fixed region scale thus making local region feature more generalizable across space, which is preferred for tasks requiring local pattern recognition (e.g. semantic point labeling)."
How is the computation cost of MSG different from MRG?,MSG is computationally more expensive than MRG,"[Compared with MSG, MRG is computationally more efficient since it avoids feature extraction in large scale neighborhoods at the lowest levels. The MSG approach is computationally expensive since it runs local PointNet at large scale neighborhoods for every centroid point. In particular, since the number of centroid points is usually quite large at the lowest level, the time cost is significant.]","The MSG approach above is computationally expensive since it runs local PointNet at large scale neighborhoods for every centroid point. In particular, since the number of centroid points is usually quite large at the lowest level, the time cost is significant.  Compared with MSG, this method is computationally more efficient since we avoids the feature extraction in large scale neighborhoods at lowest levels. "
What if we do not share the parameter?,"Sure! Here's the answer based on the given context:

The inference speed of the proposed network may not be significantly improved by not sharing parameters","[No sharing of the parameters means no sharing of computations, causing deceleration of inference speed. Instead, the paper anticipates it as worthwhile to think of how to accelerate the inference speed of the paper's proposed network especially for MSG and MRG layers by sharing more computation in each local region.]","In the future, it’s worthwhile thinking how to accelerate inference speed of our proposed network especially for MSG and MRG layers by sharing more computation in each local regions. It’s also interesting to find applications in higher dimensional metric spaces where CNN based method would be computationally unfeasible while our method can scale well."
What is the instance of quantization artifacts?,Quantization artifacts that can obscure natural invariances of the data,"[The point clouds or meshes are not in a regular format, most researchers typically transform such data to regular 3D voxel grids or collections of images (e.g. views) before feeding them to a deep net architecture. This data representation transformation, however, renders the resulting data unnecessarily voluminous, while also introducing quantization artifacts that can obscure natural invariances of the data. However, the paper doesn't mention any particular instance of quantization artifact.]","In this paper we explore deep learning architectures capable of reasoning about 3D geometric data such as point clouds or meshes. Typical convolutional architectures require highly regular input data formats, like those of image grids or 3D voxels, in order to perform weight sharing and other kernel optimizations. Since point clouds or meshes are not in a regular format, most researchers typically transform such data to regular 3D voxel grids or collections of images (e.g, views) before feeding them to a deep net architecture. This data representation transformation, however, renders the resulting data unnecessarily voluminous — while also introducing quantization artifacts that can obscure natural invariances of the data."
What is the combinatorial irregularities and complexities of meshes?,"Sure! Here's my answer:

Combinatorial irregularities and complexities of meshes refer to the varying numbers of triangles, vertices, and edges that make up a 3D mesh, leading to a complex and irregular structure","[Point clouds are simple and unified structures that avoid the combinatorial irregularities and complexities of meshes, and thus are easier to learn from. However, these combinatorial irregularities and complexities of meshes are not mentioned in the paper.]","For this reason we focus on a different input representation for 3D geometry using simply point clouds – and name our resulting deep nets PointNets. Point clouds are simple and unified structures that avoid the combinatorial irregularities and complexities of meshes, and thus are easier to learn from. The PointNet, however, still has to respect the fact that a point cloud is just a set of points and therefore invariant to permutations of its members, necessitating certain symmetrizations in the net computation. Further invariances to rigid motions also need to be considered."
What is the meaning of invariant to permutations?,"Invariant to permutations means that the model should not be affected by the order in which the points are presented in the input set. The model should be able to recognize the same local structures and relationships among points, regardless of their specific order in the input data","[Point cloud is just a set of points and therefore invariant or indifferent to permutations of its members, necessitating certain symmetrizations in the net computation. Point cloud is a set of points without a specific order, unlike pixel arrays in images or voxel arrays in volumetric grids. This implies that a network that consumes N 3D point sets needs to be invariant to N! permutations of the input set in data feeding order.]","•Unordered.Unlike pixel arrays in images or voxel arrays in volumetric grids, point cloud is a set of points without specific order. In other words, a network that consumes N 3D point sets needs to be invariant to N! permutations of the input set in data feeding order.•Interaction among points. The points are from a space with a distance metric. It means that points are not isolated, and neighboring points form a meaningful subset. Therefore, the model needs to be able to capture local structures from nearby points, and the combinatorial interactions among local structures.•Invariance under transformations.As a geometric object, the learned representation of the point set should be invariant to certain transformations. For example, rotating and translating points all together should not modify the global point cloud category nor the segmentation of the points. "
How to get local or global features in the PointCloud domain,Use a Segmentation Network to combine global and local features,"[An SVM or multi-layer perceptron classifier can be trained on the shape global features for classification. However, point segmentation requires a combination of local and global knowledge. After computing the global point cloud feature vector, the paper feeds it back to per point features by concatenating the global feature with each of the point features. Then the paper extracts new per point features based on the combined point features - this time the per point feature is aware of both the local and global information. ]","The output from the above section forms a vector [f_{1},\dots,f_{K}], which is a global signature of the input set. We can easily train a SVM or multi-layer perceptron classifier on the shape global features for classification. However, point segmentation requires a combination of local and global knowledge. We can achieve this by a simple yet highly effective manner.  Our solution can be seen in Fig 2 (Segmentation Network). After computing the global point cloud feature vector, we feed it back to per point features by concatenating the global feature with each of the point features. Then we extract new per point features based on the combined point features - this time the per point feature is aware of both the local and global information. "
Is the reason for using max pooling for permutation invariant in the paper above?,"Yes.

In the paper, the author uses max pooling to make the model invariant to input permutation. Max pooling is a symmetric function that selects the maximum value from a set of input values, and it is used to aggregate the information from each point in the point cloud. By using max pooling, the network learns a set of optimization functions/criteria that select interesting or informative points of the point cloud and encode the reason for their selection, making the model invariant to input permutation","[Note:The question is not phrased correctly. In order to make a model invariant to input permutation, one of the strategies is to use a simple symmetric function to aggregate the information from each point. In the paper, a symmetric function takes n vectors as input and outputs a new vector that is invariant to the input order. For example, + and * operators are symmetric binary functions. The key to the paper's approach is the use of a single symmetric function, max pooling.]","In order to make a model invariant to input permutation, three strategies exist: 1) sort input into a canonical order; 2) treat the input as a sequence to train an RNN, but augment the training data by all kinds of permutations; 3) use a simple symmetric function to aggregate the information from each point. Here, a symmetric function takes n vectors as input and outputs a new vector that is invariant to the input order. For example, + and * operators are symmetric binary functions. Key to our approach is the use of a single symmetric function, max pooling. Effectively the network learns a set of optimization functions/criteria that select interesting or informative points of the point cloud and encode the reason for their selection. The final fully connected layers of the network aggregate these learnt optimal values into the global descriptor for the entire shape as mentioned above (shape classification) or are used to predict per point labels (shape segmentation)."
What if we change the order of g and h in equation (1)?,The result will be the same. The symmetry of g and h ensures that the order of application does not affect the output,"[A symmetric function takes n vectors as input and outputs a new vector that is invariant to the input order. For example, + and * operators are symmetric binary functions. The paper approximates a general function defined on a point set by applying a symmetric function on transformed elements in the set, as in Equation (1), where f:2^{\mathbb{R}^{N}}\rightarrow\mathbb{R}, h:\mathbb{R}^{N}\rightarrow\mathbb{R}^{K} and g:\underbrace{\mathbb{R}^{K}\times\dots\times\mathbb{R}^{K}}_{n}\rightarrow\mathbb{R} is a symmetric function. Therefore, the result would be the same even if the order of g and h is changed.]","In order to make a model invariant to input permutation, three strategies exist: 1) sort input into a canonical order; 2) treat the input as a sequence to train an RNN, but augment the training data by all kinds of permutations; 3) use a simple symmetric function to aggregate the information from each point. Here, a symmetric function takes n vectors as input and outputs a new vector that is invariant to the input order. For example, + and * operators are symmetric binary functions. Our idea is to approximate a general function defined on a point set by applying a symmetric function on transformed elements in the set:\displaystyle f(\{x_{1},\dots,x_{n}\})\approx g(h(x_{1}),\dots,h(x_{n})),(1)where f:2^{\mathbb{R}^{N}}\rightarrow\mathbb{R}, h:\mathbb{R}^{N}\rightarrow\mathbb{R}^{K} and g:\underbrace{\mathbb{R}^{K}\times\dots\times\mathbb{R}^{K}}_{n}\rightarrow\mathbb{R} is a symmetric function."
How to construct the affine transformation matrix?,"Sure! Here's the answer to your question based on the provided context:

The affine transformation matrix is constructed by a mini-network (T-net) composed of basic modules of point independent feature extraction, max pooling, and fully connected layers","[The paper predicts an affine transformation matrix by a mini-network, called T-net, and directly applies this transformation to the coordinates of input points. The mini-network itself resembles the big network and is composed of basic modules of point-independent feature extraction, max pooling, and fully connected layers.]","Our input form of point clouds allows us to achieve this goal in a much simpler way compared with [9]. We do not need to invent any new layers and no alias is introduced as in the image case. We predict an affine transformation matrix by a mini-network (T-net in Fig 2) and directly apply this transformation to the coordinates of input points. The mini-network itself resembles the big network and is composed by basic modules of point independent feature extraction, max pooling and fully connected layers. More details about the T-net are in the supplementary."
