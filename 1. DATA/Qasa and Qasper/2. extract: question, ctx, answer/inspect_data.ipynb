{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/ubuntu/iris_repos/llm_evaluation_thesis/QASA-QASPER/2. extract: question, ctx, answer/qasa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>question_type</th>\n",
       "      <th>context</th>\n",
       "      <th>correct_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do these automated metrics for human prefe...</td>\n",
       "      <td>complex question</td>\n",
       "      <td>The ultimate aim of language technology is to ...</td>\n",
       "      <td>The automated metrics that are mentioned while...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does non-differentiable mean here? If the...</td>\n",
       "      <td>testing question</td>\n",
       "      <td>The ultimate aim of language technology is to ...</td>\n",
       "      <td>A formal definition of non-differentiability h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why is the action space of language modeling p...</td>\n",
       "      <td>shallow question</td>\n",
       "      <td>Language generation action spaces are orders o...</td>\n",
       "      <td>The action space for language modeling is equa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What do the equations for Q-value and value re...</td>\n",
       "      <td>shallow question</td>\n",
       "      <td>RL4LMs supports fine-tuning and training LMs f...</td>\n",
       "      <td>Q and V are mathematically expressed as: V_{t}...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why is it helpful to mask out less relevant to...</td>\n",
       "      <td>complex question</td>\n",
       "      <td>Specifically, NLPOmaintains a masking policy \\...</td>\n",
       "      <td>The authors hypothesize that their dynamic mas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>What are the term-based techniques they used i...</td>\n",
       "      <td>shallow question</td>\n",
       "      <td>Traditional term-based methods like BM25 Rober...</td>\n",
       "      <td>Traditional term-based methods like BM25 Rober...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>How much the quality of generated pseudo-queri...</td>\n",
       "      <td>complex question</td>\n",
       "      <td>Another interesting question is how important ...</td>\n",
       "      <td>larger generation models lead to improved gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>Will the pseudo-query generator generalize wel...</td>\n",
       "      <td>complex question</td>\n",
       "      <td>Since our approach allows us to generate queri...</td>\n",
       "      <td>The pseudo-query generator generalize well whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>What if we introduce zero-shot generation for ...</td>\n",
       "      <td>complex question</td>\n",
       "      <td>Question generation for data augmentation is a...</td>\n",
       "      <td>if we introduce zero-shot generation for the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>I am wondering why the authors used community ...</td>\n",
       "      <td>complex question</td>\n",
       "      <td>First, we observe that general-domain question...</td>\n",
       "      <td>Using open-domain information retrieval datase...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1542 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question     question_type  \\\n",
       "0     How do these automated metrics for human prefe...  complex question   \n",
       "1     What does non-differentiable mean here? If the...  testing question   \n",
       "2     Why is the action space of language modeling p...  shallow question   \n",
       "3     What do the equations for Q-value and value re...  shallow question   \n",
       "4     Why is it helpful to mask out less relevant to...  complex question   \n",
       "...                                                 ...               ...   \n",
       "1537  What are the term-based techniques they used i...  shallow question   \n",
       "1538  How much the quality of generated pseudo-queri...  complex question   \n",
       "1539  Will the pseudo-query generator generalize wel...  complex question   \n",
       "1540  What if we introduce zero-shot generation for ...  complex question   \n",
       "1541  I am wondering why the authors used community ...  complex question   \n",
       "\n",
       "                                                context  \\\n",
       "0     The ultimate aim of language technology is to ...   \n",
       "1     The ultimate aim of language technology is to ...   \n",
       "2     Language generation action spaces are orders o...   \n",
       "3     RL4LMs supports fine-tuning and training LMs f...   \n",
       "4     Specifically, NLPOmaintains a masking policy \\...   \n",
       "...                                                 ...   \n",
       "1537  Traditional term-based methods like BM25 Rober...   \n",
       "1538  Another interesting question is how important ...   \n",
       "1539  Since our approach allows us to generate queri...   \n",
       "1540  Question generation for data augmentation is a...   \n",
       "1541  First, we observe that general-domain question...   \n",
       "\n",
       "                                         correct_answer  \n",
       "0     The automated metrics that are mentioned while...  \n",
       "1     A formal definition of non-differentiability h...  \n",
       "2     The action space for language modeling is equa...  \n",
       "3     Q and V are mathematically expressed as: V_{t}...  \n",
       "4     The authors hypothesize that their dynamic mas...  \n",
       "...                                                 ...  \n",
       "1537  Traditional term-based methods like BM25 Rober...  \n",
       "1538  larger generation models lead to improved gene...  \n",
       "1539  The pseudo-query generator generalize well whe...  \n",
       "1540  if we introduce zero-shot generation for the s...  \n",
       "1541  Using open-domain information retrieval datase...  \n",
       "\n",
       "[1542 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows for question type: complex question\n",
      "                                                                                                                                                                                                                                                                             question     question_type  \\\n",
      "787                                                                How does the use of identity shortcuts in this paper, which are parameter-free and always pass information through, differ from gated shortcuts used in highway networks that can be closed and block information?  complex question   \n",
      "955                                                                                                                                                                                                                            Was Transfer learning beneficial on the CADe process?   complex question   \n",
      "660                                                                                                                                                                                                                                         Explain the meaning of N-Way and M-shot.   complex question   \n",
      "1540  What if we introduce zero-shot generation for the synthetic query generation by using large-scale generative language models such as GPT-3, to get rid of the assumption that the training datasets exist even for the general domain? Would this too generate quality queries?  complex question   \n",
      "1355                                                           Why would the fact that PR2 had a greater gripping force be a valid reason for the difference in performance if \"valid grasps which was not executed correctly by Yogi\" were still counted as true positives for Yogi?  complex question   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            context  \\\n",
      "787                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \"Concurrent with our work, “highway networks” …….depth (e.g., over 100 layers).\"   \n",
      "955   While it is a more practical CADe scheme, slice-level CNN learning [40] is very challenging, as it is restricted to only 905 CT image slices with tagged ILD labels. We only benchmark the slice-level ILD classification results in this section. Even with the help of data augmentation (described in Sec. II), the classification accuracy of GoogLeNet-TL from Table III is only 0.57. However, transfer learning from ImageNet pre-trained model is consistently beneficial, as evidenced by AlexNet-TL (0.46) versus AlexNet-RI (0.44), and GoogLeNet-TL (0.57) versus GoogLeNet-RI (0.41). It especially prevents GoogLeNet from over-fitting on the limited CADe datasets. Finally, when the cross-validation is conducted by randomly splitting the set of all 905 CT axial slices into five folds, markedly higher F-scores are obtained (Slice-Random in Table IV). This further validates the claim that the dataset poorly generalizes ILDs for different patients. Figure 10 shows examples of misclassified ILD patches (in axial view), with their ground truth labels and inaccurately classified labels. •Deep CNN architectures with 8, even 22 layers [4, 33], can be useful even for CADe problems where the available training datasets are limited. Previously, CNN models used in medical image analysis applications have often been 2\\sim 5 orders of magnitude smaller.•The trade-off between using better learning models and using more training data [51] should be carefully considered when searching for an optimal solution to any CADe problem (e.g., mediastinal and abdominal LN detection).•Limited datasets can be a bottleneck to further advancement of CADe. Building progressively growing (in scale), well annotated datasets is at least as crucial as developing new algorithms. This has been accomplished, for instance, in the field of computer vision. The well-known scene recognition problem has made tremendous progress, thanks to the steady and continuous development of Scene-15, MIT Indoor-67, SUN-397 and Place datasets [58].•Transfer learning from the large scale annotated natural image datasets (ImageNet) to CADe problems has been consistently beneficial in our experiments. This sheds some light on cross-dataset CNN learning in the medical image domain, e.g., the union of the ILD [37] and LTRC datasets [64], as suggested in this paper.•Finally, applications of off-the-shelf deep CNN image features to CADe problems can be improved by either exploring the performance-complementary properties of hand-crafted features [10, 9, 12], or by training CNNs from scratch and better fine-tuning CNNs on the target medical image dataset, as evaluated in this paper.   \n",
      "660                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We evaluate our model by performing different q-shot, K-way experiments on both datasets. For every few-shot task \\mathcal{T}, we sample K random classes from the dataset, and from each class we sample q random samples. An extra sample to classify is chosen from one of that K classes.   \n",
      "1540                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Question generation for data augmentation is a common tool, but has not been tested in the pure zero-shot setting nor for neural passage retrieval.Duan et al. (2017) use community QA as a data source, as we do, to train question generators. The generated question-passage pairs are not used to train a neural model, but QA is instead done via question-question similarity. Furthermore, they do not test on specialized domains. Alberti et al. (2019) show that augmenting supervised training resources with synthetic question-answer pairs can lead to improvements. Nogueira et al. (2019) employed query generation in the context of first-stage retrieval. In that study, the generated queries were used to augment documents to improve BM25 keyword search. Here we focus on using synthetic queries to train the neural retrieval models.   \n",
      "1355                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Our algorithm was able to consistently detect and execute valid grasps for ared cereal box, but had some failures on a white and yellow one. This is becausethe background for all objects in the dataset is white, leading the algorithmto learn features relating white areas at the edges of the gripper region tograspable cases. However, it was able to detect and execute correct grasps foran all-white ice cube tray, and so does not fail for all white objects. Thiscould be remedied by extending the dataset to include cases with differentbackground colors.Interestingly, even though the parameters of grasps detectedfor the white box were similar for PR2 and Baxter, PR2 was able to succeed inevery case while Baxter succeeded only half the time. This is because PR2’sincreased gripper strength allowed it to execute grasps across corners of thebox, crushing it slightly in the process. PR2 yielded a higher success rate as seen in Table LABEL:tbl:pr2Results,succeeding in 89% of trials. This islargely due to the much wider span of PR2’s gripper from open to closed andits ability to fully close from its widest position, as well as PR2’s abilityto apply a larger gripping force. Some specific instances where PR2 andBaxter’s performance differed are discussed below.    \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              correct_answer  \n",
      "787                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The identity shortcuts presented in this paper differ from gated shortcuts because it don’t depend on data and always learn residual functions.  \n",
      "955                                                                                                                                                                                                                                                                                                                                                                                                                                         Transfer learning was shown to be beneficial in the paper's experiments, as seen by the differences in performance between AlexNet-TL/GoogLeNet-TL and their non-transfer learning counterparts.  \n",
      "660                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 N-way and M-shot means that we sample N random classes from the dataset, and we sample M random samples from each class.  \n",
      "1540                                                                                                                                                                                                                                                                                                                                                            if we introduce zero-shot generation for the synthetic query generation by using large-scale generative language models such as GPT-3, to get rid of the assumption that the training datasets exist even for the general domain, Would this still generate  quality queries  \n",
      "1355  PR2 yielded a higher success rate as seen in Table V, succeeding in 89% of trials. This is largely due to the much wider span of PR2’s gripper from open to closed and its ability to fully close from its widest position, as well as PR2’s ability to apply a larger gripping force.  Interestingly, even though the parameters of grasps detected for the white box were similar for PR2 and Baxter, PR2 was able to succeed in every case while Baxter succeeded only half the time. This is because PR2’s increased gripper strength allowed it to execute grasps across corners of the box, crushing it slightly in the process.  \n",
      "\n",
      "\n",
      "Rows for question type: testing question\n",
      "                                                                                                                                                                                                              question     question_type  \\\n",
      "259                                                                                                                          How is the inversion of text-guided diffusion models different from the inversion of GAN?  testing question   \n",
      "1144                                                                                                                                                                                             What does “RoI” mean?  testing question   \n",
      "979                                                                                                                                                                                   What is catastrophic forgetting?  testing question   \n",
      "220                                                                                                                               What hyperparameters values were used to train both the plain and highway networks ?  testing question   \n",
      "789   How does the effectiveness of the residual representation method used in this paper compare to other methods such as Multigrid and hierarchical basis preconditioning in solving Partial Differential Equations?  testing question   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            context  \\\n",
      "259                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Real Image Editing.Editing a real image requires finding an initial noise vector that produces the given input image when fed into the diffusion process. This process, known as inversion, has recently drawn considerable attention for GANs, e.g., zhu2016generative ; abdal2019image2stylegan ; alaluf2022hyperstyle ; roich2021pivotal ; zhu2020domain ; tov2021designing ; Wang2021HighFidelityGI ; xia2021gan , but has not yet been fully addressed for text-guided diffusion models.   \n",
      "1144                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of H\\times W (e.g., 7\\times 7), where H and W are layer hyper-parameters that are independent of any particular RoI.In this paper, an RoI is a rectangular window into a conv feature map.Each RoI is defined by a four-tuple (r,c,h,w) that specifies its top-left corner (r,c) and its height and width (h,w).   \n",
      "979                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Fine-tuning the target classifier is the most critical part of the transfer learning method. Overly aggressive fine-tuning will cause catastrophic forgetting, eliminating the benefit of the information captured through language modeling; too cautious fine-tuning will lead to slow convergence (and resultant overfitting). Besides discriminative fine-tuning and triangular learning rates, we propose gradual unfreezing for fine-tuning the classifier. We show that not the idea of LM fine-tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption. LMs overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier. Compared to CV, NLP models are typically more shallow and thus require different fine-tuning methods. On all datasets, fine-tuning the full model leads to the lowest error comparatively early in training, e.g. already after the first epoch on IMDb. The error then increases as the model starts to overfit and knowledge captured through pretraining is lost. In contrast, ULMFiT is more stable and suffers from no such catastrophic forgetting; performance remains similar or improves until late epochs, which shows the positive effect of the learning rate schedule.   \n",
      "220   All networks were trained using SGD with momentum. An exponentially decaying learning rate was used in Section 3.1. For the rest of the experiments, a simpler commonly used strategy was employed where the learning rate starts at a value \\lambda and decays according to a fixed schedule by a factor \\gamma. \\lambda, \\gamma and the schedule were selected once based on validation set performance on the CIFAR-10 dataset, and kept fixed for all experiments.All convolutional highway networks utilize the rectified linear activation function [16] to compute the block state H. To provide a better estimate of the variability of classification results due to random initialization, we report our results in the format Best (mean \\pm std.dev.) based on 5 runs wherever available. Experiments were conducted using Caffe [33] and Brainstorm (https://github.com/IDSIA/brainstorm) frameworks. Source code, hyperparameter search results and related scripts are publicly available at http://people.idsia.ch/~rupesh/very_deep_learning/. We trained both plain and highway networks of varying varying depths on the MNIST digit classification dataset.All networks are thin: each layer has 50 blocks for highway networks and 71 units for plain networks, yielding roughly identical numbers of parameters (\\approx5000) per layer.In all networks, the first layer is a fully connected plain layer followed by 9, 19, 49, or 99 fully connected plain or highway layers. Finally, the network output is produced by a softmax layer.We performed a random search of 100 runs for both plain and highway networks to find good settings for the following hyperparameters: initial learning rate, momentum, learning rate exponential decay factor & activation function (either rectified linear or tanh). For highway networks, an additional hyperparameter was the initial value for the transform gate bias (between -1 and -10). Other weights were initialized using the same normalized initialization as plain networks.   \n",
      "789                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \"In low-level vision and computer…………..or preconditioning can simplify the optimization.\"   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                  correct_answer  \n",
      "259                                                                                                                                                                                                     Inversion of GANs requires finding the initial noise vector that produces the edit we want. Can't fully answer this question regarding the text guided as it's not fully addressed for text-guided diffusion models yet.  \n",
      "1144                                                                                                                                                                                                                                                                                                                                                                    The RoI is a rectangular window into a conv feature map.  \n",
      "979                                                                                                                                                                                                                                                                                            Catastrophic forgetting involved increasing error as a model start to overfit and knowledge captured through pretraining is lost.  \n",
      "220                                                                                                                                                                                                                                                                                                                           Both the plain network and the highway network set the best hyperparameters after 100 experiments.  \n",
      "789   Multigrid method reformulates the system as sub problems at multiple scales, where each sub problem is responsible for the residual solution between a coarser and a finer scale. Hierarchical basis preconditioning relies on variables that represent residual vectors between two scales. The effectiveness of residual representation method presented in this paper cannot be compared to these methods in this paper  \n",
      "\n",
      "\n",
      "Rows for question type: shallow question\n",
      "                                                                                                 question     question_type  \\\n",
      "1064                       What are the eight different LSTM variants that the authors experimented with?  shallow question   \n",
      "680                                                                What datasets did this paper used for?  shallow question   \n",
      "489   Were the baseline models implemented from scratch or from existing codes from the original authors?  shallow question   \n",
      "718                              What type of ML models have been successful on the FashionMNIST dataset?  shallow question   \n",
      "1232                                                     What is the instance of the 'geometric changes'?  shallow question   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       context  \\\n",
      "1064  The focus of our study is to empirically compare different LSTM variants, and not to achieve state-of-the-art results.Therefore, our experiments are designed to keep the setup simple and the comparisons fair.The vanilla LSTM is used as a baseline and evaluated together with eight of its variants.Each variant adds, removes, or modifies the baseline in exactly one aspect, which allows to isolate their effect.They are evaluated on three different datasets from different domains to account for cross-domain variations. This paper reports the results of a large scale study on variants of the LSTM architecture. We conclude that the most commonly used LSTM architecture (vanilla LSTM) performs reasonably well on various datasets.None of the eight investigated modifications significantly improves performance.However, certain modifications such as coupling the input and forget gates (CIFG) or removing peephole connections (NP) simplified LSTMs in our experiments without significantly decreasing performance.These two variants are also attractive because they reduce the number of parameters and the computational cost of the LSTM. The first important observation based on Figure 3 is that removing the output activation function (NOAF) or the forget gate (NFG) significantly hurt performance on all three datasets. Apart from the CEC, the ability to forget old information and the squashing of the cell state appear to be critical for the LSTM architecture. Indeed, without the output activation function, the block output can in principle grow unbounded. Coupling the input and the forget gate avoids this problem and might render the use of an output non-linearity less important, which could explain why GRU performs well without it. Adding full gate recurrence (FGR) did not significantly change performance on TIMIT or IAM Online, but led to worse results on the JSB Chorales dataset. Given that this variant greatly increases the number of parameters, we generally advise against using it. Note that this feature was present in the original proposal of LSTM [14, 15], but has been absent in all following studies. Removing the input gate (NIG), the output gate (NOG), and the input activation function (NIAF) led to a significant reduc\u0002tion in performance on speech and handwriting recognition. However, there was no significant effect on music modeling performance. A small (but statistically insignificant) average performance improvement was observed for the NIG and NIAF architectures on music modeling. We hypothesize that these behaviors will generalize to similar problems such as language modeling. For supervised learning on continuous real-valued data (such as speech and handwriting recognition), the input gate, output gate, and input activation function are all crucial for obtaining good performance.   \n",
      "680                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Datasets. The datasets we have used in the experiments include three versions of TACRED: the original TACRED Zhang et al. (2017), TACREV Alt et al. (2020), and Re-TACRED Stoica et al. (2021).Alt et al. (2020) observed that the TACRED dataset contains about 6.62\\% noisily-labeled instances and relabeled the development and test set.Stoica et al. (2021) further refined the label definitions in TACRED and relabeled the whole dataset.We provide the statistics of the datasets in Appendix A. In this paper, we present a simple yet strong RE baseline that offers new SOTA performance, along with a comprehensive study to understand its prediction generalizability and robustness.Specifically, we revisit two technical problems in sentence-level RE, namely entity representation and noisy or ill-defined labels.We propose an improved entity representation technique, which significantly outperforms existing sentence-level RE models.Especially, our improved RE baseline achieves an F_{1} score of 91.1\\% on the Re-TACRED dataset, showing that PLMs already achieve satisfactory performance on this task.We hope the proposed techniques and analyses can benefit future research on RE.   \n",
      "489                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The models are implemented and evaluated in Python 3 and the PyTorch deep learning framework of version 1.5.0. For training each model, we use one NVIDIA GeForce GTX 1080 Ti. We mostly refer to the previous implemen- tations [40], [48] when implementing the vanilla Trans- former. For implementing and training BLSTM and ONADE, we use the original settings [9], [11]. The gradients are all clipped to 1 for the learning stability during training of all models. VTHarm, rVTHarm, and ONADE are assessed with 10 test samples per melody due to their randomness.   \n",
      "718                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The reason MNIST is so popular has to do with its size, allowing deep learning researchers to quickly check and prototype their algorithms. This is also complemented by the fact that all machine learning libraries (e.g. scikit-learn) and deep learning frameworks (e.g. Tensorflow, Pytorch) provide helper functions and convenient examples that use MNIST out of the box. Our aim with this work is to create a good benchmark dataset which has all the accessibility of MNIST, namely its small size, straightforward encoding and permissive license. We took the approach of sticking to the 10 classes 70,000 grayscale images in the size of 28\\times 28 as in the original MNIST. In fact, the only change one needs to use this dataset is to change the URL from where the MNIST dataset is fetched. Moreover, Fashion-MNIST poses a more challenging classification task than the simple MNIST digits data, whereas the latter has been trained to accuracies above 99.7% as reported in Wan et al. (2013); Ciregan et al. (2012).   \n",
      "1232                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Although our method can achieve compelling results in many cases, the results are far from uniformly positive. Figure 17 shows several typical failure cases. On translation tasks that involve color and texture changes, as many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. For example, on the task of dog\\rightarrowcat transfiguration, the learned translation degenerates into making minimal changes to the input (Figure 17). This failure might be caused by our generator architectures which are tailored for good performance on the appearance changes. Handling more varied and extreme transformations, especially geometric changes, is an important problem for future work.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              correct_answer  \n",
      "1064                                                                                                                                              The authors conducted the experiment with these LSTM variants of the vanilla architecture to empirically compare different LSTM variants: No Input Gate (NIG), No Forget Gate (NFG), No Output Gate (NOG), No Input Activation Function (NIAF), No Output Activation Function (NOAF), Coupled Input and Forget Gate (CIFG), No Peepholes (NP), Full Gate Recurrence (FGR).  \n",
      "680                                                                                                                                                                                                                                                                                                                                                                                         They used three datasets, the original TACRED Zhang et al. (2017), TACREV Alt et al. (2020), and Re-TACRED Stoica et al. (2021).  \n",
      "489                                                                                                                                                                                                                                                                                                                                                                     The baseline models are implemented from scratch, where the experimental settings are referred to the original settings in the corresponding papers.  \n",
      "718                                                                                                                                                                                                                                                                                                                                                                        the author talks about using various ML and DL models and getting the accuracy as well, but exactly which models are used has not been specified.  \n",
      "1232  [The paper has explored tasks that require geometric changes, with little success. For example, on the task of dog→cat transfiguration, the learned translation degenerates into making minimal changes to the input. This failure might be caused by the paper’s generator architectures which are tailored for good performance on the appearance changes. This implies that a geometric change doesn’t include a change in appearance, but the paper doesn't explicitly mention an instance of \"geometric change\".]  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_question_types = df['question_type'].unique()\n",
    "\n",
    "for question_type in unique_question_types:\n",
    "    sampled_df = df[df['question_type'] == question_type].sample(n=5)\n",
    "    print(f\"Rows for question type: {question_type}\")\n",
    "    print(sampled_df)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Why is the action space of language modeling particularly large? Is it because of the vocab size? But then, moving in the real world also has a huge action space (degrees of movement).\n",
      "question_type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           shallow question\n",
      "context           Language generation action spaces are orders of magnitude larger than what most discrete action space RL algorithms are designed for (Ranzato et al., 2015; Ammanabrolu, 2021), e.g., GPT-2/3 and T5 have a vocabulary size of 50K and 32K respectively.We hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods.To address this issue, we introduce NLPO (Natural Language Policy Optimization), which is inspired by work on action elimination/invalid-action masking (Zahavy et al., 2018; Huang & Ontañón, 2020; Ammanabrolu & Hausknecht, 2020). NLPO, a parameterized-masked extension of PPO, learns to mask out less relevant tokens in-context as it trains. NLPO accomplishes this via top-p sampling, which restricts tokens to the smallest possible set whose cumulative probability is greater than the probability parameter p (Holtzman et al., 2018). RL for Large Action Spaces. MIXER (Ranzato et al., 2015) combined ideas from schedule sampling and REINFORCE (Williams, 1992).Bahdanau et al. (2016) proposed an actor-critic algorithm to address the variance/large action space problems when using REINFORCE for language generation; follow-up works such asKG-A2C (Ammanabrolu & Hausknecht, 2020), TrufLL (Martin et al., 2022), AE-DQN (Zahavy et al., 2018), and GALAD (Ammanabrolu et al., 2022) addressed similar issues by attempting to eliminate and reduce the action space during exploration. Each environment is an NLP task: we are given a supervised dataset \\mathcal{D}=\\{({\\bm{x}}^{i},{\\bm{y}}^{i})\\}_{i=1}^{N} of N examples, where {\\bm{x}}\\in\\mathcal{X} is an language input and {\\bm{y}}\\in\\mathcal{Y} is the target string. Generation can be viewed as a Markov Decision Process (MDP) \\langle\\mathcal{S},\\mathcal{A},\\mathcal{R},P,\\gamma,T\\rangle using a finite vocabulary \\mathcal{V}.Each episode in the MDP begins by sampling a datapoint ({\\bm{x}},{\\bm{y}}) from our dataset and ends when the current time step t exceeds the horizon T or an end of sentence (EOS) token is generated.The input {\\bm{x}}=(x_{0},\\cdots,x_{m}) is a task-specific prompt that is used as our initial state \\bm{s}_{0}=(x_{0},\\cdots,x_{m}), where \\bm{s}_{0}\\in\\mathcal{S} and \\mathcal{S} is the state space with x_{m}\\in\\mathcal{V}. An action in the environment a_{t}\\in\\mathcal{A} consists of a token from our vocabulary \\mathcal{V}. The transition function P:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathcal{S} deterministically appends an action a_{t} to the end of the state \\bm{s}_{t-1}=(x_{0},\\cdots,x_{m},a_{0},\\cdots,a_{t-1}). This continues until the end of the horizon t\\leq T and we obtain a state \\bm{s}_{T}=(x_{0},\\cdots,x_{m},a_{0},\\cdots,a_{T}).At the end of an episode a reward \\mathcal{R}:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}^{1} that depends on the (\\bm{s}_{T},{\\bm{y}}) (e.g., an automated metric like PARENT Dhingra et al. (2019)) is emitted. RL4LMs provides an OpenAI gym (Brockman et al., 2016) styleAPI for an RL environmentthat simulates this LM-Based MDP formulation.Abstracting the details of the MDP environment structure allows for new tasks to be added quickly with compatibility across all implemented algorithms.\n",
      "correct_answer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The action space for language modeling is equal to the vocabulary set of language models. Since the vocabularies are very large (i.e. tens of thousands of possible tokens), the action space is also very large. In general, locomotion in the real world can be condensed to three quantities - moving across X, Y or Z axes, or a linear combination thereof. The authors mention that typical RL problems have an action space that is an order of magnitude smaller, but do not specifically explain how the action spaces for typical problems is modeled or constructed.\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "row = df.loc[2]\n",
    "print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
