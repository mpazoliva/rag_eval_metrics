question,iris_answer,correct_answer,context
How to set Cs(critical point set),"Choose a finite subset \mathcal{C}_{S} of less or equal to K elements from S, where K is the bottleneck dimension of f","[The second part of theorem 2 in the paper implies that \mathcal{C}_{S} only contains a bounded number of points, determined by K in Equation(1). That is,  f(S) is in fact totally determined by a finite subset \mathcal{C}_{S}\subseteq S of less or equal to K elements. Hence, the paper calls \mathcal{C}_{S} the critical point set of S and K the bottleneck dimension of f.]","We explain the implications of the theorem. (a) says that f(S) is unchanged up to the input corruption if all points in \mathcal{C}_{S} are preserved; it is also unchanged with extra noise points up to \mathcal{N}_{S}. (b) says that \mathcal{C}_{S} only contains a bounded number of points, determined by K in (1). In other words, f(S) is in fact totally determined by a finite subset \mathcal{C}_{S}\subseteq S of less or equal to K elements. We therefore call \mathcal{C}_{S} the critical point set of S and K the bottleneck dimension of f."
why PointNet is highly robust to small perturbation of input points as well as to corruption through point insertion (outliers) or deletion (missing data)?,"PointNet is highly robust to small perturbation of input points and corruption through point insertion/deletion due to its sparse representation of shapes, which allows it to tolerate minor changes in the input data while maintaining its ability to accurately classify or segment objects","The paper shows the universal approximation ability of its neural network to continuous set functions. By the continuity of set functions, intuitively, a small perturbation to the input point set should not greatly change the function values, such as classification or segmentation scores. The paper's theorem 2, combined with the continuity of h, explains the robustness of the paper's model w.r.t point perturbation, corruption, and extra noise points. The robustness is gained in analogy to the sparsity principle in machine learning models. Intuitively, the paper's network learns to summarize a shape by a sparse set of key points.","Combined with the continuity of h, this explains the robustness of our model w.r.t point perturbation, corruption and extra noise points. The robustness is gained in analogy to the sparsity principle in machine learning models. Intuitively, our network learns to summarize a shape by a sparse set of key points. In experiment section we see that the key points form the skeleton of an object. We first show the universal approximation ability of our neural network to continuous set functions. By the continuity of set functions, intuitively, a small perturbation to the input point set should not greatly change the function values, such as classification or segmentation scores."
What is the definition of 'task'?,"A task is a set of observations and corresponding outputs, along with a loss function that provides task-specific feedback, as defined by the context","Task means what the agent wants to learn. It can be Supervised Learning or Reinforcement Learning, which is represented by initial state, loss function, transition distribution, and episode length H.","Few-shot learning is well-studied in the domain of supervised tasks, where the goal is to learn a new function from only a few input/output pairs for that task, using prior data from similar tasks for meta-learning. For example, the goal might be to classify images of a Segway after seeing only one or a few examples of a Segway, with a model that has previously seen many other types of objects. Likewise, in few-shot regression, the goal is to predict the outputs of a continuous-valued function from only a few datapoints sampled from that function, after training on many functions with similar statistical properties. We consider a model, denoted f, that maps observations \mathbf{x} to outputs \mathbf{a}.During meta-learning, the modelis trained to be able to adapt to a large or infinite number of tasks.Since we would like to apply our framework to a variety of learning problems, from classification to reinforcement learning, we introduce a generic notion of a learning task below.Formally, each task \mathcal{T}=\{\mathcal{L}(\mathbf{x}_{1},\mathbf{a}_{1},\dots,\mathbf{x}_{H},\mathbf{a}_{H}),q(\mathbf{x}_{1}),q(\mathbf{x}_{t+1}|\mathbf{x}_{t},\mathbf{a}_{t}),H\}consists of a loss function \mathcal{L}, a distribution over initial observations q(\mathbf{x}_{1}), a transition distribution q(\mathbf{x}_{t+1}|\mathbf{x}_{t},\mathbf{a}_{t}), and an episode length H. In i.i.d. supervised learning problems, the length H\!=\!1.The model may generate samples of length H by choosing an output \mathbf{a}_{t} at each time t.The loss \mathcal{L}(\mathbf{x}_{1},\mathbf{a}_{1},\dots,\mathbf{x}_{H},\mathbf{a}_{H})\rightarrow\mathbb{R}, provides task-specific feedback, which might be in the form of a misclassification loss or a cost function in a Markov decision process."
What is the definition of 'meta-learning'?,"Meta-learning is a machine learning technique that involves training a model on a set of tasks, such that the trained model can quickly adapt to new tasks using only a small number of examples or training iterations",Meta-learning is a learning method that enables a model to quickly adapt to tasks during learning. Few-shot meta learning means learning a good model within a small number of iterations using only a few training images. Reinforcement learning means that a good policy can be learned using a small number of experiences.,"Learning quickly is a hallmark of human intelligence, whether it involves recognizing objects from a few examples or quickly learning new skillsafter just minutes of experience. Our artificial agents should be able to do the same, learning and adapting quickly from only a few examples, and continuing to adapt as more data becomes available. This kind of fast and flexible learning is challenging, since the agent must integrate its prior experience with a small amount of new information, while avoiding overfitting to the new data. Furthermore, the form of prior experience and new data will depend on the task. As such, for the greatest applicability, the mechanism for learning to learn (or meta-learning) should be general to the task and the form of computation required to complete the task. In this work, we propose a meta-learning algorithm that is general and model-agnostic, in the sense that it can be directly applied to any learning problem and model that is trained with a gradient descent procedure. Our focus is on deep neural network models, but we illustrate how our approach can easily handle different architectures and different problem settings, including classification, regression, and policy gradient reinforcement learning, with minimal modification.In meta-learning, the goal of the trained model is to quickly learn a new task from a small amount of new data, and the model istrained by the meta-learner to be able to learn on a large number of different tasks.The key idea underlying our method is totrain the model’s initial parameters such that the model has maximal performance on a new task after the parameters have been updatedthrough one or more gradient steps computed with a small amount of data from that new task.Unlike prior meta-learning methods that learn an update function or learning rule (Schmidhuber, 1987; Bengio et al., 1992; Andrychowicz et al., 2016; Ravi & Larochelle, 2017), our algorithm does not expand the number of learned parameters nor place constraints on the model architecture (e.g. by requiring a recurrent model (Santoro et al., 2016) or a Siamese network (Koch, 2015)), and it can be readily combined with fully connected, convolutional, or recurrent neural networks. It can also be used with a variety of loss functions, including differentiable supervised losses and non-differentiable reinforcement learning objectives.  In reinforcement learning (RL), the goal of few-shot meta-learning is to enable an agent to quickly acquire a policy for a new test task using only a small amount of experience in the test setting. A new task might involve achieving a new goal or succeeding on a previously trained goal in a new environment. For example, an agent might learn to quickly figure out how to navigate mazes so that, when faced with a new maze, it can determine how to reliably reach the exit with only a few samples.In this section, we will discuss how MAML can be applied to meta-learning for RL. All of the meta-learning problems that we consider require some amount of adaptation to new tasks at test-time. When possible, we compare our results to an oracle that receives the identity of the task (which is a problem-dependent representation) as an additional input, as an upper bound on the performance of the model. All of the experiments were performed using TensorFlow (Abadi et al., 2016), which allows for automatic differentiation through the gradient update(s) during meta-learning. The code is available online111Code for the regression and supervised experiments is at github.com/cbfinn/maml and code for the RL experiments is at github.com/cbfinn/maml_rl. The primary contribution of this work is a simple model- and task-agnostic algorithm for meta-learning that trains a model’s parameters such that a small number of gradient updates will lead to fast learning on a new task.We demonstrate the algorithm on different model types, including fully connected and convolutional networks, and in several distinct domains, including few-shot regression, image classification, and reinforcement learning.Our evaluation shows that our meta-learning algorithm compares favorably to state-of-the-art one-shot learning methods designed specifically for supervised classification, while using fewer parameters, but that it can also be readily applied to regression and can accelerate reinforcement learning in the presence of task variability, substantially outperforming direct pretraining as initialization. The goal of few-shot meta-learningis to train a model that can quickly adapt to a new task using only a few datapoints and training iterations.To accomplish this, the model or learner is trained during a meta-learning phase on a set of tasks, such that the trained model can quickly adapt to new tasks using only a small number of examples or trials.In effect, the meta-learning problem treats entire tasks as training examples.In this section, we formalize this meta-learning problem setting in a general manner, including brief examples of different learning domains.We will discuss two different learning domains in detail in Section 3. We consider a model, denoted f, that maps observations \mathbf{x} to outputs \mathbf{a}.During meta-learning, the modelis trained to be able to adapt to a large or infinite number of tasks.Since we would like to apply our framework to a variety of learning problems, from classification to reinforcement learning, we introduce a generic notion of a learning task below.Formally, each task \mathcal{T}=\{\mathcal{L}(\mathbf{x}_{1},\mathbf{a}_{1},\dots,\mathbf{x}_{H},\mathbf{a}_{H}),q(\mathbf{x}_{1}),q(\mathbf{x}_{t+1}|\mathbf{x}_{t},\mathbf{a}_{t}),H\}consists of a loss function \mathcal{L}, a distribution over initial observations q(\mathbf{x}_{1}), a transition distribution q(\mathbf{x}_{t+1}|\mathbf{x}_{t},\mathbf{a}_{t}), and an episode length H. In i.i.d. supervised learning problems, the length H\!=\!1.The model may generate samples of length H by choosing an output \mathbf{a}_{t} at each time t.The loss \mathcal{L}(\mathbf{x}_{1},\mathbf{a}_{1},\dots,\mathbf{x}_{H},\mathbf{a}_{H})\rightarrow\mathbb{R}, provides task-specific feedback, which might be in the form of a misclassification loss or a cost function in a Markov decision process."
What type of parameter would be considered a 'good' initial parameter?,"A 'good' initial parameter would be one that is sensitive to changes in the task, such that small changes in the parameters lead to large improvements on the loss function of any task drawn from p(T)","A good initial parameter is a parameter that gives good performance in many tasks even with a little fine-tuning of the parameter. This means that the loss function defined in many tasks is sensitive, and this sensitive loss leads to good updates.","The process of training a model’s parameters such that a few gradient steps, or even a single gradient step, can produce good results on a new task can be viewed from a feature learning standpoint as building an internal representation that is broadly suitable for many tasks. If the internal representation is suitable to many tasks, simply fine-tuning the parameters slightly (e.g. by primarily modifying the top layer weights in a feedforward model) can produce good results. In effect, our procedure optimizes for models that are easy and fast to fine-tune, allowing the adaptation to happen in the right space for fast learning. From a dynamical systems standpoint, our learning process can be viewed as maximizing the sensitivity of the loss functions of new tasks with respect to the parameters: when the sensitivity is high, small local changes to the parameters can lead to large improvements in the task loss. Our approach is also related to methods for initialization of deep networks. In computer vision, models pretrained on large-scale image classification have been shown to learn effective features for a range of problems (Donahue et al., 2014). In contrast, our method explicitly optimizes the model for fast adaptability, allowing it to adapt to new tasks with only a few examples.Our method can also be viewed as explicitly maximizing sensitivity of new task losses to the model parameters.A number of prior works have explored sensitivity in deep networks, often in the context of initialization (Saxe et al., 2014; Kirkpatrick et al., 2016). Most of these works have considered good random initializations, though a number of papers have addressed data-dependent initializers (Krähenbühl et al., 2016; Salimans & Kingma, 2016), including learned initializations (Husken & Goerick, 2000; Maclaurin et al., 2015). In contrast, our method explicitly trains the parameters for sensitivity on a given task distribution, allowing for extremely efficient adaptation for problems such as K-shot learning and rapid reinforcement learning in only one or a few gradient steps. In contrast to prior work, which has sought to train recurrent neural networks that ingest entire datasets (Santoro et al., 2016; Duan et al., 2016b) or feature embeddings that can be combined with nonparametric methods at test time (Vinyals et al., 2016; Koch, 2015), we propose a method that can learn the parameters of any standard model via meta-learning in such a way as to prepare that model for fast adaptation. The intuition behind this approach is that some internal representations are more transferrable than others. For example, a neural network might learn internal features that are broadly applicable to all tasks in p(\mathcal{T}), rather than a single individual task. How can we encourage the emergence of such general-purpose representations? We take an explicit approach to this problem: since the model will be fine-tuned using a gradient-based learning rule on a new task, we will aim to learn a model in such a way that this gradient-based learning rule can make rapid progress on new tasks drawn from p(\mathcal{T}), without overfitting. In effect, we will aim to find model parameters that are sensitive to changes in the task, such that small changes in the parameters will produce large improvements on the loss function of any task drawn from p(\mathcal{T}), when altered in the direction of the gradient of that loss (see Figure 1). We make no assumption on the form of the model, other than to assume that it is parametrized by some parameter vector \theta, and that the loss function is smooth enough in \theta that we can use gradient-based learning techniques."
Is it true that this paper's learning process can be viewed as maximizing the sensitivity of the loss functions of new tasks with respect to the parameters?,"Yes. The paper's learning process can be viewed as maximizing the sensitivity of the loss functions of new tasks with respect to the parameters, as the goal is to learn model parameters such that small changes in the parameters will produce large improvements on the loss function of any task drawn from the task distribution p(\mathcal{T})","It is true. As many sentences mention, it can be seen as increasing the sensitivity of the loss function.","The model parameters are trained by optimizing for the performance of f_{\theta_{i}^{\prime}} with respect to \theta across tasks sampled from p(\mathcal{T}).More concretely, the meta-objective is as follows:\displaystyle\vspace{-0.2cm}\min_{\theta}\sum_{\mathcal{T}_{i}\sim p(\mathcal{T})}\mathcal{L}_{\mathcal{T}_{i}}(f_{\theta_{i}^{\prime}})=\sum_{\mathcal{T}_{i}\sim p(\mathcal{T})}\mathcal{L}_{\mathcal{T}_{i}}(f_{\theta-\alpha\nabla_{\theta}\mathcal{L}_{\mathcal{T}_{i}}(f_{\theta})})Note that the meta-optimization is performed over the model parameters \theta, whereas the objective is computed using the updated model parameters \theta^{\prime}.In effect, our proposed method aims to optimize the model parameters such that one or a small number of gradient steps on a new task will produce maximally effective behavior on that task. The process of training a model’s parameters such that a few gradient steps, or even a single gradient step, can produce good results on a new task can be viewed from a feature learning standpoint as building an internal representation that is broadly suitable for many tasks. If the internal representation is suitable to many tasks, simply fine-tuning the parameters slightly (e.g. by primarily modifying the top layer weights in a feedforward model) can produce good results. In effect, our procedure optimizes for models that are easy and fast to fine-tune, allowing the adaptation to happen in the right space for fast learning. From a dynamical systems standpoint, our learning process can be viewed as maximizing the sensitivity of the loss functions of new tasks with respect to the parameters: when the sensitivity is high, small local changes to the parameters can lead to large improvements in the task loss. Our approach is also related to methods for initialization of deep networks. In computer vision, models pretrained on large-scale image classification have been shown to learn effective features for a range of problems (Donahue et al., 2014). In contrast, our method explicitly optimizes the model for fast adaptability, allowing it to adapt to new tasks with only a few examples.Our method can also be viewed as explicitly maximizing sensitivity of new task losses to the model parameters.A number of prior works have explored sensitivity in deep networks, often in the context of initialization (Saxe et al., 2014; Kirkpatrick et al., 2016). Most of these works have considered good random initializations, though a number of papers have addressed data-dependent initializers (Krähenbühl et al., 2016; Salimans & Kingma, 2016), including learned initializations (Husken & Goerick, 2000; Maclaurin et al., 2015). In contrast, our method explicitly trains the parameters for sensitivity on a given task distribution, allowing for extremely efficient adaptation for problems such as K-shot learning and rapid reinforcement learning in only one or a few gradient steps. In contrast to prior work, which has sought to train recurrent neural networks that ingest entire datasets (Santoro et al., 2016; Duan et al., 2016b) or feature embeddings that can be combined with nonparametric methods at test time (Vinyals et al., 2016; Koch, 2015), we propose a method that can learn the parameters of any standard model via meta-learning in such a way as to prepare that model for fast adaptation. The intuition behind this approach is that some internal representations are more transferrable than others. For example, a neural network might learn internal features that are broadly applicable to all tasks in p(\mathcal{T}), rather than a single individual task. How can we encourage the emergence of such general-purpose representations? We take an explicit approach to this problem: since the model will be fine-tuned using a gradient-based learning rule on a new task, we will aim to learn a model in such a way that this gradient-based learning rule can make rapid progress on new tasks drawn from p(\mathcal{T}), without overfitting. In effect, we will aim to find model parameters that are sensitive to changes in the task, such that small changes in the parameters will produce large improvements on the loss function of any task drawn from p(\mathcal{T}), when altered in the direction of the gradient of that loss (see Figure 1). We make no assumption on the form of the model, other than to assume that it is parametrized by some parameter vector \theta, and that the loss function is smooth enough in \theta that we can use gradient-based learning techniques."
Why meta learning is better than transfer learning?,"Meta learning is better than transfer learning because it allows for more flexible and efficient adaptation to new tasks, without requiring a large amount of training data or a separate model for each task","While transfer learning requires learned parameters, meta-learning does not require learned parameters.","We introduced a meta-learning method based on learning easily adaptable model parameters through gradient descent. Our approach has a number of benefits. It is simple and does not introduce any learned parameters for meta-learning. It can be combined with any model representation that is amenable to gradient-based training, and any differentiable objective, including classification, regression, and reinforcement learning. Lastly, since our method merely produces a weight initialization, adaptation can be performed with any amount of data and any number of gradient steps, though we demonstrate state-of-the-art results on classification with only one or five examples per class. We also show that our method can adapt an RL agent using policy gradients and a very modest amount of experience."
How could the algorithm infer amplitude and phase in the case of showing only half of the input range?,The algorithm infers amplitude and phase in the case of showing only half of the input range by learning to model the periodic nature of the sine wave,"As written in the paper, the authors claim that the meta-learned model f can learn the periodicity of a sine wave without looking at the entire dataset.","We evaluate performance by fine-tuning the model learned by MAML and the pretrained model on K=\{5,10,20\} datapoints. During fine-tuning, each gradient step is computed using the same K datapoints. The qualitative results, shown in Figure 2 and further expanded on in Appendix B show that the learned model is able to quickly adapt with only 5 datapoints, shown as purple triangles, whereas the model that is pretrained using standard supervised learning on all tasks is unable to adequately adapt with so few datapoints without catastrophic overfitting. Crucially, when the K datapoints are all in one half of the input range, the model trained with MAML can still infer the amplitude and phase in the other half of the range, demonstrating that the MAML trained model f has learned to model the periodic nature of the sine wave. Furthermore, we observe both in the qualitative and quantitative results (Figure 3 and Appendix B) that the model learned with MAML continues to improve with additional gradient steps, despite being trained for maximal performance after one gradient step. This improvement suggests that MAML optimizes the parameters such that they lie in a region that is amenable to fast adaptation and is sensitive to loss functions from p(\mathcal{T}), as discussed in Section 2.2, rather than overfitting to parameters \theta that only improve after one step."
Is it true that the first-order approximation led to roughly 33% speed-up in network computation?,Yes,"According to the paper, eliminating the Hessian calculation increases the overall calculation speed by 33%.","A significant computational expense in MAML comes from the use of second derivatives when backpropagating the meta-gradient through the gradient operator in the meta-objective (see Equation (1)). On MiniImagenet, we show a comparison to a first-order approximation of MAML, where these second derivatives are omitted. Note that the resulting method still computes the meta-gradient at the post-update parameter values \theta_{i}^{\prime}, which provides for effective meta-learning. Surprisingly however, the performance of this method is nearly the same as that obtained with full second derivatives, suggesting that most of the improvement in MAML comes from the gradients of the objective at the post-update parameter values, rather than the second order updates from differentiating through the gradient update. Past work has observed that ReLU neural networks are locally almost linear (Goodfellow et al., 2015), which suggests that second derivatives may be close to zero in most cases, partially explaining the good performance of the first-order approximation. This approximation removes the need for computing Hessian-vector products in an additional backward pass, which we found led to roughly 33\% speed-up in network computation."
Why are there tradeoffs between sample variety and fidelity?,"Because reducing the truncation threshold increases IS (like precision) but also leads to a sharp drop in FID (like recall), there are tradeoffs between sample variety and fidelity","The tradeoff is as IS does not penalize lack of variety in class-conditional models, reducing the truncation threshold leads to a direct increase in IS (analogous to precision). FID penalizes lack of variety (analogous to recall) but also rewards precision, so we initially see a moderate improvement in FID, but as truncation approaches zero and variety diminishes, the FID sharply drops.","This technique allows fine-grained, post-hoc selection of the trade-off between sample quality and variety for a given G. Notably, we can compute FID and IS for a range of thresholds, obtaining the variety-fidelity curve reminiscent of the precision-recall curve (Figure 17). As IS does not penalize lack of variety in class-conditional models, reducing the truncation threshold leads to a direct increase in IS (analogous to precision). FID penalizes lack of variety (analogous to recall) but also rewards precision, so we initially see a moderate improvement in FID, but as truncation approaches zero and variety diminishes, the FID sharply drops. The distribution shift caused by sampling with different latents than those seen in training is problematic for many models. Some of our larger models are not amenable to truncation, producing saturation artifacts (Figure 2(b)) when fed truncated noise. To counteract this, we seek to enforce amenability to truncation by conditioning G to be smooth, so that the full space of z will map to good output samples. For this, we turn to Orthogonal Regularization (Brock et al., 2017), which directly enforces the orthogonality condition: We evaluate our models on ImageNet ILSVRC 2012 (Russakovsky et al., 2015) at 128\times128, 256\times256, and 512\times512 resolutions, employing the settings from Table 1, row 8.The samples generated by our models are presented in Figure 4, with additional samples in Appendix A, and online222https://drive.google.com/drive/folders/1lWC6XEPD0LT5KUnPXeve_kWeY-FxH002.We report IS and FID in Table 2. As our models are able to trade sample variety for quality, it is unclear how best to compare against prior art; we accordingly report values at three settings, with complete curves in Appendix D. First, we report the FID/IS values at the truncation setting which attains the best FID. Second, we report the FID at the truncation setting for which our model’s IS is the same as that attained by the real validation data, reasoning that this is a passable measure of maximum sample variety achieved while still achieving a good level of “objectness.” Third, we report FID at the maximum IS achieved by each model, to demonstrate how much variety must be traded off to maximize quality. In all three cases, our models outperform the previous state-of-the-art IS and FID scores achieved by Miyato et al. (2018) and Zhang et al. (2018)."
Can 512x512 be considered high resolution?,No,Information about Wether 512*512 is high resolution or not is not explicitly provided in the paper.,"We find that current GAN techniques are sufficient to enable scaling to large models and distributed, large-batch training. We find that we can dramatically improve the state of the art and train models up to 512\times512 resolution without need for explicit multiscale methods like Karras et al. (2018). Despite these improvements, our models undergo training collapse, necessitating early stopping in practice. In the next two sections we investigate why settings which were stable in previous works become unstable when applied at scale."
Is it true that GAN training is sensitive to every aspect of its setup (from optimization parameters to model architecture)?,Yes,Yes its true that  GAN training is sensitive to every aspect of its setup (from optimization parameters to model architecture.,"The state of generative image modeling has advanced dramatically in recent years, with Generative Adversarial Networks (GANs, Goodfellow et al. (2014)) at the forefront of efforts to generate high-fidelity, diverse images with models learned directly from data. GAN training is dynamic, and sensitive to nearly every aspect of its setup (from optimization parameters to model architecture), but a torrent of research has yielded empirical and theoretical insights enabling stable training in a variety of settings. Despite this progress, the current state of the art in conditional ImageNet modeling (Zhang et al., 2018) achieves an Inception Score (Salimans et al., 2016) of 52.5, compared to 233 for real data."
What is the truncation setting?,The truncation setting refers to the resolution at which the generated images are truncated to achieve the best FID score,"First, the FID/IS values at the truncation setting attain the best FID. Second, the FID at the truncation setting for which our model’s IS is the same as that acquired by the genuine validation data, reasoning that this is a satisfactory measure of maximal sample variety produced while still obtaining a fair degree of “objectness.” Third, FID at the greatest IS attained by each model, indicates how much variation must be traded off to optimize quality.","We evaluate our models on ImageNet ILSVRC 2012 (Russakovsky et al., 2015) at 128\times128, 256\times256, and 512\times512 resolutions, employing the settings from Table 1, row 8.The samples generated by our models are presented in Figure 4, with additional samples in Appendix A, and online222https://drive.google.com/drive/folders/1lWC6XEPD0LT5KUnPXeve_kWeY-FxH002.We report IS and FID in Table 2. As our models are able to trade sample variety for quality, it is unclear how best to compare against prior art; we accordingly report values at three settings, with complete curves in Appendix D. First, we report the FID/IS values at the truncation setting which attains the best FID. Second, we report the FID at the truncation setting for which our model’s IS is the same as that attained by the real validation data, reasoning that this is a passable measure of maximum sample variety achieved while still achieving a good level of “objectness.” Third, we report FID at the maximum IS achieved by each model, to demonstrate how much variety must be traded off to maximize quality. In all three cases, our models outperform the previous state-of-the-art IS and FID scores achieved by Miyato et al. (2018) and Zhang et al. (2018)."
How could we check whether D is overfitting the training set?,"D is overfitting the training set because its validation accuracy is no better than random guessing, despite its high training accuracy","By observing that D’s loss approaches zero during training, but undergoes a sharp upward jump at the collapse.","We also observe that D’s loss approaches zero during training, but undergoes a sharp upward jump at collapse (Appendix F).One possible explanation for this behavior is that D is overfitting to the training set, memorizing training examples rather than learning some meaningful boundary between real and generated images. As a simple test for D’s memorization (related to Gulrajani et al. (2017)), we evaluate uncollapsed discriminators on the ImageNet training and validation sets, and measure what percentage of samples are classified as real or generated. While the training accuracy is consistently above 98%, the validation accuracy falls in the range of 50-55%, no better than random guessing (regardless of regularization strategy). This confirms that Dis indeed memorizing the training set;we deem this in line with D’s role, which is not explicitly to generalize, but to distill the training data and provide a useful learning signal for G. Additional experiments and discussion are provided in Appendix G."
What if we train this model with small dataset?,"Sure! Here's the answer to your question based on the provided context:

The model may suffer from overfitting to the small dataset, leading to poor generalization performance on unseen data",The model generate less of that class for which data is low.,"We note that some failure modes of our partially-trained models are distinct from those previously observed. Most previous failures involve local artifacts (Odena et al., 2016), images consisting of texture blobs instead of objects (Salimans et al., 2016), or the canonical mode collapse. We observe class leakage, where images from one class contain properties of another, as exemplified by Figure 4(d). We also find that many classes on ImageNet are more difficult than others for our model; our model is more successful at generating dogs (which make up a large portion of the dataset, and are mostly distinguished by their texture) than crowds (which comprise a small portion of the dataset and have more large-scale structure). Further discussion is available in Appendix A."
What's the effect of expanding channel size?,"Significant improvements in performance are seen when expanding the channel size from 64 to 128, particularly on the larger and more diverse JFT-300M dataset",expanding channel size substantially improves the performance of the model.,"To confirm that our design choices are effective for even larger and more complex and diverse datasets, we also present results of our system on a subset of JFT-300M (Sun et al., 2017).The full JFT-300M dataset contains 300M real-world images labeled with 18K categories.Since the category distribution is heavily long-tailed, we subsample the dataset to keep only images with the 8.5K most common labels.The resulting dataset contains 292M images – two orders of magnitude larger than ImageNet. For images with multiple labels, we sample a single label randomly and independently whenever an image is sampled.To compute IS and FID for the GANs trained on this dataset, we use an Inception v2 classifier (Szegedy et al., 2016) trained on this dataset.Quantitative results are presented in Table 3.All models are trained with batch size 2048.We compare an ablated version of our model –comparable to SA-GAN (Zhang et al., 2018) but with the larger batch size –against a “full” BigGAN model that makes uses of all of the techniques applied to obtain the best results on ImageNet (shared embedding, skip-z, and orthogonal regularization).Our results show that these techniques substantially improve performance even in the setting of this much larger dataset at the same model capacity (64 base channels).We further show that for a dataset of this scale, we see significant additional improvements from expanding the capacity of our models to 128 base channels, while for ImageNet GANs that additional capacity was not beneficial."
What is the definition of intra-class variability?,Intra-class variability refers to the variation in the labels within a single class or category,intra-class variability here means images with multiple objects at a variety of scales.,"In Figure 19 (Appendix D), we present truncation plots for models trained on this dataset.Unlike for ImageNet, where truncation limits of \sigma\approx 0 tend to produce the highest fidelity scores, IS is typically maximized for our JFT-300M models when the truncation value \sigma ranges from 0.5 to 1.We suspect that this is at least partially due to the intra-class variability of JFT-300M labels, as well as the relative complexity of the image distribution, which includes images with multiple objects at a variety of scales.Interestingly, unlike models trained on ImageNet, where training tends to collapse without heavy regularization (Section 4), the models trained on JFT-300M remain stable over many hundreds of thousands of iterations.This suggests that moving beyond ImageNet to larger datasets may partially alleviate GAN stability issues."
How the architecture is chosen,"The architecture is chosen based on high-level knowledge of the classification task and the type, number, and size of layers used in the substitute DNN","The adversary (attacking part) must at least have some partial knowledge of the input (e.g., images, text) and expected output (e.g., classification) in order to select the architecture of the attacking system. The adversary selects an appropriate architecture adapted to the input-output relation. For instance, if the task is image classification or machine visioon, a convolutional neural network is the best choice. The parameters of the system (Deep Neural Network), like training epochs, number of layers , nodes etc., have relatively little impact on the success of the attack, so they do not determine the architecture.","Substitute Architecture: This factor is not the mostlimiting as the adversary must at least have some partial knowledge of theoracle input (e.g., images, text) andexpected output (e.g., classification). The adversary can thus usean architecture adapted to the input-output relation. For instance, aconvolutional neural network is suitable for image classification. Furthermore,we show in Section 6 that the type, number, and size of layers usedin the substitute DNN have relatively littleimpact on the success of the attack. Adversaries can also consider performingan architecture exploration and train several substitute models beforeselecting the one yielding the highest attack success. Substitute DNN Training Algorithm: We now describe the five-step training procedure outlined in Algorithm 1:•Initial Collection (1): The adversary collects a very small set S_{0} of inputs representative of the input domain. For instance, if the targeted oracle O classifies handwritten digits, the adversary collects 10 images of each digit 0 through 9. We show in Section 5 that this set does not necessarily have to come from the distribution from which the targeted oracle was trained.•Architecture Selection (2): The adversary selects an architecture to be trained as the substitute F. Again, this can be done using high-level knowledge of the classification task performed by the oracle (e.g., convolutional networks are appropriate for vision)•Substitute Training: The adversary iteratively trains moreaccurate substitute DNNs F_{\rho} by repeating the following for ρ∈0..ρm⁢a⁢x\rho\in 0..\rho_{max}italic_ρ ∈ 0 . . italic_ρ start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT:–Labeling (3): By querying for the labels \tilde{O}(\vec{x}) output by oracle O, the adversary labels each sample \vec{x}\in S_{\rho} in its initial substitute training set S_{\rho}.–Training (4): The adversary trains the architecture chosen at step (2) using substitute training set S_{\rho} in conjunction with classical training techniques.–Augmentation (5): The adversary applies our augmentation technique on the initial substitute training set S_{\rho} to produce a larger substitute training set S_{\rho+1} with more synthetic training points. This new training set better represents the model’s decision boundaries. The adversary repeats steps (3) and (4) with the augmented set S_{\rho+1}.Step (3) is repeated several times to increase the substitute DNN’s accuracy and the similarity of its decision boundaries with the oracle. We introduce the term substitute training epoch, indexed with \rho, to refer to each iteration performed. This leads to this formalization of the Jacobian-based Dataset Augmentation performed at step (5) of our substitute training algorithm to find more synthetic training points:S_{\rho+1}=\{\vec{x}+\lambda\cdot\operatorname{sgn}(J_{F}[\tilde{O}(\vec{x})]):\vec{x}\in S_{\rho}\}\cup S_{\rho}(4)where \lambda is a parameter of the augmentation: it defines the size of the step taken in the sensitive direction identified by the Jacobian matrix to augment the set S_{\rho} into S_{\rho+1}. Choosing an Architecture: We train substitute DNNs A and F to M (cf.Table 13) using 150 samples from the MNIST test set as the substitute training set. During eachof the 6 substitute training epochs, the DNN is trained for 5 epochsfrom scratch. Between epochs, synthetic data is added to the training set using Jacobian-baseddataset augmentations with step \lambda=0.1. The substitutearchitectures differ from the oracle’s by the type, number, and sizeof layers. In Table 1,we report the accuracy of each architecture after 2 and 6 substitute training epochs, as well as the adversarialsample transferability after 6 epochs. Adversarial samples are crafted using the Goodfellow algorithm with an inputvariation of \varepsilon=0.4 (which we justify later). The last column ofTable 1shows that the choice of architecture has a limitedimpact on adversarial sample transferability, and therefore on the attacksuccess. The most important transferability drop follows from removing allconvolutional layers. Changing the hidden layer activation function fromrectified linear to a sigmoid does not impact transferability significantly. Goodfellow’s algorithm: Recall from Equation 5 the perturbation computed in the Goodfellow attack.Its only parameter is the variation \varepsilon added inthe direction of the gradient sign. We use the same architecture set asbefore to quantify the impact of \varepsilon onadversarial sample transferability.In Figure 8, architecture A outperformsall others: it is a copy of the oracle’s and acts as a baseline. Otherarchitectures have asymptotic transferability rates ranging between 72.24\%and 80.21\%, confirming that the substitute architecture choice hasa limited impact on transferability. Increasing the value of \varepsilon above0.4 yields little improvement in transferability and should be avoidedto guarantee indistinguishability of adversarial samples to humans."
The accuracy is achieved by which ML Model used for training the substitute,"The accuracy is achieved by the Logistic Regression (LR) model used for training the substitute, with an accuracy of 81.20\% and 67.00\% on the MNIST test set after 6 substitute training epochs","The ML model used for achieving the accuracy is a DNN (Deep Neural Network) combined with LR (Logistic Regression), and the two refinements as introduced in Section 6: a periodic step size and reservoir sampling.","Substitute DNN Training: The adversary uses the initial substitute training sets and the oracle to trainsubsitute DNNs. Our substitute architecture A, a standard forimage classification, is describedin Table 13 (cf. appendix).The substitute DNN is trained on ourmachine for 6 substitute epochs.Duringeach of these 6 epochs, the model is trained for 10 epochsfrom scratch with a learning rate of 10^{-2} and momentum of 0.9. Betweensubstitute epochs, we perform a Jacobian-based dataset augmentation with a stepsize of \lambda=0.1 to generate additional synthetic training data, which we label using the MetaMind oracle. The accuracy of the two substitute DNNs is reported inFigure 4. It is computed with the MNISTtest set (minus the 150 samples used in the first initial substitute trainingset). The adversary does not have access to this full test set: wesolely use it to analyze our results. The two substituteDNNs respectively achieve a 81.20\% and 67.00\% accuracy on the MNIST test set after 6 substitute training epochs. These accuracies fallshort of current state-of-the-art accuracies on this task. However, the adversary has access to a limited number ofsamples (in this case 6,400=100\times 2^{6} instead of 50,000 forstate-of-the-art models). Furthermore, the adversarial goal is to craftadversarial samples misclassified by the oracle. Instead of learning asubstitute DNN with optimal accuracy, the adversary is interested inlearning a substitute capable of mimicking the oracle decisionboundaries. Whereas we previously trained all of our substitutes using DNNs only, we now useboth DNNs and LR as substitute models. The Jacobian-based dataset augmentationdescribed in the context of DNNs is easily adapted to logistic regression: thelater is analog to the softmax layer frequently used by the former whenoutputting probability vectors. We use 100 samples from the MNIST test set asthe initial substitute training set and use the two refinements introduced inSection 6: a periodic step size and reservoirsampling. Substitute Training: By augmenting an initial training set of 100 test set samples, wetrain a DNN and LR substitute for each of the two oracles. We measure success as the rate of adversarialsamples misclassified by the corresponding oracle, among the 10,000 produced from the test set using the fast gradient sign method with parameter \varepsilon=0.3. These rates, computed after \rho\in\{3,6\} dataset augmentation iterations, are reported in Table 3. Results reported in the last row use both a periodic step size and reservoir sampling (hence the reduced number of queries made to train the substitute)."
What is the criteria for training multiple substitute DNNs?,To maximize the similarity of the decision boundaries of the substitute DNNs with the oracle,The criteria for training multiple substitute DNNs is achieve good accuray but mainly  the goal is to create a substitute capable of mimicking the oracle decision boundaries.,"Substitute DNN Training Algorithm: We now describe the five-step training procedure outlined in Algorithm 1:•Initial Collection (1): The adversary collects a very small set S_{0} of inputs representative of the input domain. For instance, if the targeted oracle O classifies handwritten digits, the adversary collects 10 images of each digit 0 through 9. We show in Section 5 that this set does not necessarily have to come from the distribution from which the targeted oracle was trained.•Architecture Selection (2): The adversary selects an architecture to be trained as the substitute F. Again, this can be done using high-level knowledge of the classification task performed by the oracle (e.g., convolutional networks are appropriate for vision)•Substitute Training: The adversary iteratively trains moreaccurate substitute DNNs F_{\rho} by repeating the following for ρ∈0..ρm⁢a⁢x\rho\in 0..\rho_{max}italic_ρ ∈ 0 . . italic_ρ start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT:–Labeling (3): By querying for the labels \tilde{O}(\vec{x}) output by oracle O, the adversary labels each sample \vec{x}\in S_{\rho} in its initial substitute training set S_{\rho}.–Training (4): The adversary trains the architecture chosen at step (2) using substitute training set S_{\rho} in conjunction with classical training techniques.–Augmentation (5): The adversary applies our augmentation technique on the initial substitute training set S_{\rho} to produce a larger substitute training set S_{\rho+1} with more synthetic training points. This new training set better represents the model’s decision boundaries. The adversary repeats steps (3) and (4) with the augmented set S_{\rho+1}.Step (3) is repeated several times to increase the substitute DNN’s accuracy and the similarity of its decision boundaries with the oracle. We introduce the term substitute training epoch, indexed with \rho, to refer to each iteration performed. This leads to this formalization of the Jacobian-based Dataset Augmentation performed at step (5) of our substitute training algorithm to find more synthetic training points:S_{\rho+1}=\{\vec{x}+\lambda\cdot\operatorname{sgn}(J_{F}[\tilde{O}(\vec{x})]):\vec{x}\in S_{\rho}\}\cup S_{\rho}(4)where \lambda is a parameter of the augmentation: it defines the size of the step taken in the sensitive direction identified by the Jacobian matrix to augment the set S_{\rho} into S_{\rho+1}. The accuracy of the two substitute DNNs is reported inFigure 4. It is computed with the MNISTtest set (minus the 150 samples used in the first initial substitute trainingset). The adversary does not have access to this full test set: wesolely use it to analyze our results. The two substituteDNNs respectively achieve a 81.20\% and 67.00\% accuracy on the MNIST test set after 6 substitute training epochs. These accuracies fallshort of current state-of-the-art accuracies on this task. However, the adversary has access to a limited number ofsamples (in this case 6,400=100\times 2^{6} instead of 50,000 forstate-of-the-art models). Furthermore, the adversarial goal is to craftadversarial samples misclassified by the oracle. Instead of learning asubstitute DNN with optimal accuracy, the adversary is interested inlearning a substitute capable of mimicking the oracle decisionboundaries. Substitute DNN Training: The adversary uses twoinitial substitute training sets extracted from the GTSRB test set. Thefirst includes the first 1,000 samples and the second thefirst 500. The number of initial samples is higher than forMNIST substitutes as inputs have a higher dimensionality. We trainthree substitute architectures C, D, and E (cf.Table 13) using the oracle for 6 substitutetraining epochs with a Jacobian-based dataset augmentation parameter of\lambda=0.1. Substitute C and E where trained with the 1,000 sampleinitial substitute training set and achieve a 71.42\% accuracy. Substitute Dwas trained with the initial set of 500 samples. Its accuracy of 60.12\% islower than C and E. Choosing an Architecture: We train substitute DNNs A and F to M (cf.Table 13) using 150 samples from the MNIST test set as the substitute training set. During eachof the 6 substitute training epochs, the DNN is trained for 5 epochsfrom scratch. Between epochs, synthetic data is added to the training set using Jacobian-baseddataset augmentations with step \lambda=0.1. The substitutearchitectures differ from the oracle’s by the type, number, and sizeof layers. In Table 1,we report the accuracy of each architecture after 2 and 6 substitute training epochs, as well as the adversarialsample transferability after 6 epochs. Adversarial samples are crafted using the Goodfellow algorithm with an inputvariation of \varepsilon=0.4 (which we justify later). The last column ofTable 1shows that the choice of architecture has a limitedimpact on adversarial sample transferability, and therefore on the attacksuccess. The most important transferability drop follows from removing allconvolutional layers. Changing the hidden layer activation function fromrectified linear to a sigmoid does not impact transferability significantly."
What type of defense strategies are evaded in this paper ,Gradient masking and defensive distillation,"Two types of defense are evaded, Adversarial training and Defensive distillation.","We show a more general flaw in the category of gradient masking.Even if the defender attempts to prevent attacks by not publishingthe directions in which the model is sensitive, these directionscan be discovered by other means, in which case thesame attack can still succeed.We show that the black-box attack based on transfer from a substitute modelovercomes gradient masking defenses. No fully effective defense mechanism is known, but we study the two with thegreatest empirical success so far:adversarial training [4, 14], anddefensive distillation for DNNs [10]. Adversarial training:It was shown that injecting adversarial examples throughout training increasesthe robustness of significantly descriptive models, such as DNNs [4, 14, 17].We implemented an approximation of this defense using the Google Prediction API.Since the API does not support the generation of adversarial examplesat every step of training, as a correct implementation of adversarial training woulddo, we instead inject a large amount of adversarial examples infrequently.After training in this way, the model has a misclassification rate of 8.75\% onthe unperturbed test set,but the adversarial misclassification rate rises to 100\% when \rho=6.To evaluate this defense strategy using a correct implementation, we resortto training the oracle locally, using our own codebase that includes support forgenerating adversarial examples at each step.After each training batch, we compute and train on adversarial examplesgenerated with the fast gradient sign method before starting training on the next batch of theoriginal training data.Results are given in Table 4.We observe that for \varepsilon=0.15, the defense can be evaded using theblack-box attack with adversarial examples crafted on the substitute andmisclassified by the oracle at rates up to 71.25\%.However, for \varepsilon=0.3, the black-box attack is not effective anymore.Therefore, making a machine learning model robust to small and infinitesimalperturbations of its inputs is an example of gradient masking and canbe evaded using our substitute-based black-box approach.However, making the model robust to larger and finite perturbations preventsthe black-box attack.To confirm this hypothesis, we now show that defensive distillation, whichmakes the model robust to infinitesimal perturbations, can be evaded by theblack-box approach. Defensive distillation:Due to space constraints, we refer readers to [10] fora detailed presentation of defensivedistillation, which is an alternative defense.Because the remotely hosted APIs we study here do not implement defensive distillation or provideprimitives that could be used to implement it,we are forced to evaluate this defense on a locally trained oracle.Therefore, we train a distilled model as described in [10] to act as our MNIST oracle."
How black box attacks perform on larger and finite perturbation?,Not effective,Black-box attacks can not be successful against a model that is robust to larger and finite perturbations.,"Adversarial training:It was shown that injecting adversarial examples throughout training increasesthe robustness of significantly descriptive models, such as DNNs [4, 14, 17].We implemented an approximation of this defense using the Google Prediction API.Since the API does not support the generation of adversarial examplesat every step of training, as a correct implementation of adversarial training woulddo, we instead inject a large amount of adversarial examples infrequently.After training in this way, the model has a misclassification rate of 8.75\% onthe unperturbed test set,but the adversarial misclassification rate rises to 100\% when \rho=6.To evaluate this defense strategy using a correct implementation, we resortto training the oracle locally, using our own codebase that includes support forgenerating adversarial examples at each step.After each training batch, we compute and train on adversarial examplesgenerated with the fast gradient sign method before starting training on the next batch of theoriginal training data.Results are given in Table 4.We observe that for \varepsilon=0.15, the defense can be evaded using theblack-box attack with adversarial examples crafted on the substitute andmisclassified by the oracle at rates up to 71.25\%.However, for \varepsilon=0.3, the black-box attack is not effective anymore.Therefore, making a machine learning model robust to small and infinitesimalperturbations of its inputs is an example of gradient masking and canbe evaded using our substitute-based black-box approach.However, making the model robust to larger and finite perturbations preventsthe black-box attack.To confirm this hypothesis, we now show that defensive distillation, whichmakes the model robust to infinitesimal perturbations, can be evaded by theblack-box approach."
Aren't YOLO9000 and YOLOv2 essentially the same thing? Why make the distinction?,"No, YOLO9000 and YOLOv2 are not the same thing. YOLOv2 is an improved version of the base YOLO detection system, while YOLO9000 is a real-time object detector that can detect over 9000 different object categories using a combination of data from ImageNet and COCO. The distinction is made because YOLO9000 is a more advanced and capable system that can detect a much wider range of objects than YOLOv2",YOLOv2 is the improvement over the base YOLO detection system. YOLO9000 further improve YOLOv2 by using a WordTree to combine data from various sources and uses a joint optimization technique to train simultaneously on ImageNet and COCO. This shows that YOLO9000 adds a different classification head as compared to YOLO9000 to support more classes.,"Using this method we train YOLO9000, a real-time object detector that can detect over 9000 different object categories. First we improve upon the base YOLO detection system to produce YOLOv2, a state-of-the-art, real-time detector. Then we use our dataset combination method and joint training algorithm to train a model on more than 9000 classes from ImageNet as well as detection data from COCO. YOLO9000 is a real-time framework for detection more than 9000 object categories by jointly optimizing detection and classification. We use WordTree to combine data from various sources and our joint optimization technique to train simultaneously on ImageNet and COCO. YOLO9000 is a strong step towards closing the dataset size gap between detection and classification."
How does YOLO9000 achieve the feat of predicting detections for classes despite not having labelled data for them?,"YOLO9000 achieves the feat of predicting detections for classes despite not having labelled data for them by leveraging the shared object categories between COCO and ImageNet, and using the objectness predictions to generalize to new classes",YOLO9000 can perform well on classes which it has not seen during training is because of its WordTree based data combination method from various sources. For example it can learn animal categories which it has not seen because objectness properties in case of such objects can be generalized well.,"Using this joint training, YOLO9000 learns to find objects in images using the detection data in COCO and it learns to classify a wide variety of these objects using data from ImageNet. We evaluate YOLO9000 on the ImageNet detection task. The detection task for ImageNet shares on 44 object categories with COCO which means that YOLO9000 has only seen classification data for the majority of the test images, not detection data. YOLO9000 gets 19.7 mAP overall with 16.0 mAP on the disjoint 156 object classes that it has never seen any labelled detection data for. This mAP is higher than results achieved by DPM but YOLO9000 is trained on different datasets with only partial supervision [4]. It also is simultaneously detecting 9000 other object categories, all in real-time. When we analyze YOLO9000’s performance on ImageNet we see it learns new species of animals well but struggles with learning categories like clothing and equipment. New animals are easier to learn because the objectness predictions generalize well from the animals in COCO. Conversely, COCO does not have bounding box label for any type of clothing, only for person, so YOLO9000 struggles to model categories like “sunglasses” or “swimming trunks”. YOLO9000 is a real-time framework for detection more than 9000 object categories by jointly optimizing detection and classification. We use WordTree to combine data from various sources and our joint optimization technique to train simultaneously on ImageNet and COCO. YOLO9000 is a strong step towards closing the dataset size gap between detection and classification."
What differences exist in the labeling procedure for classification datasets/detection datasets for there to be such a large difference in scale?,"The main difference in labeling procedures for classification datasets and detection datasets is the level of granularity and specificity in the labels. Classification datasets have a much wider and deeper range of labels, with multiple breeds of dogs, while detection datasets typically have only common objects and general labels",The reason between different scale of availability between classification and detection datasets is due to the  fact that labelling images for detection is far more expensive than labelling for classification or tagging. For example common object detection datasets contain only 10 to 100 thousands images with dozen to hundred tags whereas image classification datasets  have million of images with thousands of classes. Object detection methods like YOLO can utilize the large amount of classification data to help the detection task.,"Current object detection datasets are limited compared to datasets for other tasks like classification and tagging. The most common detection datasets contain thousands to hundreds of thousands of images with dozens to hundreds of tags [3] [10] [2]. Classification datasets have millions of images with tens or hundreds of thousands of categories [20] [2]. We would like detection to scale to level of object classification. However, labelling images for detection is far more expensive than labelling for classification or tagging (tags are often user-supplied for free). Thus we are unlikely to see detection datasets on the same scale as classification datasets in the near future. We propose a new method to harness the large amount of classification data we already have and use it to expand the scope of current detection systems. Our method uses a hierarchical view of object classification that allows us to combine distinct datasets together. This approach presents a few challenges. Detection datasets have only common objects and general labels, like “dog” or “boat”. Classification datasets have a much wider and deeper range of labels. ImageNet has more than a hundred breeds of dog, including “Norfolk terrier”, “Yorkshire terrier”, and “Bedlington terrier”. If we want to train on both datasets we need a coherent way to merge these labels."
What are localization errors?,"Localization errors refer to the inaccuracies in the bounding box coordinates of detected objects, where the object is placed in the wrong location or the size of the bounding box is incorrect",FAST R-CNN and other state of the art methods predict the bounding boxes more accurately which means they don’t suffer from localization errors whereas YOLO model has localization problems which are addressed in this paper. Here localization errors represent the accuracy of bounding boxes.,"YOLO suffers from a variety of shortcomings relative to state-of-the-art detection systems. Error analysis of YOLO compared to Fast R-CNN shows that YOLO makes a significant number of localization errors. Furthermore, YOLO has relatively low recall compared to region proposal-based methods. Thus we focus mainly on improving recall and localization while maintaining classification accuracy."
Why does YOLO suffer from the shortcomings mentioned by the authors?,"YOLO suffers from shortcomings due to localization errors and low recall compared to region proposal-based methods, as mentioned by the authors",The problems from which YOLO model suffer are the localization errors and low recall rate. The aim of this paper is to address these problems. One reason for YOLO shortcommings is the use of low accuracy GoogleNet.,"The YOLO framework uses a custom network based on the Googlenet architecture [19]. This network is faster than VGG-16, only using 8.52 billion operations for a forward pass. However, it’s accuracy is slightly worse than VGG-16. For single-crop, top-5 accuracy at 224\times 224, YOLO’s custom model gets 88.0% ImageNet compared to 90.0% for VGG-16. YOLO suffers from a variety of shortcomings relative to state-of-the-art detection systems. Error analysis of YOLO compared to Fast R-CNN shows that YOLO makes a significant number of localization errors. Furthermore, YOLO has relatively low recall compared to region proposal-based methods. Thus we focus mainly on improving recall and localization while maintaining classification accuracy."
Was the whole ImageNet dataset used for the 10 epochs of resolution fine tuning?,No,"After initial training on images at 224\times 224 Proposed network is fine-tuned at a larger size, 448. During initial training Proposed network is first trained on the standard ImageNet 1000 class classification dataset for 160 epochs using stochastic gradient descent with a starting learning rate of 0.1, polynomial rate decay with a power of 4, weight decay of 0.0005 and momentum of 0.9 using the Darknet neural network framework. Similarly for YOLOv2 is also fine tuned on standard ImageNet 1000 class dataset. This shows that the whole ImageNet is used for the fine tuning on larger size.","For YOLOv2 we first fine tune the classification network at the full 448\times 448 resolution for 10 epochs on ImageNet. This gives the network time to adjust its filters to work better on higher resolution input. We then fine tune the resulting network on detection. This high resolution classification network gives us an increase of almost 4% mAP. Training for classification. We train the network on the standard ImageNet 1000 class classification dataset for 160 epochs using stochastic gradient descent with a starting learning rate of 0.1, polynomial rate decay with a power of 4, weight decay of 0.0005 and momentum of 0.9 using the Darknet neural network framework [13]. During training we use standard data augmentation tricks including random crops, rotations, and hue, saturation, and exposure shifts. As discussed above, after our initial training on images at 224\times 224 we fine tune our network at a larger size, 448. For this fine tuning we train with the above parameters but for only 10 epochs and starting at a learning rate of 10^{-3}. At this higher resolution our network achieves a top-1 accuracy of 76.5\% and a top-5 accuracy of 93.3\%."
"Is it true then that YOLOv2's classification network is first trained with 416 x 416 images, then finetuned with 448 x 448 images?","No. The classification network of YOLOv2 is first fine-tuned with 448x448 images for 10 epochs, then fine-tuned with detection. The input resolution of the original YOLO was 448x448, but with the addition of anchor boxes, it was changed to 416x416. However, the model is designed to be robust to different image sizes, so it can be resized on the fly",Yes YOLOv2 uses reduced size resolution of 416\times416. And during fine tuning on the ImageNet it uses  448\times 448 resolution for 10 epochs.,"For YOLOv2 we first fine tune the classification network at the full 448\times 448 resolution for 10 epochs on ImageNet. This gives the network time to adjust its filters to work better on higher resolution input. We then fine tune the resulting network on detection. This high resolution classification network gives us an increase of almost 4% mAP. Multi-Scale Training. The original YOLO uses an input resolution of 448\times 448. With the addition of anchor boxes we changed the resolution to 416\times 416. However, since our model only uses convolutional and pooling layers it can be resized on the fly. We want YOLOv2 to be robust to running on images of different sizes so we train this into the model."
How does the graph of the average IOU vs. number of clusters imply the claim that k = 5 is the optimal choice for the complexity/recall tradeoff?,"The graph of the average IOU vs. number of clusters suggests that k = 5 is the optimal choice for the complexity/recall tradeoff because it has a similar performance to 9 anchor boxes (61.0) with a much simpler model (5 centroids) compared to using 9 centroids, which results in a higher average IOU (64.3)",A graph is shown between average IOU vs. number of clusters. Number of anchar boxes are then hand-picked by comparing the average IOU closest to the prior. K=5 is choosen because At only 5 priors the centroids perform similarly to 9 anchor boxes with an average IOU of 61.0 compared to 60.9.,"Instead of choosing priors by hand, we run k-means clustering on the training set bounding boxes to automatically find good priors. If we use standard k-means with Euclidean distance larger boxes generate more error than smaller boxes. However, what we really want are priors that lead to good IOU scores, which is independent of the size of the box. Thus for our distance metric we use: We run k-means for various values of k and plot the average IOU with closest centroid, see Figure 2. We choose k=5 as a good tradeoff between model complexity and high recall. The cluster centroids are significantly different than hand-picked anchor boxes. There are fewer short, wide boxes and more tall, thin boxes. We compare the average IOU to closest prior of our clustering strategy and the hand-picked anchor boxes in Table 1. At only 5 priors the centroids perform similarly to 9 anchor boxes with an average IOU of 61.0 compared to 60.9. If we use 9 centroids we see a much higher average IOU. This indicates that using k-means to generate our bounding box starts the model off with a better representation and makes the task easier to learn."
Is there any specific reason that Bw and Bh uses an exponential function for the location prediction?,"Yes, there is a specific reason why Bw and Bh use an exponential function for location prediction.

In YOLO, the location predictions are made relative to the location of the grid cell, which means that the predictions are bound between 0 and 1. Using an exponential function, such as the softmax function, helps to concentrate the predictions around the center of the grid cell, making it more likely to detect objects near the center of the cell. This is particularly useful when objects are densely packed and have similar sizes, as it allows the network to distinguish between objects that are close to each other",Exponential function for the location prediction is used to bound the network’s predictions to fall in ground bounding boxes range of 0 to 1.,Instead of predicting offsets we follow the approach of YOLO and predict location coordinates relative to the location of the grid cell. This bounds the ground truth to fall between 0 and 1. We use a logistic activation to constrain the network’s predictions to fall in this range.
What is the batch size for multi-scale training?,"Sure! Here's my answer:

The batch size for multi-scale training is 320",Image size is changed after every 10 batches during multi-scale training. But the batch size is not mentioned in the paper.,"Instead of fixing the input image size we change the network every few iterations. Every 10 batches our network randomly chooses a new image dimension size. Since our model downsamples by a factor of 32, we pull from the following multiples of 32: \{320,352,...,608\}. Thus the smallest option is 320\times 320 and the largest is 608\times 608. We resize the network to that dimension and continue training."
Are there any differences between VGG-16 and YOLO's custom framework besides size?,"Sure! Here's the answer to your question based on the provided context:

Yes, there are differences between VGG-16 and YOLO's custom framework besides size. While YOLO's custom framework is faster and has fewer operations, it has slightly worse accuracy compared to VGG-16",The YOLO framework uses a custom network based on the Googlenet architecture. This architecture is faster than VGG-16 but its accuracy is slightly lower.,"The YOLO framework uses a custom network based on the Googlenet architecture [19]. This network is faster than VGG-16, only using 8.52 billion operations for a forward pass. However, it’s accuracy is slightly worse than VGG-16. For single-crop, top-5 accuracy at 224\times 224, YOLO’s custom model gets 88.0% ImageNet compared to 90.0% for VGG-16."
Wasn't initial training done with 416 x 416 images?,No,Yes the initial training was done with 416 x 416 images.,"Multi-Scale Training. The original YOLO uses an input resolution of 448\times 448. With the addition of anchor boxes we changed the resolution to 416\times 416. However, since our model only uses convolutional and pooling layers it can be resized on the fly. We want YOLOv2 to be robust to running on images of different sizes so we train this into the model."
"What does ""synset"" mean? ","A synset is a set of words or phrases with similar meanings, as defined in WordNet",SynSets are part of WordNet structured directed graph that represent similar concepts such as canine and domestic animals both can represent a dog. In the WordNet grph mny synsets have one path through the graph.,"WordNet is structured as a directed graph, not a tree, because language is complex. For example a “dog” is both a type of “canine” and a type of “domestic animal” which are both synsets in WordNet. Instead of using the full graph structure, we simplify the problem by building a hierarchical tree from the concepts in ImageNet. To build this tree we examine the visual nouns in ImageNet and look at their paths through the WordNet graph to the root node, in this case “physical object”. Many synsets only have one path through the graph so first we add all of those paths to our tree. Then we iteratively examine the concepts we have left and add the paths that grow the tree by as little as possible. So if a concept has two paths to the root and one path would add three edges to our tree and the other would only add one edge, we choose the shorter path."
Are the softmax values of different sets of co-hyponyms compared?,"No, the softmax values of different sets of co-hyponyms are not compared",Classification approaches use a softmax layer across all categories to predict the final probability of all classes. This technique would fail for models which combine datasets having similar classes. To overcome the proposed model also use a softmax over all sysnsets that are co-hyponyms.  Hence the final probability is computed by conditional probabilities at every node for the probability of each hyponym of that synset given that synset.,"Most approaches to classification use a softmax layer across all the possible categories to compute the final probability distribution. Using a softmax assumes the classes are mutually exclusive. This presents problems for combining datasets, for example you would not want to combine ImageNet and COCO using this model because the classes “Norfolk terrier” and “dog” are not mutually exclusive. The final result is WordTree, a hierarchical model of visual concepts. To perform classification with WordTree we predict conditional probabilities at every node for the probability of each hyponym of that synset given that synset. For example, at the “terrier” node we predict: To validate this approach we train the Darknet-19 model on WordTree built using the 1000 class ImageNet. To build WordTree1k we add in all of the intermediate nodes which expands the label space from 1000 to 1369. During training we propagate ground truth labels up the tree so that if an image is labelled as a “Norfolk terrier” it also gets labelled as a “dog” and a “mammal”, etc. To compute the conditional probabilities our model predicts a vector of 1369 values and we compute the softmax over all sysnsets that are hyponyms of the same concept, see Figure 5."
What is the mentioned threshold?,0.5,"Threshold is used to traverse the tree down, taking the highest confidence path at every split. Exact value is not mentioned in the paper.","This formulation also works for detection. Now, instead of assuming every image has an object, we use YOLOv2’s objectness predictor to give us the value of Pr(\text{physical object}). The detector predicts a bounding box and the tree of probabilities. We traverse the tree down, taking the highest confidence path at every split until we reach some threshold and we predict that object class."
What is DPM trained with?,DPM is trained with fully supervised learning,DPM is trained on the ImageNet detection task. DPM mAP is still less than YOLO9000 which is only evaluated on ImageNer detection task.,"We evaluate YOLO9000 on the ImageNet detection task. The detection task for ImageNet shares on 44 object categories with COCO which means that YOLO9000 has only seen classification data for the majority of the test images, not detection data. YOLO9000 gets 19.7 mAP overall with 16.0 mAP on the disjoint 156 object classes that it has never seen any labelled detection data for. This mAP is higher than results achieved by DPM but YOLO9000 is trained on different datasets with only partial supervision [4]. It also is simultaneously detecting 9000 other object categories, all in real-time."
Why is the assumption -- that the predicted box overlaps with the ground truth label by 0.3 IOU -- necessary?,"Sure! Here's the answer to the question based on the provided context:

The assumption that the predicted box overlaps with the ground truth label by 0.3 IOU is necessary because it ensures that the backpropagated loss is only computed for correctly predicted bounding boxes, and not for boxes that are completely incorrect or do not overlap with the ground truth at all",To backpropagate classification loss  highest probability bounding box class is used. Similarly to backpropagate objectness loss predicted box overlaps with the ground truth label by 0.3 IOU is checked.,When it sees a classification image we only backpropagate classification loss. To do this we simply find the bounding box that predicts the highest probability for that class and we compute the loss on just its predicted tree. We also assume that the predicted box overlaps what would be the ground truth label by at least .3 IOU and we backpropagate objectness loss based on this assumption.
"How would ""assigning weak labels to classification data"" improve detection results?","Sure! Here's my answer:

By providing more accurate and diverse training data, assigning weak labels to classification data can help the model learn to recognize and detect objects more effectively",Assigning weak labels to classification data can improve detection task because it also improved segmentation task.,For future work we hope to use similar techniques for weakly supervised image segmentation. We also plan to improve our detection results using more powerful matching strategies for assigning weak labels to classification data during training. Computer vision is blessed with an enormous amount of labelled data. We will continue looking for ways to bring different sources and structures of data together to make stronger models of the visual world.
What does it mean for view synthesis to be the supervisory signal?,"View synthesis serves as a supervisory signal for training the depth and pose estimation CNNs, providing a way to evaluate the accuracy of the estimated depth and pose","From view synthesis as the supervisory signal it means that proposed depth and pose prediction CNN require multiple new images of the scene from different poses given an input view. This idea is explored by recent methodsm  however all previous work requires posed image sets during training, while proposed framework can be applied to standard videos without pose information","The key supervision signal for our depth and pose prediction CNNs comes from the task of novel view synthesis: given one input view of a scene, synthesize a new image of the scene seen from a different camera pose. We can synthesize a target view given a per-pixel depth in that image, plus the pose and visibility in a nearby view. As we will show next, this synthesis process can be implemented in a fully differentiable manner with CNNs as the geometry and pose estimation modules. Visibility can be handled, along with non-rigidity and other non-modeled factors, using an “explanability” mask, which we discuss later (Sec. 3.3). Note that the idea of view synthesis as supervision has also been recently explored for learning single-view depth estimation [14, 16] and multi-view stereo [10]. However, to the best of our knowledge, all previous work requires posed image sets during training (and testing too in the case of DeepStereo), while our framework can be applied to standard videos without pose information. Furthermore, it predicts the poses as part of the learning framework. See Figure 2 for an illustration of our learning pipeline for depth and pose estimation."
What are the metrics used for monocular depth and camera motion estimation?,Absolute Trajectory Error (ATE) is used for monocular depth and camera motion estimation,Depth map are computed and matched across different scales for monocular depth metric. ATE metric is used for camera motion estimation.,"To the best of our knowledge, no previous systems exist that learn single-view depth estimation in an unsupervised manner from monocular videos. Nonetheless, here we provide comparison with prior methods with depth supervision [7] and recent methods that use calibrated stereo images (i.e. with pose supervision) for training [14, 16].Since the depth predicted by our method is defined up to a scale factor, for evaluation we multiply the predicted depth maps by a scalar \hat{s} that matches the median with the ground-truth, i.e. \hat{s}=median(D_{gt})/median(D_{pred}). To evaluate the performance of our pose estimation network, we applied our system to the official KITTI odometry split (containing 11 driving sequences with ground truth odometry obtained through the IMU/GPS readings, which we use for evaluation purpose only), and used sequences 00-08 for training and 09-10 for testing. In this experiment, we fix the length of input image sequences to our system to 5 frames. We compare our ego-motion estimation with two variants of monocular ORB-SLAM [37] (a well-established SLAM system): 1) ORB-SLAM (full), which recovers odometry using all frames of the driving sequence (i.e. allowing loop closure and re-localization), and 2) ORB-SLAM (short), which runs on 5-frame snippets (same as our input setting). Another baseline we compare with is the dataset mean of car motion (using ground-truth odometry) for 5-frame snippets. To resolve scale ambiguity during evaluation, we first optimize the scaling factor for the predictions made by each method to best align with the ground truth, and then measure the Absolute Trajectory Error (ATE) [37] as the metric. ATE is computed on 5-frame snippets and averaged over the full sequence.333For evaluating ORB-SLAM (full) we break down the trajectory of the full sequence into 5-frame snippets with the reference coordinate frame adjusted to the central frame of each snippet. As shown in Table 3 and Fig. 9, our method outperforms both baselines (mean odometry and ORB-SLAM (short)) that share the same input setting as ours, but falls short of ORB-SLAM (full), which leverages whole sequences (1591 for seq. 09 and 1201 for seq. 10) for loop closure and re-localization."
"Is it true that ""a geometric view synthesis system ONLY performs consistently well when its intermediate predictions of the scene geometry and the camera poses correspond to the physical ground-truth""?","NO. While the system performs well when the intermediate predictions are close to the physical ground truth, it can still synthesize views with reasonable quality even when the predictions are imperfect",Yes view synthesis system needs to get good geometry otherwise the model would fail on the scenes with more diverse layout and appearance structure.,"Our approach builds upon the insight that a geometric view synthesis system only performs consistently well when its intermediate predictions of the scene geometry and the camera poses correspond to the physical ground-truth. While imperfect geometry and/or pose estimation can cheat with reasonable synthesized views for certain types of scenes (e.g., textureless), the same model would fail miserably when presented with another set of scenes with more diverse layout and appearance structures. Thus, our goal is to formulate the entire view synthesis pipeline as the inference procedure of a convolutional neural network, so that by training the network on large-scale video data for the ‘meta’-task of view synthesis the network is forced to learn about intermediate tasks of depth and camera pose estimation in order to come up with a consistent explanation of the visual world. Empirical evaluation on the KITTI [15] benchmark demonstrates the effectiveness of our approach on both single-view depth and camera pose estimation. Our code will be made available at  https://github.com/tinghuiz/SfMLearner."
How does an imperfect system create a synthesized view reasonable enough to cheat metrics?,"By exploiting the tolerance of certain metrics to imperfect geometry and pose estimation, the system can create a synthesized view that appears reasonable enough to pass the metrics, but only for certain types of scenes with consistent appearance structures",Imperfect system can create a synthesized view reasonable enough to cheat metrics for only textureless scenes.,"Our approach builds upon the insight that a geometric view synthesis system only performs consistently well when its intermediate predictions of the scene geometry and the camera poses correspond to the physical ground-truth. While imperfect geometry and/or pose estimation can cheat with reasonable synthesized views for certain types of scenes (e.g., textureless), the same model would fail miserably when presented with another set of scenes with more diverse layout and appearance structures. Thus, our goal is to formulate the entire view synthesis pipeline as the inference procedure of a convolutional neural network, so that by training the network on large-scale video data for the ‘meta’-task of view synthesis the network is forced to learn about intermediate tasks of depth and camera pose estimation in order to come up with a consistent explanation of the visual world. Empirical evaluation on the KITTI [15] benchmark demonstrates the effectiveness of our approach on both single-view depth and camera pose estimation. Our code will be made available at  https://github.com/tinghuiz/SfMLearner."
"What are quantized depth planes, probabilistic disparity maps, and view-dependent flow fields?","Quantized depth planes, probabilistic disparity maps, and view-dependent flow fields are all representations of the underlying geometry used in end-to-end learning-based novel view synthesis methods","quantized depth planes, probabilistic disparity maps, and view-dependent flow fields are methods to represent the underlying geometry of the scene. These methods have been used in many recent end to end learning based view synthesis methods.","One important application of geometric scene understanding is the task of novel view synthesis, where the goal is to synthesize the appearance of the scene seen from novel camera viewpoints. A classic paradigm for view synthesis is to first either estimate the underlying 3D geometry explicitly or establish pixel correspondence among input views, and then synthesize the novel views by compositing image patches from the input views (e.g., [4, 55, 43, 6, 9]). Recently, end-to-end learning has been applied to reconstruct novel views by transforming the input based on depth or flow, e.g., DeepStereo [10], Deep3D [51] and Appearance Flows [54]. In these methods, the underlying geometry is represented by quantized depth planes (DeepStereo), probabilistic disparity maps (Deep3D) and view-dependent flow fields (Appearance Flows), respectively. Unlike methods that directly map from input views to the target view (e.g., [45]), warping-based methods are forced to learn intermediate predictions of geometry and/or correspondence. In this work, we aim to distill such geometric reasoning capability from CNNs trained to perform warping-based view synthesis."
"Is the ""classic paradigm for view synthesis"" referring to the same ""methods that directly map from input view to the target views""?","No. The ""classic paradigm for view synthesis"" refers to methods that first estimate the underlying 3D geometry or establish pixel correspondence among input views, and then synthesize the novel views by compositing image patches from the input views. These methods are different from the ""methods that directly map from input views to the target views"" which directly map the input views to the target view without estimating the underlying geometry or establishing pixel correspondence",Classic paradigm methods for view synthesis establish direct correspondence among multiple input views to get novel views. This is different from end to end learning based on depth or flow.,"One important application of geometric scene understanding is the task of novel view synthesis, where the goal is to synthesize the appearance of the scene seen from novel camera viewpoints. A classic paradigm for view synthesis is to first either estimate the underlying 3D geometry explicitly or establish pixel correspondence among input views, and then synthesize the novel views by compositing image patches from the input views (e.g., [4, 55, 43, 6, 9]). Recently, end-to-end learning has been applied to reconstruct novel views by transforming the input based on depth or flow, e.g., DeepStereo [10], Deep3D [51] and Appearance Flows [54]. In these methods, the underlying geometry is represented by quantized depth planes (DeepStereo), probabilistic disparity maps (Deep3D) and view-dependent flow fields (Appearance Flows), respectively. Unlike methods that directly map from input views to the target view (e.g., [45]), warping-based methods are forced to learn intermediate predictions of geometry and/or correspondence. In this work, we aim to distill such geometric reasoning capability from CNNs trained to perform warping-based view synthesis."
What is a calibrated stereo twin?,"A calibrated stereo twin is a second camera viewpoint that is registered with the first viewpoint, allowing for the estimation of depth from a single viewpoint",calibrated stereo twin is the supervision method used by  Garg et al. [14] to learn a single-view depth estimation CNN using projection errors.,"Our work is closely related to a line of recent research on learning single-view 3D inference from registered 2D observations. Garg et al. [14] propose to learn a single-view depth estimation CNN using projection errors to a calibrated stereo twin for supervision. Concurrently, Deep3D [51] predicts a second stereo viewpoint from an input image using stereoscopic film footage as training data. A similar approach was taken by Godard et al. [16], with the addition of a left-right consistency constraint, and a better architecture design that led to impressive performance. Like our approach, these techniques only learn from image observations of the world, unlike methods that require explicit depth for training, e.g., [20, 42, 7, 27, 30]."
How was Godard et al.'s architecture design better than Deep3D's?,"Godard et al.'s architecture design was better than Deep3D's because it included a left-right consistency constraint, which improved performance","Deep3D predicts second stereo viewpoint from an input image using stereoscopic film footage as training data. Godard approach is similar to Deep3D with the addition of a left-right consistency constraint, and a better architecture design that led to impressive performance.","Our work is closely related to a line of recent research on learning single-view 3D inference from registered 2D observations. Garg et al. [14] propose to learn a single-view depth estimation CNN using projection errors to a calibrated stereo twin for supervision. Concurrently, Deep3D [51] predicts a second stereo viewpoint from an input image using stereoscopic film footage as training data. A similar approach was taken by Godard et al. [16], with the addition of a left-right consistency constraint, and a better architecture design that led to impressive performance. Like our approach, these techniques only learn from image observations of the world, unlike methods that require explicit depth for training, e.g., [20, 42, 7, 27, 30]."
What type of scenes were used for training?,Indoor and outdoor scenes from the Cityscapes dataset,"Proposed model is trained on the Cityscapes dataset and then fine tuned on KITTI scenes. The training split used is from [7]. This exclude all the frames from the testing scenes as well as static sequences with mean optical flow magnitude less than 1 pixel for training. This results in a total of 44,540 sequences, out of which we use 40,109 for training and 4,431 for validation.","We train our system on the split provided by [7], and exclude all the frames from the testing scenes as well as static sequences with mean optical flow magnitude less than 1 pixel for training. We fix the length of image sequences to be 3 frames, and treat the central frame as the target view and the \pm 1 frames as the source views. We use images captured by both color cameras, but treated them independently when forming training sequences. This results in a total of 44,540 sequences, out of which we use 40,109 for training and 4,431 for validation. We show sample predictions made by our initial Cityscapes model and the final model (pre-trained on Cityscapes and then fine-tuned on KITTI) in Figure 7. Due to the domain gap between the two datasets, our Cityscapes model sometimes has difficulty in recovering the complete shape of the car/bushes, and mistakes them with distant objects."
Did the authors try other interpolation methods besides the bilinear sampling mechanism?,"No, the authors did not try other interpolation methods besides the bilinear sampling mechanism",No for warping only the differentiable bilinear sampling mechanism is used.,"Let p_{t} denote the homogeneous coordinates of a pixel in the target view, and K denote the camera intrinsics matrix. We can obtain p_{t}’s projected coordinates onto the source view p_{s} by222For notation simplicity, we omit showing the necessary conversion to homogeneous coordinates along the steps of matrix multiplication.p_{s}\sim K\hat{T}_{t\rightarrow s}\hat{D}_{t}(p_{t})K^{-1}p_{t}(2)Notice that the projected coordinates p_{s} are continuous values. To obtain I_{s}(p_{s}) for populating the value of \hat{I}_{s}(p_{t}) (see Figure 3), we then use the differentiable bilinear sampling mechanism proposed in the spatial transformer networks [23] that linearly interpolates the values of the 4-pixel neighbors (top-left, top-right, bottom-left, and bottom-right) of p_{s} to approximate I_{s}(p_{s}), i.e. \hat{I}_{s}(p_{t})=I_{s}(p_{s})=\sum_{i\in\{t,b\},j\in\{l,r\}}w^{ij}I_{s}(p_{s}^{ij}), where w^{ij} is linearly proportional to the spatial proximity between p_{s} and p_{s}^{ij} , and \sum_{i,j}w^{ij}=1. A similar strategy is used in [54] for learning to directly warp between different views, while here the coordinates for pixel warping are obtained through projective geometry that enables the factorization of depth and camera pose."
What is the strategy used in [54] for directly warping between different views?,"Direct warping between different views is achieved using a similar strategy as in [54], which involves learning a differentiable bilinear sampling mechanism to interpolate the values of the 4-pixel neighbors of the target pixel, and then using these interpolated values to approximate the appearance of the scene seen from the novel viewpoint",Appearance Flows [54] is an end-to-end learning method to reconstruct novel views. In this method warping coordinates for pixel warping are obtained through projective geometry that enables the factorization of depth and camera pose.,"Let p_{t} denote the homogeneous coordinates of a pixel in the target view, and K denote the camera intrinsics matrix. We can obtain p_{t}’s projected coordinates onto the source view p_{s} by222For notation simplicity, we omit showing the necessary conversion to homogeneous coordinates along the steps of matrix multiplication.p_{s}\sim K\hat{T}_{t\rightarrow s}\hat{D}_{t}(p_{t})K^{-1}p_{t}(2)Notice that the projected coordinates p_{s} are continuous values. To obtain I_{s}(p_{s}) for populating the value of \hat{I}_{s}(p_{t}) (see Figure 3), we then use the differentiable bilinear sampling mechanism proposed in the spatial transformer networks [23] that linearly interpolates the values of the 4-pixel neighbors (top-left, top-right, bottom-left, and bottom-right) of p_{s} to approximate I_{s}(p_{s}), i.e. \hat{I}_{s}(p_{t})=I_{s}(p_{s})=\sum_{i\in\{t,b\},j\in\{l,r\}}w^{ij}I_{s}(p_{s}^{ij}), where w^{ij} is linearly proportional to the spatial proximity between p_{s} and p_{s}^{ij} , and \sum_{i,j}w^{ij}=1. A similar strategy is used in [54] for learning to directly warp between different views, while here the coordinates for pixel warping are obtained through projective geometry that enables the factorization of depth and camera pose. One important application of geometric scene understanding is the task of novel view synthesis, where the goal is to synthesize the appearance of the scene seen from novel camera viewpoints. A classic paradigm for view synthesis is to first either estimate the underlying 3D geometry explicitly or establish pixel correspondence among input views, and then synthesize the novel views by compositing image patches from the input views (e.g., [4, 55, 43, 6, 9]). Recently, end-to-end learning has been applied to reconstruct novel views by transforming the input based on depth or flow, e.g., DeepStereo [10], Deep3D [51] and Appearance Flows [54]. In these methods, the underlying geometry is represented by quantized depth planes (DeepStereo), probabilistic disparity maps (Deep3D) and view-dependent flow fields (Appearance Flows), respectively. Unlike methods that directly map from input views to the target view (e.g., [45]), warping-based methods are forced to learn intermediate predictions of geometry and/or correspondence. In this work, we aim to distill such geometric reasoning capability from CNNs trained to perform warping-based view synthesis."
What does it mean for a surface to be Lambertian?,"A surface is considered Lambertian if it reflects light in all directions, following the Lambert's cosine law",Lambertian surface can show the meaningful photo-consistency error.,"Note that when applied to monocular videos the above view synthesis formulation implicitly assumes 1) the scene is static without moving objects; 2) there is no occlusion/disocclusion between the target view and the source views; 3) the surface is Lambertian so that the photo-consistency error is meaningful. If any of these assumptions are violated in a training sequence, the gradients could be corrupted and potentially inhibit training. To improve the robustness of our learning pipeline to these factors, we additionally train a explainability prediction network (jointly and simultaneously with the depth and pose networks) that outputs a per-pixel soft mask \hat{E}_{s} for each target-source pair, indicating the network’s belief in where direct view synthesis will be successfully modeled for each target pixel. Based on the predicted \hat{E}_{s}, the view synthesis objective is weighted correspondingly by\mathcal{L}_{vs}=\sum_{<I_{1},\ldots,I_{N}>\in\mathcal{S}}\sum_{p}\hat{E}_{s}(p)|I_{t}(p)-\hat{I}_{s}(p)|~{}.(3)Since we do not have direct supervision for \hat{E}_{s}, training with the above loss would result in a trivial solution of the network always predicting \hat{E}_{s} to be zero, which perfectly minimizes the loss. To resolve this, we add a regularization term \mathcal{L}_{reg}(\hat{E}_{s}) that encourages nonzero predictions by minimizing the cross-entropy loss with constant label 1 at each pixel location. In other words, the network is encouraged to minimize the view synthesis objective, but allowed a certain amount of slack for discounting the factors not considered by the model."
What values were used for lambda.s and lambda.e?,\lambda_{s} = 0.5/l and \lambda_{e} = 0.2,"For all the experiments, paper uses lambda.s =0.5 and lambda.e =0.2.","We implemented the system using the publicly available TensorFlow [1] framework. For all the experiments, we set \lambda_{s}=0.5/l (l is the downscaling factor for the corresponding scale) and \lambda_{e}=0.2. During training, we used batch normalization [21] for all the layers except for the output layers, and the Adam [28] optimizer with \beta_{1}=0.9, \beta_{2}=0.999, learning rate of 0.0002 and mini-batch size of 4. The training typically converges after about 150K iterations. All the experiments are performed with image sequences captured with a monocular camera. We resize the images to 128\times 416 during training, but both the depth and pose networks can be run fully-convolutionally for images of arbitrary size at test time."
How did the authors optimize lambda.s and lambda.e?,The authors optimized $\lambda_s$ and $\lambda_e$ using grid search,"lambda.s and lambda.e  are the weighting for the depth smoothness loss and the explainability regularization, respectively. For all the experiments, paper uses a fixed value for lambda.s =0.5 and lambda.e =0.2.","Our final objective becomes\mathcal{L}_{final}=\sum_{l}\mathcal{L}_{vs}^{l}+\lambda_{s}\mathcal{L}^{l}_{smooth}+\lambda_{e}\sum_{s}\mathcal{L}_{reg}(\hat{E}_{s}^{l})~{},(4)where l indexes over different image scales, s indexes over source images, and \lambda_{s} and \lambda_{e} are the weighting for the depth smoothness loss and the explainability regularization, respectively. We implemented the system using the publicly available TensorFlow [1] framework. For all the experiments, we set \lambda_{s}=0.5/l (l is the downscaling factor for the corresponding scale) and \lambda_{e}=0.2. During training, we used batch normalization [21] for all the layers except for the output layers, and the Adam [28] optimizer with \beta_{1}=0.9, \beta_{2}=0.999, learning rate of 0.0002 and mini-batch size of 4. The training typically converges after about 150K iterations. All the experiments are performed with image sequences captured with a monocular camera. We resize the images to 128\times 416 during training, but both the depth and pose networks can be run fully-convolutionally for images of arbitrary size at test time."
How did the authors optimize alpha and beta for the activation function of the prediction layers?,"Sure! Here's the answer to your question based on the provided context:

The authors optimized alpha and beta for the activation function of the prediction layers using experimentation",fixed values for alpha=10 and beta=0.01 are use to constrain the predicted depth to be always positive within a reasonable range.,"For single-view depth prediction, we adopt the DispNet architecture proposed in [35] that is mainly based on an encoder-decoder design with skip connections and multi-scale side predictions (see Figure 4). All conv layers are followed by ReLU activation except for the prediction layers, where we use 1/(\alpha*sigmoid(x)+\beta) with \alpha=10 and \beta=0.01 to constrain the predicted depth to be always positive within a reasonable range. We also experimented with using multiple views as input to the depth network, but did not find this to improve the results. This is in line with the observations in [47], where optical flow constraints need to be enforced to utilize multiple views effectively."
"Why does ORB-SLAM (short) perform so poorly when the turning magnitude is low, as seen in Figure 9?","ORB-SLAM (short) performs poorly when the turning magnitude is low because it relies on local feature matching, which is less effective when the car is driving straight and there are few distinct features to match",Proposed model has good performance as compared with  ORB-SLAM (short) when the turning magnitude is low. ORB-SLAM (short) perform so poorly because it could not learn ego-motion.,"In this work, we mimic this approach by training a model that observes sequences of images and aims to explain its observations by predicting likely camera motion and the scene structure (as shown in Fig. 1). We take an end-to-end approach in allowing the model to map directly from input pixels to an estimate of ego-motion (parameterized as 6-DoF transformation matrices) and the underlying scene structure (parameterized as per-pixel depth maps under a reference view). We are particularly inspired by prior work that has suggested view synthesis as a metric [44] and recent work that tackles the calibrated, multi-view 3D case in an end-to-end framework [10]. Our method is unsupervised, and can be trained simply using sequences of images with no manual labeling or even camera motion information. For better understanding of our pose estimation results, we show in Figure 9 the ATE curve with varying amount of side-rotation by the car between the beginning and the end of a sequence. Figure 9 suggests that our method is significantly better than ORB-SLAM (short) when the side-rotation is small (i.e. car mostly driving forward), and comparable to ORB-SLAM (full) across the entire spectrum. The large performance gap between ours and ORB-SLAM (short) suggests that our learned ego-motion could potentially be used as an alternative to the local estimation modules in monocular SLAM systems."
How did the authors come to the conclusion that the pose network likely uses image correspondence and the depth estimation network likely recognizes common structural features?,The authors inferred that the pose network uses image correspondence and the depth estimation network recognizes common structural features based on the synthesis process used in the novel view synthesis task,"New scenes can be synthesized given camera poses which will require image correspondence. Whereas  we can also synthesize a target view given a per-pixel depth in that image, plus the pose and visibility in a nearby view including structural features.","The key supervision signal for our depth and pose prediction CNNs comes from the task of novel view synthesis: given one input view of a scene, synthesize a new image of the scene seen from a different camera pose. We can synthesize a target view given a per-pixel depth in that image, plus the pose and visibility in a nearby view. As we will show next, this synthesis process can be implemented in a fully differentiable manner with CNNs as the geometry and pose estimation modules. Visibility can be handled, along with non-rigidity and other non-modeled factors, using an “explanability” mask, which we discuss later (Sec. 3.3). Another interesting area for future work would be to investigate in more detail the representation learned by our system. In particular, the pose network likely uses some form of image correspondence in estimating the camera motion, whereas the depth estimation network likely recognizes common structural features of scenes and objects. It would be interesting to probe these, and investigate the extent to which our network already performs, or could be re-purposed to perform, tasks such as object detection and semantic segmentation."
How do the authors come to the conclusion that the thinness is what is causing regions to be masked improperly? ,"The authors conclude that the thinness of structures is causing regions to be masked improperly because the traditional toolchain relies on accurate image correspondence, which is difficult to establish in areas of low texture and thin structures",The simultaneous estimation of structure and motion is a well studied problem where traditional methods suffer from thin structures. That is why the authors think that the thinness can cause improper masks.,"The simultaneous estimation of structure and motion is a well studied problem with an established toolchain of techniques [12, 50, 38]. Whilst the traditional toolchain is effective and efficient in many cases, its reliance on accurate image correspondence can cause problems in areas of low texture, complex geometry/photometry, thin structures, and occlusions. To address these issues, several of the pipeline stages have been recently tackled using deep learning, e.g., feature matching [18], pose estimation [26], and stereo [10, 27, 53]. These learning-based techniques are attractive in that they are able to leverage external supervision during training, and potentially overcome the above issues when applied to test data."
Why does the depth model suffer from close objects?,"Sure! Here's the answer to the question based on the given context:

The depth model struggles with close objects due to the limited view of the camera, which can cause the model to overestimate the distance of objects near the camera",Proposed model failed sometimes for objects close to the front of the camera. The reason is not discussed in the paper.,"Comparison of single-view depth estimation between Eigen et al. [7] (with ground-truth depth supervision), Garg et al. [14] (with ground-truth pose supervision), and ours  unsupervised). The ground-truth depth map is interpolated from sparse measurements for visualization purpose. The last two rows show typical failure cases of our model, which sometimes struggles in vast open scenes and objects close to the front of the camera."
How are loops detected and validated?,"Sure! Here's the answer to your question based on the provided context:

Loops are detected and validated using a pose-graph optimization",In ORB-SLAM2 a full BA optimization is used for loop detection and validation. Loop detection is part of Loop closing.,"Loop closing is performed in two steps, firstly a loop has to be detected and validated, and secondly the loop is corrected optimizing a pose-graph.In contrast to monocular ORB-SLAM, where scale drift may occur [20], the stereo/depth information makes scale observable and the geometric validation andpose-graph optimization no longer require dealing with scale drift and are based on rigid body transformations instead of similarities. In ORB-SLAM2 we have incorporated a full BA optimization after the pose-graph to achieve the optimal solution. This optimization might be very costly and therefore weperform it in a separate thread, allowing the system to continue creating map and detecting loops. However this brings the challenge of merging the bundle adjustmentoutput with the current state of the map. If a new loop is detected while the optimization is running, we abort the optimizationand proceed to close the loop, which will launch the full BA optimization again. When the full BA finishes,we need to merge the updated subset of keyframes and points optimized by the full BA, with the non-updated keyframes and pointsthat where inserted while the optimization was running. This is done by propagating the correction of updated keyframes (i.e. the transformation from the non-optimized to the optimized pose)to non-updated keyframes through the spanning tree. Non-updated points are transformed according to the correction applied to their reference keyframe."
Why are both map point matches and visual odometry matches required?,"Sure! Here's my answer:

Both map point matches and visual odometry matches are required because they serve different purposes in localization. Map point matches ensure drift-free localization to the existing map, while visual odometry matches make the localization robust to unmapped regions","map point matches and visual odometry matches are required for Localization Mode which can be useful for lightweight long-term localization in well mapped areas, as long as there are not significant changes in the environment. In this mode the local mapping and loop closing threads are deactivated and the camera is continuously localized by the tracking using relocalization","We incorporate a Localization Mode which can be useful for lightweight long-term localization in well mapped areas, as long as there are not significant changes in the environment. In this mode the local mapping and loop closing threads are deactivatedand the camera is continuously localized by the tracking using relocalization if needed. In this mode the tracking leverages visual odometry matches and matches to map points.Visual odometry matches are matches between ORB in the current frame and 3D points created in the previous frame from the stereo/depth information. These matches make the localizationrobust to unmapped regions, but drift can be accumulated. Map point matches ensure drift-free localization to the existing map.This mode is demonstrated in the accompanying video."
"What is ""bundle adjustment""?",Bundle Adjustment (BA) is a computer vision technique used to refine the camera pose and 3D point positions in a visual-inertial SLAM system by optimizing a set of keyframes and points using the Levenberg-Marquardt method,"bundle adjustment is a method to optimize the camera pose in the tracking thread, to optimize a local window of keyframes and points in the local mapping thread and after a loop closure to optimize all keyframes and points. Local BA optimizes a set of covisible keyframes \mathcal{K}_{L} and all points seen in those keyframes \mathcal{P}_{L}. Full BA is the specific case of local BA, where all keyframes and points in the map are optimized, except the origin keyframe that is fixed to eliminate the gauge freedom","Our system performs BA to optimize the camera pose in the tracking thread (motion-only BA), to optimize a local window of keyframes and points in the local mapping thread (local BA),and after a loop closure to optimize all keyframes and points (full BA). We use the Levenberg–Marquardt method implemented in g2o [19]. Local BA optimizes a set of covisible keyframes \mathcal{K}_{L} and all points seen in those keyframes \mathcal{P}_{L}.All other keyframes \mathcal{K}_{F}, not in \mathcal{K}_{L}, observing points in \mathcal{P}_{L}contribute to the cost function but remain fixed in the optimization. Defining \mathcal{X}_{k} as the set of matches between pointsin \mathcal{P}_{L} and keypoints in a keyframe k, the optimization problem is the following:\begin{gathered}\{\mathbf{X}^{i},\mathbf{R}_{l},\mathbf{t}_{l}|i\in\mathcal{P}_{L},l\in\mathcal{K}_{L}\}=\operatorname*{argmin}_{\mathbf{X}^{i},\mathbf{R}_{l},\mathbf{t}_{l}}\sum_{k\in\mathcal{K}_{L}\cup\mathcal{K}_{F}}\sum_{j\in\mathcal{X}_{k}}\rho\left(E_{kj}\right)\\E_{kj}=\left\|\mathbf{x}^{j}_{\mathrm{(\cdot)}}-\pi_{\mathrm{(\cdot)}}\left(\mathbf{R}_{k}\mathbf{X}^{j}+\mathbf{t}_{k}\right)\right\|^{2}_{\Sigma}\end{gathered}(4) Full BA is the specific case of local BA, where all keyframes and points in the map are optimized, except the origin keyframe that is fixed to eliminate the gauge freedom."
Why are pure rotations hard to track for monocular SLAM?,"Pure rotations are hard to track for monocular SLAM because the lack of depth information makes it difficult to determine the scale of the map and estimated trajectory, leading to scale drift and potential failure","Pure rotations hard to track for monocular SLAM because  depth is not observable from just one camera, the scale of the map and estimated trajectory is unknown.","Visual SLAM can be performed by using just a monocular camera, which is the cheapest and smallest sensor setup.However as depth is not observable from just one camera, the scale of the map andestimated trajectory is unknown. In addition the system bootstrapping require multi-view or filtering techniques to produce an initial map as it cannot be triangulated from the veryfirst frame. Last but not least, monocular SLAM suffers from scale drift and may fail if performing pure rotations in exploration. By using a stereo oran RGB-D camera all these issues are solved and allows for the most reliable Visual SLAM solutions."
Is the difference between ORB-SLAM and ORB-SLAM2 that ORB-SLAM only supports monocular cameras?,"NO. ORB-SLAM2 supports monocular, stereo, and RGB-D cameras, while ORB-SLAM only supports monocular cameras",ORB-SLAM2 for stereo and RGB-D cameras is built on monocular feature-based ORB-SLAM. This shows that ORB-SLAM only supports monocular cameras as compared with ORB-SLAM.,"ORB-SLAM2 for stereo and RGB-D cameras is built on our monocular feature-based ORB-SLAM [1], whose main components are summarized here for reader convenience.A general overview of the system is shown in Fig. 2. The system has three main parallel threads: 1) the tracking to localize the camera with every frame by findingfeature matches to the local map and minimizing the reprojection error applying motion-only BA, 2) the local mapping to manage the local map and optimize it, performing local BA,3) the loop closing to detect large loops and correct the accumulated drift by performing a pose-graph optimization. This thread launches a fourth thread to perform full BA afterthe pose-graph optimization, to compute the optimal structure and motion solution. In this paper we build on our monocular ORB-SLAM [1] and propose ORB-SLAM2 with the following contributions:•The first open-source111https://github.com/raulmur/ORB_SLAM2 SLAM system for monocular, stereo and RGB-D cameras, including loop closing, relocalization and map reuse.•Our RGB-D results show that by using Bundle Adjustment (BA) we achieve more accuracy than state-of-the-art methods based on ICP or photometric and depth error minimization.•By using close and far stereo points and monocular observations our stereo results are more accurate than the state-of-the-art direct stereo SLAM.•A lightweight localization mode that can effectively reuse the map with mapping disabled."
What is fusion?,"Fusion refers to the process of combining data from multiple sensors or sources to obtain a more accurate and complete estimate of the environment. In the context of SLAM, fusion is used to combine data from different sensors, such as stereo and RGB-D, to improve the accuracy and robustness of the estimated pose and map",Fusion is used in KinectFusion method in which all depth data from the sensor is fused into a volumetric dense model which is then used to track to camera pose.,"Fig. 1 shows examples of ORB-SLAM2 output from stereo and RGB-D inputs. The stereo case shows the final trajectory and sparse reconstruction of the sequence 00 fromthe KITTI dataset [2]. This is an urban sequence with multiple loop closures that ORB-SLAM2 was able to successfully detect. The RGB-D case shows the keyframe poses estimatedin sequence fr1_room from the TUM RGB-D Dataset [3], and a dense pointcloud, rendered by backprojecting sensor depth maps from the estimated keyframe poses. Note that our SLAMdoes not perform any fusion like KinectFusion [4] or similar, but the gooddefinition indicates the accuracy of the keyframe poses. More examples are shown on the attached video. One of the earliest and most famed RGB-D SLAM systems was the KinectFusion of Newcombe et al. [4]. This method fused all depth data from the sensor into a volumetricdense model that is used to track the camera pose using ICP. This system was limited to small workspaces due to its volumetric representation and the lack of loop closing.Kintinuous by Whelan et al. [12] was able to operate in large environments by using a rolling cyclical buffer and included loop closing using place recognitionand pose graph optimization."
What information from the input images do ORB features extract?,ORB features extract salient keypoint locations in the input images,"ORB features are extracted at salient keypoints in both view of image. For every left ORB image a matching feature can be found at right image. ORB extract such features from images which are robust to rotation and scale and present a good invariance to camera auto-gain and auto-exposure, and illumination changes.","The system uses the same ORB features [17] for tracking, mapping and place recognition tasks. These features are robust to rotation and scale and present a good invarianceto camera auto-gain and auto-exposure, and illumination changes. Moreover they are fast to extract and match allowing for real-time operation and show good precision/recallperformance in bag-of-word place recognition [18]. ORB-SLAM2 as a feature-based method pre-processes the input to extract features at salient keypoint locations, as shown in Fig. 2b. The inputimages are then discarded and all system operationsare based on these features, so that the system is independent of the sensor being stereo or RGB-D.Our system handles monocular and stereo keypoints, which are further classified as close or far. Stereo keypoints are defined by three coordinates \mathbf{x}_{\mathrm{s}}=\left(u_{L},v_{L},u_{R}\right), being (u_{L},v_{L}) the coordinates on the left image and u_{R} the horizontalcoordinate in the right image. For stereo cameras, we extract ORB in both images and for every left ORB we search for a match in the right image. This can be done very efficiently assumingstereo rectified images, so that epipolar lines are horizontal. We then generate the stereo keypoint with the coordinates of the left ORB and the horizontal coordinate of the rightmatch, which is subpixel refined by patch correlation. For RGB-D cameras, we extract ORB features onthe RGB image and, as proposed by Strasdat et al. [8], for each feature with coordinates\left(u_{L},v_{L}\right) we transform its depth value d into a virtual right coordinate:u_{R}=u_{L}-\frac{f_{x}b}{d}(1)where f_{x} is the horizontal focal length and b is the baseline between thestructured light projector and the infrared camera, which we approximate to 8cm for Kinectand Asus Xtion. The uncertainty of the depth sensor is represented by the uncertainty of thevirtual right coordinate. In this way, features from stereo and RGB-D input are handled equallyby the rest of the system."
"What does it mean to be ""subpixel refined by patch correction""?","Subpixel refinement by patch correlation means to refine the horizontal coordinate of a stereo keypoint by comparing small patches of the left and right images around the keypoint, allowing for more precise alignment of the images and improved accuracy of the stereo matching",subpixel is the stereo keypoint which is subpixel obtained by patch correlation generated from the  coordinates of the left ORB and the horizontal coordinate of the right match.,"Stereo keypoints are defined by three coordinates \mathbf{x}_{\mathrm{s}}=\left(u_{L},v_{L},u_{R}\right), being (u_{L},v_{L}) the coordinates on the left image and u_{R} the horizontalcoordinate in the right image. For stereo cameras, we extract ORB in both images and for every left ORB we search for a match in the right image. This can be done very efficiently assumingstereo rectified images, so that epipolar lines are horizontal. We then generate the stereo keypoint with the coordinates of the left ORB and the horizontal coordinate of the rightmatch, which is subpixel refined by patch correlation. For RGB-D cameras, we extract ORB features onthe RGB image and, as proposed by Strasdat et al. [8], for each feature with coordinates\left(u_{L},v_{L}\right) we transform its depth value d into a virtual right coordinate:u_{R}=u_{L}-\frac{f_{x}b}{d}(1)where f_{x} is the horizontal focal length and b is the baseline between thestructured light projector and the infrared camera, which we approximate to 8cm for Kinectand Asus Xtion. The uncertainty of the depth sensor is represented by the uncertainty of thevirtual right coordinate. In this way, features from stereo and RGB-D input are handled equallyby the rest of the system."
What software was used to run the optimizations for BA?,g2o,For BA optimization Levenberg–Marquardt method implemented in g2o software is used.,"Our system performs BA to optimize the camera pose in the tracking thread (motion-only BA), to optimize a local window of keyframes and points in the local mapping thread (local BA),and after a loop closure to optimize all keyframes and points (full BA). We use the Levenberg–Marquardt method implemented in g2o [19]."
What metrics are used for the evaluation of SLAM systems?,The metrics used for the evaluation of SLAM systems in the context of the KITTI dataset are absolute translation RMSE and average relative translation and rotation errors,"for evaluation of SLAM systems two different metrics, the absolute translation RMSE tabs proposed in [3], and the average relative translation trel and rotation rrel errors are used.","The KITTI dataset [2] contains stereo sequences recorded from a car in urban and highway environments. The stereo sensor has a ∼54cm baseline and works at 10Hz with a resolution after rectification of 1240 × 376 pixels. Sequences 00, 02, 05, 06, 07 and 09 contain loops. Our ORB-SLAM2 detects all loops and is able to reuse its map afterwards, except for sequence 09 where the loop happens in very few frames at the end of the sequence. Table I shows results in the 11 training sequences, which have public ground-truth, compared to the state-of-the-art Stereo LSD-SLAM [11], to our knowledge the only stereo SLAM showing detailed results for all sequences. We use two different metrics, the absolute translation RMSE tabs proposed in [3], and the average relative translation trel and rotation rrel errors proposed in [2]."
Do the differences in hardware include different gripper shapes?,Yes,"The differences in hardware include different gripper shapes, illustrating the range of variation in gripper wear and geometry as in Figure 7. Uneven wear and tear on the robots resulted in many differences in the shape of the gripper fingers.","In order to train our prediction network, we collected over 800,000 grasp attempts using a set of similar (but not identical) robotic manipulators, shown in Figure 1. We discuss the details of our hardware setup in Section 5.1, and discuss the data collection process in Section 5.2. To ensure generalization of the learned prediction network, the specific parameters of each robot varied in terms of the camera pose relative to the robot, providing independence to camera calibration. Furthermore, uneven wear and tear on each robot resulted in differences in the shape of the gripper fingers. Although accurately predicting optimal motion vectors in open-loop is not possible with this degree of variation, as demonstrated in our experiments, our continuous servoing method can correct mistakes by observing the outcomes of its past actions, achieving a high success rate even without knowledge of the precise camera calibration. Our robotic manipulator platform consists of a lightweight 7 degree of freedom arm, a compliant, underactuated, two-finger gripper, and a camera mounted behind the arm looking over the shoulder. An illustration of a single robot is shown in Figure 5. The underactuated gripper provides some degree of compliance for oddly shaped objects, at the cost of producing a loose grip that is prone to slipping. An interesting property of this gripper was uneven wear and tear over the course of data collection, which lasted several months. Images of the grippers of various robots are shown in Figure 7, illustrating the range of variation in gripper wear and geometry. Furthermore, the cameras were mounted at slightly varying angles, providing a different viewpoint for each robot. The views from the cameras of all 14 robots during data collection are shown in Figure 6."
How does the network achieve the correction of its own mistakes?,"Through continuous servoing, the network uses the CNN to continuously update the robot's motor commands, allowing the robot to correct its own mistakes by observing the outcomes of its past actions","The network achieves the correction of its own mistakes using the continuous servoing method can correct mistakes by observing the outcomes of its past actions, achieving a high success rate. Their method can use continuous feedback to correct mistakes and reposition the gripper; the servoing mechanism provides the robot with fast feedback to perturbations and object motion, as well as robustness.","In order to train our prediction network, we collected over 800,000 grasp attempts using a set of similar (but not identical) robotic manipulators, shown in Figure 1. We discuss the details of our hardware setup in Section 5.1, and discuss the data collection process in Section 5.2. To ensure generalization of the learned prediction network, the specific parameters of each robot varied in terms of the camera pose relative to the robot, providing independence to camera calibration. Furthermore, uneven wear and tear on each robot resulted in differences in the shape of the gripper fingers. Although accurately predicting optimal motion vectors in open-loop is not possible with this degree of variation, as demonstrated in our experiments, our continuous servoing method can correct mistakes by observing the outcomes of its past actions, achieving a high success rate even without knowledge of the precise camera calibration. Our method consists of two components: a grasp success predictor, which uses a deep convolutional neural network (CNN) to determine how likely a given motion is to produce a successful grasp, and a continuous servoing mechanism that uses the CNN to continuously update the robot’s motor commands. By continuously choosing the best predicted path to a successful grasp, the servoing mechanism provides the robot with fast feedback to perturbations and object motion, as well as robustness to inaccurate actuation. We presented a method for learning hand-eye coordination for robotic grasping, using deep learning to build a grasp success prediction network, and a continuous servoing mechanism to use this network to continuously control a robotic manipulator. By training on over 800,000 grasp attempts from 14 distinct robotic manipulators with variation in camera pose, we can achieve invariance to camera calibration and small variations in the hardware. Unlike most grasping and visual servoing methods, our approach does not require calibration of the camera to the robot, instead using continuous feedback to correct any errors resulting from discrepancies in calibration. Our experimental results demonstrate that our method can effectively grasp a wide range of different objects, including novel objects not seen during training. Our results also show that our method can use continuous feedback to correct mistakes and reposition the gripper in response to perturbation and movement of objects in the scene."
What does it mean for perception or feedback to be open loop?,"Open loop refers to a system that does not use continuous visual feedback to adjust its actions. Instead, it relies on pre-processed information and pre-defined rules to make decisions. In the context of grasping, open loop systems observe the scene prior to the grasp, extract image patches, and use a known camera calibration to move the gripper to a predetermined location, without considering the current state of the environment","The open-loop method observes the scene prior to the grasp, extracts image patches, chooses the patch with the highest probability of a successful grasp, and then uses a known camera calibration to move the gripper to that location. This method uses the same network architecture as our method and the same training set. We refer to this approach as “open loop,” since it does not make use of continuous visual feedback.","The goal of our evaluation was to answer the following questions: (1) does continuous servoing significantly improve grasping accuracy and success rate? (2) how well does our learning-based system perform when compared to alternative approaches? To answer question (1), we compared our approach to an open-loop method that observes the scene prior to the grasp, extracts image patches, chooses the patch with the highest probability of a successful grasp, and then uses a known camera calibration to move the gripper to that location. This method is analogous to the approach proposed by Pinto & Gupta (2015), but uses the same network architecture as our method and the same training set. We refer to this approach as “open loop,” since it does not make use of continuous visual feedback. To answer question (2), we also compared our approach to a random baseline method, as well as a hand-engineered grasping system that uses depth images and heuristic positioning of the fingers. This hand-engineered system is described in Appendix C. Note that our method requires fewer assumptions than either of the two alternative methods: unlike Pinto & Gupta (2015), we do not require knowledge of the camera to hand calibration, and unlike the hand-engineered system, we do not require either the calibration or depth images."
What differences exist between the approach of this paper and open-loop variants? ,"Continuous visual feedback. Our approach uses continuous visual feedback to adjust motor commands and maximize grasp success, while open-loop variants rely on pre-planned grasp poses and do not use visual feedback","The approach of this paper:  does not require knowledge of the camera to hand calibration and we do not require either the calibration or depth images, achieves continuous hand-eye coordination by observing the gripper and choosing the best motor command to move the gripper toward a successful grasp, does not require proposals or crops of image patches and, most importantly, does not require calibration between the robot and the camera,  learning continuous visual servoing for robotic grasping from monocular cameras, entirely data-driven, and does not rely on any human annotation either at training or test time, continuously adjusts the motor commands to maximize grasp success, providing continuous feedback. The open-loop variants: observe the scene prior to the grasp, extracts image patches, chooses the patch with the highest probability of a successful grasp, and then uses a known camera calibration to move the gripper to that location, making open-loop predictions.","The goal of our evaluation was to answer the following questions: (1) does continuous servoing significantly improve grasping accuracy and success rate? (2) how well does our learning-based system perform when compared to alternative approaches? To answer question (1), we compared our approach to an open-loop method that observes the scene prior to the grasp, extracts image patches, chooses the patch with the highest probability of a successful grasp, and then uses a known camera calibration to move the gripper to that location. This method is analogous to the approach proposed by Pinto & Gupta (2015), but uses the same network architecture as our method and the same training set. We refer to this approach as “open loop,” since it does not make use of continuous visual feedback. To answer question (2), we also compared our approach to a random baseline method, as well as a hand-engineered grasping system that uses depth images and heuristic positioning of the fingers. This hand-engineered system is described in Appendix C. Note that our method requires fewer assumptions than either of the two alternative methods: unlike Pinto & Gupta (2015), we do not require knowledge of the camera to hand calibration, and unlike the hand-engineered system, we do not require either the calibration or depth images. The main contributions of this work are a method for learning continuous visual servoing for robotic grasping from monocular cameras, a novel convolutional neural network architecture for learning to predict the outcome of a grasp attempt, and a large-scale data collection framework for robotic grasps. Our experimental evaluation demonstrates that our convolutional neural network grasping controller achieves a high success rate when grasping in clutter on a wide range of objects, including objects that are large, small, hard, soft, deformable, and translucent. Supplemental videos of our grasping system show that the robot employs continuous feedback to constantly adjust its grasp, accounting for motion of the objects and inaccurate actuation commands. We also compare our approach to open-loop variants to demonstrate the importance of continuous feedback, as well as a hand-engineering grasping baseline that uses manual hand-to-eye calibration and depth sensing. Our method achieves the highest success rates in our experiments. Our dataset is available here: https://sites.google.com/site/brainrobotdata/home Data-driven methods take a variety of different forms, including human-supervised methods that predict grasp configurations (Herzog et al., 2014; Lenz et al., 2015) and methods that predict finger placement from geometric criteria computed offline (Goldfeder et al., 2009a). Both types of data-driven grasp selection have recently incorporated deep learning (Kappler et al., 2015; Lenz et al., 2015; Redmon & Angelova, 2015). Feedback has been incorporated into grasping primarily as a way to achieve the desired forces for force closure and other dynamic grasping criteria (Hudson et al., 2012), as well as in the form of standard servoing mechanisms, including visual servoing (described below) to servo the gripper to a pre-planned grasp pose (Kragic & Christensen, 2002). The method proposed in this work is entirely data-driven, and does not rely on any human annotation either at training or test time, in contrast to prior methods based on grasp points. Furthermore, our approach continuously adjusts the motor commands to maximize grasp success, providing continuous feedback. Comparatively little prior work has addressed direct visual feedback for grasping, most of which requires manually designed features to track the end effector (Vahrenkamp et al., 2008; Hebert et al., 2012). Our approach is most closely related to recent work on self-supervised learning of grasp poses by Pinto & Gupta (2015). This prior work proposed to learn a network to predict the optimal grasp orientation for a given image patch, trained with self-supervised data collected using a heuristic grasping system based on object proposals. In contrast to this prior work, our approach achieves continuous hand-eye coordination by observing the gripper and choosing the best motor command to move the gripper toward a successful grasp, rather than making open-loop predictions. Furthermore, our approach does not require proposals or crops of image patches and, most importantly, does not require calibration between the robot and the camera, since the closed-loop servoing mechanism can compensate for offsets due to differences in camera pose by continuously adjusting the motor commands. We trained our method using over 800,000 grasp attempts on a very large variety of objects, which is more than an order of magnitude larger than prior methods based on direct self-supervision (Pinto & Gupta, 2015) and more than double the dataset size of prior methods based on synthetic grasps from 3D scans (Kappler et al., 2015). "
How were the probability percentages chosen for the two heuristics of the continuous serving algorithm?,The probability percentages of 90% and 50% were chosen based on empirical evaluation and trial-and-error,"We use two heuristics in particular: first, we close the gripper whenever the network predicts that (\mathbf{I}_{t},\emptyset), where \emptyset corresponds to no motion, will succeed with a probability that is at least 90\% of the best inferred motion \mathbf{v}_{t}^{\star}. The rationale behind this is to stop the grasp early if closing the gripper is nearly as likely to produce a successful grasp as moving it. The second heuristic is to raise the gripper off the table when (\mathbf{I}_{t},\emptyset) has a probability of success that is less than 50\% of \mathbf{v}_{t}^{\star}. The rationale behind this choice is that, if closing the gripper now is substantially worse than moving it, the gripper is most likely not positioned in a good configuration, and a large motion will be required. Therefore, raising the gripper off the table minimizes the chance of hitting other objects that are in the way.","We can use the predicted grasp success p(\ell=1) produced by the network to inform a heuristic for raising and lowering the gripper, as well as to choose when to stop moving and attempt a grasp. We use two heuristics in particular: first, we close the gripper whenever the network predicts that (\mathbf{I}_{t},\emptyset), where \emptyset corresponds to no motion, will succeed with a probability that is at least 90\% of the best inferred motion \mathbf{v}_{t}^{\star}. The rationale behind this is to stop the grasp early if closing the gripper is nearly as likely to produce a successful grasp as moving it. The second heuristic is to raise the gripper off the table when (\mathbf{I}_{t},\emptyset) has a probability of success that is less than 50\% of \mathbf{v}_{t}^{\star}. The rationale behind this choice is that, if closing the gripper now is substantially worse than moving it, the gripper is most likely not positioned in a good configuration, and a large motion will be required. Therefore, raising the gripper off the table minimizes the chance of hitting other objects that are in the way. While these heuristics are somewhat ad-hoc, we found that they were effective for successfully grasping a wide range of objects in highly cluttered situations, as discussed in Section 6. Pseudocode for the servoing mechanism f(\mathbf{I}_{t}) is presented in Algorithm 1. Further details on the servoing mechanism are presented in Appendix A."
Is the additional image I0 unchanged throughout the entire training process?,NO,"The additional image I0 is unchanged throughout the entire training process. It is recorded before the grasp begins, and does not contain the gripper. This additional image provides an unoccluded view of the scene.","The architecture of our grasp prediction CNN is shown in Figure 4. The network takes the current image \mathbf{I}_{t} as input, as well as an additional image \mathbf{I}_{0} that is recorded before the grasp begins, and does not contain the gripper. This additional image provides an unoccluded view of the scene. The two input images are concatenated and processed by 5 convolutional layers with batch normalization (Ioffe & Szegedy, 2015), following by max pooling. After the 5^{\text{th}} layer, we provide the vector \mathbf{v}_{t} as input to the network. The vector is represented by 5 values: a 3D translation vector, and a sine-cosine encoding of the change in orientation of the gripper about the vertical axis.111In this work, we only consider vertical pinch grasps, though extensions to other grasp parameterizations would be straightforward. To provide this vector to the convolutional network, we pass it through one fully connected layer and replicate it over the spatial dimensions of the response map after layer 5, concatenating it with the output of the pooling layer. After this concatenation, further convolution and pooling operations are applied, as described in Figure 4, followed by a set of small fully connected layers that output the probability of grasp success, trained with a cross-entropy loss to match \ell_{i}, causing the network to output p(\ell_{i}=1). The input matches are 512\times 512 pixels, and we randomly crop the images to a 472\times 472 region during training to provide for translation invariance."
How were the variations of the camera poses for the different robots determined? ,"The variations in camera poses for the different robots were determined using a combination of wear and tear, object interactions, and slightly different camera poses relative to the robot base","A slightly different camera pose was selected for each robot, relative to the robot base. Another related area to our method is visual servoing, which addresses moving a camera or end-effector to a desired pose using visual feedback.","The grasp prediction CNN was trained using a dataset of over 800,000 grasp attempts, collected using a cluster of similar (but not identical) robotic manipulators, shown in Figure 1, over the course of several months.Although the hardware parameters of each robot were initially identical, each unit experienced different wear and tear over the course of data collection, interacted with different objects, and used a slightly different camera pose relative to the robot base. These differences provided a diverse dataset for learning continuous hand-eye coordination for grasping. Another related area to our method is visual servoing, which addresses moving a camera or end-effector to a desired pose using visual feedback (Kragic & Christensen, 2002). In contrast to our approach, visual servoing methods are typically concerned with reaching a pose relative to objects in the scene, and often (though not always) rely on manually designed or specified features for feedback control (Espiau et al., 1992; Wilson et al., 1996; Vahrenkamp et al., 2008; Hebert et al., 2012; Mohta et al., 2014). Photometric visual servoing uses a target image rather than features (Caron et al., 2013), and several visual servoing methods have been proposed that do not directly require prior calibration between the robot and camera (Yoshimi & Allen, 1994; Jägersand et al., 1997; Kragic & Christensen, 2002). To the best of our knowledge, no prior learning-based method has been proposed that uses visual servoing to directly move into a pose that maximizes the probability of success on a given task (such as grasping)."
"What does it mean to be multimodal in the context of ""multimodal inputs""? ","To be multimodal in the context of multimodal inputs refers to the use of multiple sensors or modalities of data, such as color, depth, and surface normals, to provide a more comprehensive understanding of the environment or task at hand","It means that the system is able to handle multiple modalities of input data, such as audio and video, text and image data, and even RGB-D data; challenging tasks which require multiple modalities of information to perform well.","Multimodal data has become extremely important for robotics, due bothto the advent of new sensors such as the Kinect and the application ofrobots to more challenging tasks which require multiple modalities ofinformation to perform well. However, it can be very difficult todesign featureswhich do a good job of integrating many modalities. Whileour work focuses on color, depth, and surface normals as input modes,our structured multimodal regularization algorithm might also be applied toothers.This approach could improve performance while allowing roboticists to focus onother engineering challenges. Multimodal Deep Learning: Recent works in deep learning have extended these methods to handlemultiple modalities of input data, such as audio and video [43],text and image data [61], and even RGB-D data[59, 3].However, all of these approaches have fallen intotwo camps - either learning completely separate low-level features foreach modality [43, 61], or simply concatenating the modalities [59, 3].The former approaches have proven effective fordata where the basic modalities differ significantly, such as the aforementionedcase of text and images, while the latter is more effective in cases where themodalities are more similar, such as RGB-D data."
Which RGBD robotic grasping dataset was used for verification?,Cornell grasping dataset,"We used the extended version of the Cornell grasping dataset for our experiments. This dataset, along with code for this paper, is available athttp://pr.cs.cornell.edu/deepgrasping.We note that this is an updated version of the dataset used in[28], containing several more complex objects, and thus results for their algorithms will be different from those in[28]. This dataset contains 1035 images of 280 graspable objects, several of which are shown in Fig. 9.Each image is annotated with several ground-truth positive and negative grasping rectangles. While the vast majority of possible rectangles for most objects will be non-graspable, the dataset contains roughly equal numbers of graspable and non-graspable rectangles. We will show that this is useful for an unsupervised learning algorithm, as it allows learning a good representation for graspable rectangles even from unlabeled data.","We used the extended version of the Cornell graspingdataset for our experiments. This dataset, along with code for thispaper, is available athttp://pr.cs.cornell.edu/deepgrasping.We note that this is an updated version of the dataset used in[28], containing several more complex objects, and thusresults for their algorithms will be different from those in[28].This dataset contains 1035 images of 280 graspable objects, several ofwhich are shown in Fig. 9.Each image is annotated with several ground-truth positive and negativegrasping rectangles. While the vast majority of possible rectangles for most objectswill be non-graspable, the dataset contains roughly equal numbers ofgraspable and non-graspable rectangles. We will show that this is usefulfor an unsupervised learning algorithm, as it allows learning a goodrepresentation for graspable rectangles even from unlabeled data."
"Why is the paper's proposed multimodal feature learning algorithm better than other methods that ""ignore modality information at the first layer"" or ""train separate first-layer features for each modality""?","The paper's proposed multimodal feature learning algorithm is better than other methods that ""ignore modality information at the first layer"" or ""train separate first-layer features for each modality"" because it allows for a middle-ground in which each feature is encouraged to use only a subset of the input modalities, but is not forced to use only particular ones. This approach allows the model to learn correlated features between multiple input modalities, but regularizes the number of modalities used per feature, discouraging the model from learning weak correlations between modalities","The proposed approach incorporates a structured penalty term into the optimization problem to be solved during learning. This technique allows the model to learn correlated features between multiple input modalities,but regularizes the number of modalities used per feature (hidden unit),discouraging the model from learning weak correlations between modalities.With this regularization term, the algorithm can specify how mode-sparse or mode-dense the features should be, representing a continuum between the two extremes outlined above.  The second major contribution of this work is to propose a new method for handling multimodal data in the context of feature learning.The use of RGB-D data, as opposed to simple 2D image data, has been shown to significantly improve grasp detection results. In this work, we present a multimodal feature learning algorithm which adds a structured regularization penalty to the objective function to be optimized during learning.  ignore modality information at the first layer (i.e., encourage all features to use all modalities) or train separate first-layer features for each modality.","The second major contribution of our work is to propose a new method forhandling multimodal data in the context of feature learning.The use ofRGB-D data, as opposed to simple 2D image data, has been shown tosignificantly improve grasp detection results [28, 14, 56].In this work, we present a multimodal feature learning algorithm which adds astructured regularization penalty to the objective function to be optimizedduring learning.As opposed to previous works in deeplearning, which either ignore modality information at the first layer (i.e., encourage all features to use all modalities) [59] ortrain separate first-layer features for each modality [43, 61], ourapproach allows for a middle-ground in which each feature is encouraged touse only a subset of the input modalities, but is not forced to use onlyparticular ones. To solve these problems, we propose a new algorithm for feature learning formultimodal data.Our approach incorporates astructured penalty term into the optimization problem to be solved during learning. This techniqueallows the model to learn correlated features between multiple input modalities,but regularizes the number of modalities used per feature (hidden unit),discouraging themodel from learning weak correlations between modalities.With this regularization term, the algorithm can specify how mode-sparse or mode-dense the features should be, representing a continuum between the two extremes outlined above."
What do the seven input pixel channels represent?,"The seven input pixel channels represent the following features:

1. Image intensity in YUV color space (3 channels)
2. Depth channel of the image
3. X, Y, and Z components of surface normals computed based on the depth channel","There are seven (7) channels that worth of features to be extracted, giving 24x24x7 = 4032 input features. The first three channels are the image in YUV color space, used because it represents image intensity and color separately. The next is simply the depth channel of the image. The last three are the X, Y, and Z components of surface normals computed based on the depth channel. Seven (7) channels: the depth channel, the combination of the Y, U, and V channels, and the combination of the X, Y, and Z surface normal components.","From this 24x24 pixel image, seven channels’ worth of features are extracted,giving 24x24x7 = 4032 input features. The first three channels are the image inYUV color space, used because it representsimage intensity and color separately. The next is simply the depth channel ofthe image. The last three are the X, Y, and Z components of surface normalscomputed based on the depth channel. These are computed after theimage is aligned to the gripper so that they are always relative to the gripper plates. We also compare our algorithm to other deep learning approaches. We compareto a network trained only with standard L1 regularization, and a networktrained in a manner similar to [43], where three separate setsof first layer features are learned for the depth channel, the combinationof the Y, U, and V channels, and the combination of the X, Y, and Z surfacenormal components."
"What are the Y, U and V channels?","Y, U, and V channels are the color channels in the YUV color space","The first three channels are the image in YUV color space, used because it represents image intensity and color separately. In particular,many of these features lack weights to the U and V (3rd and 4th channels), which correspond to color, allowing the system to be more robust to different-colored objects.","From this 24x24 pixel image, seven channels’ worth of features are extracted,giving 24x24x7 = 4032 input features. The first three channels are the image inYUV color space, used because it representsimage intensity and color separately. The next is simply the depth channel ofthe image. The last three are the X, Y, and Z components of surface normalscomputed based on the depth channel. These are computed after theimage is aligned to the gripper so that they are always relative to the gripper plates. Figure 8 shows the features learned by the unsupervised phase ofour algorithm which havea high correlation to positive and negative grasping cases.Many of these features show non-zero weights to the depth channel,indicating that it learns thecorrelation of depths to graspability.We can see that weights to many of the modalities for these features have beeneliminated by our structured regularization approach. In particular,many of these features lack weights to the U and V (3^{rd} and 4^{th})channels, which correspond to color, allowing the system to be more robustto different-colored objects."
What does SAE stand for?,SAE stands for Sparse Autoencoder,"In the first phase, we will use unsupervised feature learning to initialize the hidden-layer weights W^{[1]} and W^{[2]}. Pre-training weights this way is critical to avoid overfitting. We will use a variant of a sparse auto-encoder (SAE) [21], as illustrated in Fig. 4-right.","In the first phase, we will use unsupervised feature learning to initialize thehidden-layer weights W^{[1]} and W^{[2]}. Pre-training weights this wayis critical to avoid overfitting. We will use a variant of a sparseauto-encoder (SAE) [21], as illustrated inFig. 4-right.We define g(h) as a sparsity penalty function over hidden unit activations, with \lambda controlling its weight.With f(W) as a regularization function, weighted by \beta, and\hat{x}^{(t)} as the reconstruction of x^{(t)}, SAE solves the followingto initialize hidden-layer weights:\displaystyle W^{*}\displaystyle=\underset{W}{\mbox{arg min }}\sum_{t=1}^{M}(||\hat{x}^{(t)}-x^{(t)}||_{2}^{2}+\lambda\sum_{j=1}^{K}g(h_{j}^{(t)}))+\beta f(W)\displaystyle h_{j}^{(t)}\displaystyle=\sigma(\sum_{i=1}^{N}x_{i}^{(t)}W_{i,j})\displaystyle\hat{x}_{i}^{(t)}\displaystyle=\sum_{j=1}^{K}h_{j}^{(t)}W_{i,j}(3)We first use this algorithm to initialize W^{[1]} to reconstruct x.We then fix W^{[1]} andlearn W^{[2]} to reconstruct h^{[1]}."
What is the bias responsible for the system performing poorly without mask-based scaling?,"Sure! Here's the answer to your question based on the provided context:

Inherent bias towards square rectangles","As shown in Table III our mask-based scaling technique at the visible layer improves grasping results by over 25% for both metrics. As seen in Figure 6, it removes the network’s inherent bias towards square rectangles, exhibiting a much wider range of a spect ratios that more closely matches that of the ground-truth data.","As shown in Table III our mask-based scalingtechnique at the visible layer improvesgrasping results by over 25% for both metrics. As seen inFigure 6, it removes the network’s inherentbias towards square rectangles, exhibiting a much wider range of aspectratios that more closely matches that of the ground-truth data."
How were the 100 trials for the robotic experiments split for the 30 objects?,"The 100 trials for the robotic experiments were split evenly among the 30 objects, with 3 trials per object on average","Table IV shows the results of our robotic experiments on Baxter for the remaining 30 objects, a total of 100 trials. Using our algorithm, Yogi was able to successfully execute a grasp in 84% of the trials.","Results: Table LABEL:tbl:expResults shows the results of our robotic experiments onBaxter forthe remaining 30 objects, a totalof 100 trials. Using our algorithm, Yogi was ableto successfully execute a grasp in 84% of the trials.Figure LABEL:fig:yogiGrasping shows Yogi executingseveral of these grasps. In 8% of the trials,our algorithm detected a valid grasp which was not executed correctly byYogi. Thus, we were able to successfully detect a good grasp in92% of the trials. Video of some of these trials is available athttp://pr.cs.cornell.edu/deepgrasping."
"Why would the fact that PR2 had a greater gripping force be a valid reason for the difference in performance if ""valid grasps which was not executed correctly by Yogi"" were still counted as true positives for Yogi?","PR2's greater gripping force would not be a valid reason for the difference in performance, because valid grasps that were not executed correctly by Yogi should not be counted as true positives for Yogi","PR2 yielded a higher success rate as seen in Table V, succeeding in 89% of trials. This is largely due to the much wider span of PR2’s gripper from open to closed and its ability to fully close from its widest position, as well as PR2’s ability to apply a larger gripping force.  Interestingly, even though the parameters of grasps detected for the white box were similar for PR2 and Baxter, PR2 was able to succeed in every case while Baxter succeeded only half the time. This is because PR2’s increased gripper strength allowed it to execute grasps across corners of the box, crushing it slightly in the process.","Our algorithm was able to consistently detect and execute valid grasps for ared cereal box, but had some failures on a white and yellow one. This is becausethe background for all objects in the dataset is white, leading the algorithmto learn features relating white areas at the edges of the gripper region tograspable cases. However, it was able to detect and execute correct grasps foran all-white ice cube tray, and so does not fail for all white objects. Thiscould be remedied by extending the dataset to include cases with differentbackground colors.Interestingly, even though the parameters of grasps detectedfor the white box were similar for PR2 and Baxter, PR2 was able to succeed inevery case while Baxter succeeded only half the time. This is because PR2’sincreased gripper strength allowed it to execute grasps across corners of thebox, crushing it slightly in the process. PR2 yielded a higher success rate as seen in Table LABEL:tbl:pr2Results,succeeding in 89% of trials. This islargely due to the much wider span of PR2’s gripper from open to closed andits ability to fully close from its widest position, as well as PR2’s abilityto apply a larger gripping force. Some specific instances where PR2 andBaxter’s performance differed are discussed below. "
How were the number of tasks from each type of continuous control chosen to create the 31-task benchmark?,"The number of tasks from each type of continuous control was chosen based on a combination of factors, including the diversity of tasks, the difficulty of tasks, and the representation of different challenges in the field of reinforcement learning. The goal was to create a balanced benchmark that covers a wide range of continuous control tasks, from simple to complex, and from low-dimensional to high-dimensional","They attempt to address this problem and present a benchmark consisting of 31 continuous control tasks. These tasks range from simple tasks, such as cart-pole balancing, to challenging tasks such as high-DOF locomotion, tasks with partial observations, and hierarchically structured tasks. The tasks in the presented benchmark can be divided into four categories: basic tasks, locomotion tasks, partially observable tasks, and hierarchical tasks.We briefly describe them in this section.More detailed specifications are given in the supplementary materials and in the source code.","We attempt to address this problem and present a benchmark consisting of 31 continuous control tasks. These tasks range from simple tasks, such as cart-pole balancing, to challenging tasks such as high-DOF locomotion, tasks with partial observations, and hierarchically structured tasks. Furthermore, a range of reinforcement learning algorithms are implemented on which we report novel findings based on a systematic evaluation of their effectiveness in training deep neural network policies. The benchmark and reference implementations are available at https://github.com/rllab/rllab, allowing for the development, implementation, and evaluation of new algorithms and tasks. The tasks in the presented benchmark can be divided into four categories: basic tasks, locomotion tasks, partially observable tasks, and hierarchical tasks.We briefly describe them in this section.More detailed specifications are given in the supplementary materials and in the source code."
Are the task categories the same as the types of tasks mentioned in the abstract?,"Sure! Here's my answer:

No. The task categories in the presented benchmark are not the same as the types of tasks mentioned in the abstract","Yes they are the same categories. The tasks in the presented benchmark can be divided into four categories: basic tasks, locomotion tasks, partially observable tasks, and hierarchical tasks.","The tasks in the presented benchmark can be divided into four categories: basic tasks, locomotion tasks, partially observable tasks, and hierarchical tasks.We briefly describe them in this section.More detailed specifications are given in the supplementary materials and in the source code."
How were the two tasks from each category chosen?,The two tasks from each category were chosen based on their similarity in terms of computational complexity and the type of optimization problem,"For the other tasks, we try both of the best hyperparameters found in the same category, and report the better performance of the two. This gives us insights into both the maximum possible performance when extensive hyperparameter tuning is performed, and the robustness of the best hyperparameters across different tasks.","For the other tasks, we try both of the best hyperparameters found in the same category, and report the better performance of the two. This gives us insights into both the maximum possible performance when extensive hyperparameter tuning is performed, and the robustness of the best hyperparameters across different tasks."
Is the Walker task notable for having hard-to-escape local optima?,Yes,"In this category, we implement six locomotion tasks of varying dynamics and difficulty: Swimmer (Purcell, 1977; Coulom, 2002; Levine & Koltun, 2013; Schulman et al., 2015a), Hopper (Murthy & Raibert, 1984; Erez et al., 2011; Levine & Koltun, 2013; Schulman et al., 2015a), Walker (Raibert & Hodgins, 1991; Erez et al., 2011; Levine & Koltun, 2013; Schulman et al., 2015a), Half-Cheetah (Wawrzyński, 2007; Heess et al., 2015b), Ant (Schulman et al., 2015b), Simple Humanoid (Tassa et al., 2012; Schulman et al., 2015b), and Full Humanoid (Tassa et al., 2012).The goal for all the tasks is to move forward as quickly as possible. These tasks are more challenging than the basic tasks due to high degrees of freedom. In addition, a great amount of exploration is needed to learn to move forward without getting stuck at local optima. Since we penalize for excessive controls as well as falling over, during the initial stage of learning, when the robot is not yet able to move forward for a sufficient distance without falling, apparent local optima exist including staying at the origin or diving forward slowly.","In this category, we implement six locomotion tasks of varying dynamics and difficulty: Swimmer (Purcell, 1977; Coulom, 2002; Levine & Koltun, 2013; Schulman et al., 2015a), Hopper (Murthy & Raibert, 1984; Erez et al., 2011; Levine & Koltun, 2013; Schulman et al., 2015a), Walker (Raibert & Hodgins, 1991; Erez et al., 2011; Levine & Koltun, 2013; Schulman et al., 2015a), Half-Cheetah (Wawrzyński, 2007; Heess et al., 2015b), Ant (Schulman et al., 2015b), Simple Humanoid (Tassa et al., 2012; Schulman et al., 2015b), and Full Humanoid (Tassa et al., 2012).The goal for all the tasks is to move forward as quickly as possible. These tasks are more challenging than the basic tasks due to high degrees of freedom. In addition, a great amount of exploration is needed to learn to move forward without getting stuck at local optima. Since we penalize for excessive controls as well as falling over, during the initial stage of learning, when the robot is not yet able to move forward for a sufficient distance without falling, apparent local optima exist including staying at the origin or diving forward slowly."
Why do the authors think RWR failed to perform on more challenging tasks?,"Sure! Here's my answer:

RWR's slow-down in performance on more challenging tasks is due to its reliance on gradient-based optimization, which can become less effective as the problem size increases","RWR is the only gradient-based algorithm we implemented that does not require any hyperparameter tuning. It can solve some basic tasks to a satisfactory degree, but fails to solve more challenging tasks such as locomotion. We observe empirically that RWR shows fast initial improvement followed by significant slow-down, as shown in Figure 3.","RWR: RWR is the only gradient-based algorithm we implemented that does not require any hyperparameter tuning. It can solve some basic tasks to a satisfactory degree, but fails to solve more challenging tasks such as locomotion. We observe empirically that RWR shows fast initial improvement followed by significant slow-down, as shown in Figure 3."
What other differences between CEM and CMA-ES exist that might affect performance?,"In addition to the estimation of the covariance matrix, CMA-ES and CEM differ in their treatment of the correlation between consecutive updates. CMA-ES adaptively estimates the covariance matrix through incremental adaptation along evolution paths, while CEM assumes a constant covariance matrix. This adaptation in CMA-ES can lead to better performance in complex tasks with high-dimensional observations, but also increases computational complexity and memory requirements","We also observe that CEM outperforms CMA-ES, which is remarkable as CMA-ES estimates the full covariance matrix. For higher-dimensional policy parameterizations, the computational complexity and memory requirement for CMA-ES become noticeable. On tasks with high-dimensional observations, such as the Full Humanoid, the CMA-ES algorithm runs out of memory and fails to yield any results, denoted as N/A in Table 1. Similar to CEM, CMA-ES is a gradient-free evolutionary approach for optimizing nonconvex objective functions. In our case, this objective function equals the average sampled return. In contrast to CEM, CMA-ES estimates the covariance matrix of a multivariate normal distribution through incremental adaption along evolution paths, which contain information about the correlation between consecutive updates.","Covariance Matrix Adaption Evolution Strategy (CMA-ES) (Hansen & Ostermeier, 2001): Similar to CEM, CMA-ES is a gradient-free evolutionary approach for optimizing nonconvex objective functions. In our case, this objective function equals the average sampled return. In contrast to CEM, CMA-ES estimates the covariance matrix of a multivariate normal distribution through incremental adaption along evolution paths, which contain information about the correlation between consecutive updates. Gradient-free methods: Surprisingly, even when training deep neural network policies with thousands of parameters, CEM achieves very good performance on certain basic tasks such as Cart-Pole Balancing and Mountain Car, suggesting that the dimension of the searching parameter is not always the limiting factor of the method. However, the performance degrades quickly as the system dynamics becomes more complicated. We also observe that CEM outperforms CMA-ES, which is remarkable as CMA-ES estimates the full covariance matrix. For higher-dimensional policy parameterizations, the computational complexity and memory requirement for CMA-ES become noticeable. On tasks with high-dimensional observations, such as the Full Humanoid, the CMA-ES algorithm runs out of memory and fails to yield any results, denoted as N/A in Table 1."
Did the authors ever try different criteria for choosing hyperparameters?,No,"Hyperparameter Tuning: For the DDPG algorithm, we used the hyperparametes reported in Lillicrap et al. (2015). For the other algorithms, we follow the approach in (Mnih et al., 2015), and we select two tasks in each category, on which a grid search of hyperparameters is performed. Each choice of hyperparameters is executed under five random seeds. The criterion for the best hyperparameters is defined as mean(returns)−std(returns). This metric selects against large fluctuations of performance due to overly large step sizes.","Hyperparameter Tuning: For the DDPG algorithm, we used the hyperparametes reported in Lillicrap et al. (2015). For the other algorithms, we follow the approach in (Mnih et al., 2015), and we select two tasks in each category, on which a grid search of hyperparameters is performed. Each choice of hyperparameters is executed under five random seeds. The criterion for the best hyperparameters is defined as \mathrm{mean}(\mathrm{returns})-\mathrm{std}(\mathrm{returns}). This metric selects against large fluctuations of performance due to overly large step sizes. "
What does using CLIP-based codes mean? And why is this a limitation? Why is not applicable to other methods? What do they mean with other methods here?,"Sure, I can answer your questions based on the given context!

Using CLIP-based codes means using codes generated by the Contrastive Language-Image Pre-training (CLIP) model to condition the image inversion process. This is a limitation because it only applies to the specific method used in DALL-E 2, which relies on CLIP-based codes to condition the model. Other methods may not be able to use these codes, and therefore may not be able to achieve the same level of image manipulation.

In other words, CLIP-based codes are a specific type of conditioning that is only applicable to the DALL-E 2 method, and other methods may not be able to use the same approach. This is why the authors note that their method may not be applicable to other methods","The definition of CLIP-based codes or its limitations cannot be found in this paper. The authors mention that CLIP-based codes, which are used by the semantic editing feature of DALLE-2, might not be applicable broadly to other models - in this context, other models could possibly refer to other general diffusion-based approaches. This paper does not explain why it might not be applicable to other methods as well.","In the realm of diffusion models, inversion can be performed na¨ıvely by adding noise to an image and then de-noising it through the network. However, this process tends to change the image content significantly. Choi et al. (2021) improve inversion by conditioning the denoising process on noised low-pass filter data from the target image. (Dhariwal & Nichol, 2021) demonstrate that the DDIM (Song et al., 2020) sampling process can be inverted in a closed-form manner, extracting a latent noise map that will produce a given real image. In DALL-E 2 (Ramesh et al., 2022), they build on this method and demonstrate that it can be used to induce changes in the image, such as cross-image interpolations or semantic editing. The later relies on their use of CLIP-based codes to condition the model, and may not be applicable to other methods."
Why use a single word embedding instead of multiple? Which could capture more expressivity,"Sure! Here's the answer to your question based on the provided context:

Using a single word embedding instead of multiple embeddings can capture more expressivity because it allows for a higher degree of flexibility in the textual embedding space","The authors did experiment with both single and multi word embeddings, and found that the single-pseudo word approach allowed greater editability, while still having similar accuracy and reconstruction quality when compared to multi-word approaches. These reasons might explain why the authors chose a single-word embedding as their main approach.","Our evaluation results are summarized in Figure 10(a).We highlight four observations of particular interest:First, the semantic reconstruction quality of our method and many of the baselines is comparable to simply sampling random images from the training set.Second, the single-word method achieves comparable reconstruction quality, and considerably improved editability over all multi-word baselines. These points outline the impressive flexibility of the textual embedding space, showing that it can serve to capture new concepts with a high degree of accuracy while using only a single pseudo-word. Third, we observe that our baselines outline a distortion-editability trade-off curve, where embeddings that lie closer to the true word distribution (e.g. due to regularization, fewer pseudo-words, or a lower learning rate) can be more easily modified, but fail to capture the details of the target. In contrast, deviating far from the word distribution enables improved reconstruction at the cost of severely diminished editing capabilities. Notably, our single-embedding model can be moved along this curve by simply changing the learning rate, offering a user a degree of control over this trade-off."
What existing baselines are there? Thought this was the first work.,"There are existing baselines, including LDM guided by a human caption and DALLE-2 guided by either a human caption or an image prompt","To benchmark their single-word embedding approach, the authors create a bunch of reference baselines to gauge the relative improvement their method offers. One reference baseline they create merely spews out images from the train set itself, while ignoring the new prompt. The second reference baseline that they create is a model which uses the text prompt only, while ignoring the personalization aspect of their task. In addition, they also compare the ability of their model to generate variations of an existing image to two existing approaches: namely, DALLE-2 and LDM.","We begin by demonstrating our ability to capture and recreate variations of an object using a single pseudo-word. In Figure 3 we compare our method to two baselines: LDM guided by a human caption and DALLE-2 guided by either a human caption or an image prompt. Captions were collected using Mechanical Turk. Annotators were provided with four images of a concept and asked to describe it in a manner that could allow an artist to recreate it. We asked for both a short (\leq 12 words) and a long (\leq 30 words) caption. In total, we collected 10 captions per concept — five short and five long. Figure 3 shows multiple results generated with a randomly chosen caption for each setup. Additional large-scale galleries showing our uncurated reconstructions are provided in the supplementary. To provide intuition for the scale of the results, we add two reference baselines.First, we consider the expected behavior from a model that always produces copies of the training set, regardless of the prompt. For that, we simply use the training set itself as the “generated sample”.Second, we consider a model that always aligns with the text prompt but ignores the personalized concept. We do so by synthesizing images using the evaluation prompts but without the pseudo-word.We denote these setups as “Image Only” and “Prompt Only”, respectively. Our evaluation results are summarized in Figure 10(a).We highlight four observations of particular interest:First, the semantic reconstruction quality of our method and many of the baselines is comparable to simply sampling random images from the training set.Second, the single-word method achieves comparable reconstruction quality, and considerably improved editability over all multi-word baselines. These points outline the impressive flexibility of the textual embedding space, showing that it can serve to capture new concepts with a high degree of accuracy while using only a single pseudo-word. Third, we observe that our baselines outline a distortion-editability trade-off curve, where embeddings that lie closer to the true word distribution (e.g. due to regularization, fewer pseudo-words, or a lower learning rate) can be more easily modified, but fail to capture the details of the target. In contrast, deviating far from the word distribution enables improved reconstruction at the cost of severely diminished editing capabilities. Notably, our single-embedding model can be moved along this curve by simply changing the learning rate, offering a user a degree of control over this trade-off."
What are transformation modules? Is this related to transformers?,"Transformation modules are components in machine learning models that adapt the output of the model when faced with new concepts, without requiring to re-train the entire model. They are not directly related to transformers, but are a way to modify the model's behavior without changing the underlying architecture","The authors explain how trainable “transformation modules” attached to a frozen (non-trainable) base model might allow for existing models to be used for new concepts, instead of finetuning or retraining (both of which have their associated challenges). In this context, it is probable that the authors used transformation modules as a general term to refer to a layer or module that is able to perform some transformation (or change) on its input. There is no information to suggest that they are specifically referring to transformers, nor do they explicitly define the exact structure of the aforementioned transformation modules.","Introducing new concepts into large scale models is often difficult. Re-training a model with an expanded dataset for each new concept is prohibitively expensive, and fine-tuning on few examples typically leads to catastrophic forgetting (Ding et al., 2022; Li et al., 2022). More measured approaches freeze the model and train transformation modules to adapt its output when faced with new concepts (Zhou et al., 2021; Gao et al., 2021; Skantze & Willemsen, 2022). However, these approaches are still prone to forgetting prior knowledge, or face difficulties in accessing it concurrently with newly learned concepts (Kumar et al., 2022; Cohen et al., 2022)."
How do they show that single word embeddings capture unique and varied concepts?,"By demonstrating that their method can successfully capture finer details of personalized objects that the image encoder (CLIP) is unlikely to have seen, such as color patterns on a teapot or a skull mug, using only a single word embedding","The authors explain how their single-word embedding approach is able to pick up on finer details (such as colour schemes, or complex images) that are difficult to express using natural language alone. Additionally, their results indicate that their single-word embedding approach has comparable performance to multi-word embeddings, suggesting that their single-word embeddings are not inherently limited in how much information they encode. Both these pieces of information taken together are used by the authors to show that their approach captures unique and diverse concepts.","As our results demonstrate, our method better captures the unique details of the concept. Human captioning typically captures the most prominent features of an object, but provides insufficient detail to reconstruct finer features like color patterns (e.g. of the teapot). In some cases (e.g. the skull mug) the object itself may be exceedingly difficult to describe through natural language. When provided with an image, DALLE-2 is able to recreate more appealing samples, particularly for well-known objects with limited detail (Aladdin’s lamp). However, it still struggles with unique details of personalized objects that the image encoder (CLIP) is unlikely to have seen (mug, teapot). In contrast, our method can successfully capture these finer details, and it does so using only a single word embedding. However, note that while our creations are more similar to the source objects, they are still variations that may differ from the source. Our evaluation results are summarized in Figure 10(a).We highlight four observations of particular interest:First, the semantic reconstruction quality of our method and many of the baselines is comparable to simply sampling random images from the training set.Second, the single-word method achieves comparable reconstruction quality, and considerably improved editability over all multi-word baselines. These points outline the impressive flexibility of the textual embedding space, showing that it can serve to capture new concepts with a high degree of accuracy while using only a single pseudo-word."
They claim that LSTIM can synthesize unseen compositions. Is this true? What are some examples?,"Yes, LSTIM can synthesize unseen compositions.

Examples of unseen compositions synthesized by LSTIM include:

* Novel scenes with unconventional perspectives or lighting, such as a cityscape at night with glowing neon lights and a futuristic skyline.
* Images that combine elements from different domains, such as a beach scene with a hot air balloon flying overhead.
* Scenes with fantastical or surreal elements, such as a forest filled with giant mushrooms and glowing fungi.

These examples demonstrate the ability of LSTIM to generate novel and unseen compositions, beyond the capabilities of traditional image generation methods","Assuming that “LSTIM” stands for “large-scale text-to-image models”, the authors mention a list of related work that demonstrate the efficacy of models such as these to reason over natural language queries and generate new art or images. However, whether this statement is true or not, or examples that would validate this claim are not explicitly spelled out in this paper.","Recently, large-scale text-to-image models (Rombach et al., 2021; Ramesh et al., 2021, 2022; Nichol et al., 2021; Yu et al., 2022; Saharia et al., 2022) have demonstrated an unprecedented capability to reason over natural language descriptions. They allow users to synthesize novel scenes with unseen compositions and produce vivid pictures in a myriad of styles. These tools have been used for artistic creation, as sources of inspiration, and even to design new, physical products (Yacoubian, 2022). Their use, however, is constrained by the user’s ability to describe the desired target through text. Turning back to Rose, one could then ask: How might she frame her request if she were to approach one of these models? How could we, as users, ask text-to-image models to craft a novel scene containing a cherished childhood toy? Or to pull our child’s drawing from its place on the fridge, and turn it into an artistic showpiece?"
What does low-pass filter data mean? I understand noised but not theo ther parts,"Sure, I can help you with that!

Low-pass filter data refers to the output of a low-pass filter applied to an image. In the context of image processing, a low-pass filter is a mathematical operation that smooths out the image, retaining only the large-scale features and suppressing the high-frequency details. The low-pass filter output is a representation of the image that emphasizes the larger, more gradual changes in the image, rather than the small, detailed features. In the context of inversion, conditioning the denoising process on noised low-pass filter data from the target image helps to preserve the large-scale features of the original image, while still allowing for some degree of detail to be recovered","This term appears in the paper when discussing how a related work, Choi et al (2021). The current paper does not explain what low-pass filtered data means in this context, though it could possibly refer to the signal processing technique.","In the realm of diffusion models, inversion can be performed naïvely by adding noise to an image and then de-noising it through the network. However, this process tends to change the image content significantly. Choi et al. (2021) improve inversion by conditioning the denoising process on noised low-pass filter data from the target image.(Dhariwal & Nichol, 2021) demonstrate that the DDIM (Song et al., 2020) sampling process can be inverted in a closed-form manner, extracting a latent noise map that will produce a given real image. In DALL-E 2 (Ramesh et al., 2022), they build on this method and demonstrate that it can be used to induce changes in the image, such as cross-image interpolations or semantic editing. The later relies on their use of CLIP-based codes to condition the model, and may not be applicable to other methods."
What is DDIM? What does closed-form manner mean here?,"DDIM stands for Diffusion-based Dual-Image Model, and a closed-form manner refers to a mathematical expression that can be evaluated in a straightforward and analytical way, without the need for numerical optimization or approximation",DDIM Is a sampling process that is described in detail in Song et. al 2020. The authors proposed approach samples images after performing multiple DDIM steps.,"In the realm of diffusion models, inversion can be performed naïvely by adding noise to an image and then de-noising it through the network. However, this process tends to change the image content significantly. Choi et al. (2021) improve inversion by conditioning the denoising process on noised low-pass filter data from the target image.(Dhariwal & Nichol, 2021) demonstrate that the DDIM (Song et al., 2020) sampling process can be inverted in a closed-form manner, extracting a latent noise map that will produce a given real image. In DALL-E 2 (Ramesh et al., 2022), they build on this method and demonstrate that it can be used to induce changes in the image, such as cross-image interpolations or semantic editing. The later relies on their use of CLIP-based codes to condition the model, and may not be applicable to other methods. For each prompt, we synthesize 64 samples using 50 DDIM steps, calculate the average CLIP-space embedding of the samples, and compute their cosine similarity with the CLIP-space embedding of the textual prompts, where we omit the placeholder S_{*} (i.e. “A photo of on the moon”). Here, a higher score indicates better editing capability and more faithfulness to the prompt itself. Note that our method does not involve the direct optimization of the CLIP-based objective score and, as such, is not sensitive to the adversarial scoring flaws outlined by Nichol et al. (2021)."
What is the key difference btw GANs and Diffusion models that leads to piror work on inversions not being helpful here?,"The key difference between GANs and Diffusion models that leads to prior work on inversions not being helpful here is the nature of the latent space. In GANs, the latent space is typically assumed to be a continuous, structured space, while in Diffusion models, the latent space is discrete and unstructured. This difference makes many of the solutions typically used in GAN inversion fail to generalize to the textual embedding space, and are often unhelpful or actively harmful","GANs and diffusion models are very different classes of models. GANs attempt to invert an image by transforming an input image into a latent vector - this process of inversion occurs by attempting to optimize the latent vector directly, or alternatively by training an image encoder model on a large dataset of images. Diffusion models, on the other hand, primarily function by adding noise to an image, and then training a model to denoise (or remove noise) from these noisy images. The authors attempted to extend their model using ideas from these two classes of models but stated that neither of them resulted in significantly better performance. The exact reason why these strategies do not work is not explicitly discussed in the paper.","Manipulating images with generative networks often requires one to find a corresponding latent representation of the given image, a process referred to as inversion (Zhu et al., 2016; Xia et al., 2021).In the GAN literature, this inversion is done through either an optimization-based technique (Abdal et al., 2019, 2020; Zhu et al., 2020b; Gu et al., 2020) or by using an encoder (Richardson et al., 2020; Zhu et al., 2020a; Pidhorskyi et al., 2020; Tov et al., 2021). Optimization methods directly optimize a latent vector, such that feeding it through the GAN will re-create a target image. Encoders leverage a large image set to train a network that maps images to their latent representations. In the realm of diffusion models, inversion can be performed naïvely by adding noise to an image and then de-noising it through the network. However, this process tends to change the image content significantly. Choi et al. (2021) improve inversion by conditioning the denoising process on noised low-pass filter data from the target image.(Dhariwal & Nichol, 2021) demonstrate that the DDIM (Song et al., 2020) sampling process can be inverted in a closed-form manner, extracting a latent noise map that will produce a given real image. In DALL-E 2 (Ramesh et al., 2022), they build on this method and demonstrate that it can be used to induce changes in the image, such as cross-image interpolations or semantic editing. The later relies on their use of CLIP-based codes to condition the model, and may not be applicable to other methods. Below, we outline the core details of applying our approach to a specific class of generative models — Latent Diffusion Models (Rombach et al., 2021). In Section 5, we then analyze a set of extensions to this approach, motivated by GAN-inversion literature. However, as we later show, these additional complexities fail to improve upon the initial representation, presented here. Inversion into an uncharted latent space provides us with a wide range of possible design choices. Here, we examine these choices in light of the GAN inversion literature and discover that many core premises (such as a distortion-editability tradeoff (Tov et al., 2021; Zhu et al., 2020b)) also exist in the textual embedding space. However, our analysis reveals that many of the solutions typically used in GAN inversion fail to generalize to this space, and are often unhelpful or actively harmful. We further investigate a series of extensions based on tools typically used in Generative Adversarial Network (GAN) inversion. Our analysis reveals that, while some core principles remain, applying the prior art in a naïve way is either unhelpful or actively harmful. Text-guided image synthesis has been widely studied in the context of GANs (Goodfellow et al., 2014). Typically, a conditional model is trained to reproduce samples from given paired image-caption datasets (Zhu et al., 2019; Tao et al., 2020), leveraging attention mechanisms (Xu et al., 2018) or cross-modal contrastive approaches (Zhang et al., 2021; Ye et al., 2021). More recently, impressive visual results were achieved by leveraging large scale auto-regressive (Ramesh et al., 2021; Yu et al., 2022) or diffusion models (Ramesh et al., 2022; Saharia et al., 2022; Nichol et al., 2021; Rombach et al., 2021)."
Are newly trained concepts weaker than prior ones?,"No, newly trained concepts are not weaker than prior ones","If this question on whether newly trained concepts are weaker, is with regards to existing work, it depends on what specific method is used. For example, the authors mention that finetuning based approaches suffer from catastrophic forgetting. If this question is about the authors’ proposed approach, the answer is a little more unclear, since the question does not define what “weakness” means in this context. The authors explain that some “concepts” (such as those involving specific shapes) are difficult to encode in the proposed approach, but a more thorough answer cannot be found from the text of the paper alone.","Introducing new concepts into large scale models is often difficult. Re-training a model with an expanded dataset for each new concept is prohibitively expensive, and fine-tuning on few examples typically leads to catastrophic forgetting (Ding et al., 2022; Li et al., 2022). More measured approaches freeze the model and train transformation modules to adapt its output when faced with new concepts (Zhou et al., 2021; Gao et al., 2021; Skantze & Willemsen, 2022). However, these approaches are still prone to forgetting prior knowledge, or face difficulties in accessing it concurrently with newly learned concepts (Kumar et al., 2022; Cohen et al., 2022). While our method offers increased freedom, it may still struggle with learning precise shapes, instead incorporating the “semantic” essence of a concept. For artistic creations, this is often enough. In the future, we hope to achieve better control over the accuracy of the reconstructed concepts, enabling users to leverage our method for tasks that require greater precision."
How important is data augmentation for final model accuracy using SSD?,VERY IMPORTANT,"The data augmentation is improving the performance of SSD on small datasets like PASCAL VOC. The mAP values increase by 2% - 3% on multiple datasets after data augmentation. The new augmentation trick improves the performance on small objects as well. Thus, the data augmentation is quite important for final model accuracy using SSD.","Without a follow-up feature resampling step as in Faster R-CNN, the classification task for small objects is relatively hard for SSD, as demonstrated in our analysis (see Fig. 4). The data augmentation strategy described in Sec. 2.2 helps to improve the performance dramatically, especially on small datasets such as PASCAL VOC. The random crops generated by the strategy can be thought of as a “zoom in” operation and can generate many larger training examples. To implement a “zoom out” operation that creates more small training examples, we first randomly place an image on a canvas of 16× of the original image size filled with mean values before we do any random crop operation. Because we have more training images by introducing this new “expansion” data augmentation trick, we have to double the training iterations. We have seen a consistent increase of 2%-3% mAP across multiple datasets, as shown in Table 6. In specific, Figure 6 shows that the new augmentation trick significantly improves the performance on small objects. This result underscores the importance of the data augmentation strategy for the final model accuracy."
How did we get 8732 default bounding box for 300x300 input resolution?,"We used default boxes of different scales on different output layers, with a total of 8732 boxes for a 300x300 input resolution",The 8732 default bounding boxes are obtained for 300x300 input resolution by stacking more scales of boxes on remaining layers and adjusting scales of boxes if needed.,"Multiple output layers at different resolutions is better. A major contribution of SSD is using default boxes of different scales on different output layers. To measure the advantage gained, we progressively remove layers and compare results. For a fair comparison, every time we remove a layer, we adjust the default box tiling to keep the total number of boxes similar to the original (8732). This is done by stacking more scales of boxes on remaining layers and adjusting scales of boxes if needed. We do not exhaustively optimize the tiling for each setting. Table 3 shows a decrease in accuracy with fewer layers, dropping monotonically from 74.3 to 62.4. When we stack boxes of multiple scales on a layer, many are on the image boundary and need to be handled carefully. We tried the strategy used in Faster R-CNN [2], ignoring boxes which are on the boundary. We observe some interesting trends. For example, it hurts the performance by a large margin if we use very coarse feature maps (e.g. conv11 2 (1 × 1) or conv10 2 (3 × 3)). The reason might be that we do not have enough large boxes to cover large objects after the pruning. When we use primarily finer resolution maps, the performance starts increasing again because even after pruning a sufficient number of large boxes remains. If we only use conv7 for prediction, the performance is the worst, reinforcing the message that it is critical to spread boxes of different scales over different layers. Besides, since our predictions do not rely on ROI pooling as in [6], we do not have the collapsing bins problem in low-resolution feature maps [23]. The SSD architecture combines predictions from feature maps of various resolutions to achieve comparable accuracy to Faster R-CNN, while using lower resolution input images."
What would happen if authors replace three prediction layers with DPM (Deformable Convolution Layers) ?,"Sure! Here's the answer to your question based on the provided context:

The performance of the R-CNN model would likely decrease if the authors replaced the three prediction layers with DPM (Deformable Convolution Layers)",If authors replace three prediction layers with DPM (Deformable Convolution Layers) the performance of SSD will degrade compared to R-CNN and other methods.,"There are two established classes of methods for object detection in images, one based on sliding windows and the other based on region proposal classification. Before the advent of convolutional neural networks, the state of the art for those two approaches – Deformable Part Model (DPM) [26] and Selective Search [1] – had comparable performance. However, after the dramatic improvement brought on by R-CNN [22], which combines selective search region proposals and convolutional network based post-classification, region proposal object detection methods became prevalent."
"For creating feature maps, did SSD extracted features from single or multiple layers of the network?",Multiple layers,SSD uses features from multiple layers of the network for creating the feature maps.,"To handle different object scales, some methods [4, 9] suggest processing the image at different sizes and combining the results afterwards. However, by utilizing feature maps from several different layers in a single network for prediction we can mimic the same effect, while also sharing parameters across all object scales. Previous works [10, 11] have shown that using feature maps from the lower layers can improve semantic segmentation quality because the lower layers capture more fine details of the input objects. Similarly,  [12] showed that adding global context pooled from a feature map can help smooth the segmentation results. Motivated by these methods, we use both the lower and upper feature maps for detection. Figure 1 shows two exemplar feature maps (8\times 8 and 4\times 4) which are used in the framework. In practice, we can use many more with small computational overhead."
"For matching default boxes with ground truth ones, what metric was used? ",Jaccard overlap,Best Jaccard Overlap was used to match default boxes with ground truth ones.,"During training we need to determine which default boxes correspond to a ground truth detection and train the network accordingly. For each ground truth box we are selecting from default boxes that vary over location, aspect ratio, and scale. We begin by matching each ground truth box to the default box with the best jaccard overlap (as in MultiBox [7]). Unlike MultiBox, we then match default boxes to any ground truth with jaccard overlap higher than a threshold (0.5). This simplifies the learning problem, allowing the network to predict high scores for multiple overlapping default boxes rather than requiring it to pick only the one with maximum overlap."
What kind of loss function is used in training SSD?,The SSD training objective uses a weighted sum of localization loss (L_{loc}) and confidence loss (L_{conf}),"The loss function used for training is a weighted sum of the localization loss (loc) and the confidence loss conf):L(x,c,l,g)=\frac{1}{N}(L_{conf}(x,c)+\alpha L_{loc}(x,l,g))(1)where N is the number of matched default boxes. If N=0, wet set the loss to 0. The localization loss is a Smooth L1 loss [6] between the predicted box (l) and the ground truth box (g) parameters.","The SSD training objective is derived from the MultiBox objective [7, 8] but is extended to handle multiple object categories. Let x_{ij}^{p}=\{1,0\} be an indicator for matching the i-th default box to the j-th ground truth box of category p.In the matching strategy above, we can have \sum_{i}x_{ij}^{p}\geq 1.The overall objective loss function is a weighted sum of the localization loss (loc) and the confidence loss (conf):L(x,c,l,g)=\frac{1}{N}(L_{conf}(x,c)+\alpha L_{loc}(x,l,g))(1)where N is the number of matched default boxes. If N=0, wet set the loss to 0. The localization loss is a Smooth L1 loss [6] between the predicted box (l) and the ground truth box (g) parameters. Similar to Faster R-CNN [2], we regress to offsets for the center (cx,cy) of the default bounding box (d) and for its width (w) and height (h).\begin{split}L_{loc}(x,l,g)=\sum_{i\in Pos}^{N}\sum_{m\in\{cx,cy,w,h\}}&x_{ij}^{k}\text{smooth}_{\text{L1}}(l_{i}^{m}-\hat{g}_{j}^{m})\\\hat{g}_{j}^{cx}=(g_{j}^{cx}-d_{i}^{cx})/d_{i}^{w}\quad\quad&\hat{g}_{j}^{cy}=(g_{j}^{cy}-d_{i}^{cy})/d_{i}^{h}\\\hat{g}_{j}^{w}=\log\Big{(}\frac{g_{j}^{w}}{d_{i}^{w}}\Big{)}\quad\quad&\hat{g}_{j}^{h}=\log\Big{(}\frac{g_{j}^{h}}{d_{i}^{h}}\Big{)}\end{split}(2)The confidence loss is the softmax loss over multiple classes confidences (c).L_{conf}(x,c)=-\sum_{i\in Pos}^{N}x_{ij}^{p}log(\hat{c}_{i}^{p})-\sum_{i\in Neg}log(\hat{c}_{i}^{0})\quad\text{where}\quad\hat{c}_{i}^{p}=\frac{\exp(c_{i}^{p})}{\sum_{p}\exp(c_{i}^{p})}(3)and the weight term \alpha is set to 1 by cross validation."
What baseline is used for creating feature maps in the proposed SSD framework?,VGG-16 network,"To create feature maps in SSD, VGG-16 was used as a baseline.","The SSD approach is based on a feed-forward convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum suppression step to produce the final detections. The early network layers are based on a standard architecture used for high quality image classification (truncated before any classification layers), which we will call the base network222We use the VGG-16 network as a base, but other networks should also produce good results.. We then add auxiliary structure to the network to produce detections with the following key features:"
Increasing the input size improved the detection for small objects. Is this true?,No,"According to above evidential sentence, the answer is True.","To understand the performance of our two SSD models in more details, we used the detection analysis tool from [21]. Figure 3 shows that SSD can detect various object categories with high quality (large white area). The majority of its confident detections are correct. The recall is around 85-90%, and is much higher with “weak” (0.1 jaccard overlap) criteria. Compared to R-CNN [22], SSD has less localization error, indicating that SSD can localize objects better because it directly learns to regress the object shape and classify object categories instead of using two decoupled steps. However, SSD has more confusions with similar object categories (especially for animals), partly because we share locations for multiple categories. Figure 4 shows that SSD is very sensitive to the bounding box size. In other words, it has much worse performance on smaller objects than bigger objects. This is not surprising because those small objects may not even have any information at the very top layers. Increasing the input size (e.g. from 300\times 300 to 512\times 512) can help improve detecting small objects, but there is still a lot of room to improve. On the positive side, we can clearly see that SSD performs really well on large objects. And it is very robust to different object aspect ratios because we use default boxes of various aspect ratios per feature map location."
Did the authors use different sampling strategies than F-RCNN or other detection algorithms?,"Sure! Here's the answer to your question based on the provided context:

Yes, the authors used a different sampling strategy than F-RCNN and other detection algorithms","Yes, the authors used YOLO as a sampling strategy.","Data augmentation is crucial. Fast and Faster R-CNN use the original image and the horizontal flip to train. We use a more extensive sampling strategy, similar to YOLO [5]. Table 2 shows that we can improve 8.8% mAP with this sampling strategy. We do not know how much our sampling strategy will benefit Fast and Faster R-CNN, but they are likely to benefit less because they use a feature pooling step during classification that is relatively robust to object translation by design."
Why did the authors use multi-scale feature maps for detection?,"To allow predictions of detections at multiple scales, enabling the model to capture objects of varying sizes and distances from the camera",Authors used multi-scale feature maps for detection because they allow predictions of detections at multiple scales.,Multi-scale feature maps for detection We add convolutional feature layers to the end of the truncated base network. These layers decrease in size progressively and allow predictions of detections at multiple scales. The convolutional model for predicting detections is different for each feature layer (cf Overfeat[4] and YOLO[5] that operate on a single scale feature map).
Compare the detection feature map of both of single shot detectors (SSD and YOLO) ? ,"Both SSD and YOLO use multi-scale feature maps for detection, but they differ in the number of scales and the size of the feature maps. SSD uses a larger number of scales with smaller feature maps, while YOLO uses a smaller number of scales with larger feature maps",SSD uses multi-scale feature map while YOLO operates on single scale feature map.,Multi-scale feature maps for detection We add convolutional feature layers to the end of the truncated base network. These layers decrease in size progressively and allow predictions of detections at multiple scales. The convolutional model for predicting detections is different for each feature layer (cf Overfeat[4] and YOLO[5] that operate on a single scale feature map).
How to reinject the missed predictions to help SSD to learn from negative predictions ? ,"Sure! Here's the answer to your question based on the provided context:

Reinjecting the missed predictions can help SSD learn from negative predictions by providing more diverse and balanced training examples",The negative samples are sorted using highest confidence loss for each default box and the top ones are picked. This is how missed predictions are reinjected to help SSD learn from negative predictions.,"After the matching step, most of the default boxes are negatives, especially when the number of possible default boxes is large. This introduces a significant imbalance between the positive and negative training examples. Instead of using all the negative examples, we sort them using the highest confidence loss for each default box and pick the top ones so that the ratio between the negatives and positives is at most 3:1. We found that this leads to faster optimization and a more stable training."
Increasing default box shape will increase in model performance. How many default boxes are used in the SSD framework?,6 default boxes per location,"In SSD framework, generally 6 default boxes per location are used.","More default box shapes is better. As described in Sec. 2.2.3, by default we use 6 default boxes per location. If we remove the boxes with \frac{1}{3} and 3 aspect ratios, the performance drops by 0.6%. By further removing the boxes with \frac{1}{2} and 2 aspect ratios, the performance drops another 2.1%. Using a variety of default box shapes seems to make the task of predicting boxes easier for the network."
"What do you mean by NMS(Non-Maximal Suppression) ?
","Non-Maximal Suppression (NMS) is a step in the SSD object detection algorithm where the output bounding boxes and scores are filtered to retain only the most likely detections, based on a threshold value. This helps to reduce the number of false positives and improve the accuracy of the detections",NMS is used in SSD to get the final predictions. The definition and process of NMS cannot be answered in this paper.,"The SSD approach is based on a feed-forward convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum suppression step to produce the final detections. The early network layers are based on a standard architecture used for high quality image classification (truncated before any classification layers), which we will call the base network222We use the VGG-16 network as a base, but other networks should also produce good results.. We then add auxiliary structure to the network to produce detections with the following key features:"
What input size images were tested for SSD experiments? ,300x300,SSD experiments were tested for images with input size 300x300.,"To understand SSD better, we carried out controlled experiments to examine how each component affects performance. For all the experiments, we use the same settings and input size (300\times 300), except for specified changes to the settings or component(s)."
"What did the author mean by “Hard Negative Mining”?
","The author referred to selecting only the most difficult negative examples, or those with the highest confidence loss, for training to address the imbalance issue in object detection","When the number of available default boxes is high, the majority of the default boxes after the matching phase are negatives. As a result, there is now a sizable imbalance between the training instances that are good and negative. The biggest confidence loss for each default box is used to order the negative instances, and the top ones are chosen so that the ratio of negative to positive examples is no greater than 3:1. This process is called as Hard Negative Mining.","After the matching step, most of the default boxes are negatives, especially when the number of possible default boxes is large. This introduces a significant imbalance between the positive and negative training examples. Instead of using all the negative examples, we sort them using the highest confidence loss for each default box and pick the top ones so that the ratio between the negatives and positives is at most 3:1. We found that this leads to faster optimization and a more stable training."
How scales of the default bounding boxes for a particular feature map is computed? ,"The scales of the default bounding boxes for a particular feature map are computed as:

s_k = s_min + (s_max - s_min) / (m-1)(k-1)

where s_min and s_max are the minimum and maximum scales, respectively, and m is the number of feature maps","The scale of the default boxes for each feature map is computed as:s_{k}=s_{\text{min}}+\frac{s_{\text{max}}-s_{\text{min}}}{m-1}(k-1),\quad k\in[1,m](4)where s_{\text{min}} is 0.2 and s_{\text{max}} is 0.9, meaning the lowest layer has a scale of 0.2 and the highest layer has a scale of 0.9, and all layers in between are regularly spaced.","Feature maps from different levels within a network are known to have different (empirical) receptive field sizes [13]. Fortunately, within the SSD framework, the default boxes do not necessary need to correspond to the actual receptive fields of each layer.We design the tiling of default boxes so that specific feature maps learn to be responsive to particular scales of the objects. Suppose we want to use m feature maps for prediction. The scale of the default boxes for each feature map is computed as:s_{k}=s_{\text{min}}+\frac{s_{\text{max}}-s_{\text{min}}}{m-1}(k-1),\quad k\in[1,m](4)where s_{\text{min}} is 0.2 and s_{\text{max}} is 0.9, meaning the lowest layer has a scale of 0.2 and the highest layer has a scale of 0.9, and all layers in between are regularly spaced. We impose different aspect ratios for the default boxes, and denote them as a_{r}\in\{1,2,3,\frac{1}{2},\frac{1}{3}\}. We can compute the width (w_{k}^{a}=s_{k}\sqrt{a_{r}}) and height (h_{k}^{a}=s_{k}/\sqrt{a_{r}}) for each default box. For the aspect ratio of 1, we also add a default box whose scale is s^{\prime}_{k}=\sqrt{s_{k}s_{k+1}}, resulting in 6 default boxes per feature map location. We set the center of each default box to (\frac{i+0.5}{|f_{k}|},\frac{j+0.5}{|f_{k}|}), where |f_{k}| is the size of the k-th square feature map, i,j\in[0,|f_{k}|). In practice, one can also design a distribution of default boxes to best fit a specific dataset. How to design the optimal tiling is an open question as well."
"How did SSD handle Object localization better than F-CNN ? 
","SSD directly learns to regress object shapes and classify object categories, resulting in less localization error compared to F-CNN",SSD does better object localization than F-CNN because directly learns to regress the object shape and classify object categories instead of using two decoupled steps.,"To understand the performance of our two SSD models in more details, we used the detection analysis tool from [21]. Figure 3 shows that SSD can detect various object categories with high quality (large white area). The majority of its confident detections are correct. The recall is around 85-90%, and is much higher with “weak” (0.1 jaccard overlap) criteria. Compared to R-CNN [22], SSD has less localization error, indicating that SSD can localize objects better because it directly learns to regress the object shape and classify object categories instead of using two decoupled steps. However, SSD has more confusions with similar object categories (especially for animals), partly because we share locations for multiple categories. Figure 4 shows that SSD is very sensitive to the bounding box size. In other words, it has much worse performance on smaller objects than bigger objects. This is not surprising because those small objects may not even have any information at the very top layers. Increasing the input size (e.g. from 300\times 300 to 512\times 512) can help improve detecting small objects, but there is still a lot of room to improve. On the positive side, we can clearly see that SSD performs really well on large objects. And it is very robust to different object aspect ratios because we use default boxes of various aspect ratios per feature map location."
What features of the SSD algorithm contributed to major improvements in detection speed ?,"The features of the SSD algorithm that contributed to major improvements in detection speed are:

1. Eliminating bounding box proposals and pixel resampling, which reduces computational cost and allows for faster processing.
2. Using a small convolutional filter to predict object categories and offsets in bounding box locations.
3. Using separate predictors (filters) for different aspect ratio detections.
4. Applying these filters to multiple feature maps from later stages of the network to perform detection at multiple scales","According to authors, the improvement in speed of SSD algorithm comes from eliminating bounding box proposals and the subsequent pixel or feature resampling stage. They have used a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales. With these modifications—especially using multiple layers for prediction at different scales—thye achieved high-accuracy using relatively low resolution input, further increasing detection speed.","This paper presents the first deep network based object detector that does not resample pixels or features for bounding box hypotheses and and is as accurate as approaches that do. This results in a significant improvement in speed for high-accuracy detection (59 FPS with mAP 74.3% on VOC2007 test, vs. Faster R-CNN 7 FPS with mAP 73.2% or YOLO 45 FPS with mAP 63.4%). The fundamental improvement in speed comes from eliminating bounding box proposals and the subsequent pixel or feature resampling stage. We are not the first to do this (cf [4, 5]), but by adding a series of improvements, we manage to increase the accuracy significantly over previous attempts. Our improvements include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales. With these modifications—especially using multiple layers for prediction at different scales—we can achieve high-accuracy using relatively low resolution input, further increasing detection speed. While these contributions may seem small independently, we note that the resulting system improves accuracy on real-time detection for PASCAL VOC from 63.4% mAP for YOLO to 74.3% mAP for our SSD. This is a larger relative improvement in detection accuracy than that from the recent, very high-profile work on residual networks [3]. Furthermore, significantly improving the speed of high-quality detection can broaden the range of settings where computer vision is useful."
How did the authors manage the model fast and accurately enough for real-time applications?,"By eliminating bounding box proposals and resampling stages, and adding improvements such as using small convolutional filters for object category and offset predictions, separate predictors for different aspect ratios, and applying filters to multiple feature maps for detection at multiple scales","Authors did the following things to manage fast and accurate model for real-time applications: a) A deep network based object detector that does not resample pixels or features for bounding box hypotheses and and is as accurate as approaches that do. b) Elimination of bounding box proposals and the subsequent pixel or feature resampling stage. c) Using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales.","This paper presents the first deep network based object detector that does not resample pixels or features for bounding box hypotheses and and is as accurate as approaches that do. This results in a significant improvement in speed for high-accuracy detection (59 FPS with mAP 74.3% on VOC2007 test, vs. Faster R-CNN 7 FPS with mAP 73.2% or YOLO 45 FPS with mAP 63.4%). The fundamental improvement in speed comes from eliminating bounding box proposals and the subsequent pixel or feature resampling stage. We are not the first to do this (cf [4, 5]), but by adding a series of improvements, we manage to increase the accuracy significantly over previous attempts. Our improvements include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales. With these modifications—especially using multiple layers for prediction at different scales—we can achieve high-accuracy using relatively low resolution input, further increasing detection speed. While these contributions may seem small independently, we note that the resulting system improves accuracy on real-time detection for PASCAL VOC from 63.4% mAP for YOLO to 74.3% mAP for our SSD. This is a larger relative improvement in detection accuracy than that from the recent, very high-profile work on residual networks [3]. Furthermore, significantly improving the speed of high-quality detection can broaden the range of settings where computer vision is useful."
"How does the SSD match the default bounding box with ground truth ones?
",SSD matches the default bounding box with ground truth ones using a combination of Jaccard overlap and a threshold-based matching strategy,"To match the ground truth box with default box, authors used Best Jaccard Overlap. The default boxes are matched with any ground truth box with jaccard overlap higher than a threshold which is 0.5. The SSD training objective is derived from the MultiBox objective [7, 8] but is extended to handle multiple object categories. The process to match the bxoes is - Let x_{ij}^{p}=\{1,0\} be an indicator for matching the i-th default box to the j-th ground truth box of category p.In the matching strategy above, we can have \sum_{i}x_{ij}^{p}\geq 1.The overall objective loss function is a weighted sum of the localization loss (loc) and the confidence loss (conf):L(x,c,l,g)=\frac{1}{N}(L_{conf}(x,c)+\alpha L_{loc}(x,l,g))(1)where N is the number of matched default boxes. If N=0, wet set the loss to 0. The localization loss is a Smooth L1 loss [6] between the predicted box (l) and the ground truth box (g) parameters.","The SSD training objective is derived from the MultiBox objective [7, 8] but is extended to handle multiple object categories. Let x_{ij}^{p}=\{1,0\} be an indicator for matching the i-th default box to the j-th ground truth box of category p.In the matching strategy above, we can have \sum_{i}x_{ij}^{p}\geq 1.The overall objective loss function is a weighted sum of the localization loss (loc) and the confidence loss (conf):L(x,c,l,g)=\frac{1}{N}(L_{conf}(x,c)+\alpha L_{loc}(x,l,g))(1)where N is the number of matched default boxes. If N=0, wet set the loss to 0. The localization loss is a Smooth L1 loss [6] between the predicted box (l) and the ground truth box (g) parameters. Similar to Faster R-CNN [2], we regress to offsets for the center (cx,cy) of the default bounding box (d) and for its width (w) and height (h).\begin{split}L_{loc}(x,l,g)=\sum_{i\in Pos}^{N}\sum_{m\in\{cx,cy,w,h\}}&x_{ij}^{k}\text{smooth}_{\text{L1}}(l_{i}^{m}-\hat{g}_{j}^{m})\\\hat{g}_{j}^{cx}=(g_{j}^{cx}-d_{i}^{cx})/d_{i}^{w}\quad\quad&\hat{g}_{j}^{cy}=(g_{j}^{cy}-d_{i}^{cy})/d_{i}^{h}\\\hat{g}_{j}^{w}=\log\Big{(}\frac{g_{j}^{w}}{d_{i}^{w}}\Big{)}\quad\quad&\hat{g}_{j}^{h}=\log\Big{(}\frac{g_{j}^{h}}{d_{i}^{h}}\Big{)}\end{split}(2)The confidence loss is the softmax loss over multiple classes confidences (c).L_{conf}(x,c)=-\sum_{i\in Pos}^{N}x_{ij}^{p}log(\hat{c}_{i}^{p})-\sum_{i\in Neg}log(\hat{c}_{i}^{0})\quad\text{where}\quad\hat{c}_{i}^{p}=\frac{\exp(c_{i}^{p})}{\sum_{p}\exp(c_{i}^{p})}(3)and the weight term \alpha is set to 1 by cross validation. During training we need to determine which default boxes correspond to a ground truth detection and train the network accordingly. For each ground truth box we are selecting from default boxes that vary over location, aspect ratio, and scale. We begin by matching each ground truth box to the default box with the best jaccard overlap (as in MultiBox [7]). Unlike MultiBox, we then match default boxes to any ground truth with jaccard overlap higher than a threshold (0.5). This simplifies the learning problem, allowing the network to predict high scores for multiple overlapping default boxes rather than requiring it to pick only the one with maximum overlap."
How SSD will predict boundary boxes after training as there is no ground truth anymore? ,"SSD will predict boundary boxes based on the output of the convolutional network, without relying on ground truth boundaries","After training, SSD predicts the boundary box by doing a non-maximum suppression on boundary boxes with the presence of object class instance.","The SSD approach is based on a feed-forward convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum suppression step to produce the final detections. The early network layers are based on a standard architecture used for high quality image classification (truncated before any classification layers), which we will call the base network222We use the VGG-16 network as a base, but other networks should also produce good results.. We then add auxiliary structure to the network to produce detections with the following key features:"
"SSD adds six auxiliary convolution layers after the VGG16. In three of those layers, we make 6 predictions instead of 4(YOLO does). Why is the case? ",To increase the resolution of the feature maps and improve object detection accuracy,"SSD adds 6 auxiliary convolution layers after VGG16 to produce detections with following key features: a) Multi-scale feature maps for detection b) Convolutional predictors for detection
c) Default boxes and aspect ratios","The SSD approach is based on a feed-forward convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum suppression step to produce the final detections. The early network layers are based on a standard architecture used for high quality image classification (truncated before any classification layers), which we will call the base network222We use the VGG-16 network as a base, but other networks should also produce good results.. We then add auxiliary structure to the network to produce detections with the following key features:"
"Are default box and predicted boxes are different?

","Yes, default boxes and predicted boxes are different","The default boxes are used during training to tune the model's weights. On the other hand, predicted boxes are compared with default boxes to optimise the model. There is only one predicted box but the default box number can be huge. So, the default and predicted boxes are different.","By combining predictions for all default boxes with different scales and aspect ratios from all locations of many feature maps, we have a diverse set of predictions, covering various input object sizes and shapes. For example, in Fig. 1, the dog is matched to a default box in the 4\times 4 feature map, but not to any default boxes in the 8\times 8 feature map. This is because those boxes have different scales and do not match the dog box, and therefore are considered as negatives during training. After the matching step, most of the default boxes are negatives, especially when the number of possible default boxes is large. This introduces a significant imbalance between the positive and negative training examples. Instead of using all the negative examples, we sort them using the highest confidence loss for each default box and pick the top ones so that the ratio between the negatives and positives is at most 3:1. We found that this leads to faster optimization and a more stable training. During training we need to determine which default boxes correspond to a ground truth detection and train the network accordingly. For each ground truth box we are selecting from default boxes that vary over location, aspect ratio, and scale. We begin by matching each ground truth box to the default box with the best jaccard overlap (as in MultiBox [7]). Unlike MultiBox, we then match default boxes to any ground truth with jaccard overlap higher than a threshold (0.5). This simplifies the learning problem, allowing the network to predict high scores for multiple overlapping default boxes rather than requiring it to pick only the one with maximum overlap."
How input image resolution affects the accuracy of the SSD framework?,"The SSD framework achieves high accuracy even with low resolution input images, improving the speed vs accuracy trade-off",The accuracy of SSD framework is relatively more on higher resolution images than on lower resolution images.,"This paper presents the first deep network based object detector that does not resample pixels or features for bounding box hypotheses and and is as accurate as approaches that do. This results in a significant improvement in speed for high-accuracy detection (59 FPS with mAP 74.3% on VOC2007 test, vs. Faster R-CNN 7 FPS with mAP 73.2% or YOLO 45 FPS with mAP 63.4%). The fundamental improvement in speed comes from eliminating bounding box proposals and the subsequent pixel or feature resampling stage. We are not the first to do this (cf [4, 5]), but by adding a series of improvements, we manage to increase the accuracy significantly over previous attempts. Our improvements include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales. With these modifications—especially using multiple layers for prediction at different scales—we can achieve high-accuracy using relatively low resolution input, further increasing detection speed. While these contributions may seem small independently, we note that the resulting system improves accuracy on real-time detection for PASCAL VOC from 63.4% mAP for YOLO to 74.3% mAP for our SSD. This is a larger relative improvement in detection accuracy than that from the recent, very high-profile work on residual networks [3]. Furthermore, significantly improving the speed of high-quality detection can broaden the range of settings where computer vision is useful. We summarize our contributions as follows:•We introduce SSD, a single-shot detector for multiple categories that is faster than the previous state-of-the-art for single shot detectors (YOLO), and significantly more accurate, in fact as accurate as slower techniques that perform explicit region proposals and pooling (including Faster R-CNN).•The core of SSD is predicting category scores and box offsets for a fixed set of default bounding boxes using small convolutional filters applied to feature maps.•To achieve high detection accuracy we produce predictions of different scales from feature maps of different scales, and explicitly separate predictions by aspect ratio.•These design features lead to simple end-to-end training and high accuracy, even on low resolution input images, further improving the speed vs accuracy trade-off.•Experiments include timing and accuracy analysis on models with varying input size evaluated on PASCAL VOC, COCO, and ILSVRC and are compared to a range of recent state-of-the-art approaches."
Is channel shuffle operation is similiar to that of random sparse convolution?,"No. While both channel shuffle and random sparse convolution involve rearranging the channel dimensions of a convolutional layer, they serve different purposes and have different effects on the network's performance. Random sparse convolution is designed to reduce the computational cost of convolutional layers by randomly dropping out channels, whereas channel shuffle is used to enable cross-group information flow and improve the performance of multiple group convolutional layers","The group convolution and channel shuffle are clearly described and evaluated in the paper. The authors claim that random scarce convolution is similar to channel shuffle with group convolution. However, since the purpose of random scarce convolution is different and is not described, it is hard to tell just by the paper, how exactly they are similar.","If we allow group convolution to obtain input data from different groups (as shown in Fig 1 (b)), the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups. This can be efficiently and elegantly implemented by a channel shuffle operation (Fig 1 (c)): suppose a convolutional layer with g groups whose output has g\times n channels; we first reshape the output channel dimension into (g,n), transposing and then flattening it back as the input of next layer. Note that the operation still takes effect even if the two convolutions have different numbers of groups. Moreover, channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training. Channel shuffle operation makes it possible to build more powerful structures with multiple group convolutional layers. In the next subsection we will introduce an efficient network unit with channel shuffle and group convolution. The purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. Table 3 compares the performance of ShuffleNet structures (group number is set to 3 or 8 for instance) with/without channel shuffle. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g=8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange. To the best of our knowledge, the idea of channel shuffle operation is rarely mentioned in previous work on efficient model design, although CNN library cuda-convnet [20] supports “random sparse convolution” layer, which is equivalent to random channel shuffle followed by a group convolutional layer. Such “random shuffle” operation has different purpose and been seldom exploited later. Very recently, another concurrent work  [41] also adopt this idea for a two-stage convolution. However,  [41] did not specially investigate the effectiveness of channel shuffle itself and its usage in tiny model design."
How channel shuffle operation works for two groups?,"Channel shuffle operation for two groups involves dividing the channels in each group into subgroups, feeding each group in the next layer with different subgroups, and then transposing and flattening the output. This allows for efficient and elegant information exchange between groups, improving the performance of the network","In the case of channel shuffle operation for two groups, each group is divided into two and shuffled so each new group has a subgroup from both old groups. For example, |A|B| -> |aa|bb| -> |ab|ab|. In terms of performance, two groups seem to work consistently better than the single group case and consistently worse than having more than 2 groups.","If we allow group convolution to obtain input data from different groups (as shown in Fig 1 (b)), the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups. This can be efficiently and elegantly implemented by a channel shuffle operation (Fig 1 (c)): suppose a convolutional layer with g groups whose output has g\times n channels; we first reshape the output channel dimension into (g,n), transposing and then flattening it back as the input of next layer. Note that the operation still takes effect even if the two convolutions have different numbers of groups. Moreover, channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training. In ShuffleNet units, group number g controls the connection sparsity of pointwise convolutions. Table 1 explores different group numbers and we adapt the output channels to ensure overall computation cost roughly unchanged (\sim140 MFLOPs). Obviously, larger group numbers result in more output channels (thus more convolutional filters) for a given complexity constraint, which helps to encode more information, though it might also lead to degradation for an individual convolutional filter due to limited corresponding input channels. In Sec 4.1.1 we will study the impact of this number subject to different computational constrains. Table 2 also shows that for some models (e.g. ShuffleNet 0.5\times) when group numbers become relatively large (e.g. g=8), the classification score saturates or even drops. With an increase in group number (thus wider feature maps), input channels for each convolutional filter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as ShuffleNet 0.25\times larger group numbers tend to better results consistently, which suggests wider feature maps bring more benefits for smaller models. The purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. Table 3 compares the performance of ShuffleNet structures (group number is set to 3 or 8 for instance) with/without channel shuffle. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g=8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange."
Why it is possible to say that multiple group convolutional layers works efficiently without weakening representation?,"Channel shuffle operation.

Multiple group convolutional layers can work efficiently without weakening representation because the channel shuffle operation enables cross-group information flow, allowing the model to capture more information and maintain representation","It is clearly stated in the paper that having group convolutions is a trade-off between representative capability and the computational cost of the model. The ShuffleNet allows stacking multiple group convolutions with an appropriate number of groups because of channel shuffle and it is empirically shown in the paper. However, it is also noted that having too many groups might sometimes damage the performance. Thus, multiple group convolutions work efficiently only when the number of groups is chosen carefully and channel shuffle is used.","If we allow group convolution to obtain input data from different groups (as shown in Fig 1 (b)), the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups. This can be efficiently and elegantly implemented by a channel shuffle operation (Fig 1 (c)): suppose a convolutional layer with g groups whose output has g\times n channels; we first reshape the output channel dimension into (g,n), transposing and then flattening it back as the input of next layer. Note that the operation still takes effect even if the two convolutions have different numbers of groups. Moreover, channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training. From the results, we see that models with group convolutions (g>1) consistently perform better than the counterparts without pointwise group convolutions (g=1). Smaller models tend to benefit more from groups. For example, for ShuffleNet 1\times the best entry (g=8) is 1.2% better than the counterpart, while for ShuffleNet 0.5\times and 0.25\times the gaps become 3.5% and 4.4% respectively. Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps. Table 2 also shows that for some models (e.g. ShuffleNet 0.5\times) when group numbers become relatively large (e.g. g=8), the classification score saturates or even drops. With an increase in group number (thus wider feature maps), input channels for each convolutional filter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as ShuffleNet 0.25\times larger group numbers tend to better results consistently, which suggests wider feature maps bring more benefits for smaller models. The purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. Table 3 compares the performance of ShuffleNet structures (group number is set to 3 or 8 for instance) with/without channel shuffle. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g=8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange. The concept of group convolution, which was first introduced in AlexNet [21] for distributing the model over two GPUs, has been well demonstrated its effectiveness in ResNeXt [40]. Depthwise separable convolution proposed in Xception [3] generalizes the ideas of separable convolutions in Inception series [34, 32]. Recently, MobileNet [12] utilizes the depthwise separable convolutions and gains state-of-the-art results among lightweight models. Our work generalizes group convolution and depthwise separable convolution in a novel form. Modern convolutional neural networks [30, 33, 34, 32, 9, 10] usually consist of repeated building blocks with the same structure. Among them, state-of-the-art networks such as Xception [3] and ResNeXt [40] introduce efficient depthwise separable convolutions or group convolutions into the building blocks to strike an excellent trade-off between representation capability and computational cost. However, we notice that both designs do not fully take the 1\times 1 convolutions (also called pointwise convolutions in  [12]) into account, which require considerable complexity. For example, in ResNeXt [40] only 3\times 3 layers are equipped with group convolutions. As a result, for each residual unit in ResNeXt the pointwise convolutions occupy 93.4% multiplication-adds (cardinality = 32 as suggested in  [40]). In tiny networks, expensive pointwise convolutions result in limited number of channels to meet the complexity constraint, which might significantly damage the accuracy. To address the issue, a straightforward solution is to apply channel sparse connections, for example group convolutions, also on 1\times 1 layers. By ensuring that each convolution operates only on the corresponding input channel group, group convolution significantly reduces computation cost. However, if multiple group convolutions stack together, there is one side effect: outputs from a certain channel are only derived from a small fraction of input channels. Fig 1 (a) illustrates a situation of two stacked group convolution layers. It is clear that outputs from a certain group only relate to the inputs within the group. This property blocks information flow between channel groups and weakens representation."
"What does s in ""ShuffleNet s x"" mean? ","Sure! Here's my answer:

s is a scale factor that represents the number of times the number of filters in the original ShuffleNet network is scaled up","""s"" means the scale factor by which the number of channels is multiplied to adapt the ShuffleNet to the given computational complexity.","To customize the network to a desired complexity, we can simply apply a scale factor s on the number of channels. For example, we denote the networks in Table 1 as ”ShuffleNet 1\times”, then ”ShuffleNet s×s\timesitalic_s ×” means scaling the number of filters in ShuffleNet 1\times by s times thus overall complexity will be roughly s^{2} times of ShuffleNet 1\times."
How is complexity calculated given scale factor of the ShuffleNet model? Given scale factor 0.25 and complexity of ShuffleNet 1x is 140 MFLOPS,"Sure! Here's the answer to your question based on the provided context:

The complexity of the ShuffleNet model with a scale factor of 0.25 is approximately 35 MFLOPS (25 x 140 MFLOPS)","As it is shown in Table 2, the complexity of ShufflNet 0.25x will be 13 MFLOPs.","To customize the network to a desired complexity, we can simply apply a scale factor s on the number of channels. For example, we denote the networks in Table 1 as ”ShuffleNet 1\times”, then ”ShuffleNet s×s\timesitalic_s ×” means scaling the number of filters in ShuffleNet 1\times by s times thus overall complexity will be roughly s^{2} times of ShuffleNet 1\times. Table 2. Classification error vs. number of groups g (smaller number represents better performance)"
"For channel shuffle, were they applied for first pointwise convolution or second ?",Second,The channel shuffle in the ShuffleNet unit occurs only after the first pointwise group convolution.,"If we allow group convolution to obtain input data from different groups (as shown in Fig 1 (b)), the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups. This can be efficiently and elegantly implemented by a channel shuffle operation (Fig 1 (c)): suppose a convolutional layer with g groups whose output has g\times n channels; we first reshape the output channel dimension into (g,n), transposing and then flattening it back as the input of next layer. Note that the operation still takes effect even if the two convolutions have different numbers of groups. Moreover, channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training. Taking advantage of the channel shuffle operation, we propose a novel ShuffleNet unit specially designed for small networks. We start from the design principle of bottleneck unit [9] in Fig 2 (a). It is a residual block. In its residual branch, for the 3\times 3 layer, we apply a computational economical 3\times 3 depthwise convolution [3] on the bottleneck feature map. Then, we replace the first 1\times 1 layer with pointwise group convolution followed by a channel shuffle operation, to form a ShuffleNet unit, as shown in Fig 2 (b). The purpose of the second pointwise group convolution is to recover the channel dimension to match the shortcut path. For simplicity, we do not apply an extra channel shuffle operation after the second pointwise layer as it results in comparable scores. The usage of batch normalization (BN) [15] and nonlinearity is similar to  [9, 40], except that we do not use ReLU after depthwise convolution as suggested by  [3]. As for the case where ShuffleNet is applied with stride, we simply make two modifications (see Fig 2 (c)): (i) add a 3\times 3 average pooling on the shortcut path; (ii) replace the element-wise addition with channel concatenation, which makes it easy to enlarge channel dimension with little extra computation cost. The purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. Table 3 compares the performance of ShuffleNet structures (group number is set to 3 or 8 for instance) with/without channel shuffle. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g=8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange. To the best of our knowledge, the idea of channel shuffle operation is rarely mentioned in previous work on efficient model design, although CNN library cuda-convnet [20] supports “random sparse convolution” layer, which is equivalent to random channel shuffle followed by a group convolutional layer. Such “random shuffle” operation has different purpose and been seldom exploited later. Very recently, another concurrent work  [41] also adopt this idea for a two-stage convolution. However,  [41] did not specially investigate the effectiveness of channel shuffle itself and its usage in tiny model design."
What is the activation function for a ShuffleNet Unit?,The activation function for a ShuffleNet unit is pointwise group convolution followed by a channel shuffle operation,"The use of activation functions in the ShuffleNet unit happens only after the first 1x1 group convolution and the last concatenation of shortcut and residual paths, following the suggestions of referenced papers [3, 9, 40]. And the only non-linear activation function that is used is ReLU.","Taking advantage of the channel shuffle operation, we propose a novel ShuffleNet unit specially designed for small networks. We start from the design principle of bottleneck unit [9] in Fig 2 (a). It is a residual block. In its residual branch, for the 3\times 3 layer, we apply a computational economical 3\times 3 depthwise convolution [3] on the bottleneck feature map. Then, we replace the first 1\times 1 layer with pointwise group convolution followed by a channel shuffle operation, to form a ShuffleNet unit, as shown in Fig 2 (b). The purpose of the second pointwise group convolution is to recover the channel dimension to match the shortcut path. For simplicity, we do not apply an extra channel shuffle operation after the second pointwise layer as it results in comparable scores. The usage of batch normalization (BN) [15] and nonlinearity is similar to  [9, 40], except that we do not use ReLU after depthwise convolution as suggested by  [3]. As for the case where ShuffleNet is applied with stride, we simply make two modifications (see Fig 2 (c)): (i) add a 3\times 3 average pooling on the shortcut path; (ii) replace the element-wise addition with channel concatenation, which makes it easy to enlarge channel dimension with little extra computation cost. Figure 2. ShuffleNet Units. a) bottleneck unit [9] with depthwise convolution (DWConv) [3, 12]; b) ShuffleNet unit with pointwise group"
"Given input size (3x224x224) and bottleneck channels being 64, compare the computational complexity between ResNet, ResNext and ShuffleNet.","ShuffleNet has the lowest computational complexity compared to ResNet and ResNeXt, given an input size of (3x224x224) and bottleneck channels of 64","ResNet: hw(2cm+9m^{2}) = 244^2*2 * 3 * 64 + 244^2*9 * 64^2 = 2 217 596 928
ResNeXt: hw(2cm+9m^{2}/g) = 244^2*2 * 3 * 64 + 244^2*9 * 64^2/g = 22 861 824 + 2 194 735 104 / g
ShuffleNet: hw(2cm/g + 9m) = 244^2*2 * 3 * 64 / g + 244^2*9 * 64 = 22 861 824 / g + 34 292 736
Even with the group size of 1 (g=1), ShuffleNet have much less complexity compared to ResNet and ResNeXt.","Thanks to pointwise group convolution with channel shuffle, all components in ShuffleNet unit can be computed efficiently. Compared with ResNet [9] (bottleneck design) and ResNeXt [40], our structure has less complexity under the same settings. For example, given the input size c\times h\times w and the bottleneck channels m, ResNet unit requires hw(2cm+9m^{2}) FLOPs and ResNeXt has hw(2cm+9m^{2}/g) FLOPs, while our ShuffleNet unit requires only hw(2cm/g+9m) FLOPs, where g means the number of groups for convolutions. In other words, given a computational budget, ShuffleNet can use wider feature maps. We find this is critical for small networks, as tiny networks usually have an insufficient number of channels to process the information."
How pointwise group convolutions is different from 1x1 convolutions?,"Pointwise group convolutions are different from 1x1 convolutions in terms of channel sparsity and information flow. While 1x1 convolutions operate on all channels simultaneously, pointwise group convolutions only operate on a specific group of channels, leading to reduced computation cost but potentially limited information flow between channel groups","The group convolution divides the channels into groups and applies the convolution only within the groups, thus reducing the computational complexity of 1x1 convolutions. However, when several group convolutions are stacked together it may block the information flow and weaken the representation.","We notice that state-of-the-art basic architectures such as Xception [3] and ResNeXt [40] become less efficient in extremely small networks because of the costly dense 1\times 1 convolutions. We propose using pointwise group convolutions to reduce computation complexity of 1\times 1 convolutions. To overcome the side effects brought by group convolutions, we come up with a novel channel shuffle operation to help the information flowing across feature channels. Based on the two techniques, we build a highly efficient architecture called ShuffleNet. Compared with popular structures like  [30, 9, 40], for a given computation complexity budget, our ShuffleNet allows more feature map channels, which helps to encode more information and is especially critical to the performance of very small networks. From the results, we see that models with group convolutions (g>1) consistently perform better than the counterparts without pointwise group convolutions (g=1). Smaller models tend to benefit more from groups. For example, for ShuffleNet 1\times the best entry (g=8) is 1.2% better than the counterpart, while for ShuffleNet 0.5\times and 0.25\times the gaps become 3.5% and 4.4% respectively. Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps. The concept of group convolution, which was first introduced in AlexNet [21] for distributing the model over two GPUs, has been well demonstrated its effectiveness in ResNeXt [40]. Depthwise separable convolution proposed in Xception [3] generalizes the ideas of separable convolutions in Inception series [34, 32]. Recently, MobileNet [12] utilizes the depthwise separable convolutions and gains state-of-the-art results among lightweight models. Our work generalizes group convolution and depthwise separable convolution in a novel form. Modern convolutional neural networks [30, 33, 34, 32, 9, 10] usually consist of repeated building blocks with the same structure. Among them, state-of-the-art networks such as Xception [3] and ResNeXt [40] introduce efficient depthwise separable convolutions or group convolutions into the building blocks to strike an excellent trade-off between representation capability and computational cost. However, we notice that both designs do not fully take the 1\times 1 convolutions (also called pointwise convolutions in  [12]) into account, which require considerable complexity. For example, in ResNeXt [40] only 3\times 3 layers are equipped with group convolutions. As a result, for each residual unit in ResNeXt the pointwise convolutions occupy 93.4% multiplication-adds (cardinality = 32 as suggested in  [40]). In tiny networks, expensive pointwise convolutions result in limited number of channels to meet the complexity constraint, which might significantly damage the accuracy. To address the issue, a straightforward solution is to apply channel sparse connections, for example group convolutions, also on 1\times 1 layers. By ensuring that each convolution operates only on the corresponding input channel group, group convolution significantly reduces computation cost. However, if multiple group convolutions stack together, there is one side effect: outputs from a certain channel are only derived from a small fraction of input channels. Fig 1 (a) illustrates a situation of two stacked group convolution layers. It is clear that outputs from a certain group only relate to the inputs within the group. This property blocks information flow between channel groups and weakens representation."
"What are the side effects of group convolution?
","Side effects of group convolution include:

1. Reduced information flow between channel groups, which can weaken representation.
2. Outputs from a certain group only relate to the inputs within the group, limiting the information captured",The side effects of group convolutions are: blocked flow of information between channel groups when multiple group convolutions are combined; and damaged individual convolution filters for each group due to decreased number of input channels.,"In ShuffleNet units, group number g controls the connection sparsity of pointwise convolutions. Table 1 explores different group numbers and we adapt the output channels to ensure overall computation cost roughly unchanged (\sim140 MFLOPs). Obviously, larger group numbers result in more output channels (thus more convolutional filters) for a given complexity constraint, which helps to encode more information, though it might also lead to degradation for an individual convolutional filter due to limited corresponding input channels. In Sec 4.1.1 we will study the impact of this number subject to different computational constrains. Table 2 also shows that for some models (e.g. ShuffleNet 0.5\times) when group numbers become relatively large (e.g. g=8), the classification score saturates or even drops. With an increase in group number (thus wider feature maps), input channels for each convolutional filter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as ShuffleNet 0.25\times larger group numbers tend to better results consistently, which suggests wider feature maps bring more benefits for smaller models. To address the issue, a straightforward solution is to apply channel sparse connections, for example group convolutions, also on 1\times 1 layers. By ensuring that each convolution operates only on the corresponding input channel group, group convolution significantly reduces computation cost. However, if multiple group convolutions stack together, there is one side effect: outputs from a certain channel are only derived from a small fraction of input channels. Fig 1 (a) illustrates a situation of two stacked group convolution layers. It is clear that outputs from a certain group only relate to the inputs within the group. This property blocks information flow between channel groups and weakens representation."
Is the theoretical speedup greater than the actual speedup when comparing ShuffleNet to AlexNet on real hardware?,No. The actual speedup of ShuffleNet over AlexNet on real hardware is greater than the theoretical speedup,"Yes, while the theoretical speedup of ShuffleNet is 18 times, the actual speedup is only ~13 times, compared to the AlexNet on the real hardware.","We also examine the speedup on real hardware, i.e. an off-the-shelf ARM-based computing core. The ShuffleNet model achieves \sim13\times actual speedup (theoretical speedup is 18\times) over AlexNet [21] while maintaining comparable accuracy. Table 6 compares our ShuffleNet with a few popular models. Results show that with similar accuracy ShuffleNet is much more efficient than others. For example, ShuffleNet 0.5\times is theoretically 18\times faster than AlexNet [21] with comparable classification score. We will evaluate the actual running time in Sec 4.5. Finally, we evaluate the actual inference speed of ShuffleNet models on a mobile device with an ARM platform. Though ShuffleNets with larger group numbers (e.g. g=4 or g=8) usually have better performance, we find it less efficient in our current implementation. Empirically g=3 usually has a proper trade-off between accuracy and actual inference time. As shown in Table 8, three input resolutions are exploited for the test. Due to memory access and other overheads, we find every 4\times theoretical complexity reduction usually results in \sim2.6\times actual speedup in our implementation. Nevertheless, compared with AlexNet [21] our ShuffleNet 0.5\times model still achieves \sim13\times actual speedup under comparable classification accuracy (the theoretical speedup is 18\times), which is much faster than previous AlexNet-level models or speedup approaches such as  [14, 16, 22, 42, 43, 38]."
What do you mean by model pruning?,Model pruning refers to the process of removing redundant connections or channels in a pre-trained neural network to accelerate inference while preserving accuracy,"Although model pruning is a specific concept in deep learning, it refers to reducing the model size by removing redundant network connections or channels.","Building deeper and larger convolutional neural networks (CNNs) is a primary trend for solving major visual recognition tasks [21, 9, 33, 5, 28, 24]. The most accurate CNNs usually have hundreds of layers and thousands of channels [9, 34, 32, 40], thus requiring computation at billions of FLOPs. This report examines the opposite extreme: pursuing the best accuracy in very limited computational budgets at tens or hundreds of MFLOPs, focusing on common mobile platforms such as drones, robots, and smartphones. Note that many existing works [16, 22, 43, 42, 38, 27] focus on pruning, compressing, or low-bit representing a “basic” network architecture. Here we aim to explore a highly efficient basic architecture specially designed for our desired computing ranges. This direction aims to accelerate inference while preserving accuracy of a pre-trained model.Pruning network connections [6, 7] or channels [38] reducesredundant connections in a pre-trained model while maintaining performance.Quantization [31, 27, 39, 45, 44] andfactorization [22, 16, 18, 37] are proposed inliterature to reduce redundancy in calculations to speed up inference.Without modifying the parameters, optimized convolution algorithms implemented by FFT [25, 35] and other methods [2] decrease time consumption in practice.Distilling [11] transfers knowledge fromlarge models into small ones, which makes training small models easier."
What does the tradeoff  look like when basic network architectures are represented in low-bit computations?,"A significant tradeoff in accuracy is expected when basic network architectures are represented in low-bit computations, as the reduced precision can lead to loss of information and increased error rates","Although representing network architectures in low-bit form is mentioned as a technique for reducing the computational cost of the model, the paper does not mention anything about the tradeoff of the technique.","Building deeper and larger convolutional neural networks (CNNs) is a primary trend for solving major visual recognition tasks [21, 9, 33, 5, 28, 24]. The most accurate CNNs usually have hundreds of layers and thousands of channels [9, 34, 32, 40], thus requiring computation at billions of FLOPs. This report examines the opposite extreme: pursuing the best accuracy in very limited computational budgets at tens or hundreds of MFLOPs, focusing on common mobile platforms such as drones, robots, and smartphones. Note that many existing works [16, 22, 43, 42, 38, 27] focus on pruning, compressing, or low-bit representing a “basic” network architecture. Here we aim to explore a highly efficient basic architecture specially designed for our desired computing ranges."
How ShuffleNet allowed more feature maps for a given computational complexity?,"By using pointwise group convolutions and a novel channel shuffle operation, ShuffleNet allows more feature maps for a given computational complexity, which helps to encode more information and is especially critical to the performance of very small networks","The ShuffleNet uses pointwise group convolution with channel shuffling, thus design-wise it has less complexity (requires hw(2cm/g+9m) FLOPs). This means it allows wider feature maps for a given computational budget. And the effect seems to increase the performance better as the model gets smaller.","We notice that state-of-the-art basic architectures such as Xception [3] and ResNeXt [40] become less efficient in extremely small networks because of the costly dense 1\times 1 convolutions. We propose using pointwise group convolutions to reduce computation complexity of 1\times 1 convolutions. To overcome the side effects brought by group convolutions, we come up with a novel channel shuffle operation to help the information flowing across feature channels. Based on the two techniques, we build a highly efficient architecture called ShuffleNet. Compared with popular structures like  [30, 9, 40], for a given computation complexity budget, our ShuffleNet allows more feature map channels, which helps to encode more information and is especially critical to the performance of very small networks. Thanks to pointwise group convolution with channel shuffle, all components in ShuffleNet unit can be computed efficiently. Compared with ResNet [9] (bottleneck design) and ResNeXt [40], our structure has less complexity under the same settings. For example, given the input size c\times h\times w and the bottleneck channels m, ResNet unit requires hw(2cm+9m^{2}) FLOPs and ResNeXt has hw(2cm+9m^{2}/g) FLOPs, while our ShuffleNet unit requires only hw(2cm/g+9m) FLOPs, where g means the number of groups for convolutions. In other words, given a computational budget, ShuffleNet can use wider feature maps. We find this is critical for small networks, as tiny networks usually have an insufficient number of channels to process the information. From the results, we see that models with group convolutions (g>1) consistently perform better than the counterparts without pointwise group convolutions (g=1). Smaller models tend to benefit more from groups. For example, for ShuffleNet 1\times the best entry (g=8) is 1.2% better than the counterpart, while for ShuffleNet 0.5\times and 0.25\times the gaps become 3.5% and 4.4% respectively. Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps. Table 2 also shows that for some models (e.g. ShuffleNet 0.5\times) when group numbers become relatively large (e.g. g=8), the classification score saturates or even drops. With an increase in group number (thus wider feature maps), input channels for each convolutional filter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as ShuffleNet 0.25\times larger group numbers tend to better results consistently, which suggests wider feature maps bring more benefits for smaller models. We use exactly the same settings to train these models. Results are shown in Table 4. Our ShuffleNet models outperform most others by a significant margin under different complexities. Interestingly, we find an empirical relationship between feature map channels and classification accuracy. For example, under the complexity of 38 MFLOPs, output channels of Stage 4 (see Table 1) for VGG-like, ResNet, ResNeXt, Xception-like, ShuffleNet models are 50, 192, 192, 288, 576 respectively, which is consistent with the increase of accuracy. Since the efficient design of ShuffleNet, we can use more channels for a given computation budget, thus usually resulting in better performance."
Which networks introduced efficient depthwise seperable convolution into the building blocks of a state-of-the-art network?,Xception and ResNeXt,"Although AlexNet introduced the idea of group convolutions, the Xception and ResNeXt generalized depthwise separable convolutions and achieved state-of-the-art results under large computationl budget (~1 GFLOPs).","We notice that state-of-the-art basic architectures such as Xception [3] and ResNeXt [40] become less efficient in extremely small networks because of the costly dense 1\times 1 convolutions. We propose using pointwise group convolutions to reduce computation complexity of 1\times 1 convolutions. To overcome the side effects brought by group convolutions, we come up with a novel channel shuffle operation to help the information flowing across feature channels. Based on the two techniques, we build a highly efficient architecture called ShuffleNet. Compared with popular structures like  [30, 9, 40], for a given computation complexity budget, our ShuffleNet allows more feature map channels, which helps to encode more information and is especially critical to the performance of very small networks. In addition, in ShuffleNet depthwise convolution only performs on bottleneck feature maps. Even though depthwise convolution usually has very low theoretical complexity, we find it difficult to efficiently implement on low-power mobile devices, which may result from a worse computation/memory access ratio compared with other dense operations. Such drawback is also referred in  [3], which has a runtime library based on TensorFlow [1]. In ShuffleNet units, we intentionally use depthwise convolution only on bottleneck in order to prevent overhead as much as possible. Recent leading convolutional units in VGG [30], ResNet [9], GoogleNet [33], ResNeXt [40] and Xception [3] have pursued state-of-the-art results with large models (e.g. \geq 1GFLOPs), but do not fully explore low-complexity conditions. In this section we survey a variety of building blocks and make comparisons with ShuffleNet under the same complexity constraint. Recently Howard et al. have proposed MobileNets [12] which mainly focus on efficient network architecture for mobile devices. MobileNet takes the idea of depthwise separable convolution from  [3] and achieves state-of-the-art results on small models. The concept of group convolution, which was first introduced in AlexNet [21] for distributing the model over two GPUs, has been well demonstrated its effectiveness in ResNeXt [40]. Depthwise separable convolution proposed in Xception [3] generalizes the ideas of separable convolutions in Inception series [34, 32]. Recently, MobileNet [12] utilizes the depthwise separable convolutions and gains state-of-the-art results among lightweight models. Our work generalizes group convolution and depthwise separable convolution in a novel form. Modern convolutional neural networks [30, 33, 34, 32, 9, 10] usually consist of repeated building blocks with the same structure. Among them, state-of-the-art networks such as Xception [3] and ResNeXt [40] introduce efficient depthwise separable convolutions or group convolutions into the building blocks to strike an excellent trade-off between representation capability and computational cost. However, we notice that both designs do not fully take the 1\times 1 convolutions (also called pointwise convolutions in  [12]) into account, which require considerable complexity. For example, in ResNeXt [40] only 3\times 3 layers are equipped with group convolutions. As a result, for each residual unit in ResNeXt the pointwise convolutions occupy 93.4% multiplication-adds (cardinality = 32 as suggested in  [40]). In tiny networks, expensive pointwise convolutions result in limited number of channels to meet the complexity constraint, which might significantly damage the accuracy."
How much percentage computation do pointwise convolutions take up in each residual unit?,93.4%,"Only for ResNeXt, the pointwise convolutions seem to take 93.4% of multiplication-adds. However, it is impossible to say how much percentage the pointwise convolutions take for all the models that are mentioned in the paper.","Thanks to pointwise group convolution with channel shuffle, all components in ShuffleNet unit can be computed efficiently. Compared with ResNet [9] (bottleneck design) and ResNeXt [40], our structure has less complexity under the same settings. For example, given the input size c\times h\times w and the bottleneck channels m, ResNet unit requires hw(2cm+9m^{2}) FLOPs and ResNeXt has hw(2cm+9m^{2}/g) FLOPs, while our ShuffleNet unit requires only hw(2cm/g+9m) FLOPs, where g means the number of groups for convolutions. In other words, given a computational budget, ShuffleNet can use wider feature maps. We find this is critical for small networks, as tiny networks usually have an insufficient number of channels to process the information. Modern convolutional neural networks [30, 33, 34, 32, 9, 10] usually consist of repeated building blocks with the same structure. Among them, state-of-the-art networks such as Xception [3] and ResNeXt [40] introduce efficient depthwise separable convolutions or group convolutions into the building blocks to strike an excellent trade-off between representation capability and computational cost. However, we notice that both designs do not fully take the 1\times 1 convolutions (also called pointwise convolutions in  [12]) into account, which require considerable complexity. For example, in ResNeXt [40] only 3\times 3 layers are equipped with group convolutions. As a result, for each residual unit in ResNeXt the pointwise convolutions occupy 93.4% multiplication-adds (cardinality = 32 as suggested in  [40]). In tiny networks, expensive pointwise convolutions result in limited number of channels to meet the complexity constraint, which might significantly damage the accuracy."
How did the authors handle complexity constraints for their mobile networks?,"The authors handled complexity constraints for their mobile networks by intentionally using depthwise convolution only on bottleneck feature maps, and by scaling the number of filters in ShuffleNet units by a factor of s to customize the network to a desired complexity","The authors have constructed simple scaling to reduce the size of the ShuffleNet to fit the computational constraints. Also, they report the specific outcomes of their implementation and its reasons when the network is run on mobile devices. However, it is hard to understand what does handle complexity constraints and mobile networks mean in the question.","In addition, in ShuffleNet depthwise convolution only performs on bottleneck feature maps. Even though depthwise convolution usually has very low theoretical complexity, we find it difficult to efficiently implement on low-power mobile devices, which may result from a worse computation/memory access ratio compared with other dense operations. Such drawback is also referred in  [3], which has a runtime library based on TensorFlow [1]. In ShuffleNet units, we intentionally use depthwise convolution only on bottleneck in order to prevent overhead as much as possible. To customize the network to a desired complexity, we can simply apply a scale factor s on the number of channels. For example, we denote the networks in Table 1 as ”ShuffleNet 1\times”, then ”ShuffleNet s×s\timesitalic_s ×” means scaling the number of filters in ShuffleNet 1\times by s times thus overall complexity will be roughly s^{2} times of ShuffleNet 1\times. Recent leading convolutional units in VGG [30], ResNet [9], GoogleNet [33], ResNeXt [40] and Xception [3] have pursued state-of-the-art results with large models (e.g. \geq 1GFLOPs), but do not fully explore low-complexity conditions. In this section we survey a variety of building blocks and make comparisons with ShuffleNet under the same complexity constraint. For fair comparison, we use the overall network architecture as shown in Table 1. We replace the ShuffleNet units in Stage 2-4 with other structures, then adapt the number of channels to ensure the complexity remains unchanged. The structures we explored include: Table 5 compares classification scores under a variety of complexity levels. It is clear that our ShuffleNet models are superior to MobileNet for all the complexities. Though our ShuffleNet network is specially designed for small models (<150 MFLOPs), we find it is still better than MobileNet for higher computation cost, e.g. 3.1% more accurate than MobileNet 1\times at the cost of 500 MFLOPs. For smaller networks (\sim40 MFLOPs) ShuffleNet surpasses MobileNet by 7.8%. Note that our ShuffleNet architecture contains 50 layers while MobileNet only has 28 layers. For better understanding, we also try ShuffleNet on a 26-layer architecture by removing half of the blocks in Stage 2-4 (see ”ShuffleNet 0.5\times shallow (g=3)” in Table 5). Results show that the shallower model is still significantly better than the corresponding MobileNet, which implies that the effectiveness of ShuffleNet mainly results from its efficient structure, not the depth. Finally, we evaluate the actual inference speed of ShuffleNet models on a mobile device with an ARM platform. Though ShuffleNets with larger group numbers (e.g. g=4 or g=8) usually have better performance, we find it less efficient in our current implementation. Empirically g=3 usually has a proper trade-off between accuracy and actual inference time. As shown in Table 8, three input resolutions are exploited for the test. Due to memory access and other overheads, we find every 4\times theoretical complexity reduction usually results in \sim2.6\times actual speedup in our implementation. Nevertheless, compared with AlexNet [21] our ShuffleNet 0.5\times model still achieves \sim13\times actual speedup under comparable classification accuracy (the theoretical speedup is 18\times), which is much faster than previous AlexNet-level models or speedup approaches such as  [14, 16, 22, 42, 43, 38]."
"Each convolution operates on that corresponding input channel group. If so, how the model learns the features from entire input space?","The model learns the features from the entire input space through the channel shuffle operation, which allows the features from different groups to be combined and propagated through the network","When multiple group convolutions are stacked together, the authors use channel shuffle, which divides the channels into subgroups within groups and shuffles them in a way that each group consists of subgroups from all other groups. It lets the model learn from an entire input space despite the group convolution. However, the paper does not explicitly report such side effects when group convolutions are not stacked together.","If we allow group convolution to obtain input data from different groups (as shown in Fig 1 (b)), the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups. This can be efficiently and elegantly implemented by a channel shuffle operation (Fig 1 (c)): suppose a convolutional layer with g groups whose output has g\times n channels; we first reshape the output channel dimension into (g,n), transposing and then flattening it back as the input of next layer. Note that the operation still takes effect even if the two convolutions have different numbers of groups. Moreover, channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training. Channel shuffle operation makes it possible to build more powerful structures with multiple group convolutional layers. In the next subsection we will introduce an efficient network unit with channel shuffle and group convolution. To address the issue, a straightforward solution is to apply channel sparse connections, for example group convolutions, also on 1\times 1 layers. By ensuring that each convolution operates only on the corresponding input channel group, group convolution significantly reduces computation cost. However, if multiple group convolutions stack together, there is one side effect: outputs from a certain channel are only derived from a small fraction of input channels. Fig 1 (a) illustrates a situation of two stacked group convolution layers. It is clear that outputs from a certain group only relate to the inputs within the group. This property blocks information flow between channel groups and weakens representation."
What will be the effect in performance if group numbers for convolution is increased? ,"Performance will improve, but may eventually degrade as group numbers increase","For ShuffleNet, having more than 1 group seems to show consistently better results for all complexities. As the model gets smaller, the performance gain seems to increase more as the number of groups increases. However, for larger models, a too large number of groups led to saturation or a drop in classification error, possibly due to reduced representative capabilities.","In ShuffleNet units, group number g controls the connection sparsity of pointwise convolutions. Table 1 explores different group numbers and we adapt the output channels to ensure overall computation cost roughly unchanged (\sim140 MFLOPs). Obviously, larger group numbers result in more output channels (thus more convolutional filters) for a given complexity constraint, which helps to encode more information, though it might also lead to degradation for an individual convolutional filter due to limited corresponding input channels. In Sec 4.1.1 we will study the impact of this number subject to different computational constrains. From the results, we see that models with group convolutions (g>1) consistently perform better than the counterparts without pointwise group convolutions (g=1). Smaller models tend to benefit more from groups. For example, for ShuffleNet 1\times the best entry (g=8) is 1.2% better than the counterpart, while for ShuffleNet 0.5\times and 0.25\times the gaps become 3.5% and 4.4% respectively. Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps. Table 2 also shows that for some models (e.g. ShuffleNet 0.5\times) when group numbers become relatively large (e.g. g=8), the classification score saturates or even drops. With an increase in group number (thus wider feature maps), input channels for each convolutional filter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as ShuffleNet 0.25\times larger group numbers tend to better results consistently, which suggests wider feature maps bring more benefits for smaller models. Finally, we evaluate the actual inference speed of ShuffleNet models on a mobile device with an ARM platform. Though ShuffleNets with larger group numbers (e.g. g=4 or g=8) usually have better performance, we find it less efficient in our current implementation. Empirically g=3 usually has a proper trade-off between accuracy and actual inference time. As shown in Table 8, three input resolutions are exploited for the test. Due to memory access and other overheads, we find every 4\times theoretical complexity reduction usually results in \sim2.6\times actual speedup in our implementation. Nevertheless, compared with AlexNet [21] our ShuffleNet 0.5\times model still achieves \sim13\times actual speedup under comparable classification accuracy (the theoretical speedup is 18\times), which is much faster than previous AlexNet-level models or speedup approaches such as  [14, 16, 22, 42, 43, 38]."
Why are dense 1X1 convolutions computationally expensive?,"Dense 1X1 convolutions are computationally expensive because they require a large number of multiplication-adds operations, which can be a limiting factor in extremely small networks with limited computational resources",The 1x1 convolutions are expensive in extremely reduced versions of Xception and ResNeXt as they might take 93.4% of multiplication-adds for each residual unit.,"We notice that state-of-the-art basic architectures such as Xception [3] and ResNeXt [40] become less efficient in extremely small networks because of the costly dense 1\times 1 convolutions. We propose using pointwise group convolutions to reduce computation complexity of 1\times 1 convolutions. To overcome the side effects brought by group convolutions, we come up with a novel channel shuffle operation to help the information flowing across feature channels. Based on the two techniques, we build a highly efficient architecture called ShuffleNet. Compared with popular structures like  [30, 9, 40], for a given computation complexity budget, our ShuffleNet allows more feature map channels, which helps to encode more information and is especially critical to the performance of very small networks. From the results, we see that models with group convolutions (g>1) consistently perform better than the counterparts without pointwise group convolutions (g=1). Smaller models tend to benefit more from groups. For example, for ShuffleNet 1\times the best entry (g=8) is 1.2% better than the counterpart, while for ShuffleNet 0.5\times and 0.25\times the gaps become 3.5% and 4.4% respectively. Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps. Modern convolutional neural networks [30, 33, 34, 32, 9, 10] usually consist of repeated building blocks with the same structure. Among them, state-of-the-art networks such as Xception [3] and ResNeXt [40] introduce efficient depthwise separable convolutions or group convolutions into the building blocks to strike an excellent trade-off between representation capability and computational cost. However, we notice that both designs do not fully take the 1\times 1 convolutions (also called pointwise convolutions in  [12]) into account, which require considerable complexity. For example, in ResNeXt [40] only 3\times 3 layers are equipped with group convolutions. As a result, for each residual unit in ResNeXt the pointwise convolutions occupy 93.4% multiplication-adds (cardinality = 32 as suggested in  [40]). In tiny networks, expensive pointwise convolutions result in limited number of channels to meet the complexity constraint, which might significantly damage the accuracy."
"For designing convolutional architectures for ImageNet, authors used CIFAR-10 dataset as a proxy for learning. Is this true?",Yes,"According to above evidential sentence, the answer is True. Authors used CIFAR-10 dataset as a proxy for learning.","In this paper, we study a new paradigm of designing convolutional architectures and describe a scalable method to optimize convolutional architectures on a dataset of interest, for instance the ImageNet classification dataset.Our approach is inspired by the recently proposed Neural Architecture Search (NAS) framework [71], which uses a reinforcement learning search method to optimize architecture configurations. Applying NAS, or any other search methods, directly to a large dataset, such as the ImageNet dataset, is however computationally expensive. We therefore propose to search for a good architecture on a proxy dataset, for example the smaller CIFAR-10 dataset, and then transfer the learned architecture to ImageNet. We achieve this transferrability by designing a search space (which we call “the NASNet search space”) so that the complexity of the architecture is independent of the depth of the network and the size of input images. More concretely, all convolutional networks in our search space are composed of convolutional layers (or “cells”) with identical structure but different weights. Searching for the best convolutional architectures is therefore reduced to searching for the best cell structure. Searching for the best cell structure has two main benefits: it is much faster than searching for an entire network architecture and the cell itself is more likely to generalize to other problems.In our experiments, this approach significantly accelerates the search for the best architectures using CIFAR-10 by a factor of 7×7\times7 × and learns architectures that successfully transfer to ImageNet."
"During architecture search, did the models inherently learn skip connections? ","Sure! Here's the answer to your question based on the provided context:

Yes, the models inherently learned skip connections during architecture search","Yes, the models inherently learn the skip connections.","Note we do not have residual connections between convolutional cells as the models learn skip connections on their own. We empirically found manually inserting residual connections between cells to not help performance.Our training setup on ImageNet is similar to [60], but please see Appendix A for details."
"What are the hyper-parameters used to design the neural architecture search network?
",The hyper-parameters used to design the neural architecture search network are the number of cell repeats N and the number of filters in the initial convolutional cell,The number of cell repeats and the number of filters in the initial convolutional cell are the hyper-parameters used to design the Neural Architecture Search network.,"We demonstrate the utility of the convolutional cells by employing this learned architecture on CIFAR-10 and a family of ImageNet classification tasks. The latter family of tasks is explored across a few orders of magnitude in computational budget.After having learned the convolutional cells, several hyper-parameters may be explored to build a final network for a given task: (1) the number of cell repeats N and (2) the number of filters in the initial convolutional cell. After selecting the number of initial filters, we use a common heuristic to double the number of filters whenever the stride is 2.Finally, we define a simple notation, e.g., 4 @ 64, to indicate these two parameters in all networks, where 4 and 64 indicate the number of cell repeats and the number of filters in the penultimate layer of the network, respectively."
What metric is used for measuring Computational demand of a network? ,Computational demand is measured by the number of parameters and floating-point operations (FLOPs) required by a network,"To measure the computational demand of the network, top-1 accuracy metric was used.","Additionally, by simply varying the number of the convolutional cells and number of filters in the convolutional cells, we can create different versions of NASNets with different computational demands. Thanks to this property of the cells, we can generate a family of models that achieve accuracies superior to all human-invented models at equivalent or smaller computational budgets [60, 29]. Notably, the smallest version of NASNet achieves 74.0% top-1 accuracy on ImageNet, which is 3.1% better than previously engineered architectures targeted towards mobile and embedded vision tasks [24, 70]."
Is random search (RS) more efficient that reinforcement learning (RL) for learning neural architectures?,"Sure! Here's the answer based on the given context:

No, reinforcement learning (RL) is not less efficient than random search (RS) for learning neural architectures. In fact, RL finds better architectures than RS in the NASNet search space, as shown in Figure 6","No, Reninforcement Learning is more efficient than Random Search for learning neural architectures.","Figure 6 shows the performance of reinforcement learning (RL) and random search (RS) as more model architectures are sampled. Note that the best model identified with RL is significantly better than the best model found by RS by over 1% as measured by on CIFAR-10. Additionally, RL finds an entire range of models that are of superior quality to random search. We observe this in the mean performance of the top-5 and top-25 models identified in RL versus RS.We take these results to indicate that although RS may provide a viable search strategy, RL finds better architectures in the NASNet search space."
"Does NASNets perform better than MobileNet, ShuffleNet under resource-constraint setting?","No, NASNets does not perform better than MobileNet and ShuffleNet under resource-constrained settings","From the above evidential sentence, it is obvious that NASNets with 74% accuracy perform better than MobileNet and ShuffleNet with 70.6% and 70.9% accuracies respectively.","Finally, we test how well the best convolutional cells may perform in a resource-constrained setting, e.g., mobile devices (Table 3). In these settings, the number of floating point operations is severely constrained and predictive performance must be weighed against latency requirements on a device with limited computational resources. MobileNet [24] and ShuffleNet [70] provide state-of-the-art results obtaining 70.6% and 70.9\% accuracy, respectively on 224x224 images using \sim550M multliply-add operations. An architecture constructed from the best convolutional cells achieves superior predictive performance (74.0% accuracy) surpassing previous models but with comparable computational demand. In summary, we find that the learned convolutional cells are flexible across model scales achieving state-of-the-art performance across almost 2 orders of magnitude in computational budget."
"How different versions of NASNets with different computational demands were created?
",By varying the number of convolutional cells and filters in the cells,Different versions of NASNets with different computational demands were created by varying the number of the convolutional cells and number of filters in the convolutional cells.,"Additionally, by simply varying the number of the convolutional cells and number of filters in the convolutional cells, we can create different versions of NASNets with different computational demands. Thanks to this property of the cells, we can generate a family of models that achieve accuracies superior to all human-invented models at equivalent or smaller computational budgets [60, 29]. Notably, the smallest version of NASNet achieves 74.0% top-1 accuracy on ImageNet, which is 3.1% better than previously engineered architectures targeted towards mobile and embedded vision tasks [24, 70]."
"What are the networks that were constructed from the best three searches?
","NASNet-A, NASNet-B, and NASNet-C","The networks constructed from the best three searches are NASNet-A, NASNet-B and NASNet-C.","Figure 4 shows a diagram of the top performing Normal Cell and Reduction Cell. Note the prevalence of separable convolutions and the number of branches compared with competing architectures [53, 59, 20, 60, 58]. Subsequent experiments focus on this convolutional cell architecture, although we examine the efficacy of other, top-ranked convolutional cells in ImageNet experiments (described in Appendix B) and report their results as well. We call the three networks constructed from the best three searches NASNet-A, NASNet-B and NASNet-C."
Authors used a modified version of DropPath regularization named ScheduledDropPath. What is modified?,The probability of dropping each path in the cell is linearly increased over the course of training,"In ScheduledDropPath, each path in the cell is dropped out with a probability that is linearly increased over the course of training.","For complete details of of the architecture learning algorithm and the controller system, please refer to Appendix A. Importantly, when training NASNets, we discovered ScheduledDropPath, a modified version of DropPath [33], to be an effective regularization method for NASNet. In DropPath [33], each path in the cell is stochastically dropped with some fixed probability during training. In our modified version, ScheduledDropPath, each path in the cell is dropped out with a probability that is linearly increased over the course of training. We find that DropPath does not work well for NASNets, while ScheduledDropPath significantly improves the final performance of NASNets in both CIFAR and ImageNet experiments."
Why DropPath regularization didn’t work well for NASNets?,DropPath regularization did not work well for NASNets because the fixed probability of dropping paths did not allow for sufficient exploration of the search space,"Authors found that ScheduledDropPath, a modified version of DropPath works well for NASNets. So one possible reason why DropPath regularization didn't work well for NASNets could be that in DropPath each path in the cell is stochastically dropped with some fixed probability during training. The probability should linearly increase over the course of training.","For complete details of of the architecture learning algorithm and the controller system, please refer to Appendix A. Importantly, when training NASNets, we discovered ScheduledDropPath, a modified version of DropPath [33], to be an effective regularization method for NASNet. In DropPath [33], each path in the cell is stochastically dropped with some fixed probability during training. In our modified version, ScheduledDropPath, each path in the cell is dropped out with a probability that is linearly increased over the course of training. We find that DropPath does not work well for NASNets, while ScheduledDropPath significantly improves the final performance of NASNets in both CIFAR and ImageNet experiments."
"Searching for the best cell structure is less computationally expensive than searching for an entire network. If so, how the architecture search learns to connect the network? ",The architecture search learns to connect the network by transferring the learned cell structures from the smaller CIFAR-10 dataset to the larger ImageNet dataset,The architecture learns to connect the network by searching for the best cell structure instead of searching for the best convolutional architectures.,"In this paper, we study a new paradigm of designing convolutional architectures and describe a scalable method to optimize convolutional architectures on a dataset of interest, for instance the ImageNet classification dataset.Our approach is inspired by the recently proposed Neural Architecture Search (NAS) framework [71], which uses a reinforcement learning search method to optimize architecture configurations. Applying NAS, or any other search methods, directly to a large dataset, such as the ImageNet dataset, is however computationally expensive. We therefore propose to search for a good architecture on a proxy dataset, for example the smaller CIFAR-10 dataset, and then transfer the learned architecture to ImageNet. We achieve this transferrability by designing a search space (which we call “the NASNet search space”) so that the complexity of the architecture is independent of the depth of the network and the size of input images. More concretely, all convolutional networks in our search space are composed of convolutional layers (or “cells”) with identical structure but different weights. Searching for the best convolutional architectures is therefore reduced to searching for the best cell structure. Searching for the best cell structure has two main benefits: it is much faster than searching for an entire network architecture and the cell itself is more likely to generalize to other problems.In our experiments, this approach significantly accelerates the search for the best architectures using CIFAR-10 by a factor of 7×7\times7 × and learns architectures that successfully transfer to ImageNet."
"What are the approaches that led to improved accuracy with lesser parameters for NASNets compared to Inception, ResNet and PolyNet?","The approaches that led to improved accuracy with lesser parameters for NASNets compared to Inception, ResNet, and PolyNet are:

1. Introduction of a novel loss function.
2. Use of NASNet-A image featurization.
3. Ensembling multiple inferences across multiple model instances and image crops","Ensembling multiple inferences across multiple model instances and image crops led to improved accuracy with lesser parameters for NASNets compared to Inception, ResNet and PolyNet.","For the mobile-optimized network, our resulting system achieves a mAP of 29.6% – exceeding previous mobile-optimized networks that employ Faster-RCNN by over 5.0% (Table 4). For the best NASNet network, our resulting network operating on images of the same spatial resolution (800 \times 800) achieves mAP = 40.7%, exceeding equivalent object detection systems based off lesser performing image featurization (i.e. Inception-ResNet-v2) by 4.0% [28, 52] (see Appendix for example detections on images and side-by-side comparisons). Finally, increasing the spatial resolution of the input image results in the best reported, single model result for object detection of 43.1%, surpassing the best previous best by over 4.0% [37].222A primary advance in the best reported object detection system is the introduction of a novel loss [37]. Pairing this loss with NASNet-A image featurization may lead to even further performance gains. Additionally, performance gains are achievable through ensembling multiple inferences across multiple model instances and image crops (e.g., [28]). These results provide further evidence that NASNet provides superior, generic image features that may be transferred across other computer vision tasks. Figure 10 and Figure 11 in Appendix C show four examples of object detection results produced by NASNet-A with the Faster-RCNN framework."
What does model transferability mean?,"Model transferability refers to the ability of a trained model to perform well on a different dataset or task, often with different characteristics than the training data. In this paper, the authors aim to achieve transferability of the learned architecture from the smaller CIFAR-10 dataset to the larger ImageNet dataset",Applying NAS to a large dataset is computationally expensive. So the authors find the good architecture on a proxy dataset and then transfer the learned architecture to ImageNet. This approach is called Model Transferability.,"In this paper, we study a new paradigm of designing convolutional architectures and describe a scalable method to optimize convolutional architectures on a dataset of interest, for instance the ImageNet classification dataset.Our approach is inspired by the recently proposed Neural Architecture Search (NAS) framework [71], which uses a reinforcement learning search method to optimize architecture configurations. Applying NAS, or any other search methods, directly to a large dataset, such as the ImageNet dataset, is however computationally expensive. We therefore propose to search for a good architecture on a proxy dataset, for example the smaller CIFAR-10 dataset, and then transfer the learned architecture to ImageNet. We achieve this transferrability by designing a search space (which we call “the NASNet search space”) so that the complexity of the architecture is independent of the depth of the network and the size of input images. More concretely, all convolutional networks in our search space are composed of convolutional layers (or “cells”) with identical structure but different weights. Searching for the best convolutional architectures is therefore reduced to searching for the best cell structure. Searching for the best cell structure has two main benefits: it is much faster than searching for an entire network architecture and the cell itself is more likely to generalize to other problems.In our experiments, this approach significantly accelerates the search for the best architectures using CIFAR-10 by a factor of 7×7\times7 × and learns architectures that successfully transfer to ImageNet. The key insight in our approach is to design a search space that decouples the complexity of an architecture from the depth of a network. This resulting search space permits identifying good architectures on a small dataset (i.e., CIFAR-10) and transferring the learned architecture to image classifications across a range of data and computational scales."
The complexity of the NAS architecture is independent of the depth of the network and the size of input images. How does it scale to produce better models? ,"The NAS architecture scales by searching for the best cell structure, which is more likely to generalize to other problems and is faster than searching for an entire network architecture","To scale for the better models, authors searched for the best convolutional architectures by searching for the best cell structure.","In this paper, we study a new paradigm of designing convolutional architectures and describe a scalable method to optimize convolutional architectures on a dataset of interest, for instance the ImageNet classification dataset.Our approach is inspired by the recently proposed Neural Architecture Search (NAS) framework [71], which uses a reinforcement learning search method to optimize architecture configurations. Applying NAS, or any other search methods, directly to a large dataset, such as the ImageNet dataset, is however computationally expensive. We therefore propose to search for a good architecture on a proxy dataset, for example the smaller CIFAR-10 dataset, and then transfer the learned architecture to ImageNet. We achieve this transferrability by designing a search space (which we call “the NASNet search space”) so that the complexity of the architecture is independent of the depth of the network and the size of input images. More concretely, all convolutional networks in our search space are composed of convolutional layers (or “cells”) with identical structure but different weights. Searching for the best convolutional architectures is therefore reduced to searching for the best cell structure. Searching for the best cell structure has two main benefits: it is much faster than searching for an entire network architecture and the cell itself is more likely to generalize to other problems.In our experiments, this approach significantly accelerates the search for the best architectures using CIFAR-10 by a factor of 7×7\times7 × and learns architectures that successfully transfer to ImageNet."
Is the architecture search by the original NAS and NASNet are different?,Yes,"From the Table 1, the depths, number of parameters vary for both original NAS and NASNet. So, we can conclude that architecture search by both is different.","In this paper, we study a new paradigm of designing convolutional architectures and describe a scalable method to optimize convolutional architectures on a dataset of interest, for instance the ImageNet classification dataset.Our approach is inspired by the recently proposed Neural Architecture Search (NAS) framework [71], which uses a reinforcement learning search method to optimize architecture configurations. Applying NAS, or any other search methods, directly to a large dataset, such as the ImageNet dataset, is however computationally expensive. We therefore propose to search for a good architecture on a proxy dataset, for example the smaller CIFAR-10 dataset, and then transfer the learned architecture to ImageNet. We achieve this transferrability by designing a search space (which we call “the NASNet search space”) so that the complexity of the architecture is independent of the depth of the network and the size of input images. More concretely, all convolutional networks in our search space are composed of convolutional layers (or “cells”) with identical structure but different weights. Searching for the best convolutional architectures is therefore reduced to searching for the best cell structure. Searching for the best cell structure has two main benefits: it is much faster than searching for an entire network architecture and the cell itself is more likely to generalize to other problems.In our experiments, this approach significantly accelerates the search for the best architectures using CIFAR-10 by a factor of 7×7\times7 × and learns architectures that successfully transfer to ImageNet."
Will normal cells and reduction cells that come out as search results be different for each dataset ? ,"Sure! Here's the answer to your question based on the given context:

Yes, normal cells and reduction cells that come out as search results will be different for each dataset","The number of normal and reduction cells that come out as search results depend on at least one factor, the input image size in the dataset. So, the number is different for each dataset.","Figure 2 shows our placement of Normal and Reduction Cells for CIFAR-10 and ImageNet. Note on ImageNet we have more Reduction Cells, since the incoming image size is 299x299 compared to 32x32 for CIFAR. The Reduction and Normal Cell could have the same architecture, but we empirically found it beneficial to learn two separate architectures.We use a common heuristic to double the number of filters in the output whenever the spatial activation size is reduced in order to maintain roughly constant hidden state dimension [32, 53].Importantly,much like Inception and ResNet models [59, 20, 60, 58],we consider the number of motif repetitions N and the number of initial convolutional filters as free parameters that we tailor to the scale of an image classification problem."
Which framework achieved state-of-the-art COCO object detection results with NASNets?,Faster-RCNN framework,Faster-RCNN framework along with the features learned by NASNets from ImageNet achieved state-of-the-art COCO object detection results with NASNets.,"Finally, we show that the image features learned by NASNets are generically useful and transfer to other computer vision problems. In our experiments, the features learned by NASNets from ImageNet classification can be combined with the Faster-RCNN framework [47] to achieve state-of-the-art on COCO object detection task for both the largest as well as mobile-optimized models. Our largest NASNet model achieves 43.1% mAP, which is 4% better than previous state-of-the-art. "
What is meant by “Proximal Policy Optimization”?,Proximal Policy Optimization (PPO) is an algorithm used for training the controller RNN in the described method to learn convolutional cells,Proximal Policy Optimization is an optimization algorithm used to train the controller RNN. It is done by employing a global work queue system for generating a pool of child networks controlled by the RNN.,"In this section, we describe our experiments with the method described above to learn convolutional cells. In summary, all architecture searches are performed using the CIFAR-10 classification task [31]. The controller RNN was trained using Proximal Policy Optimization (PPO) [51] by employing a global workqueue system for generating a pool of child networks controlled by the RNN. In our experiments, the pool of workers in the workqueue consisted of 500 GPUs."
Why cutout data augmentation improve NASNet-A model error rate?,"Sure! Here's the answer to the question using the provided context:

Cutout data augmentation improves the NASNet-A model error rate by providing the model with a more diverse set of training examples, which helps the model learn to recognize the features more accurately","From the above evidential paragraph, we can see that the cutout data augmentation achieves a state-of-the-art error rate of 2.40% which is better than the previous record. But, why it improves the performance cannot be answered in this paper.","For the task of image classification with CIFAR-10, we set N=4 or 6 (Figure 2). The test accuracies of the best architectures are reported in Table 1 along with other state-of-the-art models. As can be seen from the Table, a large NASNet-A model with cutout data augmentation [12] achieves a state-of-the-art error rate of 2.40% (averaged across 5 runs), which is slightly better than the previous best record of 2.56% by [12]. The best single run from our model achieves 2.19% error rate."
Why residual connections didn't help much for NASNets?,"Sure! Here's the answer to the question based on the provided context:

The lack of residual connections in NASNets did not have a significant impact on performance, as the models were able to learn skip connections on their own",Inserting residual connections between residual connections between cells doesn't improve performance. But why it doesn't help improve the performance cannot be answered in this paper.,"Note we do not have residual connections between convolutional cells as the models learn skip connections on their own. We empirically found manually inserting residual connections between cells to not help performance.Our training setup on ImageNet is similar to [60], but please see Appendix A for details."
How is normal cell different from reduction cell for NASNets?,"Normal cells and reduction cells differ in their architecture and function in NASNets. Normal cells have the same architecture but different weights, while reduction cells have a stride of two to reduce the height and width of the feature map, and are used to reduce the spatial dimensions of the input","We learn two separate architectures for reduction and normal cells. During prediction,  the first 5B predictions are for the Normal Cell and the second 5B predictions are for the Reduction Cell. For Reduction cell authors make the initial operation applied to the cell’s inputs have a stride of two to reduce the height and width which is not done for Normal cell.","In our approach, the overall architectures of the convolutional nets are manually predetermined. They are composed of convolutional cells repeated many times where each convolutional cell has the same architecture, but different weights. To easily build scalable architectures for images of any size, we need two types of convolutional cells to serve two main functions when taking in a feature map as input: (1) convolutional cells that return a feature map of the same dimension, and (2) convolutional cells that return a feature map where the feature map height and width is reduced by a factor of two. We name the first type and second type of convolutional cells Normal Cell and Reduction Cell respectively. For the Reduction Cell, we make the initial operation applied to the cell’s inputs have a stride of two to reduce the height and width. All of our operations that we consider for building our convolutional cells have an option of striding. Figure 2 shows our placement of Normal and Reduction Cells for CIFAR-10 and ImageNet. Note on ImageNet we have more Reduction Cells, since the incoming image size is 299x299 compared to 32x32 for CIFAR. The Reduction and Normal Cell could have the same architecture, but we empirically found it beneficial to learn two separate architectures.We use a common heuristic to double the number of filters in the output whenever the spatial activation size is reduced in order to maintain roughly constant hidden state dimension [32, 53].Importantly,much like Inception and ResNet models [59, 20, 60, 58],we consider the number of motif repetitions N and the number of initial convolutional filters as free parameters that we tailor to the scale of an image classification problem. What varies in the convolutional nets is the structures of the Normal and Reduction Cells, which are searched by the controller RNN.The structures of the cells can be searched within a search space defined as follows (see Appendix, Figure 7 for schematic). In our search space, each cell receives as input two initial hidden states h_{i} and h_{i-1} which are the outputs of two cells in previous two lower layers or the input image. The controller RNN recursively predicts the rest of the structure of the convolutional cell, given these two initial hidden states (Figure 3). The predictions of the controller for each cell are grouped into B blocks, where each block has 5 prediction steps made by 5 distinct softmax classifiers corresponding to discrete choices of the elements of a block: To allow the controller RNN to predict both Normal Cell and Reduction Cell, we simply make the controller have 2\times 5B predictions in total, where the first 5B predictions are for the Normal Cell and the second 5B predictions are for the Reduction Cell."
The authors explored the possibility of using residual networks on the inception model to reduce complexity. Is that true?,Yes,True. The authors explored the possibility of using residual networks on the inception model to reduce complexity.,"In this work we study the combination of the two most recent ideas:Residual connections introduced by He et al. in  [5] and the latestrevised version of the Inception architecture [15].In [5], it is argued that residual connections are of inherentimportance for training very deep architectures. Since Inception networkstend to be very deep, it is natural to replace thefilter concatenation stage of the Inception architecture with residual connections. Thiswould allow Inception to reap all the benefits of the residual approachwhile retaining its computational efficiency. We studied how the introduction of residual connections leads to dramaticallyimproved training speed for the Inception architecture. Also our latest models(with and without residual connections) outperform all our previous networks,just by virtue of the increased model size."
"What are the metrics used for comparing Inception-v4, Inception- ResNet-v1/2 and their ensembles?",Top-5 error,Metrics used for the comparison of ensembles are: a) Computational Cost b) Recognition Performance c) Step Time d) top-5 error.,"We tried several versions of the residual version of Inception. Only twoof them are detailed here. The first one “Inception-ResNet-v1”roughly the computational cost of Inception-v3, while “Inception-ResNet-v2”matches the raw cost of the newly introduced Inception-v4 network. SeeFigure 15 for the large scale structure of bothvarianets. (However, the step time of Inception-v4 proved to be significantlyslower in practice, probably due to the larger number of layers.) •Inception-ResNet-v1: a hybrid Inception version that has asimilar computational cost to Inception-v3from [15].•Inception-ResNet-v2: a costlier hybrid Inception version withsignificantly improved recognition performance.•Inception-v4: a pure Inception variant without residual connectionswith roughly the same recognition performance as Inception-ResNet-v2. The last experiment reported here is an evaluation of an ensemble ofall the best performing models presented here. As it wasapparent that both Inception-v4 and Inception-ResNet-v2 performedsimilarly well, exceeding state-of-the art single frame performanceon the ImageNet validation dataset, we wanted to see how a combinationof those pushes the state of the art on this well studied dataset.Surprisingly, we found that gains on the single-frame performance do nottranslate into similarly large gains on ensembled performance. Nonetheless,it still allows us to report 3.1% top-5 error on the validation set withfour models ensembled setting a new state of the art, to our bestknowledge."
Which part of the Inception architecture was replaced with residual connections? ,The filter concatenation stage of the Inception architecture was replaced with residual connections,The filter concatenation stage of the Inception Architecture was replaced with Residual connections.,"In this work we study the combination of the two most recent ideas:Residual connections introduced by He et al. in  [5] and the latestrevised version of the Inception architecture [15].In [5], it is argued that residual connections are of inherentimportance for training very deep architectures. Since Inception networkstend to be very deep, it is natural to replace thefilter concatenation stage of the Inception architecture with residual connections. Thiswould allow Inception to reap all the benefits of the residual approachwhile retaining its computational efficiency."
"What is the role of adding 1x1 convolution before the 3x3 and 1x7 convolutions, How does it help?","Sure! Here's my answer:

The 1x1 convolution before the 3x3 and 1x7 convolutions helps to scale up the dimensionality of the filter bank, compensating for the dimensionality reduction induced by the Inception block",1x1 convolution block is added before 3x3 and 1x7 convolutions for scaling up the dimensionality of the filter bank before the additionto match the depth of the input. This is done to compensate for the dimensionality reduction induced by the Inception block.,"For the residual versions of the Inception networks, we use cheaper Inceptionblocks than the original Inception. Each Inception block is followed byfilter-expansion layer (1\times 1 convolution without activation) which isused for scaling up the dimensionality of the filter bank before the additionto match the depth of the input. This is needed to compensate for the dimensionalityreduction induced by the Inception block."
Why does inception-v4’s inception module use average pooling instead of max pooling? ,"Sure! Here's the answer to your question based on the provided context:

To reduce computational cost.

Inception-v4's use of average pooling instead of max pooling in its inception module is a deliberate design choice aimed at reducing computational cost. By using average pooling, the model can capture local features more efficiently and with less computational overhead, which is particularly important in the early stages of the network",One possible reason to use average pooling instead of max pooling is that max pooling introduces some kind of technical constraints which are reduced by average pooling. But these constraint have not been explained in the paper. So this question cannot be fully answered in this paper.,"Besides a straightforward integration, we have also studied whetherInception itself can be made more efficient by making it deeper and wider.For that purpose, we designed a new version named Inception-v4which has a more uniform simplified architecture and more inception modulesthan Inception-v3. Historically, Inception-v3 had inherited a lot of thebaggage of the earlier incarnations. The technical constraints chiefly came fromthe need for partitioning the model for distributed training usingDistBelief [2].Now, after migrating our training setup to TensorFlow [1]these constraints have been lifted, which allowed us to simplify the architecturesignificantly. The details of that simplified architecture are described in Section 3."
Is 7x7 convolution is similiar in computational complexity with two 7x1 and 1x7 convolution ?,"Sure! Here's the answer to your question based on the provided context:

No.

In the context of the report, the author mentions that the Inception-ResNet variants were chosen in an ad hoc manner with the main constraint being that the parameters and computational complexity of the models should be similar to the cost of the non-residual models. This suggests that the computational complexity of the models was not the only factor considered when choosing the Inception-ResNet variants, and therefore, the computational complexity of a 7x7 convolution may not be directly comparable to that of two 7x1 and 1x7 convolutions","From the above evidential sentence, it can be concluded that the computational complexity of 7x7 convolution is similar to that of 7x1 and 1x7 convolution.","In this report, we will compare the two pure Inception variants,Inception-v3 and v4, with similarly expensive hybrid Inception-ResNetversions. Admittedly, those models were picked in a somewhat ad hoc mannerwith the main constraint being that the parameters and computationalcomplexity of the models should be somewhat similar to the costof the non-residual models. In fact we have tested bigger and widerInception-ResNet variants and they performed very similarly on theImageNet classification challenge  [11]dataset."
"How Inception-ResNet v1 compare with Inception-ResNet v2 in terms of structure, stem and settings?","Inception-ResNet v1 and Inception-ResNet v2 have the same structure and settings, but Inception-ResNet v2 has a different stem",Both Inception-ResNet-v1 and Inception-ResNet-v2 are Inception style networksthat utilize residual connections instead of filter concatenation.,"Finally, we present some comparisons, between various versions of Inceptionand Inception-ResNet. The models Inception-v3 and Inception-v4 are deepconvolutional networks not utilizing residual connections whileInception-ResNet-v1 and Inception-ResNet-v2 are Inception style networksthat utilize residual connections instead of filter concatenation."
Why 1x1 convolution is used after the original convolutions in Inception-Resnet architectures?,"Sure! Here's the answer to the question using the provided context:

To compensate for the dimensionality reduction induced by the Inception block",1x1 convolution is used after the original convolutions in Inception-Resnet architectures for scaling up the dimensionality of the filter bank before the additionto match the depth of the input. This is needed to compensate for the dimensionalityreduction induced by the Inception block.,"For the residual versions of the Inception networks, we use cheaper Inceptionblocks than the original Inception. Each Inception block is followed byfilter-expansion layer (1\times 1 convolution without activation) which isused for scaling up the dimensionality of the filter bank before the additionto match the depth of the input. This is needed to compensate for the dimensionalityreduction induced by the Inception block."
What do you mean by Top-1 and Top-5 error rate?,"Sure! Here's my answer:

Top-1 and Top-5 error rate refer to the proportion of incorrectly classified instances among the top-1 and top-5 predictions, respectively",Top-1 and Top-5 error rate are the evaluation metrics used to compare the performace of various models. The definition of these metrics cannot be answered in this paper.,"First we observe the top-1 and top-5 validation-error evolution of thefour variants during training. After the experiment was conducted, we havefound that our continuous evaluation was conducted on a subset of thevalidation set which omitted about 1700 blacklisted entities due to poorbounding boxes. It turned out that the omission should have been only performedfor the CLSLOC benchmark, but yields somewhat incomparable (more optimistic)numbers when compared to other reports including some earlier reports by ourteam. The difference is about 0.3% for top-1 error and about 0.15% forthe top-5 error. However, since the differences are consistent, we thinkthe comparison between the curves is a fair one."
Why the training was unstable without these activations scaled before addition?,"The training was unstable without the activations scaled before addition because the high number of filters in the network caused vanishing gradients, and scaling the residuals helped to stabilize the training by amplifying the gradients","The training was stabilised after scaling down the residuals before adding them to the previous layer activations. Even when this scaling was not strictly necessary, it helped stabilise the training without affecting the accuracy. But this paper does not answer why the training was unstable without these scaled activations.","We found that scaling down the residuals before adding them tothe previous layer activation seemed to stabilize the training. In generalwe picked some scaling factors between 0.1 and 0.3 to scale the residualsbefore their being added to the accumulated layer activations(cf. Figure 20). A similar instability was observed by He et al. in [5] inthe case of very deep residual networks and they suggested a two-phasetraining where the first “warm-up” phase is done with very low learningrate, followed by a second phase with high learning rata. We found thatif the number of filters is very high, then even a very low (0.00001) learningrate is not sufficient to cope with the instabilities and the training withhigh learning rate had a chance to destroy its effects. We found it muchmore reliable to just scale the residuals. Even where the scaling was not strictly necessary, it neverseemed to harm the final accuracy, but it helped to stabilize the training."
Will is the Inception- ResNet-v2 trained faster than pure Inception-v4 although their computational complexity is similar ?,"Sure! Here's the answer to your question based on the provided context:

No","Since the step time of Inception-v4 is significantly slower in practice, we can conclude that Inception-ResNet-v2 trained faster than pure Inception-v4 even though the computational complexity is similar. This is also supported by the Figure 24.","We tried several versions of the residual version of Inception. Only twoof them are detailed here. The first one “Inception-ResNet-v1”roughly the computational cost of Inception-v3, while “Inception-ResNet-v2”matches the raw cost of the newly introduced Inception-v4 network. SeeFigure 15 for the large scale structure of bothvarianets. (However, the step time of Inception-v4 proved to be significantlyslower in practice, probably due to the larger number of layers.)"
"By ignoring IoU between 0.4 and 0.5, Are we losing some positive samples too?","Yes.

By ignoring the IoU between 0.4 and 0.5, you are indeed losing some positive samples. This is because some true positive anchors may have an IoU value between 0.4 and 0.5, and by ignoring these values, you may be missing out on some correctly assigned anchors","As evident from the above sentence, since we are ignoring the anchors, it is possible that we may lose some positive samples if their IoU is between 0.4 and 0.5.","Each anchor is assigned a length K one-hot vector of classification targets, where K is the number of object classes, and a 4-vector of box regression targets. We use the assignment rule from RPN [28] but modified for multiclass detection and with adjusted thresholds. Specifically, anchors are assigned to ground-truth object boxes using an intersection-over-union (IoU) threshold of 0.5; and to background if their IoU is in [0, 0.4). As each anchor is assigned to at most one object box, we set the corresponding entry in its length K label vector to 1 and all other entries to 0. If an anchor is unassigned, which may happen with overlap in [0.4, 0.5), it is ignored during training. Box regression targets are computed as the offset between each anchor and its assigned object box, or omitted if there is no assignment."
One stage detectors are computationally inexpensive than two stage detectors. Is it true?,False,"From the above evidential sentence, we can see that a one-stage detector has to process larger set of candidate object locations regularly sampled across an image. So, we can say that one stage detectors are computationally expensive than two stage detectors. Thus, the answer is False.","In contrast, a one-stage detector must process a much larger set of candidate object locations regularly sampled across an image. In practice this often amounts to enumerating \scriptstyle\sim100k locations that densely cover spatial positions, scales, and aspect ratios. While similar sampling heuristics may also be applied, they are inefficient as the training procedure is still dominated by easily classified background examples. This inefficiency is a classic problem in object detection that is typically addressed via techniques such as bootstrapping [33, 29] or hard example mining [37, 8, 31]."
How foreground-background class imbalance is encountered for two stage detectors ?,"The foreground-background class imbalance is a common challenge encountered by two-stage object detectors, particularly in datasets with an imbalanced class distribution, such as COCO","In the two-stage mechanism for object detection, the first stage generates a sparse set of candidate object locations and the second stage classifies each candidate location as one of the foreground or background classes using a CNN. If the CNN outputs more foreground or background classes than the latter, we can say that foreground-background imbalance has occurred.","Current state-of-the-art object detectors are based on a two-stage, proposal-driven mechanism. As popularized in the R-CNN framework [11], the first stage generates a sparse set of candidate object locations and the second stage classifies each candidate location as one of the foreground classes or as background using a convolutional neural network. Through a sequence of advances [10, 28, 20, 14], this two-stage framework consistently achieves top accuracy on the challenging COCO benchmark [21]."
How many sets of candidate object location is sampled across an image for RetinaNet,"approximately 100,000 sets of candidate object locations",~100k sets of candidate object locations were sampled accross an image for RetinaNet.,"In contrast, a one-stage detector must process a much larger set of candidate object locations regularly sampled across an image. In practice this often amounts to enumerating \scriptstyle\sim100k locations that densely cover spatial positions, scales, and aspect ratios. While similar sampling heuristics may also be applied, they are inefficient as the training procedure is still dominated by easily classified background examples. This inefficiency is a classic problem in object detection that is typically addressed via techniques such as bootstrapping [33, 29] or hard example mining [37, 8, 31]."
"What makes the performance of one-stage detectors inferior to two-stage detectors ?
",Density of candidate object locations,"Two-stage detectors can classify boxes at any position, scale, and aspect ratio using a region pooling operation. In contrast, one-stage detectors use a fixed sampling grid. Two-stage detectors can be made fast simply by reducing input image resolution and the number of proposals, but one-stage methods trailed in accuracy even with a larger compute budget. Also, one stage detector has to process large set of candidate object locations regularly sampled across an image. These are the reasons why one stage detectors perform worse than two stage detectors.","In contrast, a one-stage detector must process a much larger set of candidate object locations regularly sampled across an image. In practice this often amounts to enumerating \scriptstyle\sim100k locations that densely cover spatial positions, scales, and aspect ratios. While similar sampling heuristics may also be applied, they are inefficient as the training procedure is still dominated by easily classified background examples. This inefficiency is a classic problem in object detection that is typically addressed via techniques such as bootstrapping [33, 29] or hard example mining [37, 8, 31]. One of the most important design factors in a one-stage detection system is how densely it covers the space of possible image boxes. Two-stage detectors can classify boxes at any position, scale, and aspect ratio using a region pooling operation [10]. In contrast, as one-stage detectors use a fixed sampling grid, a popular approach for achieving high coverage of boxes in these approaches is to use multiple ‘anchors’ [28] at each spatial position to cover boxes of various scales and aspect ratios. OverFeat [30] was one of the first modern one-stage object detector based on deep networks. More recently SSD [22, 9] and YOLO [26, 27] have renewed interest in one-stage methods. These detectors have been tuned for speed but their accuracy trails that of two-stage methods. SSD has a 10-20% lower AP, while YOLO focuses on an even more extreme speed/accuracy trade-off. See Figure 2. Recent work showed that two-stage detectors can be made fast simply by reducing input image resolution and the number of proposals, but one-stage methods trailed in accuracy even with a larger compute budget [17]. In contrast, the aim of this work is to understand if one-stage detectors can match or surpass the accuracy of two-stage detectors while running at similar or faster speeds."
"What does ""online hard example mining (OHEM)"" means ?","Sure! Here's my answer:

Online Hard Example Mining (OHEM) is a technique used to improve the training of two-stage detectors by selectively choosing the most challenging examples for training","In OHEM, each example is scored by its loss, non-maximum suppression (nms) is then applied, and a minibatch is constructed with the highest-loss examples.","[31] proposed to improve training of two-stage detectors by constructing minibatches using high-loss examples. Specifically, in OHEM each example is scored by its loss, non-maximum suppression (nms) is then applied, and a minibatch is constructed with the highest-loss examples. The nms threshold and batch size are tunable parameters. Like the focal loss, OHEM puts more emphasis on misclassified examples, but unlike FL, OHEM completely discards easy examples. We also implement a variant of OHEM used in SSD [22]: after applying nms to all examples, the minibatch is constructed to enforce a 1:3 ratio between positives and negatives to help ensure each minibatch has enough positives."
Is RPN is used as a secondary classifier for proposing boundary boxes for F-RCNN framework ?,"No. RPN is not used as a secondary classifier for proposing boundary boxes in the Faster R-CNN (F-RCNN) framework. Instead, it is used to generate a set of candidate proposals that are then fed into the F-RCNN network for classification and refinement","Yes, RPN is used as a secondary classifier for proposing boundary boxes for F-RCNN framework","The dominant paradigm in modern object detection is based on a two-stage approach. As pioneered in the Selective Search work [35], the first stage generates a sparse set of candidate proposals that should contain all objects while filtering out the majority of negative locations, and the second stage classifies the proposals into foreground classes / background. R-CNN [11] upgraded the second-stage classifier to a convolutional network yielding large gains in accuracy and ushering in the modern era of object detection. R-CNN was improved over the years, both in terms of speed [15, 10] and by using learned object proposals [6, 24, 28]. Region Proposal Networks (RPN) integrated proposal generation with the second-stage classifier into a single convolution network, forming the Faster R-CNN framework [28]. Numerous extensions to this framework have been proposed, e.g. [20, 31, 32, 16, 14]."
What are the problems associated with class imbalance for single stage detectors ?,"Class imbalance for single stage detectors causes two problems: (1) training is inefficient as most locations are easy negatives that contribute no useful learning signal, and (2) en masse, the easy negatives can overwhelm training and lead to degenerate models",The problems associated with class imbalance for single stage detectors are: 1) Training is inefficient as most locations are easy negatives that contribute no useful learning signal. 2) The large number of easy negatives can overwhelm training and lead to degenerate models.,"Both classic one-stage object detection methods, like boosted detectors [37, 5] and DPMs [8], and more recent methods, like SSD [22], face a large class imbalance during training. These detectors evaluate 10^{4}-10^{5} candidate locations per image but only a few locations contain objects. This imbalance causes two problems: (1) training is inefficient as most locations are easy negatives that contribute no useful learning signal; (2) en masse, the easy negatives can overwhelm training and lead to degenerate models. A common solution is to perform some form of hard negative mining [33, 37, 8, 31, 22] that samples hard examples during training or more complex sampling/reweighing schemes [2]. In contrast, we show that our proposed focal loss naturally handles the class imbalance faced by a one-stage detector and allows us to efficiently train on all examples without sampling and without easy negatives overwhelming the loss and computed gradients."
What are the other loss functions experimented by the authors'? ,Huber loss and hinge loss,"The main loss function used by authors is The Focal Loss. Besides this, the other loss functions experimented on are: 1) Hinge Loss 2) Dynamically scaled cross entropy loss 3) \alpha-balanced CE loss 4) \alpha-balanced variant of the focal loss 5) Huber loss 6) The CE loss","There has been much interest in designing robust loss functions (e.g., Huber loss [13]) that reduce the contribution of outliers by down-weighting the loss of examples with large errors (hard examples). In contrast, rather than addressing outliers, our focal loss is designed to address class imbalance by down-weighting inliers (easy examples) such that their contribution to the total loss is small even if their number is large. In other words, the focal loss performs the opposite role of a robust loss: it focuses training on a sparse set of hard examples. The CE loss can be seen as the blue (top) curve in Figure 1. One notable property of this loss, which can be easily seen in its plot, is that even examples that are easily classified (p_{\textrm{t}}\gg.5) incur a loss with non-trivial magnitude. When summed over a large number of easy examples, these small loss values can overwhelm the rare class. In practice we use an \alpha-balanced variant of the focal loss:\textrm{FL}(p_{\textrm{t}})=-\alpha_{\textrm{t}}(1-p_{\textrm{t}})^{\gamma}\log(p_{\textrm{t}}).(5)We adopt this form in our experiments as it yields slightly improved accuracy over the non-\alpha-balanced form. Finally, we note that the implementation of the loss layer combines the sigmoid operation for computing p with the loss computation, resulting in greater numerical stability. Our next attempt to improve learning involved using the \alpha-balanced CE loss described in §3.1. Results for various \alpha are shown in Table 1a. Setting \alpha=.75 gives a gain of 0.9 points AP. Finally, in early experiments, we attempted to train with the hinge loss [13] on p_{\textrm{t}}, which sets loss to 0 above a certain value of p_{\textrm{t}}. However, this was unstable and we did not manage to obtain meaningful results. Results exploring alternate loss functions are in the appendix. In this paper, we propose a new loss function that acts as a more effective alternative to previous approaches for dealing with class imbalance. The loss function is a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases, see Figure 1. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. Experiments show that our proposed Focal Loss enables us to train a high-accuracy, one-stage detector that significantly outperforms the alternatives of training with the sampling heuristics or hard example mining, the previous state-of-the-art techniques for training one-stage detectors. Finally, we note that the exact form of the focal loss is not crucial, and we show other instantiations can achieve similar results."
How focal loss can be extended to use for multi class problem?,"The focal loss can be extended to the multi-class problem by using a softmax function to convert the predicted probabilities to class probabilities, and then applying the focal loss formula to each class separately","The focal loss can be extended to multi-class as follows:
Extending the focal loss to the multi-class case is straightforward and works well; for simplicity we focus on the binary loss in this work.:\textrm{CE}(p,y)=\begin{cases}-\log(p)&\text{if $y=1$}\\-\log(1-p)&\text{otherwise.}\end{cases}(1)In the above y\in\{\pm 1\} specifies the ground-truth class and p\in[0,1] is the model’s estimated probability for the class with label y=1. For notational convenience, we define p_{\textrm{t}}:p_{\textrm{t}}=\begin{cases}p&\text{if $y=1$}\\1-p&\text{otherwise,}\end{cases}(2)and rewrite \textrm{CE}(p,y)=\textrm{CE}(p_{\textrm{t}})=-\log(p_{\textrm{t}}).","The Focal Loss is designed to address the one-stage object detection scenario in which there is an extreme imbalance between foreground and background classes during training (e.g., 1:1000). We introduce the focal loss starting from the cross entropy (CE) loss for binary classification111Extending the focal loss to the multi-class case is straightforward and works well; for simplicity we focus on the binary loss in this work.:\textrm{CE}(p,y)=\begin{cases}-\log(p)&\text{if $y=1$}\\-\log(1-p)&\text{otherwise.}\end{cases}(1)In the above y\in\{\pm 1\} specifies the ground-truth class and p\in[0,1] is the model’s estimated probability for the class with label y=1. For notational convenience, we define p_{\textrm{t}}:p_{\textrm{t}}=\begin{cases}p&\text{if $y=1$}\\1-p&\text{otherwise,}\end{cases}(2)and rewrite \textrm{CE}(p,y)=\textrm{CE}(p_{\textrm{t}})=-\log(p_{\textrm{t}})."
"If we have 100000 easy examples (0.1 each) and 100 hard examples (2.3 each),Is it possible calculate percentage loss difference between them?","Yes, it is possible to calculate the percentage loss difference between the easy and hard examples.

In this case, the easy examples have a loss of 0.1 each, and the hard examples have a loss of 2.3 each. To calculate the percentage loss difference, we can subtract the loss of the easy examples from the loss of the hard examples, and then divide by the loss of the hard examples.

(2.3 - 0.1) / 2.3 = 0.9524 (or 95.24%)

So, the percentage loss difference between the easy and hard examples is 95.24%","Since the imbalance between easy and hard examples is large(100000 vs 100), it is not possible to calculate the percentage loss difference between them.","As our experiments will show, the large class imbalance encountered during training of dense detectors overwhelms the cross entropy loss. Easily classified negatives comprise the majority of the loss and dominate the gradient. While \alpha balances the importance of positive/negative examples, it does not differentiate between easy/hard examples. Instead, we propose to reshape the loss function to down-weight easy examples and thus focus training on hard negatives."
What is the effect of increasing modulating factor (γ )?,"Increasing the modulating factor (γ) reduces the loss contribution from easy examples and extends the range in which an example receives low loss, making it more important to correct misclassified examples","As specified wrongly in the question, modulating factor is (1-p_{\textrm{t}})^{\gamma} and not \gamma. Besides this as the modulating factor increases, the loss contribution from easy examples is reduced and the range in which an example receives low loss is extended.","More formally, we propose to add a modulating factor (1-p_{\textrm{t}})^{\gamma} to the cross entropy loss, with tunable focusing parameter \gamma\geq 0. We define the focal loss as:\textrm{FL}(p_{\textrm{t}})=-(1-p_{\textrm{t}})^{\gamma}\log(p_{\textrm{t}}).(4) Intuitively, the modulating factor reduces the loss contribution from easy examples and extends the range in which an example receives low loss. For instance, with \gamma=2, an example classified with p_{\textrm{t}}=0.9 would have 100×100\times100 × lower loss compared with CE and with p_{\textrm{t}}\approx 0.968 it would have 1000×1000\times1000 × lower loss. This in turn increases the importance of correcting misclassified examples (whose loss is scaled down by at most 4×4\times4 × for p_{\textrm{t}}\leq.5 and \gamma=2)."
Why P needs initialization at the start of the training?,To improve training stability,"Since P is one of the parameters in the loss function, we have to initialise P with some value before training. This P is then tuned while training such that the loss value is optimised.","Binary classification models are by default initialized to have equal probability of outputting either y=-1 or 1. Under such an initialization, in the presence of class imbalance, the loss due to the frequent class can dominate total loss and cause instability in early training. To counter this, we introduce the concept of a ‘prior’ for the value of p estimated by the model for the rare class (foreground) at the start of training. We denote the prior by \pi and set it so that the model’s estimated p for examples of the rare class is low, e.g. 0.01. We note that this is a change in model initialization (see §4.1) and not of the loss function. We found this to improve training stability for both the cross entropy and focal loss in the case of heavy class imbalance."
What value of π is used for experimentations ?,π = 0.01,The authors have used \pi = 0.01 for all the experimentations.,"We experiment with ResNet-50-FPN and ResNet-101-FPN backbones [20]. The base ResNet-50 and ResNet-101 models are pre-trained on ImageNet1k; we use the models released by [16]. New layers added for FPN are initialized as in [20]. All new conv layers except the final one in the RetinaNet subnets are initialized with bias b=0 and a Gaussian weight fill with \sigma=0.01. For the final conv layer of the classification subnet, we set the bias initialization to b=-\log((1-\pi)/\pi), where \pi specifies that at the start of training every anchor should be labeled as foreground with confidence of \scriptstyle\sim\pi. We use \pi=.01 in all experiments, although results are robust to the exact value. As explained in §3.3, this initialization prevents the large number of background anchors from generating a large, destabilizing loss value in the first iteration of training. Our first attempt to train RetinaNet uses standard cross entropy (CE) loss without any modifications to the initialization or learning strategy. This fails quickly, with the network diverging during training. However, simply initializing the last layer of our model such that the prior probability of detecting an object is \pi=.01 (see §4.1) enables effective learning. Training RetinaNet with ResNet-50 and this initialization already yields a respectable AP of 30.2 on COCO. Results are insensitive to the exact value of \pi so we use \pi=.01 for all experiments."
What was used as the backbone network for RetinaNet? ,Feature Pyramid Network (FPN) from [20],"For RetinaNet, Feature Pyramid Network (FPN) was used as a backbone.","We adopt the Feature Pyramid Network (FPN) from [20] as the backbone network for RetinaNet. In brief, FPN augments a standard convolutional network with a top-down pathway and lateral connections so the network efficiently constructs a rich, multi-scale feature pyramid from a single resolution input image, see Figure 3(a)-(b). Each level of the pyramid can be used for detecting objects at a different scale. FPN improves multi-scale predictions from fully convolutional networks (FCN) [23], as shown by its gains for RPN [28] and DeepMask-style proposals [24], as well at two-stage detectors such as Fast R-CNN [10] or Mask R-CNN [14]."
"How does it guess if it is an object or background? (Since using K classes and not K+1, 1 extra for background class)","Sure! Here's the answer to your question based on the provided context:

It guesses based on the intersection-over-union (IoU) threshold of 0.5","To decide whether there is an object in the image or not, IoU is used. If the value of IoU is equal to or above 0.5 then its an object else if IoU is between 0 and 0.4 then its background.","Each anchor is assigned a length K one-hot vector of classification targets, where K is the number of object classes, and a 4-vector of box regression targets. We use the assignment rule from RPN [28] but modified for multi-class detection and with adjusted thresholds. Specifically, anchors are assigned to ground-truth object boxes using an intersection-over-union (IoU) threshold of 0.5; and to background if their IoU is in [0, 0.4). As each anchor is assigned to at most one object box, we set the corresponding entry in its length K label vector to 1 and all other entries to 0. If an anchor is unassigned, which may happen with overlap in [0.4, 0.5), it is ignored during training. Box regression targets are computed as the offset between each anchor and its assigned object box, or omitted if there is no assignment."
What scale and aspect ratios is used for designing anchor boxes?,"Sure! Here's the answer to your question based on the provided context:

3 scales (2^k/4, for k≤3) and 3 aspect ratios (0.5, 1, 2) per location","Authors have used anchor boxes spanning 4 sub-octave scales (2^{k/4}, for k\leq 3) and with 3 aspect ratios [0.5, 1, 2].","We sweep over the number of scale and aspect ratio anchors used at each spatial position and each pyramid level in FPN. We consider cases from a single square anchor at each location to 12 anchors per location spanning 4 sub-octave scales (2^{k/4}, for k\leq 3) and 3 aspect ratios [0.5, 1, 2]. Results using ResNet-50 are shown in Table 1c. A surprisingly good AP (30.3) is achieved using just one square anchor. However, the AP can be improved by nearly 4 points (to 34.0) when using 3 scales and 3 aspect ratios per location. We used this setting for all other experiments in this work."
How classification subnet is different from regression subnet?,"The classification subnet is different from the regression subnet in that the classification subnet predicts class probabilities, while the regression subnet predicts the relative offset between the anchor and the ground-truth box","Even though the design of box regression subnet is similar to classification subnet, the design terminates in 4A linear outputs per spatial location. Also both the subnets use separate parameters.","In parallel with the object classification subnet, we attach another small FCN to each pyramid level for the purpose of regressing the offset from each anchor box to a nearby ground-truth object, if one exists. The design of the box regression subnet is identical to the classification subnet except that it terminates in 4A linear outputs per spatial location, see Figure 3 (d). For each of the A anchors per spatial location, these 4 outputs predict the relative offset between the anchor and the ground-truth box (we use the standard box parameterization from R-CNN [11]). We note that unlike most recent work, we use a class-agnostic bounding box regressor which uses fewer parameters and we found to be equally effective. The object classification subnet and the box regression subnet, though sharing a common structure, use separate parameters."
Why the normalization wasn't done taken all anchors into account?,The normalization was not done taking all anchors into account because the vast majority of anchors are easy negatives and receive negligible loss values under the focal loss,The normalisation is not done by taking all anchors into account because vast majority of anchors are easy negatives and receive negligible loss values under the focal loss.,"We use the focal loss introduced in this work as the loss on the output of the classification subnet. As we will show in §5, we find that \gamma=2 works well in practice and the RetinaNet is relatively robust to \gamma\in[0.5,5]. We emphasize that when training RetinaNet, the focal loss is applied to all \scriptstyle\sim100k anchors in each sampled image. This stands in contrast to common practice of using heuristic sampling (RPN) or hard example mining (OHEM, SSD) to select a small set of anchors (e.g., 256) for each minibatch. The total focal loss of an image is computed as the sum of the focal loss over all \scriptstyle\sim100k anchors, normalized by the number of anchors assigned to a ground-truth box. We perform the normalization by the number of assigned anchors, not total anchors, since the vast majority of anchors are easy negatives and receive negligible loss values under the focal loss. Finally we note that \alpha, the weight assigned to the rare class, also has a stable range, but it interacts with \gamma making it necessary to select the two together (see Tables 1a and 1b). In general \alpha should be decreased slightly as \gamma is increased (for \gamma=2, \alpha=0.25 works best)."
Why aren't we using α=0.75 since the positive samples are our minority classes?,"Sure! Here's the answer to your question based on the given context:

We are not using α=0.75 because it does not lead to the best performance. Although the positive samples are our minority classes, the best α value is found to be around α=.25, which is lower than α=0.75","Setting the \alpha to 0.75 gives a gain of 0.9 in AP and for \gamma = 2.0, \alpha = .25 or .5 gives the best results i.e. it lowers the AP by .4.  This is why using \alpha = .75 is not preferred.","Our next attempt to improve learning involved using the \alpha-balanced CE loss described in §3.1. Results for various \alpha are shown in Table 1a. Setting \alpha=.75 gives a gain of 0.9 points AP. For the experiments in Table 1b, for a fair comparison we find the best \alpha for each \gamma. We observe that lower \alpha’s are selected for higher \gamma’s (as easy negatives are down-weighted, less emphasis needs to be placed on the positives). Overall, however, the benefit of changing \gamma is much larger, and indeed the best \alpha’s ranged in just [.25,.75] (we tested \alpha\in[.01,.999]). We use \gamma=2.0 with \alpha=.25 for all experiments but \alpha=.5 works nearly as well (.4 AP lower)."
How do you compare background and foreground samples as γ changes?,"Sure! Here's the answer to your question based on the provided context:

The distribution of background and foreground samples does not change significantly as γ changes","The effect of changing γ on the distribution of the loss for positive examples is minor. For negatives, however, increasing γ heavily concentrates the loss on hard examples, focusing nearly all attention away from easy negatives.","Cumulative distribution functions for positive and negative samples are shown in Figure 4. If we observe the positive samples, we see that the CDF looks fairly similar for different values of \gamma. For example, approximately 20% of the hardest positive samples account for roughly half of the positive loss, as \gamma increases more of the loss gets concentrated in the top 20% of examples, but the effect is minor."
Did increase in Anchor Density improves AP?,"Sure! Here's the answer to your question based on the provided context:

YES, increasing anchor density does improve AP","Increasing anchor density does improve the AP value, but beyond 6-9 anchors, there was no further gain.","We sweep over the number of scale and aspect ratio anchors used at each spatial position and each pyramid level in FPN. We consider cases from a single square anchor at each location to 12 anchors per location spanning 4 sub-octave scales (2^{k/4}, for k\leq 3) and 3 aspect ratios [0.5, 1, 2]. Results using ResNet-50 are shown in Table 1c. A surprisingly good AP (30.3) is achieved using just one square anchor. However, the AP can be improved by nearly 4 points (to 34.0) when using 3 scales and 3 aspect ratios per location. We used this setting for all other experiments in this work. Finally, we note that increasing beyond 6-9 anchors did not shown further gains. Thus while two-stage systems can classify arbitrary boxes in an image, the saturation of performance w.r.t. density implies the higher potential density of two-stage systems may not offer an advantage."
What is the issue with intractable posterior distribution?,"The issue with intractable posterior distribution is that it makes it difficult to perform Bayesian inference using traditional methods such as the EM algorithm and sampling-based solutions like Monte Carlo EM, because these methods rely on the ability to evaluate and differentiate the posterior distribution","The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. The existing method, mean-field variational inference requires factorial. Therefore, authors suggest SGVB (Stochastic Gradient Variational Bayes) estimator that can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters. For coding or data representation tasks, efficient approximation of posterior inference is required.","Very importantly, we do not make the common simplifying assumptions about the marginal or posterior probabilities. Conversely, we are here interested in a general algorithm that even works efficiently in the case of:1.Intractability: the case where the integral of the marginal likelihood p_{\boldsymbol{\theta}}(\mathbf{x})=\int p_{\boldsymbol{\theta}}(\mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})\,d\mathbf{z} is intractable (so we cannot evaluate or differentiate the marginal likelihood), where the true posterior density p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x})=p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{z})/p_{\boldsymbol{\theta}}(\mathbf{x}) is intractable (so the EM algorithm cannot be used), and where the required integrals for any reasonable mean-field VB algorithm are also intractable. These intractabilities are quite common and appear in cases of moderately complicated likelihood functions p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), e.g. a neural network with a nonlinear hidden layer.2.A large dataset: we have so much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. Sampling-based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint."
What are the steps in AEVB algorithm?,"The AEVB algorithm consists of the following steps:

1. Initialize the variational parameters \boldsymbol{\phi} and the generative model parameters \boldsymbol{\theta} randomly from \mathcal{N}(0,0.01).
2. Use the SGVB estimator to optimize the variational lower bound with respect to \boldsymbol{\phi}.
3. Use the optimized \boldsymbol{\phi} to perform efficient approximate posterior inference using simple ancestral sampling.
4. Use the approximate posterior inference model to learn the generative model parameters \boldsymbol{\theta} using stochastic gradient ascent.
5. Jointly optimize \boldsymbol{\phi} and \boldsymbol{\theta} using the MAP criterion.
6. Adapt the stepsize using Adagrad.
7. Use minibatches of size M=100 and L=1 samples per datapoint","The AEVB algorithm connects between directed probabilistic models
(trained with a variational objective) and auto-encoders. It is connected between linear auto-encoders and a certain class of generative linear-Gaussian models.
They use a neural network for the probabilistic encoder q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) (the approximation to the posterior of the generative model p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})) and where the parameters \boldsymbol{\phi} and \boldsymbol{\theta} are optimized jointly with the AEVB algorithm. Using the SGVB estimator to optimize a recognition model allows us to perform very efficient approximate posterior inference using simple ancestral sampling.","How can we perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions?The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques. For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-Encoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling, which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint. The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. When a neural network is used for the recognition model, we arrive at the variational auto-encoder. In this section we’ll give an example where we use a neural network for the probabilistic encoder q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) (the approximation to the posterior of the generative model p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})) and where the parameters \boldsymbol{\phi} and \boldsymbol{\theta} are optimized jointly with the AEVB algorithm. The AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders. A connection between linear auto-encoders and a certain class of generative linear-Gaussian models has long been known. In  [Row98] it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior p(\mathbf{z})=\mathcal{N}(0,\mathbf{I}) and a conditional distribution p(\mathbf{x}|\mathbf{z})=\mathcal{N}(\mathbf{x};\mathbf{W}\mathbf{z},\epsilon\mathbf{I}), specifically the case with infinitesimally small \epsilon. We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational auto-encoder. All parameters, both variational and generative, were initialized by random sampling from \mathcal{N}(0,0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01, 0.02, 0.1} based on performance on the training set in the first few iterations. Minibatches of size M=100 were used, with L=1 samples per datapoint."
What are the two steps specified to generate data?,"Step 1: Generate a value \mathbf{z}^{(i)} from a prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}).

Step 2: Generate a value \mathbf{x}^{(i)} from a conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z})",The process consists of two steps: (1) a value \mathbf{z}^{(i)} is generated from some prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}); (2) a value \mathbf{x}^{(i)} is generated from some conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}).,"In order to solve our problem we invoked an alternative method for generating samples from q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}). The essential parameterization trick is quite simple. Let \mathbf{z} be a continuous random variable, and \mathbf{z}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) be some conditional distribution. It is then often possible to express the random variable \mathbf{z} as a deterministic variable \mathbf{z}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}), where \boldsymbol{\epsilon} is an auxiliary variable with independent marginal p(\boldsymbol{\epsilon}), and gϕ(.)g_{\boldsymbol{\phi}}(.)italic_g start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( . ) is some vector-valued function parameterized by \boldsymbol{\phi}. Let us consider some dataset \mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^{N} consisting of N i.i.d. samples of some continuous or discrete variable \mathbf{x}. We assume that the data are generated by some random process, involving an unobserved continuous random variable \mathbf{z}. The process consists of two steps: (1) a value \mathbf{z}^{(i)} is generated from some prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}); (2) a value \mathbf{x}^{(i)} is generated from some conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}). We assume that the prior p_{\boldsymbol{\theta}^{*}}(\mathbf{z}) and likelihood p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}) come from parametric families of distributions p_{\boldsymbol{\theta}}(\mathbf{z}) and p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), and that their PDFs are differentiable almost everywhere w.r.t. both \boldsymbol{\theta} and \mathbf{z}. Unfortunately, a lot of this process is hidden from our view: the true parameters \boldsymbol{\theta}^{*} as well as the values of the latent variables \mathbf{z}^{(i)} are unknown to us."
Why can’t we use sampling based solutions instead of this algorithm in case of large datasets?,"Sampling-based solutions, such as Monte Carlo EM, are not feasible for large datasets because they involve an expensive sampling loop per datapoint, which would be too slow","It is hard to use sampling based solutions because batch optimization with so much data is too expensive. If you want to inference in almost any model with continuous latent variables and/or parameters, sampling based solution is not applicable. For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator which is one of sampling based solution. But we need to deal with high dimensional data and the AEVB algorithm is useful.","How can we perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions?The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques. For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-Encoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling, which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint. The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. When a neural network is used for the recognition model, we arrive at the variational auto-encoder. For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. More information about the marginal likelihood estimator is available in the appendix. For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. Again, the MNIST dataset was used.The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo (HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in figure 3. Very importantly, we do not make the common simplifying assumptions about the marginal or posterior probabilities. Conversely, we are here interested in a general algorithm that even works efficiently in the case of:1.Intractability: the case where the integral of the marginal likelihood p_{\boldsymbol{\theta}}(\mathbf{x})=\int p_{\boldsymbol{\theta}}(\mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})\,d\mathbf{z} is intractable (so we cannot evaluate or differentiate the marginal likelihood), where the true posterior density p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x})=p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{z})/p_{\boldsymbol{\theta}}(\mathbf{x}) is intractable (so the EM algorithm cannot be used), and where the required integrals for any reasonable mean-field VB algorithm are also intractable. These intractabilities are quite common and appear in cases of moderately complicated likelihood functions p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), e.g. a neural network with a nonlinear hidden layer.2.A large dataset: we have so much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. Sampling-based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint."
What are the uses of approximate posterior inference of the latent variable z given an observed value x for parameters θ?,"Based on the context provided, the uses of approximate posterior inference of the latent variable z given an observed value x for parameters θ include:

1. Efficient approximate ML or MAP estimation for the parameters θ.
2. Efficient approximate posterior inference of the latent variable z given an observed value x for a choice of parameters θ.
3. Efficient approximate marginal inference of the variable x.

These uses are relevant in computer vision applications such as image denoising, inpainting, and super-resolution, where the ability to perform efficient inference on the latent variables and the parameters of the generative model is important. Additionally, the recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) can be interpreted as a probabilistic encoder, and p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) as a probabilistic decoder, which can be useful in coding theory and data representation tasks","For coding or data representation tasks, it is useful to approximate posterior inference of the latent variable \mathbf{z} given an observed value \mathbf{x} efficiently because the unobserved variables z have an interpretation as a latent representation or code. In this paper, authors assume an approximate posterior in the form q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}). They introduce a recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}): an approximation to the intractable true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}). Contrast to mean-field variational inference, this algorithm can compute its parameters \phi from some closed-form expectation by introducing learning the recognition model parameters \boldsymbol{\phi} jointly with the generative model parameters \boldsymbol{\theta}. Given a datapoint \mathbf{x}, it produces a distribution (e.g. a Gaussian) over the possible values of the code \mathbf{z} from which the datapoint \mathbf{x} could have been generated.","Under certain mild conditions outlined in section 2.4 for a chosen approximate posterior q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) we can reparameterize the random variable \widetilde{\mathbf{z}}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) using a differentiable transformation g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}) of an (auxiliary) noise variable \boldsymbol{\epsilon}:\displaystyle\widetilde{\mathbf{z}}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x})\text{\quad with \quad}\boldsymbol{\epsilon}\sim p(\boldsymbol{\epsilon})(4)See section 2.4 for general strategies for chosing such an approriate distribution p(\boldsymbol{\epsilon}) and function g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}).We can now form Monte Carlo estimates of expectations of some function f(\mathbf{z}) w.r.t. q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) as follows:\displaystyle\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[f(\mathbf{z})\right]=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}^{(i)}))\right]\displaystyle\simeq\frac{1}{L}\sum_{l=1}^{L}{f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(l)},\mathbf{x}^{(i)}))}\text{\quad where \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(5)We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic Gradient Variational Bayes (SGVB) estimator \widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}):\displaystyle\widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\displaystyle=\frac{1}{L}\sum_{l=1}^{L}\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)},\mathbf{z}^{(i,l)})-\log q_{\boldsymbol{\phi}}(\mathbf{z}^{(i,l)}|\mathbf{x}^{(i)})\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(6)Often, the KL-divergence DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right] requires estimation by sampling. The KL-divergence term can then be interpreted as regularizing \boldsymbol{\phi}, encouraging the approximate posterior to be close to the prior p_{\boldsymbol{\theta}}(\mathbf{z}).This yields a second version of the SGVB estimator \widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}), corresponding to eq. (3), which typically has less variance than the generic estimator:\displaystyle\widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})=−DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))+1L∑l=1L(logp𝜽(𝐱(i)|𝐳(i,l)))\displaystyle=-D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))+\frac{1}{L}\sum_{l=1}^{L}(\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)}))= - italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) + divide start_ARG 1 end_ARG start_ARG italic_L end_ARG ∑ start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ( roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT ) )\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(7)Given multiple datapoints from a dataset \mathbf{X} with N datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches:\displaystyle\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X})\simeq\widetilde{\mathcal{L}}^{M}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}^{M})=\frac{N}{M}\sum_{i=1}^{M}\widetilde{\mathcal{L}}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})(8)where the minibatch \mathbf{X}^{M}=\{\mathbf{x}^{(i)}\}_{i=1}^{M} is a randomly drawn sample of M datapoints from the full dataset \mathbf{X} with N datapoints. In our experiments we found that the number of samples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M=100. Derivatives \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\widetilde{\mathcal{L}}(\boldsymbol{\theta};\mathbf{X}^{M}) can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to compute the stochastic gradients. Let the prior over the latent variables be the centered isotropic multivariate Gaussian p_{\boldsymbol{\theta}}(\mathbf{z})=\mathcal{N}(\mathbf{z};\mathbf{0},\mathbf{I}). Note that in this case, the prior lacks parameters. We let p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution parameters are computed from \mathbf{z} with a MLP (a fully-connected neural network with a single hidden layer, see appendix C). Note the true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}) is in this case intractable.While there is much freedom in the form q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}), we’ll assume the true (but intractable) posterior takes on a approximate Gaussian form with an approximately diagonal covariance. In this case, we can let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure222Note that this is just a (simplifying) choice, and not a limitation of our method.:\displaystyle\log q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\displaystyle=\log\mathcal{N}(\mathbf{z};\boldsymbol{\mu}^{(i)},\boldsymbol{\sigma}^{2(i)}\mathbf{I})(9)where the mean and s.d. of the approximate posterior, \boldsymbol{\mu}^{(i)} and \boldsymbol{\sigma}^{(i)}, are outputs of the encoding MLP, i.e. nonlinear functions of datapoint \mathbf{x}^{(i)} and the variational parameters \boldsymbol{\phi} (see appendix C). As explained in section 2.4, we sample from the posterior \mathbf{z}^{(i,l)}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)}) using \mathbf{z}^{(i,l)}=g_{\boldsymbol{\phi}}(\mathbf{x}^{(i)},\boldsymbol{\epsilon}^{(l)})=\boldsymbol{\mu}^{(i)}+\boldsymbol{\sigma}^{(i)}\odot\boldsymbol{\epsilon}^{(l)} where \boldsymbol{\epsilon}^{(l)}\sim\mathcal{N}(\mathbf{0},\mathbf{I}). With \odot we signify an element-wise product.In this model both p_{\boldsymbol{\theta}}(\mathbf{z}) (the prior) and q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) are Gaussian; in this case, we can use the estimator of eq. (7) where the KL divergence can be computed and differentiated without estimation (see appendix B). The resulting estimator for this model and datapoint \mathbf{x}^{(i)} is:\displaystyle\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\displaystyle\simeq\frac{1}{2}\sum_{j=1}^{J}\left(1+\log((\sigma_{j}^{(i)})^{2})-(\mu_{j}^{(i)})^{2}-(\sigma_{j}^{(i)})^{2}\right)+\frac{1}{L}\sum_{l=1}^{L}\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)})\displaystyle\text{where\quad}\mathbf{z}^{(i,l)}\displaystyle=\boldsymbol{\mu}^{(i)}+\boldsymbol{\sigma}^{(i)}\odot\boldsymbol{\epsilon}^{(l)}\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim\mathcal{N}(0,\mathbf{I})(10)As explained above and in appendix C, the decoding term \log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)}) is a Bernoulli or Gaussian MLP, depending on the type of data we are modelling. The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example, straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity. Let us consider some dataset \mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^{N} consisting of N i.i.d. samples of some continuous or discrete variable \mathbf{x}. We assume that the data are generated by some random process, involving an unobserved continuous random variable \mathbf{z}. The process consists of two steps: (1) a value \mathbf{z}^{(i)} is generated from some prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}); (2) a value \mathbf{x}^{(i)} is generated from some conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}). We assume that the prior p_{\boldsymbol{\theta}^{*}}(\mathbf{z}) and likelihood p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}) come from parametric families of distributions p_{\boldsymbol{\theta}}(\mathbf{z}) and p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), and that their PDFs are differentiable almost everywhere w.r.t. both \boldsymbol{\theta} and \mathbf{z}. Unfortunately, a lot of this process is hidden from our view: the true parameters \boldsymbol{\theta}^{*} as well as the values of the latent variables \mathbf{z}^{(i)} are unknown to us. We are interested in, and propose a solution to, three related problems in the above scenario:1.Efficient approximate ML or MAP estimation for the parameters \boldsymbol{\theta}. The parameters can be of interest themselves, e.g. if we are analyzing some natural process. They also allow us to mimic the hidden random process and generate artificial data that resembles the real data.2.Efficient approximate posterior inference of the latent variable \mathbf{z} given an observed value \mathbf{x} for a choice of parameters \boldsymbol{\theta}. This is useful for coding or data representation tasks.3.Efficient approximate marginal inference of the variable \mathbf{x}. This allows us to perform all kinds of inference tasks where a prior over \mathbf{x} is required. Common applications in computer vision include image denoising, inpainting and super-resolution. For the purpose of solving the above problems, let us introduce a recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}): an approximation to the intractable true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}). Note that in contrast with the approximate posterior in mean-field variational inference, it is not necessarily factorial and its parameters \boldsymbol{\phi} are not computed from some closed-form expectation. Instead, we’ll introduce a method for learning the recognition model parameters \boldsymbol{\phi} jointly with the generative model parameters \boldsymbol{\theta}. From a coding theory perspective, the unobserved variables \mathbf{z} have an interpretation as a latent representation or code. In this paper we will therefore also refer to the recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) as a probabilistic encoder, since given a datapoint \mathbf{x} it produces a distribution (e.g. a Gaussian) over the possible values of the code \mathbf{z} from which the datapoint \mathbf{x} could have been generated. In a similar vein we will refer to p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) as a probabilistic decoder, since given a code \mathbf{z} it produces a distribution over the possible corresponding values of \mathbf{x}. In this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the parameters. We assume an approximate posterior in the form q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}), but please note that the technique can be applied to the case q_{\boldsymbol{\phi}}(\mathbf{z}), i.e. where we do not condition on \mathbf{x}, as well. The fully variational Bayesian method for inferring a posterior over the parameters is given in the appendix."
What is used as a recognition model in variational auto encoder?,a neural network,"The AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders. Authors introduce a recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}). All parameters, both variational and generative, were initialized by random sampling from \mathcal{N}(0,0.01), and were jointly stochastically optimized using the MAP criterion.","For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-Encoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling, which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint. The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. When a neural network is used for the recognition model, we arrive at the variational auto-encoder. The AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders. A connection between linear auto-encoders and a certain class of generative linear-Gaussian models has long been known. In  [Row98] it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior p(\mathbf{z})=\mathcal{N}(0,\mathbf{I}) and a conditional distribution p(\mathbf{x}|\mathbf{z})=\mathcal{N}(\mathbf{x};\mathbf{W}\mathbf{z},\epsilon\mathbf{I}), specifically the case with infinitesimally small \epsilon. The generative model (encoder) and variational approximation (decoder) from section 3 were used, where the described encoder and decoder have an equal number of hidden units. Since the Frey Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except that the means were constrained to the interval (0,1) using a sigmoidal activation function at the decoder output.Note that with hidden units we refer to the hidden layer of the neural networks of the encoder and decoder. We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational auto-encoder. All parameters, both variational and generative, were initialized by random sampling from \mathcal{N}(0,0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01, 0.02, 0.1} based on performance on the training set in the first few iterations. Minibatches of size M=100 were used, with L=1 samples per datapoint. For the purpose of solving the above problems, let us introduce a recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}): an approximation to the intractable true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}). Note that in contrast with the approximate posterior in mean-field variational inference, it is not necessarily factorial and its parameters \boldsymbol{\phi} are not computed from some closed-form expectation. Instead, we’ll introduce a method for learning the recognition model parameters \boldsymbol{\phi} jointly with the generative model parameters \boldsymbol{\theta}."
Why is the recognition model also referred to as a probabilistic encoder?,The recognition model is referred to as a probabilistic encoder because it produces a distribution over the possible values of the latent code \mathbf{z} from which the observed data point \mathbf{x} could have been generated,"A connection between linear auto-encoders and a certain class of generative linear-Gaussian models has long been known. Therefore, in this paper, given a datapoint \mathbf{x} it produces a distribution (e.g. a Gaussian) over the possible values of the code \mathbf{z} from which the datapoint \mathbf{x} could have been generated.","For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-Encoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling, which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint. The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. When a neural network is used for the recognition model, we arrive at the variational auto-encoder. In this section we’ll give an example where we use a neural network for the probabilistic encoder q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) (the approximation to the posterior of the generative model p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})) and where the parameters \boldsymbol{\phi} and \boldsymbol{\theta} are optimized jointly with the AEVB algorithm. The AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders. A connection between linear auto-encoders and a certain class of generative linear-Gaussian models has long been known. In  [Row98] it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior p(\mathbf{z})=\mathcal{N}(0,\mathbf{I}) and a conditional distribution p(\mathbf{x}|\mathbf{z})=\mathcal{N}(\mathbf{x};\mathbf{W}\mathbf{z},\epsilon\mathbf{I}), specifically the case with infinitesimally small \epsilon. The recently proposed DARN method  [GMW13], also learns a directed probabilistic model using an auto-encoding structure, however their method applies to binary latent variables.Even more recently,  [RMW14] also make the connection between auto-encoders, directed proabilistic models and stochastic variational inference using the reparameterization trick we describe in this paper. Their work was developed independently of ours and provides an additional perspective on AEVB. From a coding theory perspective, the unobserved variables \mathbf{z} have an interpretation as a latent representation or code. In this paper we will therefore also refer to the recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) as a probabilistic encoder, since given a datapoint \mathbf{x} it produces a distribution (e.g. a Gaussian) over the possible values of the code \mathbf{z} from which the datapoint \mathbf{x} could have been generated. In a similar vein we will refer to p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) as a probabilistic decoder, since given a code \mathbf{z} it produces a distribution over the possible corresponding values of \mathbf{x}."
What’s the effect of the gradient of the lower bound w.r.t. φ on the naïve Monte Carlo estimator?,The effect ofhh-h-h-n---h-p-----------------------------nt---ntnt least-nt least leastnt least least leastnt- least least least least least least least least least least least least leastnt least least least least leastnt least least leastnt leastnt least leastnt least least least least least least least least least least least least least least least least least least least least least least least leastnt leastnt least least least leastnt least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least leastnt least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least,"The gradient of the lower bound w.r.t. \boldsymbol{\phi} is a bit problematic. The usual (naïve) Monte Carlo gradient estimator for this type of problem is impractical for our purposes. Because gradient estimator exhibits exhibits very high variance. Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound.","Under certain mild conditions outlined in section 2.4 for a chosen approximate posterior q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) we can reparameterize the random variable \widetilde{\mathbf{z}}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) using a differentiable transformation g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}) of an (auxiliary) noise variable \boldsymbol{\epsilon}:\displaystyle\widetilde{\mathbf{z}}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x})\text{\quad with \quad}\boldsymbol{\epsilon}\sim p(\boldsymbol{\epsilon})(4)See section 2.4 for general strategies for chosing such an approriate distribution p(\boldsymbol{\epsilon}) and function g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}).We can now form Monte Carlo estimates of expectations of some function f(\mathbf{z}) w.r.t. q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) as follows:\displaystyle\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[f(\mathbf{z})\right]=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}^{(i)}))\right]\displaystyle\simeq\frac{1}{L}\sum_{l=1}^{L}{f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(l)},\mathbf{x}^{(i)}))}\text{\quad where \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(5)We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic Gradient Variational Bayes (SGVB) estimator \widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}):\displaystyle\widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\displaystyle=\frac{1}{L}\sum_{l=1}^{L}\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)},\mathbf{z}^{(i,l)})-\log q_{\boldsymbol{\phi}}(\mathbf{z}^{(i,l)}|\mathbf{x}^{(i)})\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(6)Often, the KL-divergence DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right] requires estimation by sampling. The KL-divergence term can then be interpreted as regularizing \boldsymbol{\phi}, encouraging the approximate posterior to be close to the prior p_{\boldsymbol{\theta}}(\mathbf{z}).This yields a second version of the SGVB estimator \widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}), corresponding to eq. (3), which typically has less variance than the generic estimator:\displaystyle\widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})=−DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))+1L∑l=1L(logp𝜽(𝐱(i)|𝐳(i,l)))\displaystyle=-D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))+\frac{1}{L}\sum_{l=1}^{L}(\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)}))= - italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) + divide start_ARG 1 end_ARG start_ARG italic_L end_ARG ∑ start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ( roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT ) )\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(7)Given multiple datapoints from a dataset \mathbf{X} with N datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches:\displaystyle\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X})\simeq\widetilde{\mathcal{L}}^{M}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}^{M})=\frac{N}{M}\sum_{i=1}^{M}\widetilde{\mathcal{L}}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})(8)where the minibatch \mathbf{X}^{M}=\{\mathbf{x}^{(i)}\}_{i=1}^{M} is a randomly drawn sample of M datapoints from the full dataset \mathbf{X} with N datapoints. In our experiments we found that the number of samples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M=100. Derivatives \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\widetilde{\mathcal{L}}(\boldsymbol{\theta};\mathbf{X}^{M}) can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to compute the stochastic gradients. This reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) such that the Monte Carlo estimate of the expectation is differentiable w.r.t. \boldsymbol{\phi}. A proof is as follows. Given the deterministic mapping \mathbf{z}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}) we know that q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})\prod_{i}dz_{i}=p(\boldsymbol{\epsilon})\prod_{i}d\epsilon_{i}. Therefore111Note that for infinitesimals we use the notational convention d\mathbf{z}=\prod_{i}dz_{i}, \int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})f(\mathbf{z})\,d\mathbf{z}=\int p(\boldsymbol{\epsilon})f(\mathbf{z})\,d\boldsymbol{\epsilon}=\int p(\boldsymbol{\epsilon})f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}))\,d\boldsymbol{\epsilon}. It follows that a differentiable estimator can be constructed: \int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})f(\mathbf{z})\,d\mathbf{z}\simeq\frac{1}{L}\sum_{l=1}^{L}f(g_{\boldsymbol{\phi}}(\mathbf{x},\boldsymbol{\epsilon}^{(l)})) where \boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon}). In section 2.3 we applied this trick to obtain a differentiable estimator of the variational lower bound. The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example, straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity. Stochastic variational inference [HBWP13] has recently received increasing interest. Recently, [BJP12] introduced a control variate schemes to reduce the high variance of the naïve gradient estimator discussed in section 2.1, and applied to exponential family approximations of the posterior. In [RGB13] some general methods, i.e. a control variate scheme, were introduced for reducing the variance of the original gradient estimator. In [SK13], a similar reparameterization as in this paper was used in an efficient version of a stochastic variational inference algorithm for learning the natural parameters of exponential-family approximating distributions. Parameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}) (see algorithm  1), plus a small weight decay term corresponding to a prior p(\boldsymbol{\theta})=\mathcal{N}(0,\mathbf{I}). Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound. The marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints \log p_{\boldsymbol{\theta}}(\mathbf{x}^{(1)},\cdots,\mathbf{x}^{(N)})=\sum_{i=1}^{N}\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}), which can each be rewritten as:logp𝜽(𝐱(i))=DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳|𝐱(i)))+ℒ(𝜽,ϕ;𝐱(i))\displaystyle\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)})=D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}^{(i)}))+\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) = italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) ) + caligraphic_L ( bold_italic_θ , bold_italic_ϕ ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT )(1)The first RHS term is the KL divergence of the approximate from the true posterior. Since this KL-divergence is non-negative, the second RHS term \mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}) is called the (variational) lower bound on the marginal likelihood of datapoint i, and can be written as:\displaystyle\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)})\geq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\displaystyle=\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})}\left[-\log q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})+\log p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})\right](2)which can also be written as:ℒ(𝜽,ϕ;𝐱(i))=−DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))+𝔼qϕ⁢(𝐳|𝐱(i))[logp𝜽(𝐱(i)|𝐳)]\displaystyle\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})=-D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))+\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right]caligraphic_L ( bold_italic_θ , bold_italic_ϕ ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) = - italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) + blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT [ roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z ) ](3)We want to differentiate and optimize the lower bound \mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}) w.r.t. both the variational parameters \boldsymbol{\phi} and generative parameters \boldsymbol{\theta}. However, the gradient of the lower bound w.r.t. \boldsymbol{\phi} is a bit problematic. The usual (naïve) Monte Carlo gradient estimator for this type of problem is:\nabla_{\boldsymbol{\phi}}\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z})}\left[f(\mathbf{z})\right]=\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z})}\left[f(\mathbf{z})\nabla_{q_{\boldsymbol{\phi}}(\mathbf{z})}\log q_{\boldsymbol{\phi}}(\mathbf{z})\right]\simeq\frac{1}{L}\sum_{l=1}^{L}f(\mathbf{z})\nabla_{q_{\boldsymbol{\phi}}(\mathbf{z}^{(l)})}\log q_{\boldsymbol{\phi}}(\mathbf{z}^{(l)}) where \mathbf{z}^{(l)}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)}). This gradient estimator exhibits exhibits very high variance (see e.g.  [BJP12]) and is impractical for our purposes."
How to obtain SGVB estimator from variational lower bound?,"The SGVB estimator can be obtained from the variational lower bound by reparameterizing the random variable \widetilde{\mathbf{z}}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) using a differentiable transformation g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}) of an (auxiliary) noise variable \boldsymbol{\epsilon}. This allows for the computation of expectations w.r.t. the approximate posterior using Monte Carlo estimation with the transformed noise variables \boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon}). The SGVB estimator is given by:

$$\mathcal{L}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}) \approx \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})\right] - D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) || p_{\boldsymbol{\theta}}(\mathbf{z}))$$

where $\mathbf{z} = g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x})$ and $\boldsymbol{\epsilon} \sim p(\boldsymbol{\epsilon})$","Authors apply Monte Carlo estimates of expectations of some function f(\mathbf{z}) w.r.t. q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) to the variational lower bound (eq. (2)), yielding generic Stochastic Gradient Variational Bayes (SGVB) estimator. The KL-diverdnence in eq. (3) can be integrated, such that only the reconstruction error requires estimation by sampling. The KL-divergence term regularizes \phi, encouraging the approximate posterior to be close to the prior p_\theta(z) yielding a SGVB estimator. Parameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator.","How can we perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions?The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques. Under certain mild conditions outlined in section 2.4 for a chosen approximate posterior q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) we can reparameterize the random variable \widetilde{\mathbf{z}}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) using a differentiable transformation g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}) of an (auxiliary) noise variable \boldsymbol{\epsilon}:\displaystyle\widetilde{\mathbf{z}}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x})\text{\quad with \quad}\boldsymbol{\epsilon}\sim p(\boldsymbol{\epsilon})(4)See section 2.4 for general strategies for chosing such an approriate distribution p(\boldsymbol{\epsilon}) and function g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}).We can now form Monte Carlo estimates of expectations of some function f(\mathbf{z}) w.r.t. q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) as follows:\displaystyle\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[f(\mathbf{z})\right]=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}^{(i)}))\right]\displaystyle\simeq\frac{1}{L}\sum_{l=1}^{L}{f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(l)},\mathbf{x}^{(i)}))}\text{\quad where \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(5)We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic Gradient Variational Bayes (SGVB) estimator \widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}):\displaystyle\widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\displaystyle=\frac{1}{L}\sum_{l=1}^{L}\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)},\mathbf{z}^{(i,l)})-\log q_{\boldsymbol{\phi}}(\mathbf{z}^{(i,l)}|\mathbf{x}^{(i)})\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(6)Often, the KL-divergence DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right] requires estimation by sampling. The KL-divergence term can then be interpreted as regularizing \boldsymbol{\phi}, encouraging the approximate posterior to be close to the prior p_{\boldsymbol{\theta}}(\mathbf{z}).This yields a second version of the SGVB estimator \widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}), corresponding to eq. (3), which typically has less variance than the generic estimator:\displaystyle\widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})=−DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))+1L∑l=1L(logp𝜽(𝐱(i)|𝐳(i,l)))\displaystyle=-D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))+\frac{1}{L}\sum_{l=1}^{L}(\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)}))= - italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) + divide start_ARG 1 end_ARG start_ARG italic_L end_ARG ∑ start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ( roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT ) )\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(7)Given multiple datapoints from a dataset \mathbf{X} with N datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches:\displaystyle\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X})\simeq\widetilde{\mathcal{L}}^{M}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}^{M})=\frac{N}{M}\sum_{i=1}^{M}\widetilde{\mathcal{L}}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})(8)where the minibatch \mathbf{X}^{M}=\{\mathbf{x}^{(i)}\}_{i=1}^{M} is a randomly drawn sample of M datapoints from the full dataset \mathbf{X} with N datapoints. In our experiments we found that the number of samples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M=100. Derivatives \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\widetilde{\mathcal{L}}(\boldsymbol{\theta};\mathbf{X}^{M}) can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to compute the stochastic gradients. The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example, straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity. Stochastic variational inference [HBWP13] has recently received increasing interest. Recently, [BJP12] introduced a control variate schemes to reduce the high variance of the naïve gradient estimator discussed in section 2.1, and applied to exponential family approximations of the posterior. In [RGB13] some general methods, i.e. a control variate scheme, were introduced for reducing the variance of the original gradient estimator. In [SK13], a similar reparameterization as in this paper was used in an efficient version of a stochastic variational inference algorithm for learning the natural parameters of exponential-family approximating distributions. Parameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}) (see algorithm  1), plus a small weight decay term corresponding to a prior p(\boldsymbol{\theta})=\mathcal{N}(0,\mathbf{I}). Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound. We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables. The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an efficient algorithm for efficient inference and learning, Auto-Encoding VB (AEVB), that learns an approximate inference model using the SGVB estimator. The theoretical advantages are reflected in experimental results."
"Just like AEVB, Wake Sleep algorithm employs a recognition model that approximates the true posterior. Is this true?",Yes,"Yes, is it true. Both of them employ a recognition model that approximates the true posterior. Authors compare  performance of AEVB to the wake-sleep algorithm [HDFN95] employing the same encoder (also called recognition model) for the wake-sleep algorithm and the variational autoencoder. All parameters, both variational and generative, were initialized by random sampling from N (0, 0.01), and were jointly stochastically optimized using the MAP criterion.","The wake-sleep algorithm [HDFN95] is, to the best of our knowledge, the only other on-line learning method in the literature that is applicable to the same general class of continuous latent variable models. Like our method, the wake-sleep algorithm employs a recognition model that approximates the true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood.An advantage of wake-sleep is that it also applies to models with discrete latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint. We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational auto-encoder. All parameters, both variational and generative, were initialized by random sampling from \mathcal{N}(0,0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01, 0.02, 0.1} based on performance on the training set in the first few iterations. Minibatches of size M=100 were used, with L=1 samples per datapoint. Since the SGVB estimator and the AEVB algorithm can be applied to almost any inference and learning problem with continuous latent variables, there are plenty of future directions: (i) learning hierarchical generative architectures with deep neural networks (e.g. convolutional networks) used for the encoders and decoders, trained jointly with AEVB; (ii) time-series models (i.e. dynamic Bayesian networks); (iii) application of SGVB to the global parameters; (iv) supervised models with latent variables, useful for learning complicated noise distributions."
"Compared to AEVB, what is the drawback of Wake Sleep algorithm?","Sure! Here's the answer to your question based on the provided context:

Wake Sleep algorithm requires a concurrent optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood","Wake-Sleep has the same computational complexity as AEVB per datapoint. Moreover, a drawback of the wake-sleep algorithm is that it requires a concurrent optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood and its optimization is slow compared to AEVB. Figure 3 demonstrates the AEVB algorithm to the wake-sleep algorithm, in terms of the estimated marginal likelihood, for a different number of training points.","The wake-sleep algorithm [HDFN95] is, to the best of our knowledge, the only other on-line learning method in the literature that is applicable to the same general class of continuous latent variable models. Like our method, the wake-sleep algorithm employs a recognition model that approximates the true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood.An advantage of wake-sleep is that it also applies to models with discrete latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint. For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. More information about the marginal likelihood estimator is available in the appendix. For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. Again, the MNIST dataset was used.The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo (HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in figure 3."
"According to authors, there’s a connection between PCA and maximum-likelihood (ML) solution of a special case of the linear-Gaussian model. Is this true?",Yes,"Yes, it is. In  [Row98] it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior p(\mathbf{z})=\mathcal{N}(0,\mathbf{I}) and a conditional distribution p(\mathbf{x}|\mathbf{z})=\mathcal{N}(\mathbf{x};\mathbf{W}\mathbf{z},\epsilon\mathbf{I}), specifically the case with infinitesimally small \epsilon.","The AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders. A connection between linear auto-encoders and a certain class of generative linear-Gaussian models has long been known. In  [Row98] it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior p(\mathbf{z})=\mathcal{N}(0,\mathbf{I}) and a conditional distribution p(\mathbf{x}|\mathbf{z})=\mathcal{N}(\mathbf{x};\mathbf{W}\mathbf{z},\epsilon\mathbf{I}), specifically the case with infinitesimally small \epsilon."
"As the data in Frey Face dataset is continuous, how did the authors process it?",The authors processed the continuous data in the Frey Face dataset using a decoder with Gaussian outputs and a sigmoidal activation function at the decoder output,"They consider some dataset \mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^{N} consisting of N i.i.d. samples of some continuous or discrete variable \mathbf{x}. They assume that the data are generated by some random process, involving an unobserved continuous random variable \mathbf{z}. The process consists of two steps: (1) a value \mathbf{z}^{(i)} is generated from some prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}); (2) a value \mathbf{x}^{(i)} is generated from some conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}).","The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example, straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity. The generative model (encoder) and variational approximation (decoder) from section 3 were used, where the described encoder and decoder have an equal number of hidden units. Since the Frey Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except that the means were constrained to the interval (0,1) using a sigmoidal activation function at the decoder output.Note that with hidden units we refer to the hidden layer of the neural networks of the encoder and decoder. Let us consider some dataset \mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^{N} consisting of N i.i.d. samples of some continuous or discrete variable \mathbf{x}. We assume that the data are generated by some random process, involving an unobserved continuous random variable \mathbf{z}. The process consists of two steps: (1) a value \mathbf{z}^{(i)} is generated from some prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}); (2) a value \mathbf{x}^{(i)} is generated from some conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}). We assume that the prior p_{\boldsymbol{\theta}^{*}}(\mathbf{z}) and likelihood p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}) come from parametric families of distributions p_{\boldsymbol{\theta}}(\mathbf{z}) and p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), and that their PDFs are differentiable almost everywhere w.r.t. both \boldsymbol{\theta} and \mathbf{z}. Unfortunately, a lot of this process is hidden from our view: the true parameters \boldsymbol{\theta}^{*} as well as the values of the latent variables \mathbf{z}^{(i)} are unknown to us."
What do we mean by hidden units?,The hidden units refer to the hidden layer of the neural networks in the encoder and decoder,"The chosen number of hidden units is based on prior literature on auto-encoders and as the choice of dataset, hidden units affect overfitting.
As the number of hidden units, we can learn latent representation and it affects the performance of applications such as image denoising, inpainting and super-resolution.","In relevant recent work on autoencoders [VLL{}^{+}10] it was shown that the training criterion of unregularized autoencoders corresponds to maximization of a lower bound (see the infomax principle [Lin89]) of the mutual information between input X and latent representation Z. Maximizing (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional entropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding model [VLL{}^{+}10], i.e. the negative reconstrution error.However, it is well known that this reconstruction criterion is in itself not sufficient for learning useful representations [BCV13].Regularization techniques have been proposed to make autoencoders learn useful representations, such as denoising, contractive and sparse autoencoder variants  [BCV13]. The SGVB objective contains a regularization term dictated by the variational bound (e.g. eq. (10)), lacking the usual nuisance regularization hyperparameter required to learn useful representations.Related are also encoder-decoder architectures such as the predictive sparse decomposition (PSD) [KRL08], from which we drew some inspiration. Also relevant are the recently introduced Generative Stochastic Networks [BTL13] where noisy auto-encoders learn the transition operator of a Markov chain that samples from the data distribution. In [SL10] a recognition model was employed for efficient learning with Deep Boltzmann Machines.These methods are targeted at either unnormalized models (i.e. undirected models like Boltzmann machines) or limited to sparse coding models, in contrast to our proposed algorithm for learning a general class of directed probabilistic models. The generative model (encoder) and variational approximation (decoder) from section 3 were used, where the described encoder and decoder have an equal number of hidden units. Since the Frey Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except that the means were constrained to the interval (0,1) using a sigmoidal activation function at the decoder output.Note that with hidden units we refer to the hidden layer of the neural networks of the encoder and decoder. We trained generative models (decoders) and corresponding encoders (a.k.a. recognition models) having 500 hidden units in case of MNIST, and 200 hidden units in case of the Frey Face dataset (to prevent overfitting, since it is a considerably smaller dataset). The chosen number of hidden units is based on prior literature on auto-encoders, and the relative performance of different algorithms was not very sensitive to these choices. Figure 2 shows the results when comparing the lower bounds. Interestingly, superfluous latent variables did not result in overfitting, which is explained by the regularizing nature of the variational bound. For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. More information about the marginal likelihood estimator is available in the appendix. For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. Again, the MNIST dataset was used.The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo (HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in figure 3."
How can the SGVB be optimised?,Answer: The SGVB can be optimized using standard stochastic gradient methods,"Authors form Monte Carlo estimates of expectations of some function f(\mathbf{z}) w.r.t. q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) as Eq. (5). They use this technique to the variational lower bound (eq. (2)), yielding generic Stochastic Gradient Variational Bayes (SGVB) estimator. All parameters, both variational and generative, were initialized by random sampling from \mathcal{N}(0,0.01), and were jointly stochastically optimized using the MAP criterion. Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound.","How can we perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions?The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques. Under certain mild conditions outlined in section 2.4 for a chosen approximate posterior q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) we can reparameterize the random variable \widetilde{\mathbf{z}}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) using a differentiable transformation g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}) of an (auxiliary) noise variable \boldsymbol{\epsilon}:\displaystyle\widetilde{\mathbf{z}}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x})\text{\quad with \quad}\boldsymbol{\epsilon}\sim p(\boldsymbol{\epsilon})(4)See section 2.4 for general strategies for chosing such an approriate distribution p(\boldsymbol{\epsilon}) and function g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}).We can now form Monte Carlo estimates of expectations of some function f(\mathbf{z}) w.r.t. q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) as follows:\displaystyle\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[f(\mathbf{z})\right]=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}^{(i)}))\right]\displaystyle\simeq\frac{1}{L}\sum_{l=1}^{L}{f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(l)},\mathbf{x}^{(i)}))}\text{\quad where \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(5)We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic Gradient Variational Bayes (SGVB) estimator \widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}):\displaystyle\widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\displaystyle=\frac{1}{L}\sum_{l=1}^{L}\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)},\mathbf{z}^{(i,l)})-\log q_{\boldsymbol{\phi}}(\mathbf{z}^{(i,l)}|\mathbf{x}^{(i)})\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(6)Often, the KL-divergence DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right] requires estimation by sampling. The KL-divergence term can then be interpreted as regularizing \boldsymbol{\phi}, encouraging the approximate posterior to be close to the prior p_{\boldsymbol{\theta}}(\mathbf{z}).This yields a second version of the SGVB estimator \widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}), corresponding to eq. (3), which typically has less variance than the generic estimator:\displaystyle\widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})=−DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))+1L∑l=1L(logp𝜽(𝐱(i)|𝐳(i,l)))\displaystyle=-D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))+\frac{1}{L}\sum_{l=1}^{L}(\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)}))= - italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) + divide start_ARG 1 end_ARG start_ARG italic_L end_ARG ∑ start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ( roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT ) )\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(7)Given multiple datapoints from a dataset \mathbf{X} with N datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches:\displaystyle\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X})\simeq\widetilde{\mathcal{L}}^{M}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}^{M})=\frac{N}{M}\sum_{i=1}^{M}\widetilde{\mathcal{L}}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})(8)where the minibatch \mathbf{X}^{M}=\{\mathbf{x}^{(i)}\}_{i=1}^{M} is a randomly drawn sample of M datapoints from the full dataset \mathbf{X} with N datapoints. In our experiments we found that the number of samples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M=100. Derivatives \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\widetilde{\mathcal{L}}(\boldsymbol{\theta};\mathbf{X}^{M}) can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to compute the stochastic gradients. Parameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}) (see algorithm  1), plus a small weight decay term corresponding to a prior p(\boldsymbol{\theta})=\mathcal{N}(0,\mathbf{I}). Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound. We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational auto-encoder. All parameters, both variational and generative, were initialized by random sampling from \mathcal{N}(0,0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01, 0.02, 0.1} based on performance on the training set in the first few iterations. Minibatches of size M=100 were used, with L=1 samples per datapoint. We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables. The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an efficient algorithm for efficient inference and learning, Auto-Encoding VB (AEVB), that learns an approximate inference model using the SGVB estimator. The theoretical advantages are reflected in experimental results."
Sometimes the posterior distribution is intractable in nature. What do we mean by intractable?,"Intractable refers to a distribution that cannot be computed or optimized using standard mathematical techniques, typically due to its complex or high-dimensional structure","In this paper, authors want to solve several inference tasks including image denoising, inpainting, and super-resolution by approximating marginal inference of the variable \mathbf{x}. To solve these problems, they introduce a recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}): an approximation to the intractable true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}). The posterior distribution is intractable, thus in this paper, authors introduce the stragegy that can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables.","How can we perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions?The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques. Let the prior over the latent variables be the centered isotropic multivariate Gaussian p_{\boldsymbol{\theta}}(\mathbf{z})=\mathcal{N}(\mathbf{z};\mathbf{0},\mathbf{I}). Note that in this case, the prior lacks parameters. We let p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution parameters are computed from \mathbf{z} with a MLP (a fully-connected neural network with a single hidden layer, see appendix C). Note the true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}) is in this case intractable.While there is much freedom in the form q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}), we’ll assume the true (but intractable) posterior takes on a approximate Gaussian form with an approximately diagonal covariance. In this case, we can let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure222Note that this is just a (simplifying) choice, and not a limitation of our method.:\displaystyle\log q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\displaystyle=\log\mathcal{N}(\mathbf{z};\boldsymbol{\mu}^{(i)},\boldsymbol{\sigma}^{2(i)}\mathbf{I})(9)where the mean and s.d. of the approximate posterior, \boldsymbol{\mu}^{(i)} and \boldsymbol{\sigma}^{(i)}, are outputs of the encoding MLP, i.e. nonlinear functions of datapoint \mathbf{x}^{(i)} and the variational parameters \boldsymbol{\phi} (see appendix C). The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example, straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity. Let us consider some dataset \mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^{N} consisting of N i.i.d. samples of some continuous or discrete variable \mathbf{x}. We assume that the data are generated by some random process, involving an unobserved continuous random variable \mathbf{z}. The process consists of two steps: (1) a value \mathbf{z}^{(i)} is generated from some prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}); (2) a value \mathbf{x}^{(i)} is generated from some conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}). We assume that the prior p_{\boldsymbol{\theta}^{*}}(\mathbf{z}) and likelihood p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}) come from parametric families of distributions p_{\boldsymbol{\theta}}(\mathbf{z}) and p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), and that their PDFs are differentiable almost everywhere w.r.t. both \boldsymbol{\theta} and \mathbf{z}. Unfortunately, a lot of this process is hidden from our view: the true parameters \boldsymbol{\theta}^{*} as well as the values of the latent variables \mathbf{z}^{(i)} are unknown to us. We are interested in, and propose a solution to, three related problems in the above scenario:1.Efficient approximate ML or MAP estimation for the parameters \boldsymbol{\theta}. The parameters can be of interest themselves, e.g. if we are analyzing some natural process. They also allow us to mimic the hidden random process and generate artificial data that resembles the real data.2.Efficient approximate posterior inference of the latent variable \mathbf{z} given an observed value \mathbf{x} for a choice of parameters \boldsymbol{\theta}. This is useful for coding or data representation tasks.3.Efficient approximate marginal inference of the variable \mathbf{x}. This allows us to perform all kinds of inference tasks where a prior over \mathbf{x} is required. Common applications in computer vision include image denoising, inpainting and super-resolution. For the purpose of solving the above problems, let us introduce a recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}): an approximation to the intractable true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}). Note that in contrast with the approximate posterior in mean-field variational inference, it is not necessarily factorial and its parameters \boldsymbol{\phi} are not computed from some closed-form expectation. Instead, we’ll introduce a method for learning the recognition model parameters \boldsymbol{\phi} jointly with the generative model parameters \boldsymbol{\theta}."
Is the computational complexity per data point of wake sleep algorithm same as AEVB or is it different?,The computational complexity per data point of the wake-sleep algorithm is the same as AEVB,Wake-Sleep has the same computational complexity as AEVB per datapoint.,"The wake-sleep algorithm [HDFN95] is, to the best of our knowledge, the only other on-line learning method in the literature that is applicable to the same general class of continuous latent variable models. Like our method, the wake-sleep algorithm employs a recognition model that approximates the true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood.An advantage of wake-sleep is that it also applies to models with discrete latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint. We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational auto-encoder. All parameters, both variational and generative, were initialized by random sampling from \mathcal{N}(0,0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01, 0.02, 0.1} based on performance on the training set in the first few iterations. Minibatches of size M=100 were used, with L=1 samples per datapoint."
A reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Is this estimator differentiable?,"Yes, the lower bound estimator obtained by reparameterizing the variational lower bound is differentiable","Yes, the reparameterization trick is useful and it can be constructed a differentiable estimator. This trick can be used to obtain a differentiable estimator of the variational lower bound. Authours show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound. The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods.","How can we perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions?The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques. Under certain mild conditions outlined in section 2.4 for a chosen approximate posterior q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) we can reparameterize the random variable \widetilde{\mathbf{z}}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) using a differentiable transformation g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}) of an (auxiliary) noise variable \boldsymbol{\epsilon}:\displaystyle\widetilde{\mathbf{z}}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x})\text{\quad with \quad}\boldsymbol{\epsilon}\sim p(\boldsymbol{\epsilon})(4)See section 2.4 for general strategies for chosing such an approriate distribution p(\boldsymbol{\epsilon}) and function g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}).We can now form Monte Carlo estimates of expectations of some function f(\mathbf{z}) w.r.t. q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) as follows:\displaystyle\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[f(\mathbf{z})\right]=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}^{(i)}))\right]\displaystyle\simeq\frac{1}{L}\sum_{l=1}^{L}{f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(l)},\mathbf{x}^{(i)}))}\text{\quad where \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(5)We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic Gradient Variational Bayes (SGVB) estimator \widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}):\displaystyle\widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\displaystyle=\frac{1}{L}\sum_{l=1}^{L}\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)},\mathbf{z}^{(i,l)})-\log q_{\boldsymbol{\phi}}(\mathbf{z}^{(i,l)}|\mathbf{x}^{(i)})\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(6)Often, the KL-divergence DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right] requires estimation by sampling. The KL-divergence term can then be interpreted as regularizing \boldsymbol{\phi}, encouraging the approximate posterior to be close to the prior p_{\boldsymbol{\theta}}(\mathbf{z}).This yields a second version of the SGVB estimator \widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}), corresponding to eq. (3), which typically has less variance than the generic estimator:\displaystyle\widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})=−DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))+1L∑l=1L(logp𝜽(𝐱(i)|𝐳(i,l)))\displaystyle=-D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))+\frac{1}{L}\sum_{l=1}^{L}(\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)}))= - italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) + divide start_ARG 1 end_ARG start_ARG italic_L end_ARG ∑ start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ( roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT ) )\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(7)Given multiple datapoints from a dataset \mathbf{X} with N datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches:\displaystyle\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X})\simeq\widetilde{\mathcal{L}}^{M}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}^{M})=\frac{N}{M}\sum_{i=1}^{M}\widetilde{\mathcal{L}}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})(8)where the minibatch \mathbf{X}^{M}=\{\mathbf{x}^{(i)}\}_{i=1}^{M} is a randomly drawn sample of M datapoints from the full dataset \mathbf{X} with N datapoints. In our experiments we found that the number of samples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M=100. Derivatives \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\widetilde{\mathcal{L}}(\boldsymbol{\theta};\mathbf{X}^{M}) can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to compute the stochastic gradients. This reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) such that the Monte Carlo estimate of the expectation is differentiable w.r.t. \boldsymbol{\phi}. A proof is as follows. Given the deterministic mapping \mathbf{z}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}) we know that q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})\prod_{i}dz_{i}=p(\boldsymbol{\epsilon})\prod_{i}d\epsilon_{i}. Therefore111Note that for infinitesimals we use the notational convention d\mathbf{z}=\prod_{i}dz_{i}, \int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})f(\mathbf{z})\,d\mathbf{z}=\int p(\boldsymbol{\epsilon})f(\mathbf{z})\,d\boldsymbol{\epsilon}=\int p(\boldsymbol{\epsilon})f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}))\,d\boldsymbol{\epsilon}. It follows that a differentiable estimator can be constructed: \int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})f(\mathbf{z})\,d\mathbf{z}\simeq\frac{1}{L}\sum_{l=1}^{L}f(g_{\boldsymbol{\phi}}(\mathbf{x},\boldsymbol{\epsilon}^{(l)})) where \boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon}). In section 2.3 we applied this trick to obtain a differentiable estimator of the variational lower bound. The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example, straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity. Parameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}) (see algorithm  1), plus a small weight decay term corresponding to a prior p(\boldsymbol{\theta})=\mathcal{N}(0,\mathbf{I}). Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound. We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables. The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an efficient algorithm for efficient inference and learning, Auto-Encoding VB (AEVB), that learns an approximate inference model using the SGVB estimator. The theoretical advantages are reflected in experimental results. In this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the parameters. We assume an approximate posterior in the form q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}), but please note that the technique can be applied to the case q_{\boldsymbol{\phi}}(\mathbf{z}), i.e. where we do not condition on \mathbf{x}, as well. The fully variational Bayesian method for inferring a posterior over the parameters is given in the appendix."
Why are the parameters important? What can we do with them?,"The parameters are important because they allow us to mimic the hidden random process and generate artificial data that resembles the real data, and they also allow us to perform all kinds of inference tasks where a prior over the variable \mathbf{x} is required, such as image denoising, inpainting, and super-resolution",Authors introduce a practical estimator of the lower bound and its derivatives w.r.t. the parameters. They introduce a recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}): an approximation to the intractable true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}). It is possible to perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions,"How can we perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions?The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques. Parameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}) (see algorithm  1), plus a small weight decay term corresponding to a prior p(\boldsymbol{\theta})=\mathcal{N}(0,\mathbf{I}). Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound. Let us consider some dataset \mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^{N} consisting of N i.i.d. samples of some continuous or discrete variable \mathbf{x}. We assume that the data are generated by some random process, involving an unobserved continuous random variable \mathbf{z}. The process consists of two steps: (1) a value \mathbf{z}^{(i)} is generated from some prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}); (2) a value \mathbf{x}^{(i)} is generated from some conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}). We assume that the prior p_{\boldsymbol{\theta}^{*}}(\mathbf{z}) and likelihood p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}) come from parametric families of distributions p_{\boldsymbol{\theta}}(\mathbf{z}) and p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), and that their PDFs are differentiable almost everywhere w.r.t. both \boldsymbol{\theta} and \mathbf{z}. Unfortunately, a lot of this process is hidden from our view: the true parameters \boldsymbol{\theta}^{*} as well as the values of the latent variables \mathbf{z}^{(i)} are unknown to us. Very importantly, we do not make the common simplifying assumptions about the marginal or posterior probabilities. Conversely, we are here interested in a general algorithm that even works efficiently in the case of:1.Intractability: the case where the integral of the marginal likelihood p_{\boldsymbol{\theta}}(\mathbf{x})=\int p_{\boldsymbol{\theta}}(\mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})\,d\mathbf{z} is intractable (so we cannot evaluate or differentiate the marginal likelihood), where the true posterior density p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x})=p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{z})/p_{\boldsymbol{\theta}}(\mathbf{x}) is intractable (so the EM algorithm cannot be used), and where the required integrals for any reasonable mean-field VB algorithm are also intractable. These intractabilities are quite common and appear in cases of moderately complicated likelihood functions p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), e.g. a neural network with a nonlinear hidden layer.2.A large dataset: we have so much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. Sampling-based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint. We are interested in, and propose a solution to, three related problems in the above scenario:1.Efficient approximate ML or MAP estimation for the parameters \boldsymbol{\theta}. The parameters can be of interest themselves, e.g. if we are analyzing some natural process. They also allow us to mimic the hidden random process and generate artificial data that resembles the real data.2.Efficient approximate posterior inference of the latent variable \mathbf{z} given an observed value \mathbf{x} for a choice of parameters \boldsymbol{\theta}. This is useful for coding or data representation tasks.3.Efficient approximate marginal inference of the variable \mathbf{x}. This allows us to perform all kinds of inference tasks where a prior over \mathbf{x} is required. Common applications in computer vision include image denoising, inpainting and super-resolution. For the purpose of solving the above problems, let us introduce a recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}): an approximation to the intractable true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}). Note that in contrast with the approximate posterior in mean-field variational inference, it is not necessarily factorial and its parameters \boldsymbol{\phi} are not computed from some closed-form expectation. Instead, we’ll introduce a method for learning the recognition model parameters \boldsymbol{\phi} jointly with the generative model parameters \boldsymbol{\theta}. In this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the parameters. We assume an approximate posterior in the form q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}), but please note that the technique can be applied to the case q_{\boldsymbol{\phi}}(\mathbf{z}), i.e. where we do not condition on \mathbf{x}, as well. The fully variational Bayesian method for inferring a posterior over the parameters is given in the appendix."
What should be the size of the latent space for generative model in case of MNIST datasets for getting the best results?,The best results for generative models on MNIST datasets are achieved with a latent space size of 200-300 hidden units,"For higher dimensional latent space the estimates became unreliable and authors use the MNIST dataset which is a low dimensional dataset. For likelihood lower bound, they trained generative models (decoders) and corresponding encoders
(a.k.a. recognition models) having 500 hidden units in case of MNIST. And for very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. For the encoder and decoder they used neural networks with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable.","We trained generative models (decoders) and corresponding encoders (a.k.a. recognition models) having 500 hidden units in case of MNIST, and 200 hidden units in case of the Frey Face dataset (to prevent overfitting, since it is a considerably smaller dataset). The chosen number of hidden units is based on prior literature on auto-encoders, and the relative performance of different algorithms was not very sensitive to these choices. Figure 2 shows the results when comparing the lower bounds. Interestingly, superfluous latent variables did not result in overfitting, which is explained by the regularizing nature of the variational bound. For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. More information about the marginal likelihood estimator is available in the appendix. For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. Again, the MNIST dataset was used.The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo (HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in figure 3."
What do we mean by i.i.d. datasets?,Independent and identically distributed (i.i.d.) datasets,"They show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. For the case of i.i.d. datasets, they can inference and learning efficiently.","Let us consider some dataset \mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^{N} consisting of N i.i.d. samples of some continuous or discrete variable \mathbf{x}. We assume that the data are generated by some random process, involving an unobserved continuous random variable \mathbf{z}. The process consists of two steps: (1) a value \mathbf{z}^{(i)} is generated from some prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}); (2) a value \mathbf{x}^{(i)} is generated from some conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}). We assume that the prior p_{\boldsymbol{\theta}^{*}}(\mathbf{z}) and likelihood p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}) come from parametric families of distributions p_{\boldsymbol{\theta}}(\mathbf{z}) and p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), and that their PDFs are differentiable almost everywhere w.r.t. both \boldsymbol{\theta} and \mathbf{z}. Unfortunately, a lot of this process is hidden from our view: the true parameters \boldsymbol{\theta}^{*} as well as the values of the latent variables \mathbf{z}^{(i)} are unknown to us. We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables. The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an efficient algorithm for efficient inference and learning, Auto-Encoding VB (AEVB), that learns an approximate inference model using the SGVB estimator. The theoretical advantages are reflected in experimental results."
How is the conditioning vector obtained?,"The conditioning vector \mathbf{c} is obtained by using a pre-trained language model, specifically T5-XXL, to generate an embedding of a tokenized text prompt \mathbf{P}. The tokenizer used is SentencePiece, and the language model is conditioned on the token identifier vector f(\mathbf{P}) to produce the embedding \mathbf{c}","From text prompts, the conditioning embedding vector is obtained. The text prompt is first tokenized to provide a fixed length token identification vector, and then the language model maps this vector to produce an embedding that serves as a conditioning vector.","Diffusion models are probabilistic generative models that are trained to learn a data distribution by the gradual denoising of a variable sampled from a Gaussian distribution. Specifically, this corresponds to learning the reverse process of a fixed-length Markovian forward process. In simple terms, a conditional diffusion model \hat{\mathbf{x}}_{\theta} is trained using a squared error loss to denoise a variably-noised image \mathbf{z}_{t}\coloneqq\alpha_{t}\mathbf{x}+\sigma_{t}{\bm{\epsilon}} as follows:\mathbb{E}_{\mathbf{x},\mathbf{c},{\bm{\epsilon}},t}\!\left[w_{t}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t}\mathbf{x}+\sigma_{t}{\bm{\epsilon}},\mathbf{c})-\mathbf{x}\|^{2}_{2}\right](1)where \mathbf{x} is the ground-truth image, \mathbf{c} is a conditioning vector (e.g., obtained from a text prompt), {\bm{\epsilon}}\sim\mathcal{N}(\mathbf{0},\mathbf{I}) is a noise term and \alpha_{t},\sigma_{t},w_{t} are terms that control the noise schedule and sample quality, and are functions of the diffusion process time t\sim\mathcal{U}([0,1]).At inference time, the diffusion model is sampled by iteratively denoising \mathbf{z}_{t_{1}}\sim\mathcal{N}(\mathbf{0},\mathbf{I}) using either the deterministic DDIM song2020denoising  or the stochastic ancestral sampler ho2020denoising . Intermediate points \mathbf{z}_{t_{1}},\dotsc,\mathbf{z}_{t_{T}}, where 1=t_{1}>\cdots>t_{T}=0, are generated, with decreasing noise levels. These points, \hat{\mathbf{x}}^{t}_{0}\coloneqq\hat{\mathbf{x}}_{\theta}(\mathbf{z}_{t},\mathbf{c}), are functions of the \mathbf{x}-predictions. The details of text-conditioning in text-to-image diffusion models are of high importance for visual quality and semantic fidelity. Ramesh et al. ramesh2022hierarchical  use CLIP text embeddings that are translated into image embeddings using a learned prior, while Saharia et al. saharia2022photorealistic  use a pre-trained T5-XXL language model raffel2020exploring . In our work, we use the latter.Language models like T5-XXL generate embeddings of a tokenized text prompt, and vocabulary encoding is an important pre-processing step for prompt embedding. In order to transform a text prompt \mathbf{P} into a conditioning embedding \mathbf{c}, the text is first tokenized using a tokenizer f using a learned vocabulary. Following saharia2022photorealistic , we use the SentencePiece tokenizer kudo2018sentencepiece . After tokenizing a prompt \mathbf{P} using tokenizer f we obtain a fixed-length vector f(\mathbf{P}). The language model \Gamma is conditioned on this token identifier vector to produce an embedding \mathbf{c}\coloneqq\Gamma(f(\mathbf{P})). Finally, the text-to-image diffusion model is directly conditioned on \mathbf{c}."
Which language model is used for vocabulary encoding and why?,"T5-XXL.

The reason is that the text-to-image diffusion model used in the context is conditioned on the embedding produced by the T5-XXL language model, which is a pre-trained language model that generates embeddings of a tokenized text prompt. The use of T5-XXL for vocabulary encoding is important for semantic fidelity and visual quality in the text-to-image diffusion process",CLIP and T5-XXL language models are used for translating noise into image embeddings. Authors’ used T5-XXL for their model. Vocabulary encoding is a pre-processing step for these language models which is used to convert the prompt into a condition embedding vector.,"The details of text-conditioning in text-to-image diffusion models are of high importance for visual quality and semantic fidelity. Ramesh et al. ramesh2022hierarchical  use CLIP text embeddings that are translated into image embeddings using a learned prior, while Saharia et al. saharia2022photorealistic  use a pre-trained T5-XXL language model raffel2020exploring . In our work, we use the latter.Language models like T5-XXL generate embeddings of a tokenized text prompt, and vocabulary encoding is an important pre-processing step for prompt embedding. In order to transform a text prompt \mathbf{P} into a conditioning embedding \mathbf{c}, the text is first tokenized using a tokenizer f using a learned vocabulary. Following saharia2022photorealistic , we use the SentencePiece tokenizer kudo2018sentencepiece . After tokenizing a prompt \mathbf{P} using tokenizer f we obtain a fixed-length vector f(\mathbf{P}). The language model \Gamma is conditioned on this token identifier vector to produce an embedding \mathbf{c}\coloneqq\Gamma(f(\mathbf{P})). Finally, the text-to-image diffusion model is directly conditioned on \mathbf{c}."
What kind of output variations are possible with dreambooth?,"Vast variations are possible with dreambooth, including changes to the subject's place, properties such as color, species, shape, pose, expression, material, and other semantic modifications","Output variants include changing the subject's location, species, color, shape, pose, expression, material, and semantics.","Given only a few (3-5) casually captured images of a specific subject, without any textual description, our objective is to generate new images of the subject with high detail fidelity and with variations guided by text prompts.We do not impose any restrictions on input image capture settings and the subject image can have varying contexts.Examples of output variations include: changing the place where the subject is, changing a property of the subject such as color, species, or shape, and modifying the subject’s pose, expression, material, and other semantic modifications. We find that the breadth of these modifications is very large given the powerful prior of these models. A high-level overview of our method is presented in Figure 3."
How are the input images labelled while fine tuning the model?,"The input images are labelled with a unique identifier and a coarse class descriptor (e.g. cat, dog, watch, etc.) to tether the prior of the class to the subject and leverage the diffusion model's prior for generating new poses and articulations","Authors fine-tuned all the layers that contain text embeddings and these embeddings were created from labels( ""a [identifier] [class noun”) of all input photographs of the subject.","We opt for a simpler approach and label all input images of the subject “a [identifier] [class noun]”, where [identifier] is a unique identifier linked to the subject and [class noun] is a coarse class descriptor of the subject (e.g. cat, dog, watch, etc.). The class descriptor can be obtained using a classifier. We specifically use a class descriptor in the sentence in order to tether the prior of the class to our unique subject. We found that using only a unique identifier, without a class noun, as a key for our subject increased training time and decreased performance. In essence, we want to leverage the diffusion model’s prior of the specific class and entangle it with the embedding of our subject’s unique identifier. In this way, it can leverage the visual prior to generate new poses and articulations of the subject in different contexts. Since our input image set is quite small, fine-tuning the large image generation models can overfit to both the context and the appearance of the subject in the given input images (e.g., subject pose).Figure 12 (top) shows some sample generated images with naive fine-tuning where we clearly see that both the subject dog’s appearance and context are overfitted to those in the input images.There are many techniques that can be used to address these problems, such as regularization or selectively fine-tuning certain parts of the model. There is uncertainty on which parts of the model need to be frozen to both obtain good subject fidelity and semantic modification flexibility. In our experience, the best results that achieve maximum subject fidelity are achieved by fine-tuning all layers of the model. Nevertheless, this includes fine-tuning layers that are conditioned on the text embeddings, which gives rise to the problem of language drift."
Why do we use a class descriptor while fine tuning the model?,"We use a class descriptor while fine-tuning the model to leverage the diffusion model's prior of the specific class and entangle it with the embedding of our subject's unique identifier, allowing the model to generate new poses and articulations of the subject in different contexts","Visual prior to specific class can generate new poses and articulations of the subject in different contexts. Additionally, incorporating the class descriptor speeds up training and improves results.","We opt for a simpler approach and label all input images of the subject “a [identifier] [class noun]”, where [identifier] is a unique identifier linked to the subject and [class noun] is a coarse class descriptor of the subject (e.g. cat, dog, watch, etc.). The class descriptor can be obtained using a classifier. We specifically use a class descriptor in the sentence in order to tether the prior of the class to our unique subject. We found that using only a unique identifier, without a class noun, as a key for our subject increased training time and decreased performance. In essence, we want to leverage the diffusion model’s prior of the specific class and entangle it with the embedding of our subject’s unique identifier. In this way, it can leverage the visual prior to generate new poses and articulations of the subject in different contexts."
Why shouldn’t an existing word be used as an identifier while fine tuning?,"Using an existing word as an identifier can lead to a strong prior in the language model and diffusion model, causing the model to entangle the word's original meaning with the subject's appearance, resulting in decreased performance and training time","Existing words in the training set of text-to-image diffusion models have stronger priors, hence they shouldn't be employed as identifiers during fine tuning.","A naive way of constructing an identifier for our subject is to use an existing word. For example, using the words like “unique” or “special”. One problem is that existing English words tend to have a stronger prior due to occurrence in the training set of text-to-image diffusion models. We generally find increased training time and decreased performance when using such generic words to index our subject, since the model has to both learn to disentangle them from their original meaning and to re-entangle them to reference our subject. This approach can also fail by entangling the meaning of the word with the appearance of our object, for example in the extreme case if the identifier chosen is the adjective “blue” and our subject is grey, colors will be entangled at inference and we will sample a mix of grey and blue subjects (as well as mixes of both).This motivates the need for an identifier that has a weak prior in both the language model and the diffusion model.A hazardous way of doing this is to select random characters in the English language and concatenate them to generate a rare identifier (e.g. “xxy5syt00”). In reality, the tokenizer might tokenize each letter separately, and the prior for the diffusion model is strong for these letters. Specifically, if we sample the model with such an identifier before fine-tuning we will get pictorial depictions of the letters or concepts that are linked to those letters. We often find that these tokens incur the same weaknesses as using common English words to index the subject."
Why can’t we use a string of random letters as an identifier while fine tuning the model?,"The use of a string of random letters as an identifier can be problematic because the tokenizer may tokenize each letter separately, resulting in a strong prior for the diffusion model and pictorial depictions of the letters or concepts linked to those letters","Theoretically, concatenating random characters to create an unique identifier has weak prior in both the language model and the diffusion model. However, in practice, it has strong prior since the model tokenizes each letter independently, which increases training and reduces performance.","A naive way of constructing an identifier for our subject is to use an existing word. For example, using the words like “unique” or “special”. One problem is that existing English words tend to have a stronger prior due to occurrence in the training set of text-to-image diffusion models. We generally find increased training time and decreased performance when using such generic words to index our subject, since the model has to both learn to disentangle them from their original meaning and to re-entangle them to reference our subject. This approach can also fail by entangling the meaning of the word with the appearance of our object, for example in the extreme case if the identifier chosen is the adjective “blue” and our subject is grey, colors will be entangled at inference and we will sample a mix of grey and blue subjects (as well as mixes of both).This motivates the need for an identifier that has a weak prior in both the language model and the diffusion model.A hazardous way of doing this is to select random characters in the English language and concatenate them to generate a rare identifier (e.g. “xxy5syt00”). In reality, the tokenizer might tokenize each letter separately, and the prior for the diffusion model is strong for these letters. Specifically, if we sample the model with such an identifier before fine-tuning we will get pictorial depictions of the letters or concepts that are linked to those letters. We often find that these tokens incur the same weaknesses as using common English words to index the subject."
How to create rare token identifier?,"Use a tokenizer to map character sequences to tokens, perform a rare-token lookup in the vocabulary to obtain a sequence of rare token identifiers, and then invert the vocabulary using a de-tokenizer to obtain a unique identifier","In order to get a list of rare token IDs, we first search for rare tokens in the vocabulary. The de-tokenizer is then used to invert the vocabulary, resulting in a string of characters that serves as our rare-token identifier.","In a nutshell, our approach is to find relatively rare tokens in the vocabulary, and then invert these rare tokens into text space. In order to do this, we first perform a rare-token lookup in the vocabulary and obtain a sequence of rare token identifiers f(\hat{\mathbf{V}}), where f is a tokenizer; a function that maps character sequences to tokens and \hat{\mathbf{V}} is the decoded text stemming from the tokens f(\hat{\mathbf{V}}). This sequence can be of variable length k with k being a hyperparameter of our method. We find that relatively short sequences of k=\{1,...,3\} work well. Then, by inverting the vocabulary using the de-tokenizer on f(\hat{\mathbf{V}}) we obtain a sequence of characters that define our unique identifier \hat{\mathbf{V}}. We observe that using uniform random sampling without replacement of tokens that correspond to 3 or fewer Unicode characters (without spaces) and using tokens in the T5-XXL tokenizer range of \{5000,...,10000\} works well."
How to deal with overfitting due to small input set while fine tuning the text-to-img model?,"Regularization and selective fine-tuning can help address overfitting due to small input sets, but the best results for maximum subject fidelity are achieved by fine-tuning all layers of the model, including those conditioned on text embeddings, which can lead to language drift","For small input sets, Fine-tuning  large image generation models can overfit context and subject appearance. Regularization or selective model fine-tuning can solve these issues. Fine-tuning all model layers achieves maximum subject fidelity and gives the best result.","Since our input image set is quite small, fine-tuning the large image generation models can overfit to both the context and the appearance of the subject in the given input images (e.g., subject pose).Figure 12 (top) shows some sample generated images with naive fine-tuning where we clearly see that both the subject dog’s appearance and context are overfitted to those in the input images.There are many techniques that can be used to address these problems, such as regularization or selectively fine-tuning certain parts of the model. There is uncertainty on which parts of the model need to be frozen to both obtain good subject fidelity and semantic modification flexibility. In our experience, the best results that achieve maximum subject fidelity are achieved by fine-tuning all layers of the model. Nevertheless, this includes fine-tuning layers that are conditioned on the text embeddings, which gives rise to the problem of language drift."
What is language drift and how to deal with it?,"Language drift refers to the phenomenon where a language model, pre-trained on a large text corpus and fine-tuned for a specific task, gradually forgets the syntactic and semantic knowledge of the language as it adapts to the target task. To deal with language drift, the authors propose an autogenous class-specific prior-preserving loss that encourages the diffusion model to retain the prior knowledge of the class, even when fine-tuned on a small set of subject images",Language drift occurs when a language model pre-trained on a large text corpus and fine-tuned for a specific task loses syntactic and semantic understanding as it improves learning the target task only. Authors suggested that their novel autogenous class-specific prior-preserving loss solves this issue.,"In order to accomplish this, our first task is to implant the subject instance into the output domain of the model and to bind the subject with a unique identifier. We present our method to design the identifier below as well as a new approach to supervise a fine-tuning process of the model such that it re-uses its prior for our subject instance. A key problem is that fine-tuning on a small set of images showing our subject is prone to overfitting on the given images. In addition, language drift Lee2019CounteringLD ; lu2020countering  is a common problem in language models, and manifests itself in text-to-image diffusion models as well: the model can forget how to generate other subjects of the same class, and lose the embedded knowledge on the diversity and natural variations of instances belonging to that class. For this, we present an autogenous class-specific prior preservation loss, where we alleviate overfitting and prevent language drift by encouraging the diffusion model to keep generating diverse instances of the same class as our subject. The phenomenon of language drift has been an observed problem in the language model literature Lee2019CounteringLD ; lu2020countering , where a language model that is pre-trained on a large text corpus and later fine-tuned for a specific task progressively loses syntactic and semantic knowledge of the language as it learns to improve in the target task. To the best of our knowledge, we are the first to find a similar phenomenon affecting diffusion models. Since our text prompt contains both the [identifier] and [class noun], when a diffusion model is fine-tuned on a small set of subject images, we observe that it slowly forgets how to generate subjects of the same class and progressively forgets the class-specific prior and can not generate different instances of the class in question. Figure 13 (middle) shows some sample generated images of “a dog” after fine-tuning the model on specific dog images. The results clearly show that the model loses the capability of generating generic dog images with naive fine-tuning. We propose an autogenous class-specific prior-preserving loss to counter both the overfitting and language drift issues. In essence, our method is to supervise the model with its own generated samples, in order for it to retain the prior once the few-shot fine-tuning begins. Specifically, we generate data \mathbf{x}_{\text{pr}}=\hat{\mathbf{x}}(\mathbf{z}_{t_{1}},\mathbf{c}_{\text{pr}}) by using the ancestral sampler on the frozen pre-trained diffusion model with random initial noise \mathbf{z}_{t_{1}}\sim\mathcal{N}(\mathbf{0},\mathbf{I}) and conditioning vector \mathbf{c}_{\text{pr}}\coloneqq\Gamma(f(\text{""a [class noun]""})). The loss becomes:\mathbb{E}_{\mathbf{x},\mathbf{c},{\bm{\epsilon}},{\bm{\epsilon}}^{\prime},t}\!\left[w_{t}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t}\mathbf{x}+\sigma_{t}{\bm{\epsilon}},\mathbf{c})-\mathbf{x}\|^{2}_{2}+\lambda w_{t^{\prime}}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t^{\prime}}\mathbf{x}_{\text{pr}}+\sigma_{t^{\prime}}{\bm{\epsilon}}^{\prime},\mathbf{c}_{\text{pr}})-\mathbf{x}_{\text{pr}}\|^{2}_{2}\right],(2)where \lambda controls for the relative weight of the prior-preservation term. Figure 4 illustrates the model fine-tuning with the class-generated samples and prior-preservation loss.Despite being simple, we find this prior-preservation loss is effective in overcoming the overfitting and language-drift issues.We find that \sim 200 epochs at learning rate 10^{-5} with \lambda=1 is enough to achieve good results. During this process, \sim 200\times N “a [class noun]” samples are generated, with N being the size of the subject dataset, usually ranging from 3-5 images. The training process takes about 15 minutes on one TPUv4."
What is prior preservation loss?,"A loss function that encourages the model to retain the prior distribution of the data, in order to overcome overfitting and language drift issues",Prior preservation loss supervises the model with its own samples to keep the prior during few-shot fine-tuning.The loss equation is presented in the evidential paragraph.,"We propose an autogenous class-specific prior-preserving loss to counter both the overfitting and language drift issues. In essence, our method is to supervise the model with its own generated samples, in order for it to retain the prior once the few-shot fine-tuning begins. Specifically, we generate data \mathbf{x}_{\text{pr}}=\hat{\mathbf{x}}(\mathbf{z}_{t_{1}},\mathbf{c}_{\text{pr}}) by using the ancestral sampler on the frozen pre-trained diffusion model with random initial noise \mathbf{z}_{t_{1}}\sim\mathcal{N}(\mathbf{0},\mathbf{I}) and conditioning vector \mathbf{c}_{\text{pr}}\coloneqq\Gamma(f(\text{""a [class noun]""})). The loss becomes:\mathbb{E}_{\mathbf{x},\mathbf{c},{\bm{\epsilon}},{\bm{\epsilon}}^{\prime},t}\!\left[w_{t}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t}\mathbf{x}+\sigma_{t}{\bm{\epsilon}},\mathbf{c})-\mathbf{x}\|^{2}_{2}+\lambda w_{t^{\prime}}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t^{\prime}}\mathbf{x}_{\text{pr}}+\sigma_{t^{\prime}}{\bm{\epsilon}}^{\prime},\mathbf{c}_{\text{pr}})-\mathbf{x}_{\text{pr}}\|^{2}_{2}\right],(2)where \lambda controls for the relative weight of the prior-preservation term. Figure 4 illustrates the model fine-tuning with the class-generated samples and prior-preservation loss.Despite being simple, we find this prior-preservation loss is effective in overcoming the overfitting and language-drift issues.We find that \sim 200 epochs at learning rate 10^{-5} with \lambda=1 is enough to achieve good results. During this process, \sim 200\times N “a [class noun]” samples are generated, with N being the size of the subject dataset, usually ranging from 3-5 images. The training process takes about 15 minutes on one TPUv4."
What are the applications of dreambooth? ,"Text-based image generation applications, including recontextualization, property modification, original art renditions, and more","Applications of Text-based image generation includes recontextualization & manipulation of subjects, , original art renditions, novel view synthesis and much more.","We discuss various insights related to the suggested mechanism and its analogy to related work, as well as highlight the contribution of each component via ablation studies, and compare our method with alternative baselines. We apply our approach to a myriad of text-based image generation applications including recontextualization of subjects, modification of their properties, original art renditions, and more, paving the way to a new stream of previously unassailable tasks."
Explain Class Prior Ablation,"Sure! Here's my answer:

Class Prior Ablation refers to the phenomenon where the model becomes entangled with the wrong class prior, hindering its ability to generate new images of the subject when trained with no or incorrect class nouns","We observe that the class prior of the erroneous class remains entangled and we cannot create new images of our subject when the model is trained in this manner. With a longer fitting time, we can help resolve the class noun prior, but the model loses the ability to produce subjects and performance decreases. The model struggles to learn the subject instance and associate the class prior with it without a class noun.","We show the results of using no class noun, a wrong class noun, and the correct class noun for text supervision of our subject images. Results are shown in Figure 11. We observe that the class prior of the wrong class (dog) remains entangled and we are not able to generate new images of our subject when the model is trained in this manner. Given a longer fitting time, we are able to disentangle the prior of the class noun “dog”, but at the cost of losing the ability to generate dogs with the model and ultimately with decreased performance. If we are to train without a class noun, the model has difficulty learning the subject instance and does not easily entangle the class prior with the instance. The model takes longer to converge and can generate erroneous samples."
Which pretrained large text to image models have authors used?,The authors have used the pre-trained Imagen model as a base model in their experiments,Authors used pre-trained Imagen text-to-image diffusion model.,"More formally, given a few images of a subject (\sim3-5), our objective is to implant the subject into the output domain of the model such that it can be synthesized with a unique identifier. To that end, we propose techniques to represent a given subject with rare token identifiers and fine-tune a pre-trained, diffusion-based text-to-image framework that operates in two steps; generating a low-resolution image from text and subsequently applying super-resolution (SR) diffusion models. We first fine-tune the low-resolution text-to-image model with the input images and text prompts containing a unique identifier followed by the class name of the subject (e.g., “A [V] dog”). In order to prevent overfitting and language drift Lee2019CounteringLD ; lu2020countering  that cause the model to associate the class name (e.g., “dog”) with the specific instance, we propose an autogenous, class-specific prior preservation loss, which leverages the semantic prior on the class that is embedded in the model, and encourages it to generate diverse instances of the same class as our subject. In the second step, we fine-tune the super-resolution component with pairs of low-resolution and high-resolution versions of the input images. This allows the model to maintain high fidelity to small (but important) details of the subject. We use the pre-trained Imagen model saharia2022photorealistic  as a base model in our experiments, although our method is not constrained to any specific text-to-image diffusion model."
Is it true that large text to image models cannot mimic and create novel rendition of images in a reference set?,Yes,This is true that large text to image models cannot mimic and create novel rendition of images in a reference set.,"Recently developed large text-to-image models achieve a remarkable leap in the evolution of AI, by enabling high-quality and diverse synthesis of images based on a text prompt written in natural language saharia2022photorealistic ; ramesh2022hierarchical . One of the main advantages of such models is the strong semantic prior learned from a large collection of image-caption pairs. Such a prior learns, for instance, to bind the word “dog” with various instances of dogs that can appear in different poses and contexts in an image.While the synthesis capabilities of these models are unprecedented, they lack the ability to mimic the appearance of subjects in a given reference set, and synthesize novel renditions of those same subjects in different contexts. The main reason is that the expressiveness of their output domain is limited; even the most detailed textual description of an object may yield instances with different appearances. Furthermore, even models whose text embedding lies in a shared language-vision space radford2021learning  cannot accurately reconstruct the appearance of given subjects but only create variations of the image content (Figure 2)."
Why can’t we create novel rendition of reference images using the pretrained model itself? ,The pretrained model lacks the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of those same subjects in different contexts due to the limited expressiveness of its output domain,"Because the output domain of the pretrained model is limited, we cannot use it to create novel renditions of reference images.","Recently developed large text-to-image models achieve a remarkable leap in the evolution of AI, by enabling high-quality and diverse synthesis of images based on a text prompt written in natural language saharia2022photorealistic ; ramesh2022hierarchical . One of the main advantages of such models is the strong semantic prior learned from a large collection of image-caption pairs. Such a prior learns, for instance, to bind the word “dog” with various instances of dogs that can appear in different poses and contexts in an image.While the synthesis capabilities of these models are unprecedented, they lack the ability to mimic the appearance of subjects in a given reference set, and synthesize novel renditions of those same subjects in different contexts. The main reason is that the expressiveness of their output domain is limited; even the most detailed textual description of an object may yield instances with different appearances. Furthermore, even models whose text embedding lies in a shared language-vision space radford2021learning  cannot accurately reconstruct the appearance of given subjects but only create variations of the image content (Figure 2)."
What is the problem authors have tried to solve?,"Subject-driven generation, which involves synthesizing novel renditions of a subject in different contexts while maintaining high fidelity to its key visual features",The author tried to solve subject-driven generation that is to synthesize novel depictions of the subject in different contexts.,"In summary, our two main contributions in this work are:•A new problem: subject-driven generation. Given a few casually captured images of a subject, the goal is to synthesize novel renditions of the subject in different contexts, while maintaining high fidelity to its key visual features.•A new technique for fine-tuning text-to-image diffusion models in a few-shot setting, while preserving the model’s semantic knowledge on the class of the subject."
What is the loss function used by authors?,"The authors use a prior-preserving loss function, which is a combination of two terms: the reconstruction loss and a class-specific prior-preserving term. The loss function is given by:

L = L_r + L_p

where L_r is the reconstruction loss and L_p is the prior-preserving loss. The prior-preserving loss is a class-specific term that encourages the model to preserve the prior distribution of the class, and is given by:

L_p = λ ||hat(x) - x||^2_2

where λ is a hyperparameter, hat(x) is the generated image, and x is the target image. The goal of the prior-preserving loss is to prevent the model from overfitting to the small set of subject images and to preserve the diversity of the class",Authors of the paper used a prior-preserving loss function.,"In order to accomplish this, our first task is to implant the subject instance into the output domain of the model and to bind the subject with a unique identifier. We present our method to design the identifier below as well as a new approach to supervise a fine-tuning process of the model such that it re-uses its prior for our subject instance. A key problem is that fine-tuning on a small set of images showing our subject is prone to overfitting on the given images. In addition, language drift Lee2019CounteringLD ; lu2020countering  is a common problem in language models, and manifests itself in text-to-image diffusion models as well: the model can forget how to generate other subjects of the same class, and lose the embedded knowledge on the diversity and natural variations of instances belonging to that class. For this, we present an autogenous class-specific prior preservation loss, where we alleviate overfitting and prevent language drift by encouraging the diffusion model to keep generating diverse instances of the same class as our subject. We propose an autogenous class-specific prior-preserving loss to counter both the overfitting and language drift issues. In essence, our method is to supervise the model with its own generated samples, in order for it to retain the prior once the few-shot fine-tuning begins. Specifically, we generate data \mathbf{x}_{\text{pr}}=\hat{\mathbf{x}}(\mathbf{z}_{t_{1}},\mathbf{c}_{\text{pr}}) by using the ancestral sampler on the frozen pre-trained diffusion model with random initial noise \mathbf{z}_{t_{1}}\sim\mathcal{N}(\mathbf{0},\mathbf{I}) and conditioning vector \mathbf{c}_{\text{pr}}\coloneqq\Gamma(f(\text{""a [class noun]""})). The loss becomes:\mathbb{E}_{\mathbf{x},\mathbf{c},{\bm{\epsilon}},{\bm{\epsilon}}^{\prime},t}\!\left[w_{t}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t}\mathbf{x}+\sigma_{t}{\bm{\epsilon}},\mathbf{c})-\mathbf{x}\|^{2}_{2}+\lambda w_{t^{\prime}}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t^{\prime}}\mathbf{x}_{\text{pr}}+\sigma_{t^{\prime}}{\bm{\epsilon}}^{\prime},\mathbf{c}_{\text{pr}})-\mathbf{x}_{\text{pr}}\|^{2}_{2}\right],(2)where \lambda controls for the relative weight of the prior-preservation term. Figure 4 illustrates the model fine-tuning with the class-generated samples and prior-preservation loss.Despite being simple, we find this prior-preservation loss is effective in overcoming the overfitting and language-drift issues.We find that \sim 200 epochs at learning rate 10^{-5} with \lambda=1 is enough to achieve good results. During this process, \sim 200\times N “a [class noun]” samples are generated, with N being the size of the subject dataset, usually ranging from 3-5 images. The training process takes about 15 minutes on one TPUv4. More formally, given a few images of a subject (\sim3-5), our objective is to implant the subject into the output domain of the model such that it can be synthesized with a unique identifier. To that end, we propose techniques to represent a given subject with rare token identifiers and fine-tune a pre-trained, diffusion-based text-to-image framework that operates in two steps; generating a low-resolution image from text and subsequently applying super-resolution (SR) diffusion models. We first fine-tune the low-resolution text-to-image model with the input images and text prompts containing a unique identifier followed by the class name of the subject (e.g., “A [V] dog”). In order to prevent overfitting and language drift Lee2019CounteringLD ; lu2020countering  that cause the model to associate the class name (e.g., “dog”) with the specific instance, we propose an autogenous, class-specific prior preservation loss, which leverages the semantic prior on the class that is embedded in the model, and encourages it to generate diverse instances of the same class as our subject. In the second step, we fine-tune the super-resolution component with pairs of low-resolution and high-resolution versions of the input images. This allows the model to maintain high fidelity to small (but important) details of the subject. We use the pre-trained Imagen model saharia2022photorealistic  as a base model in our experiments, although our method is not constrained to any specific text-to-image diffusion model. Our proposed loss acts as a powerful regularizer. A naively fine-tuned network can quickly overfit to the small set of subject images. In order to explore this phenomenon, we train two models for 200 epochs, one using naive fine-tuning (i.e. using the loss in Equation 1), and a network using our prior-preservation loss shown in Equation 2. We show results for different context captions in Figure 12. We observe that the regularization effect of our loss allows us to capture a wider range of poses for our subject dog without sacrificing subject fidelity. Importantly, we observe that using naive fine-tuning the dog usually lies on a fabric-type material similar to the training images, whereas this is avoided using our method. Further, we evaluate how our prior preservation loss described in Section 4.2 conserves variability in the prior and show sample results in Figure 13. We verify that a vanilla model is able to generate a large variety of dogs, while a naively fine-tuned model on the subject dog exhibits language drift and generates our subject dog given the prompt “a dog”. Our proposed loss preserves the variability of the prior and the model is able to generate new instances of our dog given a prompt of the style “a [V] dog” but also varied instances of dogs given a “a dog” prompt."
Explain limitations of dreambooth,"The limitations of DreamBooth include difficulty generating accurate contexts, context-appearance entanglement, overfitting to real images, and variability in fidelity and hallucinated features","Authors presented numerous drawbacks, the first of which is that it cannot accurately produce the required context. The second failure mode is context-appearance entanglement, in which the subject's appearance alters as a result of the prompted context. Third, we observe overfitting to real images when the prompt is similar to the original setting in which the subject was observed. Other limits discovered include the fact that some subjects are considerably easier to learn than others.","Our method has several limitations, which we demonstrate in Figure 17, grouped into three main failure modes. The first is related to not being able to accurately generate the prompted context. For example, in Figure 17 we observe that when we prompt the model with “a [V] backpack in the ISS” and “a [V] backpack on the moon” it is not able to generate the desired contexts. Possible reasons are that the generative model does not have a strong prior for these contexts, or that representing both the subject and the context together is a difficult task for the model. The second failure mode is context-appearance entanglement, where the appearance of the subject changes due to the prompted context. In Figure 17 we show examples of a backpack that changes colors due to the desired context being rare (“a [V] backpack in the Bolivian Salt Flats”) or entangling the color of the context with that of the subject (“a [V] backpack on top of blue fabric”). Third, we also observe overfitting to the real images that happens when the prompt is similar to the original setting in which the subject was seen. An example is shown in Figure 17. Other limitations observed are that some subjects are much easier to learn than others. For example, the model has a very strong prior for dogs and cats, with many learned variations. Occasionally, with subjects that are rarer or more complex, the model is unable to support as many subject variations. Finally, there is also variability in the fidelity of the subject and some generated images might contain hallucinated features on the subject, depending on the strength of the model prior, and the complexity of the semantic modification."
What are the FID values achieved by authors using Diffusion Model on ImageNet?,"The authors achieved state-of-the-art FID values on ImageNet using their improved diffusion model, with the best FID values obtained on the 256x256 and 512x512 resolutions","They obtain state-of-the-art image generation on ImageNet 64×64. For higher resolution ImageNet. Table 5 shows the performance of ADM. Metrics include FID, sFID, Prec, Rec.","For all comparisons in this section, we train models on ImageNet 128\times128 with batch size 256, and sample using 250 sampling steps. We train models with the above architecture changes and compare them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual connections, all of the other modifications improve performance and have a positive compounding effect. We observe in Figure 2 that while increased depth helps performance, it increases training time and takes longer to reach the same performance as a wider model, so we opt not to use this change in further experiments. Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also compare our guidance with the truncation trick from BigGAN in Figure 5. We find that classifier guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up until a certain precision threshold, after which point it cannot achieve better precision. Table 5 summarizes our results. Our diffusion models can obtain the best FID on each task, and the best sFID on all but one task. With the improved architecture, we already obtain state-of-the-art image generation on LSUN and ImageNet 64\times64. For higher resolution ImageNet, we observe that classifier guidance allows our models to substantially outperform the best GANs. These models obtain perceptual quality similar to GANs, while maintaining a higher coverage of the distribution as measured by recall, and can even do so using only 25 diffusion steps. We also compare guidance to using a two-stage upsampling stack. Nichol and Dhariwal [43] and Saharia et al. [53] train two-stage diffusion models by combining a low-resolution diffusion model with a corresponding upsampling diffusion model. In this approach, the upsampling model is trained to upsample images from the training set, and conditions on low-resolution images that are concatenated channel-wise to the model input using a simple interpolation (e.g. bilinear). During sampling, the low-resolution model produces a sample, and then the upsampling model is conditioned on this sample. This greatly improves FID on ImageNet 256\times256, but does not reach the same performance as state-of-the-art models like BigGAN-deep Nichol and Dhariwal (2021); Saharia et al. (2021), as seen in Table 5. The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et al. [25] and the improvements from Nichol and Dhariwal [43] and Song et al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples Szegedy et al. (2013).Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-the-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256\times256 and 512\times512."
Why are GANs so difficult to train?,"GANs are difficult to train due to their tendency to collapse into suboptimal solutions, requiring careful hyperparameter tuning and regularization","GANs are often difficult to train, collapsing without carefully selected hyperparameters and regularizers. Much work has been done to achieve GAN-like sample quality with likelihood-based models and they are typically easier to scale and train than GANs.","GANs Goodfellow et al. (2014) currently hold the state-of-the-art on most image generation tasks Brock et al. (2018); Wu et al. (2019); Karras et al. (2019b) as measured by sample quality metrics such as FID Heusel et al. (2017), Inception Score Salimans et al. (2016) and Precision Kynkäänniemi et al. (2019). However, some of these metrics do not fully capture diversity, and it has been shown that GANs capture less diversity than state-of-the-art likelihood-based models Razavi et al. (2019); Nichol and Dhariwal (2021); Nash et al. (2021). Furthermore, GANs are often difficult to train, collapsing without carefully selected hyperparameters and regularizers Brock et al. (2018); Miyato et al. (2018); Brock et al. (2016). While GANs hold the state-of-the-art, their drawbacks make them difficult to scale and apply to new domains. As a result, much work has been done to achieve GAN-like sample quality with likelihood-based models Razavi et al. (2019); Ho et al. (2020); Nash et al. (2021); Child (2021). While these models capture more diversity and are typically easier to scale and train than GANs, they still fall short in terms of visual sample quality. Furthermore, except for VAEs, sampling from these models is slower than GANs in terms of wall-clock time."
How do we obtain the noise(epsilon) in a diffusion model?,We obtain the noise (ε) in a diffusion model by drawing it randomly from a diagonal Gaussian distribution,"Diffusion models sample from a distribution by reversing a gradual noising process. In particular, sampling starts with noise xT and produces gradually less-noisy samples xT −1, xT −2, ... until reaching a final sample x0. We assume that the noise ε is drawn from a diagonal Gaussian distribution, which works well for natural images and simplifies various derivations.","On a high level, diffusion models sample from a distribution by reversing a gradual noising process. In particular, sampling starts with noise x_{T} and produces gradually less-noisy samples x_{T-1},x_{T-2},... until reaching a final sample x_{0}. Each timestep t corresponds to a certain noise level, and x_{t} can be thought of as a mixture of a signal x_{0} with some noise \epsilon where the signal to noise ratio is determined by the timestep t. For the remainder of this paper, we assume that the noise \epsilon is drawn from a diagonal Gaussian distribution, which works well for natural images and simplifies various derivations. A diffusion model learns to produce a slightly more “denoised” x_{t-1} from x_{t}. Ho et al. [25] parameterize this model as a function \epsilon_{\theta}(x_{t},t) which predicts the noise component of a noisy sample x_{t}. To train these models, each sample in a minibatch is produced by randomly drawing a data sample x_{0}, a timestep t, and noise \epsilon, which together give rise to a noised sample x_{t} (Equation 17).The training objective is then ||\epsilon_{\theta}(x_{t},t)-\epsilon||^{2}, i.e. a simple mean-squared error loss between the true noise and the predicted noise (Equation 26)."
Which are the metrics used by authors to compare the performance of the models?,"FID, Precision, Recall, and IS","They use FID as our default metric for overall sample quality comparisons as it captures both diversity and fidelity and has been the de facto standard metric for state-of-the-art generative modeling work. Moreover, they use Precision or IS to measure fidelity, and Recall to measure diversity or distribution coverage. In Table 4, they report FID, sFID, IS, Precision, and Recall as metrics.","Inception Score (IS) was proposed by Salimans et al. [54], and it measures how well a model captures the full ImageNet class distribution while still producing individual samples that are convincing examples of a single class. One drawback of this metric is that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS Barratt and Sharma (2018). To better capture diversity than IS, Fréchet Inception Distance (FID) was proposed by Heusel et al. [23], who argued that it is more consistent with human judgement than Inception Score. FID provides a symmetric measure of the distance between two image distributions in the Inception-V3 Szegedy et al. (2015) latent space. Recently, sFID was proposed by Nash et al. [42] as a version of FID that uses spatial features rather than the standard pooled features. They find that this metric better captures spatial relationships, rewarding image distributions with coherent high-level structure. Finally, Kynkäänniemi et al. [32] proposed Improved Precision and Recall metrics to separately measure sample fidelity as the fraction of model samples which fall into the data manifold (precision), and diversity as the fraction of data samples which fall into the sample manifold (recall). We use FID as our default metric for overall sample quality comparisons as it captures both diversity and fidelity and has been the de facto standard metric for state-of-the-art generative modeling work Karras et al. (2019a, b); Brock et al. (2018); Ho et al. (2020). We use Precision or IS to measure fidelity, and Recall to measure diversity or distribution coverage. When comparing against other methods, we re-compute these metrics using public samples or models whenever possible. This is for two reasons: first, some papers Karras et al. (2019a, b); Ho et al. (2020) compare against arbitrary subsets of the training set which are not readily available; and second, subtle implementation differences can affect the resulting FID values Parmar et al. (2021). To ensure consistent comparisons, we use the entire training set as the reference batch Heusel et al. (2017); Brock et al. (2018), and evaluate metrics for all models using the same codebase. For all comparisons in this section, we train models on ImageNet 128\times128 with batch size 256, and sample using 250 sampling steps. We train models with the above architecture changes and compare them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual connections, all of the other modifications improve performance and have a positive compounding effect. We observe in Figure 2 that while increased depth helps performance, it increases training time and takes longer to reach the same performance as a wider model, so we opt not to use this change in further experiments."
What is the final improved architecture used by authors for experiments in this paper?,"The final improved architecture used by the authors for experiments in this paper is a variable width model with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16, and 8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization for injecting timestep and class embeddings into residual blocks","They use variable width with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and 8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization for injecting timestep and class embeddings into residual blocks.","In this section we conduct several architecture ablations to find the model architecture that provides the best sample quality for diffusion models. We explore the following architectural changes: For all comparisons in this section, we train models on ImageNet 128\times128 with batch size 256, and sample using 250 sampling steps. We train models with the above architecture changes and compare them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual connections, all of the other modifications improve performance and have a positive compounding effect. We observe in Figure 2 that while increased depth helps performance, it increases training time and takes longer to reach the same performance as a wider model, so we opt not to use this change in further experiments. In the rest of the paper, we use this final improved model architecture as our default: variable width with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and 8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization for injecting timestep and class embeddings into residual blocks. The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et al. [25] and the improvements from Nichol and Dhariwal [43] and Song et al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples Szegedy et al. (2013).Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-the-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256\times256 and 512\times512."
Why did the authors have to scale the classifier gradients by a constant factor larger than 1?,"To remedy the problem of the classifier assigning reasonable probabilities to the desired classes for the final samples, but these samples not matching the intended classes upon visual inspection. Scaling up the classifier gradients increased the class probabilities from the classifier to nearly 100%, but also focused more on the modes of the classifier, potentially desirable for producing higher fidelity samples","When using a scale of 1, they observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection. Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%. When using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples.","We can safely ignore the constant term C_{4}, since it corresponds to the normalizing coefficient Z in Equation 2. We have thus found that the conditional transition operator can be approximated by a Gaussian similar to the unconditional transition operator, but with its mean shifted by \Sigma g. Algorithm 1 summaries the corresponding sampling algorithm. We include an optional scale factor s for the gradients, which we describe in more detail in Section 4.3. In initial experiments with unconditional ImageNet models, we found it necessary to scale the classifier gradients by a constant factor larger than 1. When using a scale of 1, we observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection.Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%. Figure 3 shows an example of this effect. To understand the effect of scaling classifier gradients, note that s\cdot\mathop{}\!\nabla_{\!x}\log p(y|x)=\mathop{}\!\nabla_{\!x}\log\frac{1}{Z}p(y|x)^{s}, where Z is an arbitrary constant. As a result, the conditioning process is still theoretically grounded in a re-normalized classifier distribution proportional to p(y|x)^{s}. When s>1, this distribution becomes sharper than p(y|x), since larger values are amplified by the exponent. In other words, using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples. Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also compare our guidance with the truncation trick from BigGAN in Figure 5. We find that classifier guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up until a certain precision threshold, after which point it cannot achieve better precision. The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et al. [25] and the improvements from Nichol and Dhariwal [43] and Song et al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples Szegedy et al. (2013).Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-the-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256\times256 and 512\times512."
How does the trade-off between fidelity and diversity vary with the Gradient Scale?,"The trade-off between fidelity and diversity varies smoothly with the Gradient Scale, with higher scales trading off recall (diversity) for higher precision and IS (fidelity)","When using a scale of 1, we observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection. Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%. Using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples.","We hypothesize that the gap between diffusion models and GANs stems from at least two factors: first, that the model architectures used by recent GAN literature have been heavily explored and refined; second, that GANs are able to trade off diversity for fidelity, producing high quality samples but not covering the whole distribution. We aim to bring these benefits to diffusion models, first by improving model architecture and then by devising a scheme for trading off diversity for fidelity. With these improvements, we achieve a new state-of-the-art, surpassing GANs on several different metrics and datasets. In initial experiments with unconditional ImageNet models, we found it necessary to scale the classifier gradients by a constant factor larger than 1. When using a scale of 1, we observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection.Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%. Figure 3 shows an example of this effect. To understand the effect of scaling classifier gradients, note that s\cdot\mathop{}\!\nabla_{\!x}\log p(y|x)=\mathop{}\!\nabla_{\!x}\log\frac{1}{Z}p(y|x)^{s}, where Z is an arbitrary constant. As a result, the conditioning process is still theoretically grounded in a re-normalized classifier distribution proportional to p(y|x)^{s}. When s>1, this distribution becomes sharper than p(y|x), since larger values are amplified by the exponent. In other words, using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples. Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also compare our guidance with the truncation trick from BigGAN in Figure 5. We find that classifier guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up until a certain precision threshold, after which point it cannot achieve better precision. The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et al. [25] and the improvements from Nichol and Dhariwal [43] and Song et al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples Szegedy et al. (2013).Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-the-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256\times256 and 512\times512. We have shown that diffusion models, a class of likelihood-based models with a stationary training objective, can obtain better sample quality than state-of-the-art GANs. Our improved architecture is sufficient to achieve this on unconditional image generation tasks, and our classifier guidance technique allows us to do so on class-conditional tasks. In the latter case, we find that the scale of the classifier gradients can be adjusted to trade off diversity for fidelity. These guided diffusion models can reduce the sampling time gap between GANs and diffusion models, although diffusion models still require multiple forward passes during sampling. Finally, by combining guidance with upsampling, we can further improve sample quality on high-resolution conditional image synthesis."
What is IS as a measure of fidelity?,Inception Score (IS) as a measure of fidelity,"IS measures of fidelity but it has a drawback that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS.","Inception Score (IS) was proposed by Salimans et al. [54], and it measures how well a model captures the full ImageNet class distribution while still producing individual samples that are convincing examples of a single class. One drawback of this metric is that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS Barratt and Sharma (2018). To better capture diversity than IS, Fréchet Inception Distance (FID) was proposed by Heusel et al. [23], who argued that it is more consistent with human judgement than Inception Score. FID provides a symmetric measure of the distance between two image distributions in the Inception-V3 Szegedy et al. (2015) latent space. Recently, sFID was proposed by Nash et al. [42] as a version of FID that uses spatial features rather than the standard pooled features. They find that this metric better captures spatial relationships, rewarding image distributions with coherent high-level structure. Finally, Kynkäänniemi et al. [32] proposed Improved Precision and Recall metrics to separately measure sample fidelity as the fraction of model samples which fall into the data manifold (precision), and diversity as the fraction of data samples which fall into the sample manifold (recall). Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also compare our guidance with the truncation trick from BigGAN in Figure 5. We find that classifier guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up until a certain precision threshold, after which point it cannot achieve better precision."
"In terms of image synthesis, do the GANs perform better than VQ-VAE or not?","No, GANs do not perform better than VQ-VAE in terms of image synthesis. In fact, VQ-VAE has been shown to produce diverse and high-quality images, while GANs are known to suffer from mode collapse and other training difficulties. Additionally, VQ-VAE has been shown to be more computationally efficient than GANs","Fidelity can be higher, but GANs are not always better in terms of low diversity. In table 5 ImageNet256x256 experiment, BigGAN-deep beats VA-VAE2 about FID, sFID, Precision but lose about Recall.","GANs Goodfellow et al. (2014) currently hold the state-of-the-art on most image generation tasks Brock et al. (2018); Wu et al. (2019); Karras et al. (2019b) as measured by sample quality metrics such as FID Heusel et al. (2017), Inception Score Salimans et al. (2016) and Precision Kynkäänniemi et al. (2019). However, some of these metrics do not fully capture diversity, and it has been shown that GANs capture less diversity than state-of-the-art likelihood-based models Razavi et al. (2019); Nichol and Dhariwal (2021); Nash et al. (2021). Furthermore, GANs are often difficult to train, collapsing without carefully selected hyperparameters and regularizers Brock et al. (2018); Miyato et al. (2018); Brock et al. (2016). While GANs hold the state-of-the-art, their drawbacks make them difficult to scale and apply to new domains. As a result, much work has been done to achieve GAN-like sample quality with likelihood-based models Razavi et al. (2019); Ho et al. (2020); Nash et al. (2021); Child (2021). While these models capture more diversity and are typically easier to scale and train than GANs, they still fall short in terms of visual sample quality. Furthermore, except for VAEs, sampling from these models is slower than GANs in terms of wall-clock time. Table 5 summarizes our results. Our diffusion models can obtain the best FID on each task, and the best sFID on all but one task. With the improved architecture, we already obtain state-of-the-art image generation on LSUN and ImageNet 64\times64. For higher resolution ImageNet, we observe that classifier guidance allows our models to substantially outperform the best GANs. These models obtain perceptual quality similar to GANs, while maintaining a higher coverage of the distribution as measured by recall, and can even do so using only 25 diffusion steps. Other likelihood-based models have been shown to produce high-fidelity image samples. VQ-VAE van den Oord et al. (2017) and VQ-VAE-2 Razavi et al. (2019) are autoregressive models trained on top of quantized latent codes, greatly reducing the computational resources required to train these models on large images. These models produce diverse and high quality images, but still fall short of GANs without expensive rejection sampling and special metrics to compensate for blurriness. DCTransformer Nash et al. (2021) is a related method which relies on a more intelligent compression scheme. VAEs are another promising class of likelihood-based models, and recent methods such as NVAE Vahdat and Kautz (2020) and VDVAE Child (2021) have successfully been applied to difficult image generation domains. Energy-based models are another class of likelihood-based models with a rich history Ackley et al. (1985); Dayan et al. (1995); Hinton (2002). Sampling from the EBM distribution is challenging, and Xie et al. [70] demonstrate that Langevin dynamics can be used to sample coherent images from these models. Du and Mordatch [15] further improve upon this approach, obtaining high quality images. More recently, Gao et al. [18] incorporate diffusion steps into an energy-based model, and find that doing so improves image samples from these models."
Why did authors recompute some of the metrics using public samples or models?,To ensure consistent comparisons and account for subtle implementation differences that can affect FID values,"P1 demonstrates why did authors recompute some of the metrics. There are two reasons: first, some papers compare against arbitrary subsets of the training set which are not readily available; and second, subtle implementation differences can affect the resulting FID values.","We use FID as our default metric for overall sample quality comparisons as it captures both diversity and fidelity and has been the de facto standard metric for state-of-the-art generative modeling work Karras et al. (2019a, b); Brock et al. (2018); Ho et al. (2020). We use Precision or IS to measure fidelity, and Recall to measure diversity or distribution coverage. When comparing against other methods, we re-compute these metrics using public samples or models whenever possible. This is for two reasons: first, some papers Karras et al. (2019a, b); Ho et al. (2020) compare against arbitrary subsets of the training set which are not readily available; and second, subtle implementation differences can affect the resulting FID values Parmar et al. (2021). To ensure consistent comparisons, we use the entire training set as the reference batch Heusel et al. (2017); Brock et al. (2018), and evaluate metrics for all models using the same codebase."
What is AdaGN?,Adaptive Group Normalization (AdaGN),"AdaGN incorporates the timestep and class embedding into each residual block after a group normalization operation [69], similar to adaptive instance norm [27] and FiLM. They define AdaGN(h, y) = ys GroupNorm(h)+yb, where h is the intermediate activations of the residual block following the first convolution, and y = [ys, yb] is obtained from a linear projection of the timestep and class embedding.","We also experiment with a layer Nichol and Dhariwal (2021) that we refer to as adaptive group normalization (AdaGN), which incorporates the timestep and class embedding into each residual block after a group normalization operation Wu and He (2018), similar to adaptive instance norm Karras et al. (2019a) and FiLM Perez et al. (2017). We define this layer as \text{AdaGN}(h,y)=y_{s}\text{ GroupNorm}(h)+y_{b}, where h is the intermediate activations of the residual block following the first convolution, and y=[y_{s},y_{b}] is obtained from a linear projection of the timestep and class embedding."
"How did the authors decide lambda, and what is the optimal value?","Sure! Here's the answer to your question based on the provided context:

The authors did not perform any domain-targeted tuning and set λ=1.0 for all models","lambda=1.0 is used to avoid any domain-targeted tuning. Authors train the term and neural components independently, combing them only at inference.","This is identical to QGen, but instead of using the pure neural model, we train the hybrid model in Section 4.4 setting \lambda=1.0 for all models to avoid any domain-targeted tuning. We train the term and neural components independently, combing them only at inference."
"How will the generator perform when the target corpus have different query/document (answer) distributions from the training datasets? For example, what if users in target application often ask much longer questions (e.g., longer than 12 tokens), to express more complex query intents?","Sure! Here's the answer to your question based on the provided context:

The generator may not perform as well when the target corpus has different query/document (answer) distributions from the training datasets. This is because the generator is trained on a specific distribution of queries and documents, and may not generalize well to different distributions. For example, if users in the target application often ask much longer questions (e.g., longer than 12 tokens), the generator may not be able to handle such long queries effectively, leading to reduced performance","when the target corpus have different query/document (answer) distributions from the training datasets, the generator will show performance based on learned system to show optimal performance.","Since our approach allows us to generate queries on every passage of the target corpus, one question is that whether retrieval system trained this way simply memorizes the target corpus or it also generalize on unseen passages. Furthermore, from an efficiency standpoint, how many synthetic training examples are required to achieve maximum performance.To answer these questions, we uniformly sample a subset of documents and then generate synthetic queries only on that subset.Results on BIOASQ 7 are shown in Figure 4, where x-axis denotes the percentage of sampled documents.We can see that retrieval accuracy improves as passage coverage increases.The peak is achieved when using a 20\% subset, which covers 21\% of the reference passages.This is not surprising because the number of frequently discussed entities/topics are typically limited, and a subset of the passages covers most of them.This result also indicates that the learned system does generalize, otherwise optimal performance would be seen with 100\% of the data."
"For the query/passage encoder, did the authors use siamese network (i.e., parameters are shared) or non-siamese network?",Siamese network (parameters are shared),Siamese networks are used For the query/passage encoder,"In this work, both query and document encoders are based on BERT Devlin et al. (2019),which has been shown to lead to large performance gains across a number of tasks, including document ranking Nogueira and Cho (2019a); MacAvaney et al. (2019); Yang et al. (2019b).In addition, we share parameters between the query and passage encoder – i.e., f_{Q}=f_{P}, so called Siamese networks – as we found this greatly increased performance while reducing parameters."
"I agree that increasing the size of the passage embedding vectors is better, especially for long passages. What I am curious about is the motivation behind introducing the weight metirx. What if we just use CLS vector?","The weight matrix is introduced to preserve the original size of the CLS vector, which is important for maintaining the context and information content of the passage. Using only the CLS vector without the weight matrix would result in a loss of information and potentially degrade the performance of the model",weight matrix preserves the original size of h_{\text{CLS}} and perform better than down-projecting to a lower dimensional of CLS vector.,"We encode P as (\text{CLS, }p_{1},\ldots,p_{m},\text{ SEP}). For some datasets, a passage contains both a title T=(t_{1},...,t_{l}) and content C=(c_{1},...,c_{o}), in which case we encode the passage as (\text{CLS, }t_{1},...,t_{l},\text{SEP},c_{1},...,c_{o},\text{ SEP}).These sequences are fed to the BERT encoder.Let h_{\text{CLS}}\in\mathbb{R}^{N} be the final representation of the “CLS” token.Passage encodings p are computed by applying a linear projection, i.e., \textbf{p}=\textbf{W}*h_{\text{CLS}}, where W is a N\times N weight matrix (thus N=768), which preserves the original size of h_{\text{CLS}}. This has been shown to perform better than down-projecting to a lower dimensional vector Luan et al. (2020), especially for long passages."
"For training, did the authors intentionally use a single relevant passage or have no choice but to do that becuase the training dataset provides only one relevant passage, i.e., because of annotation scarcity?",The authors had no choice but to use a single relevant passage for training due to annotation scarcity,"The authors had no choice because the training dataset provides only one relevant passage, i.e., because of annotation scarcity","In this work, we are specifically investigating the zero-shot scenario where there exists neither user issued questions nor domain specific data except the passage collection itself. We propose to address the training data scarcity issue by generating synthetic questions Zhou et al. (2017); Duan et al. (2017); Alberti et al. (2019); Nogueira et al. (2019).Leverage the fact that there are large question-answer data sources freely available from the web Shah and Pomerantz (2010); Duan et al. (2017).we first train a question generator using general domain question-answer pairs.The passage collection of a target domain is then fed into this generator to create pairs of noisy question-passage pairs, which are used to train a retrieval model (see Figure 2).In this work, we mine English question-answer pairs from community resources, primarily StackExchange444archive.org/details/stackexchange and Yahoo! Answers555webscope.sandbox.yahoo.com/catalog.php?datatype=l.Note we use stackexchange as it covers a wide range of topics, and we focus on investigating thedomain adaptability of using a question generation approach.We leave comparing question generator trained on different datasets or using different architectures to future work."
How much does the the exact retrieval increase the latency compared to approximate nearest neighbor search?,"Sure! Here's the answer to your question based on the provided context:

The exact retrieval increases the latency significantly compared to approximate nearest neighbor search",increase the latency in the exact retrieval cannot be answered in this paper,"Since the relevance-based model encodes questions and passages independently, we run the encoder over every passage in a collection offline to create a distributed lookup-table as a backend. At inference, we run the question encoder online and then perform nearest neighbor search to find relevant passages, as illustrated in the bottom half of Figure 3.While there has been extensive work in fast approximate nearest neighbour retrieval for dense representations Liu et al. (2011); Johnson et al. (2017), we simply use distributed brute-force search as our passage collections are at most in the millions, resulting in exact retrieval."
What sentence splitter did you use for chunking?,"Sure! Here's my answer:

Stanford Parser",we split its abstract into chunks with sentence boundaries preserved.A passage is constructed by concatenating the title and one chunk. Chunk size is set so that each passage has no more than 200 wordpiece tokens. It does not split sentences but passages.,"Biomedical questions from Task B Phase A of BioASQ Tsatsaronis et al. (2015).We use BioASQ 7 and 8 test data for evaluation.The collection contains all abstracts from MEDLINE articles.Given an article, we split its abstract into chunks with sentence boundaries preserved.A passage is constructed by concatenating the title and one chunk.Chunk size is set so that each passage has no more than 200 wordpiece tokens."
Why did they use different number of tokens for BioASQ and Forum/NQ datasets?,To account for differences in question and answer length across domains,they use different number of tokens for BioASQ and Forum/NQ datasets due to difference in average length of of questions and answers in each dataset,"To ensure data quality, we further filter the data by only keeping question-answer pairs that were positively rated by at least one user on these sites.In total, the final dataset contains 2 millions pairs, and the average length of questions and answers are 12 tokens and 155 tokens respectively. This dataset is general domain in that it contains question-answer pairs from a wide variety of topics."
What are examples of noise for generated questions?,"Examples of noise for generated questions include irrelevant or nonsensical questions, duplicate questions, or questions that are too similar to the original passage",The passage collection of a target domain is fed into this generator to create pairs of noisy question-passage pairs,"In this work, we are specifically investigating the zero-shot scenario where there exists neither user issued questions nor domain specific data except the passage collection itself. We propose to address the training data scarcity issue by generating synthetic questions Zhou et al. (2017); Duan et al. (2017); Alberti et al. (2019); Nogueira et al. (2019).Leverage the fact that there are large question-answer data sources freely available from the web Shah and Pomerantz (2010); Duan et al. (2017).we first train a question generator using general domain question-answer pairs.The passage collection of a target domain is then fed into this generator to create pairs of noisy question-passage pairs, which are used to train a retrieval model (see Figure 2).In this work, we mine English question-answer pairs from community resources, primarily StackExchange444archive.org/details/stackexchange and Yahoo! Answers555webscope.sandbox.yahoo.com/catalog.php?datatype=l.Note we use stackexchange as it covers a wide range of topics, and we focus on investigating thedomain adaptability of using a question generation approach.We leave comparing question generator trained on different datasets or using different architectures to future work."
How many questions are generated for each passage?,"Sure! Here's my answer:

5",At most 5 salient sentences are generated from a passage,"For QGen models, each passage is truncated to 512 sentence tokens and feed to the question generation system. We also run the question generator on individual sentences from each passage to promote questions that focus on different aspects of the same document. We select at most 5 salient sentences from a passage, where sentence saliency is the max term IDF value in a sentence."
"What does ""real information seeking queries"" mean in NQ dataset, compared to other datasets?","In the NQ dataset, ""real information seeking queries"" refers to queries that are more similar to the types of questions people would ask when seeking information, as opposed to other datasets like BioASQ and Forum, which contain more specific and targeted questions","Natural Questions contains general domain queries, which aligns well with the question-answer pairs for training the QA model. Its comparison with other datasets cannot be answered in this paper.","For NaturalQuestions since there is a single relevant passage annotation, we report Precision@1 and Mean reciprocal rank (MRR)999MRR = MAP when there is one relevant item..Results are show in Table 4.We can see here that while QGen still significantly outperform other baselines, the gap between QGen and QA is smaller.Unlike BioASQ and Forum datasets, NaturalQuestions contains general domain queries,which aligns well with the question-answer pairs for training the QA model.Another difference is that NaturalQuestions consists of real information seeking queries,in this case QGen performs better than BM25."
What are the rescorers they used?,A supervised neural rescorer based on BM25,A supervised neural rescorer is used.,"One question we can ask is how close to the state-of-the-art in supervised passage retrieval are these zero-shot models. To test this we looked at BioASQ 8 dataset and compare to the top-participant systems.101010participants-area.bioasq.org Since BioASQ provides annotated training data, the top teams typically use supervised models with a first-stage retrieval plus rescorer architecture. For instance, the AUEB group, which is the top or near top system for BioASQ 6, 7 and 8, uses a BM25 first-stage retrieval model plus a supervised neural rescorer Brokos et al. (2018); Pappas et al. (2019)."
Is it true that the fact that the optimal performance was not seen with 100% of data indicates the strong generalization ability? I suspect that there can be tail documents that can be challenging for the system to memorize even when 100% data is used for training.,"Sure! Here's the answer to your question based on the provided context:

Yes.

The fact that the optimal performance was not seen with 100% of the data indicates that the system has strong generalization ability, as it is able to perform well on unseen passages. If the system were to simply memorize the target corpus, it would achieve optimal performance with 100% of the data. However, the fact that the peak performance is achieved with a smaller subset of passages suggests that the system is able to generalize to unseen passages and is not simply memorizing the training data","its true that the fact that the optimal performance was not seen with 100% of data indicates the strong generalization ability. Since, the peak of retrieval accuracy is achieved when using a 20\% subset, which covers 21\% of the reference passages. Therefore, Tail documents can not be challenging for system because the learned system does generalize.","Since our approach allows us to generate queries on every passage of the target corpus, one question is that whether retrieval system trained this way simply memorizes the target corpus or it also generalize on unseen passages. Furthermore, from an efficiency standpoint, how many synthetic training examples are required to achieve maximum performance.To answer these questions, we uniformly sample a subset of documents and then generate synthetic queries only on that subset.Results on BIOASQ 7 are shown in Figure 4, where x-axis denotes the percentage of sampled documents.We can see that retrieval accuracy improves as passage coverage increases.The peak is achieved when using a 20\% subset, which covers 21\% of the reference passages.This is not surprising because the number of frequently discussed entities/topics are typically limited, and a subset of the passages covers most of them.This result also indicates that the learned system does generalize, otherwise optimal performance would be seen with 100\% of the data."
How did the authors tackle the zero-shot passage retrieval?,"The authors tackled the zero-shot passage retrieval by generating synthetic questions using a question generator trained on general domain question-answer pairs, and using these noisy question-passage pairs to train a retrieval model",the authors tackle the zero-shot passage retrieval by  generating synthetic questions,"In this work, we are specifically investigating the zero-shot scenario where there exists neither user issued questions nor domain specific data except the passage collection itself. We propose to address the training data scarcity issue by generating synthetic questions Zhou et al. (2017); Duan et al. (2017); Alberti et al. (2019); Nogueira et al. (2019).Leverage the fact that there are large question-answer data sources freely available from the web Shah and Pomerantz (2010); Duan et al. (2017).we first train a question generator using general domain question-answer pairs.The passage collection of a target domain is then fed into this generator to create pairs of noisy question-passage pairs, which are used to train a retrieval model (see Figure 2).In this work, we mine English question-answer pairs from community resources, primarily StackExchange444archive.org/details/stackexchange and Yahoo! Answers555webscope.sandbox.yahoo.com/catalog.php?datatype=l.Note we use stackexchange as it covers a wide range of topics, and we focus on investigating thedomain adaptability of using a question generation approach.We leave comparing question generator trained on different datasets or using different architectures to future work."
What are the term-based techniques they used in their experiments?,BM25 Robertson et al. (1995) and rescoring systems,Traditional term-based methods like BM25 Robertson et al. (1995)  is used,"Traditional term-based methods like BM25 Robertson et al. (1995) are powerful zero-shot models and can outperform supervised neural models in many cases Lin (2019). Rescoring systems have shown that integrating BM25 into a neural model improves performance McDonald et al. (2018). However, for first-stage retrieval most work focuses on approximations via re-ranking Karpukhin et al. (2020); Luan et al. (2020). Here we present a technique for exact hybrid first-stage retrieval without the need for a re-ranking stage. Our method is motivated by the work of Seo et al. (2019) for sparse-dense QA."
How much the quality of generated pseudo-queries affect retrieval performance on target domain? ,Little to no difference,"larger generation models lead to improved generators. However, there is little difference in retrieval metrics,suggesting that large domain targeted data is the more important criteria.","Another interesting question is how important is the quality of the question generator relative to retrieval performance.Below we measured generation quality (via Rouge-based metrics Lin and Hovy (2002)) versus retrieval quality for three systems. The base generator contains 12 transformer layers, the lite version only uses the first 3 layer. The large one contains 24 transformer layers and each layer with larger hidden layer size, 4096, and more attention heads, 16.Retrieval quality was measured on BIOASQ 7 and generation quality with a held out set of the community question-answer data set.Results are shown in Table 6. We can see that larger generation models lead to improved generators. However, there is little difference in retrieval metrics,suggesting that large domain targeted data is the more important criteria."
Will the pseudo-query generator generalize well when the target corpus is significantly different from the source domain on which the generator was trained?,"Sure! Here's the answer to your question based on the provided context:

The pseudo-query generator is likely to generalize well to a target corpus that is significantly different from the source domain on which it was trained",The pseudo-query generator generalize well when the target corpus is significantly different from the source domain on which the generator was trained. The result of querying through subset shows increase in performance as passage coverage increases and peak I obtained with only 21%of the dataset. It indicates that learned system does generalize otherwise peak would be seen with 100% of data,"Since our approach allows us to generate queries on every passage of the target corpus, one question is that whether retrieval system trained this way simply memorizes the target corpus or it also generalize on unseen passages. Furthermore, from an efficiency standpoint, how many synthetic training examples are required to achieve maximum performance.To answer these questions, we uniformly sample a subset of documents and then generate synthetic queries only on that subset.Results on BIOASQ 7 are shown in Figure 4, where x-axis denotes the percentage of sampled documents.We can see that retrieval accuracy improves as passage coverage increases.The peak is achieved when using a 20\% subset, which covers 21\% of the reference passages.This is not surprising because the number of frequently discussed entities/topics are typically limited, and a subset of the passages covers most of them.This result also indicates that the learned system does generalize, otherwise optimal performance would be seen with 100\% of the data."
"What if we introduce zero-shot generation for the synthetic query generation by using large-scale generative language models such as GPT-3, to get rid of the assumption that the training datasets exist even for the general domain? Would this too generate quality queries?","Yes, introducing zero-shot generation for synthetic query generation using large-scale generative language models like GPT-3 could generate quality queries, but it may also introduce new challenges and limitations","if we introduce zero-shot generation for the synthetic query generation by using large-scale generative language models such as GPT-3, to get rid of the assumption that the training datasets exist even for the general domain, Would this still generate  quality queries","Question generation for data augmentation is a common tool, but has not been tested in the pure zero-shot setting nor for neural passage retrieval.Duan et al. (2017) use community QA as a data source, as we do, to train question generators. The generated question-passage pairs are not used to train a neural model, but QA is instead done via question-question similarity. Furthermore, they do not test on specialized domains. Alberti et al. (2019) show that augmenting supervised training resources with synthetic question-answer pairs can lead to improvements. Nogueira et al. (2019) employed query generation in the context of first-stage retrieval. In that study, the generated queries were used to augment documents to improve BM25 keyword search. Here we focus on using synthetic queries to train the neural retrieval models."
"I am wondering why the authors used community QA datasets for training the question generator. How about open-domain information retrieval datasets, such as MS MARCO, which also cover diverse domains?","Sure! Here's the answer to your question based on the provided context:

The authors did not use open-domain information retrieval datasets like MS MARCO for training the question generator because such datasets are not as effective for training question generators as community QA datasets, as shown in Guo et al. (2020) and later experiments",Using open-domain information retrieval datasets for training neural retrieval models do not transfer well especially for specialized domains.,"First, we observe that general-domain question-passage pairs can be acquired from community platforms Shah and Pomerantz (2010); Duan et al. (2017) or high quality academic datasets that are publicly available Kwiatkowski et al. (2019); Bajaj et al. (2016). Such resources have been used to create open domain QA passage retrieval models. However, as shown in Guo et al. (2020) and in our later experiments, neural retrieval models trained on the general domain data often do not transfer well, especially for specialized domains."
