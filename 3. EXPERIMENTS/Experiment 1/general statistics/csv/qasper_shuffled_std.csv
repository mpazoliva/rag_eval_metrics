question,context,correct_answer,iris_answer,Rouge1,Rouge2,RougeL,Bleu,Chrf,ChrfPlus,Meteor,Ter,Bert,WMS,SMS,Wisdm,Bleurt,BEM,Bart,Prometheus,Faithfullnes,Relevancy,Correctness,RSim,Consistency,TSim,LLM,std
what language pairs are explored?,"For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation. We use 80K BPE splits as the vocabulary. Note that all sentences are tokenized by the tokenize.perl script, and we lowercase all data to avoid a large vocabulary for the MultiUN corpus. The statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\rightarrow $Es and De$\rightarrow $Fr. For distant language pair Ro$\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets. For vocabulary, we use 60K sub-word tokens based on Byte Pair Encoding (BPE) BIBREF33. FLOAT SELECTED: Table 1: Data Statistics. The statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\rightarrow $Es and De$\rightarrow $Fr. For distant language pair Ro$\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets. For vocabulary, we use 60K sub-word tokens based on Byte Pair Encoding (BPE) BIBREF33. For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation. We use 80K BPE splits as the vocabulary. Note that all sentences are tokenized by the tokenize.perl script, and we lowercase all data to avoid a large vocabulary for the MultiUN corpus.","De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru ","No, they do not train a different training method except from scheduled sampling",0.0,0.0,0.0,2.627961710408444,3.9184952978056415,3.836767697760776,0.0335570469798657,0.0099009900990099,0.0575825050473213,1.1102230246251563e-16,0.0575825050473213,0.0,-1.2767623662948608,0.0385153219103813,0.0016196685142402,1,0.0,0.7503475353901473,0.1835910106392618,0.7343681677209992,0.0,0.0,0.0,1.252702276596723
what are the topics pulled from Reddit?,"Data collection. Subreddits are sub-communities on Reddit oriented around specific interests or topics, such as technology or politics. Sampling from Reddit as a whole would bias the model towards the most commonly discussed content. But by sampling posts from individual subreddits, we can control the kinds of posts we use to train our model. To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. All posts in our sample appeared between January 2007 and March 2015, and to control for length effects, contain between 300 and 400 characters. This results in a total training dataset of 5000 posts. Data collection. Subreddits are sub-communities on Reddit oriented around specific interests or topics, such as technology or politics. Sampling from Reddit as a whole would bias the model towards the most commonly discussed content. But by sampling posts from individual subreddits, we can control the kinds of posts we use to train our model. To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. All posts in our sample appeared between January 2007 and March 2015, and to control for length effects, contain between 300 and 400 characters. This results in a total training dataset of 5000 posts. We now apply our dogmatism classifier to a larger dataset of posts, examining how dogmatic language shapes the Reddit community. Concretely, we apply the BOW+LING model trained on the full Reddit dataset to millions of new unannotated posts, labeling these posts with a probability of dogmatism according to the classifier (0=non-dogmatic, 1=dogmatic). We then use these dogmatism annotations to address four research questions."," training data has posts from politics, business, science and other popular topics; the trained model is applied to millions of unannotated posts on all of Reddit","The offensive dataset is not biased by topic, dialect, or target because the methodology used to build the dataset does not rely on a seed list of offensive words",0.2040816276551438,0.0,0.0,1.953461640690244,21.412522877196384,18.921457601921503,0.136986301369863,0.0110308101939901,0.3567977547645569,0.4621408283518208,0.3567976653575897,0.1776250451803207,-0.8219454884529114,0.045437354594469,0.006789864100509,1,1.0,0.7369136880174181,0.4376224787522025,0.7504899150088099,0.0,0.0,0.0,5.753336228802329
What accuracy does the proposed system achieve?,FLOAT SELECTED: Table 2: Main results on the DL-PS data. FLOAT SELECTED: Table 3: Main results on the EC-MT and EC-UQ datasets. FLOAT SELECTED: Table 2: Main results on the DL-PS data. FLOAT SELECTED: Table 3: Main results on the EC-MT and EC-UQ datasets.,"F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data  F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)",Attention mechanism,0.0,0.0,0.0,0.0,7.675076634092316,5.756307475569237,0.0,0.0005402485143165,0.0463928058743476,0.2255455031991005,0.0463928095996379,0.0446039475500583,-1.104270339012146,0.0526079460978508,0.0013962122392519,1,0.75,0.7457954522573966,0.565060576546057,0.7602423061842282,0.75,0.0,0.0,1.9524282371306798
What crowdsourcing platform is used?,"With the purpose of obtaining evaluation datasets from crowd annotators, we collect the sentences from two domains: Dialog and E-commerce domain. We hire undergraduate students to annotate the sentences. They are required to identify the predefined types of entities in the sentences. Together with the guideline document, the annotators are educated some tips in fifteen minutes and also provided with 20 exemplifying sentences."," They did not use any platform, instead they hired undergraduate students to do the annotation.","* For Wikipedia BIBREF4 and Twitter BIBREF8 datasets, the previous best F1 scores were 0.68 and 0.93, respectively. Our model achieved F1 scores of 0.94 for both datasets.
* For the Formspring dataset, the authors reported an accuracy score of 78.5%, and our model achieved an F1 score of 0.95 with an accuracy score of 98%",0.0370370330246917,0.0,0.0,0.1896014968732814,9.059635301650411,7.492133005639633,0.0691244239631336,0.0100791936645068,0.1944621950387954,0.3939882728164314,0.220764547586441,0.3071621358394623,-1.498700499534607,0.0440955981612205,0.0161322483828159,1,0.0,0.7509206249831356,,0.7206341035416315,0.0,0.0,0.0,2.454321510275144
On how many language pairs do they show that preordering assisting language sentences helps translation quality?,"We experimented with English INLINEFORM0 Hindi translation as the parent task. English is the assisting source language. Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks. Hindi, Bengali, Gujarati and Marathi are Indo-Aryan languages, while Malayalam and Tamil are Dravidian languages. All these languages have a canonical SOV word order. Languages We experimented with English INLINEFORM0 Hindi translation as the parent task. English is the assisting source language. Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks. Hindi, Bengali, Gujarati and Marathi are Indo-Aryan languages, while Malayalam and Tamil are Dravidian languages. All these languages have a canonical SOV word order.",5 ,"The slot filling dataset has approximately 22,000 examples",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0099009900990099,0.1380243897438049,0.0,0.1380243897438049,,-1.2532391548156738,0.0457586385309696,0.0019657027571769,1,0.6666666666666666,0.7708949923319336,0.1886237472986044,0.7544949891944178,0.5,0.0,0.0,0.4421762198561672
Which information about text structure is included in the corpus?,"The paper at hand introduces a corpus developed for use in automatic readability assessment and automatic text simplification of German. The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information. The importance of considering such information has repeatedly been asserted theoretically BIBREF11, BIBREF12, BIBREF0. The remainder of this paper is structured as follows: Section SECREF2 presents previous corpora used for automatic readability assessment and text simplification. Section SECREF3 describes our corpus, introducing its novel aspects and presenting the primary data (Section SECREF7), the metadata (Section SECREF10), the secondary data (Section SECREF28), the profile (Section SECREF35), and the results of machine learning experiments carried out on the corpus (Section SECREF37). Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element in the textstructure layer The paper at hand introduces a corpus developed for use in automatic readability assessment and automatic text simplification of German. The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information. The importance of considering such information has repeatedly been asserted theoretically BIBREF11, BIBREF12, BIBREF0. The remainder of this paper is structured as follows: Section SECREF2 presents previous corpora used for automatic readability assessment and text simplification. Section SECREF3 describes our corpus, introducing its novel aspects and presenting the primary data (Section SECREF7), the metadata (Section SECREF10), the secondary data (Section SECREF28), the profile (Section SECREF35), and the results of machine learning experiments carried out on the corpus (Section SECREF37). Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element in the textstructure layer"," paragraph, lines, textspan element (paragraph segmentation, line segmentation, Information on physical page segmentation(for PDF only))",Europarl corpus BIBREF5,0.0,0.0,0.0,0.0,6.962857686274374,5.22214326470578,0.0,0.0019960079840319,0.1867566257715225,0.1641758376476809,0.1867566257715225,0.2652724981307983,-1.175006628036499,0.0624553263187408,0.0001943120271995,1,1.0,0.7227676703785774,0.3196929637325835,0.7333173094757888,0.6666666666666666,0.0,0.0,1.775213399885673
What experiments are proposed to test that upper layers produce context-specific embeddings?,"We measure how contextual a word representation is using three different metrics: self-similarity, intra-sentence similarity, and maximum explainable variance. Recall from Definition 1 that the self-similarity of a word, in a given layer of a given model, is the average cosine similarity between its representations in different contexts, adjusted for anisotropy. If the self-similarity is 1, then the representations are not context-specific at all; if the self-similarity is 0, that the representations are maximally context-specific. In Figure FIGREF24, we plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2. For example, the self-similarity is 1.0 in ELMo's input layer because representations in that layer are static character-level embeddings. In all three models, the higher the layer, the lower the self-similarity is on average. In other words, the higher the layer, the more context-specific the contextualized representations. This finding makes intuitive sense. In image classification models, lower layers recognize more generic features such as edges while upper layers recognize more class-specific features BIBREF19. Similarly, upper layers of LSTMs trained on NLP tasks learn more task-specific representations BIBREF4. Therefore, it follows that upper layers of neural language models learn more context-specific representations, so as to predict the next word for a given context more accurately. Of all three models, representations in GPT-2 are the most context-specific, with those in GPT-2's last layer being almost maximally context-specific. As seen in Figure FIGREF20, for GPT-2, the average cosine similarity between uniformly randomly words is roughly 0.6 in layers 2 through 8 but increases exponentially from layers 8 through 12. In fact, word representations in GPT-2's last layer are so anisotropic that any two words have on average an almost perfect cosine similarity! This pattern holds for BERT and ELMo as well, though there are exceptions: for example, the anisotropy in BERT's penultimate layer is much higher than in its final layer. As word representations in a sentence become more context-specific in upper layers, they drift away from one another, although there are exceptions (see layer 12 in Figure FIGREF25). However, in all layers, the average similarity between words in the same sentence is still greater than the average similarity between randomly chosen words (i.e., the anisotropy baseline). This suggests a more nuanced contextualization than in ELMo, with BERT recognizing that although the surrounding sentence informs a word's meaning, two words in the same sentence do not necessarily have a similar meaning because they share the same context.","They measure self-similarity, intra-sentence similarity and maximum explainable variance of the embeddings in the upper layers. They plot the average cosine similarity between uniformly random words increases exponentially from layers 8 through 12.  
They plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2 and shown that the higher layer produces more context-specific embeddings.
They plot that word representations in a sentence become more context-specific in upper layers, they drift away from one another.","By generating a query based on the claim's verbs, nouns, and adjectives, and augmenting it with named entities identified using IBM's AlchemyAPI",0.0810810770160703,0.0,0.0,1.1054253444986326,17.60471545535522,15.864069294747022,0.0642549818840579,0.0028125799028381,0.3161437213420868,0.3844949249920852,0.3349822759628296,0.4434097707271576,-1.242074489593506,0.0418825559318065,0.0058337323753338,1,,0.7279798194166927,0.787432571410286,0.7497302856411436,0.3333333333333333,0.0,0.2,4.876294534077982
How do they calculate a static embedding for each word?,"FLOAT SELECTED: Table 1: The performance of various static embeddings on word embedding benchmark tasks. The best result for each task is in bold. For the contextualizing models (ELMo, BERT, GPT-2), we use the first principal component of a word’s contextualized representations in a given layer as its static embedding. The static embeddings created using ELMo and BERT’s contextualized representations often outperform GloVe and FastText vectors. As noted earlier, we can create static embeddings for each word by taking the first principal component (PC) of its contextualized representations in a given layer. In Table TABREF34, we plot the performance of these PC static embeddings on several benchmark tasks. These tasks cover semantic similarity, analogy solving, and concept categorization: SimLex999 BIBREF21, MEN BIBREF22, WS353 BIBREF23, RW BIBREF24, SemEval-2012 BIBREF25, Google analogy solving BIBREF0 MSR analogy solving BIBREF26, BLESS BIBREF27 and AP BIBREF28. We leave out layers 3 - 10 in Table TABREF34 because their performance is between those of Layers 2 and 11.",They use the first principal component of a word's contextualized representation in a given layer as its static embedding. ,"A very simple logistic regression classifier with default parameters, representing input instances with the length of the sentence",0.1142857093224491,0.0,0.0,2.276859592073037,24.91505608294899,19.991766794265267,0.0721153846153846,0.0093847758081334,0.2144036293029785,0.4482210875915231,0.2144036144018173,0.4578044414520263,-0.5789394378662109,0.0415281280875206,0.0046220723354463,1,0.0,0.7251241286775024,0.6838149988528754,0.7352599954115017,0.2,0.0,0.0,6.444222814161389
What is the performance of BERT on the task?,"To finish with this experiment set, Table also shows the strict classification precision, recall and F1-score for the compared systems. Despite the fact that, in general, the systems obtain high values, BERT outperforms them again. BERT's F1-score is 1.9 points higher than the next most competitive result in the comparison. More remarkably, the recall obtained by BERT is about 5 points above. FLOAT SELECTED: Table 5: Results of Experiment A: NUBES-PHI The results of the two MEDDOCAN scenarios –detection and classification– are shown in Table . These results follow the same pattern as in the previous experiments, with the CRF classifier being the most precise of all, and BERT outperforming both the CRF and spaCy classifiers thanks to its greater recall. We also show the results of mao2019hadoken who, despite of having used a BERT-based system, achieve lower scores than our models. The reason why it should be so remain unclear. FLOAT SELECTED: Table 8: Results of Experiment B: MEDDOCAN In this experiment set, our BERT implementation is compared to several systems that participated in the MEDDOCAN challenge: a CRF classifier BIBREF18, a spaCy entity recogniser BIBREF18, and NLNDE BIBREF12, the winner of the shared task and current state of the art for sensitive information detection and classification in Spanish clinical text. Specifically, we include the results of a domain-independent NLNDE model (S2), and the results of a model enriched with domain-specific embeddings (S3). Finally, we include the results obtained by mao2019hadoken with a CRF output layer on top of BERT embeddings. MEDDOCAN consists of two scenarios: The results of the two MEDDOCAN scenarios –detection and classification– are shown in Table . These results follow the same pattern as in the previous experiments, with the CRF classifier being the most precise of all, and BERT outperforming both the CRF and spaCy classifiers thanks to its greater recall. We also show the results of mao2019hadoken who, despite of having used a BERT-based system, achieve lower scores than our models. The reason why it should be so remain unclear. FLOAT SELECTED: Table 8: Results of Experiment B: MEDDOCAN","F1 scores are:
HUBES-PHI: Detection(0.965), Classification relaxed (0.95), Classification strict (0.937)
Medoccan: Detection(0.972), Classification (0.967) ","N400 and P600, and N400 and N200",0.0,0.0,0.0,1.123099644603982,6.129617601303545,5.826725348509672,0.0163934426229508,0.0046449900464499,0.231983482837677,0.0827846945943059,0.231983482837677,0.0068297944962978,-0.6928975582122803,0.0513953864574432,0.0070829441820553,1,0.0,0.7647979406042472,0.1950069798604814,0.7800279194419256,0.0,0.0,0.0,1.7164312038922311
how is model compactness measured?,"Even if LangID-High does not present a more accurate result, it does present a more compact one: LangID-High is 15.4 MB, while the combined wFST high resource models are 197.5 MB. Even if LangID-High does not present a more accurate result, it does present a more compact one: LangID-High is 15.4 MB, while the combined wFST high resource models are 197.5 MB.",Using file size on disk ,"The multilingual encoder was trained on the WMT 2014 En $\leftrightarrow $ Fr parallel corpus, which consists of 36 million En $\rightarrow $ Fr sentence pairs, as well as the corresponding parallel data for the Fr $\rightarrow $ En translation task",0.0571428546938776,0.0,0.0,0.0016101571105611,3.1305214554378,2.67885816679614,0.0549450549450549,0.0099009900990099,0.0054566813632845,0.5729552942017715,0.0054566818289458,0.3663171231746673,-1.193795919418335,0.0469453260302543,0.0018252829554981,1,0.0,0.7142640250151162,0.5525164745697007,0.7100658982788027,0.0,0.0,0.0,0.9017693946777907
Who were the human evaluators used?,"Human Evaluation Results. While automated evaluation metrics like ROUGE measure lexical similarity between machine and human summaries, humans can better measure how coherent and readable a summary is. Our evaluation study investigates whether tuning the PG-net model increases summary coherence, by asking evaluators to select which of three summaries for the same document they like most: the PG-net model trained on CNN/DM; the model trained on student reflections; and finally the model trained on CNN/DM and tuned on student reflections. 20 evaluators were recruited from our institution and asked to each perform 20 annotations. Summaries are presented to evaluators in random order. Evaluators are then asked to select the summary they feel to be most readable and coherent. Unlike ROUGE, which measures the coverage of a generated summary relative to a reference summary, our evaluators don't read the reflections or reference summary. They choose the summary that is most coherent and readable, regardless of the source of the summary. For both courses, the majority of selected summaries were produced by the tuned model (49% for CS and 41% for Stat2015), compared to (31% for CS and 30.9% for Stat2015) for CNN/DM model, and (19.7% for CS and 28.5% for Stat2015) for student reflections model. These results again suggest that domain transfer can remedy the size of in-domain data and improve performance. Human Evaluation Results. While automated evaluation metrics like ROUGE measure lexical similarity between machine and human summaries, humans can better measure how coherent and readable a summary is. Our evaluation study investigates whether tuning the PG-net model increases summary coherence, by asking evaluators to select which of three summaries for the same document they like most: the PG-net model trained on CNN/DM; the model trained on student reflections; and finally the model trained on CNN/DM and tuned on student reflections. 20 evaluators were recruited from our institution and asked to each perform 20 annotations. Summaries are presented to evaluators in random order. Evaluators are then asked to select the summary they feel to be most readable and coherent. Unlike ROUGE, which measures the coverage of a generated summary relative to a reference summary, our evaluators don't read the reflections or reference summary. They choose the summary that is most coherent and readable, regardless of the source of the summary. For both courses, the majority of selected summaries were produced by the tuned model (49% for CS and 41% for Stat2015), compared to (31% for CS and 30.9% for Stat2015) for CNN/DM model, and (19.7% for CS and 28.5% for Stat2015) for student reflections model. These results again suggest that domain transfer can remedy the size of in-domain data and improve performance.", 20 annotatos from author's institution,The MILR classifier,0.0,0.0,0.0,0.0,7.395214213050627,5.54641065978797,0.0,0.0059642147117296,0.2391301244497299,0.0771547734737396,0.2391301095485687,0.0270476732403039,-1.231857419013977,0.0476314797997474,0.0008363044715033,1,0.5,0.0,0.1925712408220683,0.7702849632882732,0.0,0.0,0.0,1.9002079285275817
Which methods are considered to find examples of biases and unwarranted inferences??,"It may be difficult to spot patterns by just looking at a collection of images. Another method is to tag all descriptions with part-of-speech information, so that it becomes possible to see e.g. which adjectives are most commonly used for particular nouns. One method readers may find particularly useful is to leverage the structure of Flickr30K Entities BIBREF8 . This dataset enriches Flickr30K by adding coreference annotations, i.e. which phrase in each description refers to the same entity in the corresponding image. I have used this data to create a coreference graph by linking all phrases that refer to the same entity. Following this, I applied Louvain clustering BIBREF9 to the coreference graph, resulting in clusters of expressions that refer to similar entities. Looking at those clusters helps to get a sense of the enormous variation in referring expressions. To get an idea of the richness of this data, here is a small sample of the phrases used to describe beards (cluster 268): a scruffy beard; a thick beard; large white beard; a bubble beard; red facial hair; a braided beard; a flaming red beard. In this case, `red facial hair' really stands out as a description; why not choose the simpler `beard' instead? We don't know whether or not an entity belongs to a particular social class (in this case: ethnic group) until it is marked as such. But we can approximate the proportion by looking at all the images where the annotators have used a marker (in this case: adjectives like black, white, asian), and for those images count how many descriptions (out of five) contain a marker. This gives us an upper bound that tells us how often ethnicity is indicated by the annotators. Note that this upper bound lies somewhere between 20% (one description) and 100% (5 descriptions). Figure TABREF22 presents count data for the ethnic marking of babies. It includes two false positives (talking about a white baby stroller rather than a white baby). In the Asian group there is an additional complication: sometimes the mother gets marked rather than the baby. E.g. An Asian woman holds a baby girl. I have counted these occurrences as well. One interesting pattern is that the ethnicity/race of babies doesn't seem to be mentioned unless the baby is black or asian. In other words: white seems to be the default, and others seem to be marked. How can we tell whether or not the data is actually biased? It may be difficult to spot patterns by just looking at a collection of images. Another method is to tag all descriptions with part-of-speech information, so that it becomes possible to see e.g. which adjectives are most commonly used for particular nouns. One method readers may find particularly useful is to leverage the structure of Flickr30K Entities BIBREF8 . This dataset enriches Flickr30K by adding coreference annotations, i.e. which phrase in each description refers to the same entity in the corresponding image. I have used this data to create a coreference graph by linking all phrases that refer to the same entity. Following this, I applied Louvain clustering BIBREF9 to the coreference graph, resulting in clusters of expressions that refer to similar entities. Looking at those clusters helps to get a sense of the enormous variation in referring expressions. To get an idea of the richness of this data, here is a small sample of the phrases used to describe beards (cluster 268): a scruffy beard; a thick beard; large white beard; a bubble beard; red facial hair; a braided beard; a flaming red beard. In this case, `red facial hair' really stands out as a description; why not choose the simpler `beard' instead?"," Looking for adjectives marking the noun ""baby"" and also looking for most-common adjectives related to certain nouns using POS-tagging",WN18 and FB15k,0.09999999745,0.0,0.0,1.8160849415439309,3.647174807421668,4.730548927952983,0.0260416666666666,0.0016638935108153,-0.0142627470195293,0.0,-0.0142627460882067,,-1.3463913202285769,0.036650575697422,0.0003104549570133,1,,0.7003843584322093,0.1897201983965624,0.7588807935862499,0.0,0.0,0.0,1.3502250167161052
What biases are found in the dataset?,"Ethnicity/race One interesting pattern is that the ethnicity/race of babies doesn't seem to be mentioned unless the baby is black or asian. In other words: white seems to be the default, and others seem to be marked. How can we tell whether or not the data is actually biased? The numbers in Table TABREF22 are striking: there seems to be a real, systematic difference in ethnicity marking between the groups. We can take one step further and look at all the 697 pictures with the word `baby' in it. If there turn out to be disproportionately many white babies, this strengthens the conclusion that the dataset is biased. One well-studied example BIBREF4 , BIBREF5 is sexist language, where the sex of a person tends to be mentioned more frequently if their role or occupation is inconsistent with `traditional' gender roles (e.g. female surgeon, male nurse). Beukeboom also notes that adjectives are used to create “more narrow labels [or subtypes] for individuals who do not fit with general social category expectations” (p. 3). E.g. tough woman makes an exception to the `rule' that women aren't considered to be tough.",Ethnic bias ,"Sure! Here's the answer to the question at the end of the context:

Clipping negative PMI can lead to a loss of information about word relationships",0.0,0.0,0.0,0.0,1.675085185364886,1.2563138890236647,0.0,0.0099009900990099,0.0703744515776634,0.5470214801676133,0.1208497062325477,0.3435068130493164,-1.0378003120422363,0.0332166962325573,0.0041869134875503,1,,0.8001218051389712,0.1838132320769979,0.7352529283079919,0.0,0.0,0.0,0.5612600038372301
What discourse relations does it work best/worst for?,"The second row shows the performance of our basic paragraph-level model which predicts both implicit and explicit discourse relations in a paragraph. Compared to the variant system (the first row), the basic model further improved the classification performance on the first three implicit relations. Especially on the contingency relation, the classification performance was improved by another 1.42 percents. Moreover, the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 ). After untying parameters in the softmax prediction layer, implicit discourse relation classification performance was improved across all four relations, meanwhile, the explicit discourse relation classification performance was also improved. The CRF layer further improved implicit discourse relation recognition performance on the three small classes. In summary, our full paragraph-level neural network model achieves the best macro-average F1-score of 48.82% in predicting implicit discourse relations, which outperforms previous neural tensor network models (e.g., BIBREF18 ) by more than 2 percents and outperforms the best previous system BIBREF19 by 1 percent. As we explained in section 4.2, we ran our models for 10 times to obtain stable average performance. Then we also created ensemble models by applying majority voting to combine results of ten runs. From table 5 , each ensemble model obtains performance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively. Furthermore, the ensemble model achieves the best performance for predicting both implicit and explicit discourse relations simultaneously. FLOAT SELECTED: Table 3: Multi-class Classification Results on PDTB. We report accuracy (Acc) and macro-average F1scores for both explicit and implicit discourse relation predictions. We also report class-wise F1 scores. The Penn Discourse Treebank (PDTB): We experimented with PDTB v2.0 BIBREF7 which is the largest annotated corpus containing 36k discourse relations in 2,159 Wall Street Journal (WSJ) articles. In this work, we focus on the top-level discourse relation senses which are consist of four major semantic classes: Comparison (Comp), Contingency (Cont), Expansion (Exp) and Temporal (Temp). We followed the same PDTB section partition BIBREF12 as previous work and used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set. Table 1 presents the data distributions we collected from PDTB. Multi-way Classification: The first section of table 3 shows macro average F1-scores and accuracies of previous works. The second section of table 3 shows the multi-class classification results of our implemented baseline systems. Consistent with results of previous works, neural tensors, when applied to Bi-LSTMs, improved implicit discourse relation prediction performance. However, the performance on the three small classes (Comp, Cont and Temp) remains low.", Best: Expansion (Exp). Worst: Comparison (Comp).,"The proposed model is significantly better in perplexity and BLEU score than typical UMT models.

In Table TABREF19, the proposed model with +Anti OT and +Anti UT achieves a mean perplexity of 12.3 and a BLEU score of 0.75, which is much better than the typical UMT models that have a mean perplexity of 18.5 and a BLEU score of 0.65",0.0,0.0,0.0,0.0692149463784062,3.023221865385048,2.4955300684770187,0.0526315789473684,0.0099009900990099,0.2149389833211898,0.5302982525093349,0.1818648427724838,0.1631723642349243,-1.2884936332702637,0.0411076061427593,0.0089446337495811,1,0.5,0.7515318134705868,0.8289995997254881,0.7445698274733813,0.3333333333333333,0.0,0.0,0.8669169955107022
On top of BERT does the RNN layer work better or the transformer layer?,"In this paper, we propose a method that builds upon BERT's architecture. We split the input text sequence into shorter segments in order to obtain a representation for each of them using BERT. Then, we use either a recurrent LSTM BIBREF10 network, or another Transformer, to perform the actual classification. We call these techniques Recurrence over BERT (RoBERT) and Transformer over BERT (ToBERT). Given that these models introduce a hierarchy of representations (segment-wise and document-wise), we refer to them as Hierarchical Transformers. To the best of our knowledge, no attempt has been done before to use the Transformer architecture for classification of such long sequences. In this paper, we presented two methods for long documents using BERT model: RoBERT and ToBERT. We evaluated our experiments on two classification tasks - customer satisfaction prediction and topic identification - using 3 datasets: CSAT, 20newsgroups and Fisher. We observed that ToBERT outperforms RoBERT on pre-trained BERT features and fine-tuned BERT features for all our tasks. Also, we noticed that fine-tuned BERT performs better than pre-trained BERT. We have shown that both RoBERT and ToBERT improved the simple baselines of taking an average (or the most frequent) of segment-wise predictions for long documents to obtain final prediction. Position embeddings did not significantly affect our models performance, but slightly improved the accuracy on the CSAT task. We obtained the best results on Fisher dataset and good improvements for CSAT task compared to the CNN baseline. It is interesting to note that the longer the average input in a given task, the bigger improvement we observe w.r.t. the baseline for that task. Our results confirm that both RoBERT and ToBERT can be used for long sequences with competitive performance and quick fine-tuning procedure. For future work, we shall focus on training models on long documents directly (i.e. in an end-to-end manner). In this paper, we propose a method that builds upon BERT's architecture. We split the input text sequence into shorter segments in order to obtain a representation for each of them using BERT. Then, we use either a recurrent LSTM BIBREF10 network, or another Transformer, to perform the actual classification. We call these techniques Recurrence over BERT (RoBERT) and Transformer over BERT (ToBERT). Given that these models introduce a hierarchy of representations (segment-wise and document-wise), we refer to them as Hierarchical Transformers. To the best of our knowledge, no attempt has been done before to use the Transformer architecture for classification of such long sequences. Table TABREF25 presents results using pre-trained BERT features. We extracted features from the pooled output of final transformer block as these were shown to be working well for most of the tasks BIBREF1. The features extracted from a pre-trained BERT model without any fine-tuning lead to a sub-par performance. However, We also notice that ToBERT model exploited the pre-trained BERT features better than RoBERT. It also converged faster than RoBERT. Table TABREF26 shows results using features extracted after fine-tuning BERT model with our datasets. Significant improvements can be observed compared to using pre-trained BERT features. Also, it can be noticed that ToBERT outperforms RoBERT on Fisher and 20newsgroups dataset by 13.63% and 0.81% respectively. On CSAT, ToBERT performs slightly worse than RoBERT but it is not statistically significant as this dataset is small.", The transformer layer,"Four annotators who are proficient in English judged the irony accuracy, sentiment preservation, and content preservation",0.0,0.0,0.0,0.0,4.910395816766281,3.682796862574711,0.1111111111111111,0.0105540897097625,0.0689787864685058,0.3015300631523132,0.0689787790179252,0.1411786526441574,-1.3221168518066406,0.0344307757914066,0.0016366100304614,1,1.0,0.7800244390288221,0.3626923142687328,0.7841025904082647,0.0,0.0,0.0,1.291884043785584
How was this data collected?,"However, getting access to systems with real users is usually hard. Therefore, we used the crowdsourcing platform CrowdFlower (CF) for our data collection. However, getting access to systems with real users is usually hard. Therefore, we used the crowdsourcing platform CrowdFlower (CF) for our data collection. A CF worker gets a task instructing them to use our chat-like interface to help the system with a question which is randomly selected from training examples of Simple questions BIBREF7 dataset. To complete the task user has to communicate with the system through the three phase dialog discussing question paraphrase (see Section ""Interactive Learning Evaluation"" ), explanation (see Section ""Future Work"" ) and answer of the question (see Section ""Conclusion"" ). To avoid poor English level of dialogs we involved CF workers from English speaking countries only. The collected dialogs has been annotated (see Section ""Acknowledgments"" ) by expert annotators afterwards."," The crowdsourcing platform CrowdFlower was used to obtain natural dialog data that prompted the user to paraphrase, explain, and/or answer a question from a Simple questions BIBREF7 dataset. The CrowdFlower users were restricted to English-speaking countries to avoid dialogs  with poor English.","The proposed model outperforms the lexicon-based models (TF-IDF and NVDM) by a significant margin, achieving improvement in all relevance metrics",0.1071428525510206,0.0,0.0,1.137287189899225,19.659565131818635,16.5562808110666,0.045766590389016,0.0051020408163265,0.2347733825445175,0.3906765940679193,0.2450708299875259,0.4269139766693115,-0.9614282846450806,0.0507908202707767,0.0049504908432844,2,,0.7772972507109032,0.7003610988693171,0.8014443954772682,0.0,1.0,0.0,5.279211622296481
What is the average length of dialog?,We collected the dataset with 1900 dialogs and 8533 turns. Topics discussed in dialogs are questions randomly chosen from training examples of Simple questions BIBREF7 dataset. From this dataset we also took the correct answers in form of Freebase entities. We collected the dataset with 1900 dialogs and 8533 turns. Topics discussed in dialogs are questions randomly chosen from training examples of Simple questions BIBREF7 dataset. From this dataset we also took the correct answers in form of Freebase entities.,4.49 turns 4.5 turns per dialog (8533 turns / 1900 dialogs),"The Twitter corpus used to train the word vectors was the ""filter"" endpoint, specifically the Russian language tweets",0.0,0.0,0.0,0.0,7.069495037469043,5.302121278101782,0.0,0.0099009900990099,0.0897168740630149,0.2786967966239899,0.0897168815135955,0.2618821859359741,-1.0650838613510132,0.0520048737525939,0.0014550176394534,1,0.0,0.0,0.7777318221165637,0.7109272884662544,0.0,0.0,0.0,1.8123247509968878
How does the IPA label data after interacting with users?,"Named Entity Recognition: We defined a sequence labeling task to extract custom entities from user input. We assumed seven (7) possible entities (see Table TABREF43) to be recognized by the model: topic, subtopic, examination mode and level, question number, intent, as well as the entity other for remaining words in the utterance. Since the data obtained from the rule-based system already contains information on the entities extracted from each user query (i.e., by means of Elasticsearch), we could use it to train a domain-specific NER unit. However, since the user-input was informal, the same information could be provided in different writing styles. That means that a single entity could have different surface forms (e.g., synonyms, writing styles) (although entities that we extracted from the rule-based system were all converted to a universal standard, e.g., official chapter names). To consider all of the variable entity forms while post-labeling the original dataset, we defined generic entity names (e.g., chapter, question nr.) and mapped variations of entities from the user input (e.g., Chapter = [Elementary Calculus, Chapter $I$, ...]) to them. Next Action Prediction: We defined a classification problem to predict the system's next action according to the given user input. We assumed 13 custom actions (see Table TABREF42) that we considered being our labels. In the conversational dataset, each input was automatically labeled by the rule-based system with the corresponding next action and the dialogue-id. Thus, no additional post-labeling was required. We investigated two settings: Plain dialogues with unique dialogue indexes; Plain Information Dictionary information (e.g., extracted entities) collected for the whole dialogue; Pairs of questions (i.e., user requests) and responses (i.e., bot responses) with the unique dialogue- and turn-indexes; Triples in the form of (User Request, Next Action, Response). Information on the next system's action could be employed to train a Dialogue Manager unit with (deep-) machine learning algorithms;",It defined a sequence labeling task to extract custom entities from user input and label the next action (out of 13  custom actions defined). ,"By visualizing the syntactic distance estimated by the Parsing Network, which tends to be higher between the last character of a word and a space, indicating that the model autonomously discovers to avoid inter-word attention connections and use hidden states of space tokens to summarize previous information",0.1587301540136055,0.0,0.0,0.9187679544673912,15.305358771278138,12.890385929432789,0.0856164383561643,0.0108120542903151,0.2757587432861328,0.4778845783662878,0.2757587134838104,0.6122891902923584,-0.8928296566009521,0.0504501536488533,0.0039113345427074,1,0.0,0.7462059073176168,0.1861520190501124,0.7446080762004497,0.0,0.0,0.0,4.0343741349571856
How was the audio data gathered?,"In this paper, we explore multiple pooling strategies for language identification task. Mainly we propose Ghost-VLAD based pooling method for language identification. Inspired by the recent work by W. Xie et al. [9] and Y. Zhong et al. [10], we use Ghost-VLAD to improve the accuracy of language identification task for Indian languages. We explore multiple pooling strategies including NetVLAD pooling [11], Average pooling and Statistics pooling( as proposed in X-vectors [7]) and show that Ghost-VLAD pooling is the best pooling strategy for language identification. Our model obtains the best accuracy of 98.24%, and it outperforms all the other previously proposed pooling methods. We conduct all our experiments on 635hrs of audio data for 7 Indian languages collected from $\textbf {All India Radio}$ news channel. The paper is organized as follows. In section 2, we explain the proposed pooling method for language identification. In section 3, we explain our dataset. In section 4, we describe the experiments, and in section 5, we describe the results. In this section, we describe our dataset collection process. We collected and curated around 635Hrs of audio data for 7 Indian languages, namely Kannada, Hindi, Telugu, Malayalam, Bengali, and English. We collected the data from the All India Radio news channel where an actor will be reading news for about 5-10 mins. To cover many speakers for the dataset, we crawled data from 2010 to 2019. Since the audio is very long to train any deep neural network directly, we segment the audio clips into smaller chunks using Voice activity detector. Since the audio clips will have music embedded during the news, we use Inhouse music detection model to remove the music segments from the dataset to make the dataset clean and our dataset contains 635Hrs of clean audio which is divided into 520Hrs of training data containing 165K utterances and 115Hrs of testing data containing 35K utterances. The amount of audio data for training and testing for each of the language is shown in the table bellow. In this paper, we explore multiple pooling strategies for language identification task. Mainly we propose Ghost-VLAD based pooling method for language identification. Inspired by the recent work by W. Xie et al. [9] and Y. Zhong et al. [10], we use Ghost-VLAD to improve the accuracy of language identification task for Indian languages. We explore multiple pooling strategies including NetVLAD pooling [11], Average pooling and Statistics pooling( as proposed in X-vectors [7]) and show that Ghost-VLAD pooling is the best pooling strategy for language identification. Our model obtains the best accuracy of 98.24%, and it outperforms all the other previously proposed pooling methods. We conduct all our experiments on 635hrs of audio data for 7 Indian languages collected from $\textbf {All India Radio}$ news channel. The paper is organized as follows. In section 2, we explain the proposed pooling method for language identification. In section 3, we explain our dataset. In section 4, we describe the experiments, and in section 5, we describe the results.",Through the All India Radio new channel where actors read news. ,"The model captures biases from data collection and rules of annotation, specifically biases related to language and geography",0.0,0.0,0.0,0.0,11.03966604207148,8.279749531553609,0.0393700787401574,0.0099009900990099,0.085066445171833,0.362497361394509,0.085066445171833,0.3125749230384826,-1.0797830820083618,0.3185626268386841,0.0023038381911323,1,0.0,0.0,0.4387875930271275,0.7551503721085101,0.0,0.0,0.0,2.8068643065301315
What is the GhostVLAD approach?,"GhostVLAD is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4). The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage, as shown in Figure 1 (Right side). Which means that we compute the matrix V for both normal cluster K and ghost clusters G, but we will not include the vectors belongs to ghost cluster from V during concatenation of the features. Due to which, during feature aggregation stage the contribution of the noisy and unwanted features to normal VLAD clusters are assigned less weights while Ghost clusters absorb most of the weight. We illustrate this in Figure 1(Right Side), where the ghost clusters are shown in red color. We use Ghost clusters when we are computing the V matrix, but they are excluded during the concatenation stage. These concatenated features are fed into the projection layer, followed by softmax to predict the language label. GhostVLAD is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4). The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage, as shown in Figure 1 (Right side). Which means that we compute the matrix V for both normal cluster K and ghost clusters G, but we will not include the vectors belongs to ghost cluster from V during concatenation of the features. Due to which, during feature aggregation stage the contribution of the noisy and unwanted features to normal VLAD clusters are assigned less weights while Ghost clusters absorb most of the weight. We illustrate this in Figure 1(Right Side), where the ghost clusters are shown in red color. We use Ghost clusters when we are computing the V matrix, but they are excluded during the concatenation stage. These concatenated features are fed into the projection layer, followed by softmax to predict the language label. The NetVLAD pooling strategy was initially developed for place recognition by R. Arandjelovic et al. [11]. The NetVLAD is an extension to VLAD [18] approach where they were able to replace the hard assignment based clustering with soft assignment based clustering so that it can be trained with neural network in an end to end fashion. In our case, we use the NetVLAD layer to map N local features of dimension D into a fixed dimensional vector, as shown in Figure 1 (Left side).", An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content.,The CRC model embeddings work better,0.0,0.0,0.0,0.0,12.396276116383849,9.297207087287886,0.0208333333333333,0.0024937655860349,0.1991153210401535,0.3805819803102005,0.1991153359413147,0.3475640714168548,-0.8955117464065552,0.0614325776696205,0.001512858476475,1,,0.6932009258775492,0.1842072332171424,0.7368289328685698,0.0,0.0,0.0,3.20528804919324
Which 7 Indian languages do they experiment with?,"FLOAT SELECTED: Table 1: Dataset In this section, we describe our dataset collection process. We collected and curated around 635Hrs of audio data for 7 Indian languages, namely Kannada, Hindi, Telugu, Malayalam, Bengali, and English. We collected the data from the All India Radio news channel where an actor will be reading news for about 5-10 mins. To cover many speakers for the dataset, we crawled data from 2010 to 2019. Since the audio is very long to train any deep neural network directly, we segment the audio clips into smaller chunks using Voice activity detector. Since the audio clips will have music embedded during the news, we use Inhouse music detection model to remove the music segments from the dataset to make the dataset clean and our dataset contains 635Hrs of clean audio which is divided into 520Hrs of training data containing 165K utterances and 115Hrs of testing data containing 35K utterances. The amount of audio data for training and testing for each of the language is shown in the table bellow. FLOAT SELECTED: Table 1: Dataset","Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)",Both GloVe and SGNS are evaluating word embeddings,0.0799999956480002,0.0,0.0,1.0885011049519644,11.11536733193527,9.30043249841576,0.0159235668789808,0.0039840637450199,0.2166370451450348,0.2553363162237154,0.2166370302438736,0.1242141053080558,-0.9455806016921996,0.0619915351271629,0.0013345792644892,1,,0.8110901987815996,0.4087401180166933,0.7778176149239162,0.0,0.0,0.0,2.9871707488056702
What is the invertibility condition?,"In this section, we introduce an invertibility condition for our neural projector to tackle the optimization challenge. Specifically, we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists. Invertible transformations have been explored before in independent components analysis BIBREF14 , gaussianization BIBREF15 , and deep density models BIBREF16 , BIBREF17 , BIBREF18 , for unstructured data. Here, we generalize this style of approach to structured learning, and augment it with discrete latent variables ( INLINEFORM2 ). Under the invertibility condition, we derive a learning algorithm and give another view of our approach revealed by the objective function. Then, we present the architecture of a neural projector we use in experiments: a volume-preserving invertible neural network proposed by BIBREF16 for independent components estimation. In this section, we introduce an invertibility condition for our neural projector to tackle the optimization challenge. Specifically, we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists. Invertible transformations have been explored before in independent components analysis BIBREF14 , gaussianization BIBREF15 , and deep density models BIBREF16 , BIBREF17 , BIBREF18 , for unstructured data. Here, we generalize this style of approach to structured learning, and augment it with discrete latent variables ( INLINEFORM2 ). Under the invertibility condition, we derive a learning algorithm and give another view of our approach revealed by the objective function. Then, we present the architecture of a neural projector we use in experiments: a volume-preserving invertible neural network proposed by BIBREF16 for independent components estimation.",The neural projector must be invertible. ,A small amount of training data from the non-English language is used by the system,0.0,0.0,0.0,0.0,9.69904445275707,7.274283339567801,0.1282051282051282,0.0106007067137809,0.0661557838320732,0.2813456999451926,0.0661557763814926,0.113385185599327,-1.268668293952942,0.0523720346391201,0.0112737999027012,1,0.0,0.7936911401195653,0.7870250300019628,0.748100120007851,0.0,0.0,0.0,2.472979903295254
Which neural architecture do they use as a base for their attention conflict mechanisms?,"We create two models both of which constitutes of three main parts: encoder, interaction and classifier and take two sequences as input. Except interaction, all the other parts are exactly identical between the two models. The encoder is shared among the sequences simply uses two stacked GRU layers. The interaction part consists of only attention for one model while for the another one it consists of attention and conflict combined as shown in (eqn.11) . The classifier part is simply stacked fully-connected layers. Figure 3 shows a block diagram of how our model looks like. We create two models both of which constitutes of three main parts: encoder, interaction and classifier and take two sequences as input. Except interaction, all the other parts are exactly identical between the two models. The encoder is shared among the sequences simply uses two stacked GRU layers. The interaction part consists of only attention for one model while for the another one it consists of attention and conflict combined as shown in (eqn.11) . The classifier part is simply stacked fully-connected layers. Figure 3 shows a block diagram of how our model looks like.","GRU-based encoder, interaction block, and classifier consisting of stacked fully-connected layers. ",Descriptive statistics and visualizations,0.1249999962500001,0.0,0.0,2.839838722567789,18.289580978841904,15.87062261765456,0.0384615384615384,0.0039840637450199,0.0647909417748451,0.2780277650980721,0.0647909343242645,0.1445539146661758,-0.8406585454940796,0.0417972840368747,0.0086205095131519,1,,0.7644955655812654,0.57643978464986,0.8057591385994402,0.0,0.0,0.0,4.9992573723304465
What metric is used for evaluation?,"FLOAT SELECTED: Table 2: Clustering results on the labeled dataset. We compare our algorithm (with and without timestamps) with the online micro-clustering routine of Aggarwal and Yu (2006) (denoted by CluStream). The F1 values are for the precision (P) and recall (R) in the following columns. See Table 3 for a legend of the different models. Best result for each language is in bold. FLOAT SELECTED: Table 3: Accuracy of the SVM ranker on the English training set. TOKENS are the word token features, LEMMAS are the lemma features for title and body, ENTS are named entity features and TS are timestamp features. All features are described in detail in §4, and are listed for both the title and the body. To investigate the importance of each feature, we now consider in Table TABREF37 the accuracy of the SVM ranker for English as described in § SECREF19 . We note that adding features increases the accuracy of the SVM ranker, especially the timestamp features. However, the timestamp feature actually interferes with our optimization of INLINEFORM0 to identify when new clusters are needed, although they improve the SVM reranking accuracy. We speculate this is true because high accuracy in the reranking problem does not necessarily help with identifying when new clusters need to be opened. FLOAT SELECTED: Table 2: Clustering results on the labeled dataset. We compare our algorithm (with and without timestamps) with the online micro-clustering routine of Aggarwal and Yu (2006) (denoted by CluStream). The F1 values are for the precision (P) and recall (R) in the following columns. See Table 3 for a legend of the different models. Best result for each language is in bold. Table TABREF35 gives the final monolingual results on the three datasets. For English, we see that the significant improvement we get using our algorithm over the algorithm of aggarwal2006framework is due to an increased recall score. We also note that the trained models surpass the baseline for all languages, and that the timestamp feature (denoted by TS), while not required to beat the baseline, has a very relevant contribution in all cases. Although the results for both the baseline and our models seem to differ across languages, one can verify a consistent improvement from the latter to the former, suggesting that the score differences should be mostly tied to the different difficulty found across the datasets for each language. The presented scores show that our learning framework generalizes well to different languages and enables high quality clustering results.","F1, precision, recall, accuracy Precision, recall, F1, accuracy",The Transformer layer works better than the RNN layer on top of BERT,0.0,0.0,0.0,0.0,8.046536796536797,6.034902597402598,0.0,0.0099009900990099,0.0430273748934268,0.2434087853346553,0.0430273711681366,0.0747648850083351,-1.0448318719863892,0.0503700040280818,0.0018424826513012,1,0.0,0.7473849702754007,0.1875886767908776,0.7503965738133629,0.5,0.0,0.0,2.0519151691731503
Which eight NER tasks did they evaluate on?,FLOAT SELECTED: Table 2: Top: Examples of within-space and cross-space nearest neighbors (NNs) by cosine similarity in GreenBioBERT’s wordpiece embedding layer. Blue: Original wordpiece space. Green: Aligned Word2Vec space. Bottom: Biomedical NER test set precision / recall / F1 (%) measured with the CoNLL NER scorer. Boldface: Best model in row. Underlined: Best inexpensive model (without target-domain pretraining) in row. FLOAT SELECTED: Table 2: Top: Examples of within-space and cross-space nearest neighbors (NNs) by cosine similarity in GreenBioBERT’s wordpiece embedding layer. Blue: Original wordpiece space. Green: Aligned Word2Vec space. Bottom: Biomedical NER test set precision / recall / F1 (%) measured with the CoNLL NER scorer. Boldface: Best model in row. Underlined: Best inexpensive model (without target-domain pretraining) in row.,"BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800 BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800",Attention differs from alignment for VERB and NOUN POS tags,0.0,0.0,0.0,0.0,5.726642712288897,4.294982034216673,0.0,0.0062111801242236,-0.0014228506479412,0.0408562192550071,-0.0014228533254936,0.0,-1.5158227682113647,0.0479659028351306,0.0004651775433512,1,0.0,0.7385582753052798,0.6874441917688853,0.7497784596130439,0.0,0.0,0.7,1.5052597488039785
Do they focus on Reading Comprehension or multiple choice question answering?,"Automatically answering questions, especially in the open-domain setting (i.e., where minimal or no contextual knowledge is explicitly provided), requires bringing to bear considerable amount of background knowledge and reasoning abilities. For example, knowing the answers to the two questions in Figure FIGREF1 requires identifying a specific ISA relation (i.e., that cooking is a type of learned behavior) as well as recalling the definition of a concept (i.e., that global warming is defined as a worldwide increase in temperature). In the multiple-choice setting, which is the variety of question-answering (QA) that we focus on in this paper, there is also pragmatic reasoning involved in selecting optimal answer choices (e.g., while greenhouse effect might in some other context be a reasonable answer to the second question in Figure FIGREF1, global warming is a preferable candidate). Our probing methodology starts by constructing challenge datasets (Figure FIGREF1, yellow box) from a target set of knowledge resources. Each of our probing datasets consists of multiple-choice questions that include a question $\textbf {q}$ and a set of answer choices or candidates $\lbrace a_{1},...a_{N}\rbrace $. This section describes in detail the 5 different datasets we build, which are drawn from two sources of expert knowledge, namely WordNet BIBREF35 and the GNU Collaborative International Dictionary of English (GCIDE). We describe each resource in turn, and explain how the resulting dataset probes, which we call WordNetQA and DictionaryQA, are constructed.",MULTIPLE CHOICE QUESTION ANSWERING ,GMM-based attention performs better in experiments,0.0,0.0,0.0,0.0,0.7898894154818324,0.5924170616113744,0.0,0.0099009900990099,0.1820982247591018,0.2741011758061016,0.1820982247591018,0.1235760301351547,-1.2783433198928833,0.5864516496658325,0.0002537157730277,3,0.6666666666666666,0.8797482146337742,0.7170870493819398,0.8683481975277592,1.0,1.0,0.0,0.7566144769649512
After how many hops does accuracy decrease?,"Our comprehensive assessment reveals several interesting nuances to the overall positive trend. For example, the performance of even the best QA models degrades substantially on our hyponym probes (by 8-15%) when going from 1-hop links to 2-hops. Further, the accuracy of even our best models on the WordNetQA probe drops by 14-44% under our cluster-based analysis, which assesses whether a model knows several facts about each individual concept, rather than just being good at answering isolated questions. State-of-the-art QA models thus have much room to improve even in some fundamental building blocks, namely definitions and taxonomic hierarchies, of more complex forms of reasoning. Our comprehensive assessment reveals several interesting nuances to the overall positive trend. For example, the performance of even the best QA models degrades substantially on our hyponym probes (by 8-15%) when going from 1-hop links to 2-hops. Further, the accuracy of even our best models on the WordNetQA probe drops by 14-44% under our cluster-based analysis, which assesses whether a model knows several facts about each individual concept, rather than just being good at answering isolated questions. State-of-the-art QA models thus have much room to improve even in some fundamental building blocks, namely definitions and taxonomic hierarchies, of more complex forms of reasoning.", one additional hop,Simulated datasets of document revisions,0.0,0.0,0.0,0.0,9.46050862604402,7.0953814695330175,0.0,0.0099009900990099,-0.0564035959541797,0.2021563822223293,-0.0564035885035991,0.0798369199037551,-1.3967366218566897,0.0446565747261047,0.0006303577025828,1,0.3333333333333333,0.7659862123053426,0.8696291218299397,0.7512437600470319,0.0,0.0,0.0,2.421524839418908
What languages are evaluated?,FLOAT SELECTED: Table 2: Official shared task test set results.,"German, English, Spanish, Finnish, French, Russian,  Swedish. ",Labeled features as prior knowledge,0.0,0.0,0.0,0.0,9.390393763073794,7.042795322305344,0.0,0.0070921985815602,0.1607354432344436,0.1124381292907018,0.1607354432344436,0.0437205545604229,-1.108210206031799,0.0464985966682434,0.0005045423062883,1,0.5,0.7433474132611929,0.1917945872339618,0.7672602362668801,0.0,0.0,0.0,2.390431783080243
What is MSD prediction?,"FLOAT SELECTED: Table 1: Example input sentence. Context MSD tags and lemmas, marked in gray, are only available in Track 1. The cyan square marks the main objective of predicting the word form made. The magenta square marks the auxiliary objective of predicting the MSD tag V;PST;V.PTCP;PASS. There are two tracks of Task 2 of CoNLL–SIGMORPHON 2018: in Track 1 the context is given in terms of word forms, lemmas and morphosyntactic descriptions (MSD); in Track 2 only word forms are available. See Table TABREF1 for an example. Task 2 is additionally split in three settings based on data size: high, medium and low, with high-resource datasets consisting of up to 70K instances per language, and low-resource datasets consisting of only about 1K instances.","The task of predicting MSD tags: V, PST, V.PCTP, PASS. ","* Semantic Enhancement GANs: Use of attention mechanisms to enhance the semantic content of generated images.
* Resolution Enhancement GANs: Use of upsampling layers to increase the resolution of generated images.
* Diversity Enhancement GANs: Use of multiple GANs with different weights to generate diverse images.
* Motion Enhancement GANs: Use of optical flow estimation to incorporate motion into generated videos",0.041666662916667,0.0,0.0,0.1610235914760363,2.435964774707738,2.485524613687812,0.0985221674876847,0.0102331823519543,0.1640789955854416,0.3084818890476331,0.1590106785297393,0.456783264875412,-1.1424885988235474,0.0380295813083648,0.0095755581757168,1,1.0,0.7576661353232534,0.1963122661814411,0.7852490647257646,0.0,0.0,0.0,0.7846941172547862
What other models do they compare to?,"Table TABREF21 reports comparison results in literature published . Our model achieves state-of-the-art on development dataset in setting without pre-trained large language model (ELMo). Comparing with the much complicated model R.M.-Reader + Verifier, which includes several components, our model still outperforms by 0.7 in terms of F1 score. Furthermore, we observe that ELMo gives a great boosting on the performance, e.g., 2.8 points in terms of F1 for DocQA. This encourages us to incorporate ELMo into our model in future. The results in terms of EM and F1 is summarized in Table TABREF20 . We observe that Joint SAN outperforms the SAN baseline with a large margin, e.g., 67.89 vs 69.27 (+1.38) and 70.68 vs 72.20 (+1.52) in terms of EM and F1 scores respectively, so it demonstrates the effectiveness of the joint optimization. By incorporating the output information of classifier into Joint SAN, it obtains a slight improvement, e.g., 72.2 vs 72.66 (+0.46) in terms of F1 score. By analyzing the results, we found that in most cases when our model extract an NULL string answer, the classifier also predicts it as an unanswerable question with a high probability. FLOAT SELECTED: Table 2: Comparison with published results in literature. 1: results are extracted from (Rajpurkar et al., 2018); 2: results are extracted from (Hu et al., 2018). ∗: it is unclear which model is used. #: we only evaluate the Joint SAN in the submission. FLOAT SELECTED: Table 2: Comparison with published results in literature. 1: results are extracted from (Rajpurkar et al., 2018); 2: results are extracted from (Hu et al., 2018). ∗: it is unclear which model is used. #: we only evaluate the Joint SAN in the submission. Table TABREF21 reports comparison results in literature published . Our model achieves state-of-the-art on development dataset in setting without pre-trained large language model (ELMo). Comparing with the much complicated model R.M.-Reader + Verifier, which includes several components, our model still outperforms by 0.7 in terms of F1 score. Furthermore, we observe that ELMo gives a great boosting on the performance, e.g., 2.8 points in terms of F1 for DocQA. This encourages us to incorporate ELMo into our model in future.","SAN Baseline, BNA, DocQA, R.M-Reader, R.M-Reader+Verifier and DocQA+ELMo BNA, DocQA, R.M-Reader, R.M-Reader + Verifier, DocQA + ELMo, R.M-Reader+Verifier+ELMo","Conversational flow features such as self-coverage, opponent-coverage, drop in self-coverage, and number of discussion points adopted by each side",0.0606060556473833,0.0,0.0,1.1498052049318417,11.126282198234216,10.522066343015712,0.0754716981132075,0.0099009900990099,0.2010533809661865,0.0734080961772372,0.2010533958673477,0.0,-1.3469021320343018,0.0499603152275085,0.0117896791684635,1,0.6666666666666666,0.7142520654437385,0.3387396837250588,0.7549587349002354,0.25,0.0,0.0,3.099638818800565
What evaluation metric do they use?,"To test all the possible combinations of parameters, we divided the bilingual dictionary into 4500 noun pairs used as a training set and 500 noun pairs used as a test set. We then learned transformation matrices on the training set using both training algorithms (CBOW and SkipGram) and several values of regularization $\lambda $ from 0 to 5, with a step of 0.5. The resulting matrices were applied to the Ukrainian vectors from the test set and the corresponding Russian `translations' were calculated. The ratio of correct `translations' (matches) was used as an evaluation measure. It came out that regularization only worsened the results for both algorithms, so in the Table 1 we report the results without regularization. To test all the possible combinations of parameters, we divided the bilingual dictionary into 4500 noun pairs used as a training set and 500 noun pairs used as a test set. We then learned transformation matrices on the training set using both training algorithms (CBOW and SkipGram) and several values of regularization $\lambda $ from 0 to 5, with a step of 0.5. The resulting matrices were applied to the Ukrainian vectors from the test set and the corresponding Russian `translations' were calculated. The ratio of correct `translations' (matches) was used as an evaluation measure. It came out that regularization only worsened the results for both algorithms, so in the Table 1 we report the results without regularization.",Accuracy ,100 baseline features,0.0,0.0,0.0,0.0,4.031459475929311,3.455536693653696,0.0,0.0099009900990099,0.2880012691020965,0.4702857732772827,0.2880012989044189,,-1.1614198684692385,0.0536131970584392,0.0002480285575108,1,0.3333333333333333,0.7523855421165683,0.4347811776980542,0.7390004512917718,0.0,0.0,0.0,1.1369408688709424
What are the results from these proposed strategies?,"FLOAT SELECTED: Figure 2: Ablation results on Zork1, averaged across 5 independent runs. Figure FIGREF10 shows that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it.","Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore. ","Task C, which involves predicting the relationships of the three pairs (oriQ/relQ, oriQ/relC, relQ/relC) in a multitask learning framework",0.1176470538408306,0.0,0.0,1.7954191306802214,11.159754552138594,11.11405585382969,0.1351351351351351,0.0099009900990099,0.2728228271007538,0.2385848305410884,0.272822767496109,0.6303232908248901,-0.7729122638702393,0.0496352650225162,0.0074354588964996,1,0.0,0.7564778068789466,0.6853525979341003,0.7414103917364012,0.0,0.0,0.0,3.165891308220172
How much better than the baseline is LiLi?,"Baselines. As none of the existing KBC methods can solve the OKBC problem, we choose various versions of LiLi as baselines. Single: Version of LiLi where we train a single prediction model INLINEFORM0 for all test relations. Sep: We do not transfer (past learned) weights for initializing INLINEFORM0 , i.e., we disable LL. F-th): Here, we use a fixed prediction threshold 0.5 instead of relation-specific threshold INLINEFORM0 . BG: The missing or connecting links (when the user does not respond) are filled with “@-RelatedTo-@"" blindly, no guessing mechanism. w/o PTS: LiLi does not ask for additional clues via past task selection for skillset improvement. Evaluation-I: Strategy Formulation Ability. Table 5 shows the list of inference strategies formulated by LiLi for various INLINEFORM0 and INLINEFORM1 , which control the strategy formulation of LiLi. When INLINEFORM2 , LiLi cannot interact with user and works like a closed-world method. Thus, INLINEFORM3 drops significantly (0.47). When INLINEFORM4 , i.e. with only one interaction per query, LiLi acquires knowledge well for instances where either of the entities or relation is unknown. However, as one unknown entity may appear in multiple test triples, once the entity becomes known, LiLi doesn’t need to ask for it again and can perform inference on future triples causing significant increase in INLINEFORM5 (0.97). When INLINEFORM6 , LiLi is able to perform inference on all instances and INLINEFORM7 becomes 1. For INLINEFORM8 , LiLi uses INLINEFORM9 only once (as only one MLQ satisfies INLINEFORM10 ) compared to INLINEFORM11 . In summary, LiLi’s RL-model can effectively formulate query-specific inference strategies (based on specified parameter values). Evaluation-II: Predictive Performance. Table 6 shows the comparative performance of LiLi with baselines. To judge the overall improvements, we performed paired t-test considering +ve F1 scores on each relation as paired data. Considering both KBs and all relation types, LiLi outperforms Sep with INLINEFORM12 . If we set INLINEFORM13 (training with very few clues), LiLi outperforms Sep with INLINEFORM14 on Freebase considering MCC. Thus, the lifelong learning mechanism is effective in transferring helpful knowledge. Single model performs better than Sep for unknown relations due to the sharing of knowledge (weights) across tasks. However, for known relations, performance drops because, as a new relation arrives to the system, old weights get corrupted and catastrophic forgetting occurs. For unknown relations, as the relations are evaluated just after training, there is no chance for catastrophic forgetting. The performance improvement ( INLINEFORM15 ) of LiLi over F-th on Freebase signifies that the relation-specific threshold INLINEFORM16 works better than fixed threshold 0.5 because, if all prediction values for test instances lie above (or below) 0.5, F-th predicts all instances as +ve (-ve) which degrades its performance. Due to the utilization of contextual similarity (highly correlated with class labels) of entity-pairs, LiLi’s guessing mechanism works better ( INLINEFORM17 ) than blind guessing (BG). The past task selection mechanism of LiLi also improves its performance over w/o PTS, as it acquires more clues during testing for poorly performed tasks (evaluated on validation set). For Freebase, due to a large number of past tasks [9 (25% of 38)], the performance difference is more significant ( INLINEFORM18 ). For WordNet, the number is relatively small [3 (25% of 14)] and hence, the difference is not significant. FLOAT SELECTED: Table 6: Comparison of predictive performance of various versions of LiLi [kwn = known, unk = unknown, all = overall].","In case of Freebase knowledge base, LiLi model had better F1 score than the single model by 0.20 , 0.01, 0.159 for kwn, unk, and all test Rel type.  The values for WordNet are 0.25, 0.1, 0.2. 
 ",The proposed model outperforms the state-of-the-art on all question types except for Exact Match (EM) on Table 2,0.2222222179080933,0.0,0.0,1.2157709822493183,19.306311222197603,17.128205649171786,0.0705882352941176,0.0052662375658279,0.2924466431140899,0.4808378745801748,0.2781812250614166,0.3708662688732147,-1.0168774127960205,0.0393722616136074,0.0121549367533395,1,,0.7629746339619395,0.5697366161070438,0.7789464644281753,0.3333333333333333,0.0,0.0,5.310466618476166
What are the components of the general knowledge learning engine?,"We solve the OKBC problem by mimicking how humans acquire knowledge and perform reasoning in an interactive conversation. Whenever we encounter an unknown concept or relation while answering a query, we perform inference using our existing knowledge. If our knowledge does not allow us to draw a conclusion, we typically ask questions to others to acquire related knowledge and use it in inference. The process typically involves an inference strategy (a sequence of actions), which interleaves a sequence of processing and interactive actions. A processing action can be the selection of related facts, deriving inference chain, etc., that advances the inference process. An interactive action can be deciding what to ask, formulating a suitable question, etc., that enable us to interact. The process helps grow the knowledge over time and the gained knowledge enables us to communicate better in the future. We call this lifelong interactive learning and inference (LiLi). Lifelong learning is reflected by the facts that the newly acquired facts are retained in the KB and used in inference for future queries, and that the accumulated knowledge in addition to the updated KB including past inference performances are leveraged to guide future interaction and learning. LiLi should have the following capabilities: As lifelong learning needs to retain knowledge learned from past tasks and use it to help future learning BIBREF31 , LiLi uses a Knowledge Store (KS) for knowledge retention. KS has four components: (i) Knowledge Graph ( INLINEFORM0 ): INLINEFORM1 (the KB) is initialized with base KB triples (see §4) and gets updated over time with the acquired knowledge. (ii) Relation-Entity Matrix ( INLINEFORM2 ): INLINEFORM3 is a sparse matrix, with rows as relations and columns as entity-pairs and is used by the prediction model. Given a triple ( INLINEFORM4 , INLINEFORM5 , INLINEFORM6 ) INLINEFORM7 , we set INLINEFORM8 [ INLINEFORM9 , ( INLINEFORM10 , INLINEFORM11 )] = 1 indicating INLINEFORM12 occurs for pair ( INLINEFORM13 , INLINEFORM14 ). (iii) Task Experience Store ( INLINEFORM15 ): INLINEFORM16 stores the predictive performance of LiLi on past learned tasks in terms of Matthews correlation coefficient (MCC) that measures the quality of binary classification. So, for two tasks INLINEFORM17 and INLINEFORM18 (each relation is a task), if INLINEFORM19 [ INLINEFORM20 ] INLINEFORM21 INLINEFORM22 [ INLINEFORM23 ] [where INLINEFORM24 [ INLINEFORM25 ]=MCC( INLINEFORM26 )], we say C-PR has learned INLINEFORM27 well compared to INLINEFORM28 . (iv) Incomplete Feature DB ( INLINEFORM29 ): INLINEFORM30 stores the frequency of an incomplete path INLINEFORM31 in the form of a tuple ( INLINEFORM32 , INLINEFORM33 , INLINEFORM34 ) and is used in formulating MLQs. INLINEFORM35 [( INLINEFORM36 , INLINEFORM37 , INLINEFORM38 )] = INLINEFORM39 implies LiLi has extracted incomplete path INLINEFORM40 INLINEFORM41 times involving entity-pair INLINEFORM42 [( INLINEFORM43 , INLINEFORM44 )] for query relation INLINEFORM45 . The RL model learns even after training whenever it encounters an unseen state (in testing) and thus, gets updated over time. KS is updated continuously over time as a result of the execution of LiLi and takes part in future learning. The prediction model uses lifelong learning (LL), where we transfer knowledge (parameter values) from the model for a past most similar task to help learn for the current task. Similar tasks are identified by factorizing INLINEFORM0 and computing a task similarity matrix INLINEFORM1 . Besides LL, LiLi uses INLINEFORM2 to identify poorly learned past tasks and acquire more clues for them to improve its skillset over time. LiLi also uses a stack, called Inference Stack ( INLINEFORM0 ) to hold query and its state information for RL. LiLi always processes stack top ( INLINEFORM1 [top]). The clues from the user get stored in INLINEFORM2 on top of the query during strategy execution and processed first. Thus, the prediction model for INLINEFORM3 is learned before performing inference on query, transforming OKBC to a KBC problem. Table 1 shows the parameters of LiLi used in the following sections.","Answer with content missing: (list)
LiLi should have the following capabilities:
1. to formulate an inference strategy for a given query that embeds processing and interactive actions.
2. to learn interaction behaviors (deciding what to ask and when to ask the user).
3. to leverage the acquired knowledge in the current and future inference process.
4. to perform 1, 2 and 3 in a lifelong manner for continuous knowledge learning. ",The answer styles refer to the two types of answers required for the two tasks in the MS MARCO 2.1 BIBREF5 experiment: a well-formed answer for the NLG task and a more concise answer for the Q&A task,0.1999999954500001,0.0194174713054963,0.0194174713054963,1.3254863986762977,23.53113403538027,21.05520894908113,0.0974025974025974,0.0059955822025875,0.1991555541753769,0.4572053133833165,0.2810714244842529,0.6403648257255554,-0.3981356620788574,0.06873669475317,0.0105210218744761,1,0.0,0.7024866155792933,0.1903994454232418,0.7615977816929672,0.25,0.0,0.0,6.365383561038023
How many labels do the datasets have?,"FLOAT SELECTED: Table 1: Summary of datasets. Large-scale datasets: We further conduct experiments on four much larger datasets: IMDB (I), Yelp2014 (Y), Cell Phone (C), and Baby (B). IMDB and Yelp2014 were previously used in BIBREF25 , BIBREF26 . Cell phone and Baby are from the large-scale Amazon dataset BIBREF24 , BIBREF27 . Detailed statistics are summarized in Table TABREF9 . We keep all reviews in the original datasets and consider a transductive setting where all target examples are used for both training (without label information) and evaluation. We perform sampling to balance the classes of labeled source data in each minibatch INLINEFORM3 during training. Small-scale datasets: Our new dataset was derived from the large-scale Amazon datasets released by McAuley et al. ( BIBREF24 ). It contains four domains: Book (BK), Electronics (E), Beauty (BT), and Music (M). Each domain contains two datasets. Set 1 contains 6000 instances with exactly balanced class labels, and set 2 contains 6000 instances that are randomly sampled from the large dataset, preserving the original label distribution, which we believe better reflects the label distribution in real life. The examples in these two sets do not overlap. Detailed statistics of the generated datasets are given in Table TABREF9 . FLOAT SELECTED: Table 1: Summary of datasets.","719313 Book, Electronics, Beauty and Music each have 6000, IMDB 84919, Yelp 231163, Cell Phone 194792 and Baby 160792 labeled data.","The training dataset that allowed for the best generalization to benchmark sets is the ""WikiText-103"" dataset, with a test accuracy of 96.5% and a difference of 3.5% from the baseline",0.0425531866002721,0.0,0.0,1.227186497868276,9.765533420837064,8.09188469457075,0.0359712230215827,0.0102389078498293,0.1853525191545486,0.2236933484279289,0.1853524893522262,0.0748038664460182,-1.0126217603683472,0.0695925876498222,0.0079495115340285,1,1.0,0.7983963839889836,0.8670235336786604,0.6988633654838723,0.0,0.0,0.0,2.5569771697617365
What are the source and target domains?,"FLOAT SELECTED: Table 1: Summary of datasets. Most previous works BIBREF0 , BIBREF1 , BIBREF6 , BIBREF7 , BIBREF29 carried out experiments on the Amazon benchmark released by Blitzer et al. ( BIBREF0 ). The dataset contains 4 different domains: Book (B), DVDs (D), Electronics (E), and Kitchen (K). Following their experimental settings, we consider the binary classification task to predict whether a review is positive or negative on the target domain. Each domain consists of 1000 positive and 1000 negative reviews respectively. We also allow 4000 unlabeled reviews to be used for both the source and the target domains, of which the positive and negative reviews are balanced as well, following the settings in previous works. We construct 12 cross-domain sentiment classification tasks and split the labeled data in each domain into a training set of 1600 reviews and a test set of 400 reviews. The classifier is trained on the training set of the source domain and is evaluated on the test set of the target domain. The comparison results are shown in Table TABREF37 . Small-scale datasets: Our new dataset was derived from the large-scale Amazon datasets released by McAuley et al. ( BIBREF24 ). It contains four domains: Book (BK), Electronics (E), Beauty (BT), and Music (M). Each domain contains two datasets. Set 1 contains 6000 instances with exactly balanced class labels, and set 2 contains 6000 instances that are randomly sampled from the large dataset, preserving the original label distribution, which we believe better reflects the label distribution in real life. The examples in these two sets do not overlap. Detailed statistics of the generated datasets are given in Table TABREF9 . In all our experiments on the small-scale datasets, we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain. Since we cannot control the label distribution of unlabeled data during training, we consider two different settings:","Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen ",B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11,0.0,0.0,0.0,0.0,12.221924577961166,9.166443433470876,0.0,0.0099009900990099,0.1363058239221573,0.1427461121588537,0.1497617810964584,0.1136717721819877,-1.04932963848114,0.0291707199066877,0.0006254068893593,1,0.3333333333333333,0.7611597353448056,0.7612835453368503,0.7374418736550934,0.0,0.0,0.0,3.0988320685229582
How do they deal with unknown distribution senses?,"Monosemous relatives have been employed multiple times (see Section 2), but results remain unsatisfactory. The aim of my study is to explore the limitations of this technique by implementing and evaluating such a tool for Polish. Firstly, the method is expanded by waiving the requirement of monosemy and proposing several new sources of relatives. These previously unexplored sources are based on wordnet data and help gather many training cases from the corpus. Secondly, a well-known problem of uneven yet unknown distribution of word senses is alleviated by modifying a naïve Bayesian classifier. Thanks to this correction, the classifier is no longer biased towards senses that have more training data. Finally, a very large corpus (600 million documents), gathered from the web by a Polish search engine NEKST, is used to build models based on training corpora of different sizes. Those experiments show what amount of data is sufficient for such a task. The proposed solution is compared to baselines that use wordnet structure only, with no training corpora. The algorithm works as follows. First, a set of relatives is obtained for each sense of a target word using the Polish wordnet: plWordNet BIBREF18 . Some of the replacements may have multiple senses, however usually one of them covers most cases. Secondly, a set of context features is extracted from occurrences of relatives in the NEKST corpus. Finally, the aggregated feature values corresponding to target word senses are used to build a naïve Bayesian classifier adjusted to a situation of unknown a priori probabilities. Which could be rewritten as: INLINEFORM0 The expression has been formulated as a product of two factors: INLINEFORM0 , independent from observed features and corresponding to empty word context, and INLINEFORM1 that depends on observed context. To weaken the influence of improper distribution of training cases, we omit INLINEFORM2 , so that when no context features are observed, every word sense is considered equally probable. In this paper the limitations and improvements of unsupervised word sense disambiguation have been investigated. The main problem – insufficient number and quality of replacements has been tackled by adding new rich sources of replacements. The quality of the models has indeed improved, especially thanks to replacements based on sense ordering in plWordNet. To deal with the problem of unknown sense distribution, the Bayesian classifier has been modified, removing the bias towards frequent labels in the training data. Finally, the experiments with very large corpus have shown the sufficient amount of training data for this task, which is only 6 million documents.",The Näive-Bayes classifier is corrected so it is not biased to most frequent classes ,"By swapping gendered words with their counterfactual counterparts, CDA aims to reduce bias in word embeddings by exposing the model to a more balanced representation of gendered language",0.0512820468376072,0.0,0.0,0.9726983079028162,11.229546203467416,8.903200001436284,0.064516129032258,0.0102639296187683,0.2109064608812332,0.3858469811352816,0.2109064608812332,0.5340114831924438,-1.1800851821899414,0.0542010851204395,0.0096147091026106,1,0.0,0.7314215499502422,0.1860034968561603,0.7440139874246412,0.0,0.0,0.0,2.9004185917658405
What conclusions do the authors draw from their finding that the emotional appeal of ISIS and Catholic materials are similar?,"Comparing these topics with those that appeared on a Catholic women forum, it seems that both ISIS and non-violent groups use topics about motherhood, spousal relationship, and marriage/divorce when they address women. Moreover, we used Depechemood methods to analyze the emotions that these materials are likely to elicit in readers. The result of our emotion analysis suggests that both corpuses used words that aim to inspire readers while avoiding fear. However, the actual words that lead to these effects are very different in the two contexts. Overall, our findings indicate that, using proper methods, automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda that can assist the counterterrorism community. We rely on Depechemood dictionaries to analyze emotions in both corpora. These dictionaries are freely available and come in multiple arrangements. We used a version that includes words with their part of speech (POS) tags. Only words that exist in the Depechemood dictionary with the same POS tag are considered for our analysis. We aggregated the score for each word and normalized each article by emotions. To better compare the result, we added a baseline of 100 random articles from a Reuters news dataset as a non-religious general resource which is available in an NLTK python library. Figure FIGREF22 shows the aggregated score for different feelings in our corpora. Both Catholic and ISIS related materials score the highest in “inspired” category. Furthermore, in both cases, being afraid has the lowest score. However, this is not the case for random news material such as the Reuters corpus, which are not that inspiring and, according to this method, seems to cause more fear in their audience. We investigate these results further by looking at the most inspiring words detected in these two corpora. Table TABREF24 presents 10 words that are among the most inspiring in both corpora. The comparison of the two lists indicate that the method picks very different words in each corpus to reach to the same conclusion. Also, we looked at separate articles in each of the issues of ISIS material addressing women. Figure FIGREF23 shows emotion scores in each of the 20 issues of ISIS propaganda. As demonstrated, in every separate article, this method gives the highest score to evoking inspirations in the reader. Also, in most of these issues the method scored “being afraid” as the lowest score in each issue. Comparing these topics with those that appeared on a Catholic women forum, it seems that both ISIS and non-violent groups use topics about motherhood, spousal relationship, and marriage/divorce when they address women. Moreover, we used Depechemood methods to analyze the emotions that these materials are likely to elicit in readers. The result of our emotion analysis suggests that both corpuses used words that aim to inspire readers while avoiding fear. However, the actual words that lead to these effects are very different in the two contexts. Overall, our findings indicate that, using proper methods, automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda that can assist the counterterrorism community."," By comparing scores for each word calculated using Depechemood dictionary and normalize emotional score for each article, they found Catholic and ISIS materials show similar scores",They get the formal languages from a context-free grammar containing hierarchical dependencies,0.0,0.0,0.0,0.0,22.723293588152472,17.042470191114354,0.0196078431372549,0.004594180704441,0.1467264890670776,0.437159198380652,0.1467265039682388,0.3418187499046325,-0.5703084468841553,0.0563939884305,0.0009424729441846,1,0.0,0.7032740313623164,0.3671376452250022,0.8017604540117823,0.0,0.0,0.4,5.749203796754192
How id Depechemood trained?,"Depechemood is a lexicon-based emotion detection method gathered from crowd-annotated news BIBREF24. Drawing on approximately 23.5K documents with average of 500 words per document from rappler.com, researchers asked subjects to report their emotions after reading each article. They then multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words. Due to limitations of their experiment setup, the emotion categories that they present does not exactly match the emotions from the Plutchik wheel categories. However, they still provide a good sense of the general feeling of an individual after reading an article. The emotion categories of Depechemood are: AFRAID, AMUSED, ANGRY, ANNOYED, DON'T CARE, HAPPY, INSPIRED, SAD. Depechemood simply creates dictionaries of words where each word has scores between 0 and 1 for all of these 8 emotion categories. We present our finding using this approach in the result section. Depechemood is a lexicon-based emotion detection method gathered from crowd-annotated news BIBREF24. Drawing on approximately 23.5K documents with average of 500 words per document from rappler.com, researchers asked subjects to report their emotions after reading each article. They then multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words. Due to limitations of their experiment setup, the emotion categories that they present does not exactly match the emotions from the Plutchik wheel categories. However, they still provide a good sense of the general feeling of an individual after reading an article. The emotion categories of Depechemood are: AFRAID, AMUSED, ANGRY, ANNOYED, DON'T CARE, HAPPY, INSPIRED, SAD. Depechemood simply creates dictionaries of words where each word has scores between 0 and 1 for all of these 8 emotion categories. We present our finding using this approach in the result section.",By multiplying crowd-annotated document-emotion matrix with emotion-word matrix.  ,Linear Support Vector Machine (BIBREF7),0.0,0.0,0.0,0.0,9.408518937401814,7.056389203051361,0.0,0.0062111801242236,0.2727604210376739,0.2406232136267203,0.2727603912353515,0.3757247924804687,-0.6876804828643799,0.0605335608124733,0.0019558127484719,1,0.0,0.8296016285197104,0.4991607271735452,0.7966429086941806,0.0,1.0,0.2,2.364146753275001
How are similarities and differences between the texts from violent and non-violent religious groups analyzed?,"What similarities and/or differences do these topics have with non-violent, non-Islamic religious material addressed specifically to women? As these questions suggest, to understand what, if anything, makes extremist appeals distinctive, we need a point of comparison in terms of the outreach efforts to women from a mainstream, non-violent religious group. For this purpose, we rely on an online Catholic women's forum. Comparison between Catholic material and the content of ISIS' online magazines allows for novel insight into the distinctiveness of extremist rhetoric when targeted towards the female population. To accomplish this task, we employ topic modeling and an unsupervised emotion detection method. Results ::: Emotion Analysis We rely on Depechemood dictionaries to analyze emotions in both corpora. These dictionaries are freely available and come in multiple arrangements. We used a version that includes words with their part of speech (POS) tags. Only words that exist in the Depechemood dictionary with the same POS tag are considered for our analysis. We aggregated the score for each word and normalized each article by emotions. To better compare the result, we added a baseline of 100 random articles from a Reuters news dataset as a non-religious general resource which is available in an NLTK python library. Figure FIGREF22 shows the aggregated score for different feelings in our corpora. Results ::: Content Analysis After pre-processing the text, both corpora were analyzed for word frequencies. These word frequencies have been normalized by the number of words in each corpus. Figure FIGREF17 shows the most common words in each of these corpora. A comparison of common words suggests that those related to marital relationships ( husband, wife, etc.) appear in both corpora, but the religious theme of ISIS material appears to be stronger. A stronger comparison can be made using topic modeling techniques to discover main topics of these documents. Although we used LDA, our results by using NMF outperform LDA topics, due to the nature of these corpora. Also, fewer numbers of ISIS documents might contribute to the comparatively worse performance. Therefore, we present only NMF results. Based on their coherence, we selected 10 topics for analyzing within both corporas. Table TABREF18 and Table TABREF19 show the most important words in each topic with a general label that we assigned to the topic manually. Based on the NMF output, ISIS articles that address women include topics mainly about Islam, women's role in early Islam, hijrah (moving to another land), spousal relations, marriage, and motherhood.",By using topic modeling and unsupervised emotion detection on ISIS materials and articles from Catholic women forum ,English,0.0,0.0,0.0,0.0,5.990859263439619,5.135022225805389,0.0,0.0005878894767783,0.017376109957695,0.4308532476425171,0.0173761155456304,,-1.2297565937042236,0.0511420667171478,0.0006724228817889,1,0.0,0.7380460471600641,0.5618887029908326,0.7475185366688668,0.0,0.0,0.0,1.663934024786704
How are prominent topics idenified in Dabiq and Rumiyah?,"Topic modeling methods are the more powerful technique for understanding the contents of a corpus. These methods try to discover abstract topics in a corpus and reveal hidden semantic structures in a collection of documents. The most popular topic modeling methods use probabilistic approaches such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA). LDA is a generalization of pLSA where documents are considered as a mixture of topics and the distribution of topics is governed by a Dirichlet prior ($\alpha $). Figure FIGREF12 shows plate notation of general LDA structure where $\beta $ represents prior of word distribution per topic and $\theta $ refers to topics distribution for documents BIBREF19. Since LDA is among the most widely utilized algorithms for topic modeling, we applied it to our data. However, the coherence of the topics produced by LDA is poorer than expected. To address this lack of coherence, we applied non-negative matrix factorization (NMF). This method decomposes the term-document matrix into two non-negative matrices as shown in Figure FIGREF13. The resulting non-negative matrices are such that their product closely approximate the original data. Mathematically speaking, given an input matrix of document-terms $V$, NMF finds two matrices by solving the following equation BIBREF20: A comparison of common words suggests that those related to marital relationships ( husband, wife, etc.) appear in both corpora, but the religious theme of ISIS material appears to be stronger. A stronger comparison can be made using topic modeling techniques to discover main topics of these documents. Although we used LDA, our results by using NMF outperform LDA topics, due to the nature of these corpora. Also, fewer numbers of ISIS documents might contribute to the comparatively worse performance. Therefore, we present only NMF results. Based on their coherence, we selected 10 topics for analyzing within both corporas. Table TABREF18 and Table TABREF19 show the most important words in each topic with a general label that we assigned to the topic manually. Based on the NMF output, ISIS articles that address women include topics mainly about Islam, women's role in early Islam, hijrah (moving to another land), spousal relations, marriage, and motherhood.", Using NMF based topic modeling and their coherence prominent topics are identified,"Using a generated summary vs. a user-written one has a performance difference, with the generated summary model outperforming the user-written one",0.076923071952663,0.0,0.0,1.3540902819918827,17.381605202883318,13.637479709058692,0.0769230769230769,0.0103908955962394,0.1418700665235519,0.375072541062012,0.1418700367212295,0.1293675005435943,-0.8899595737457275,0.0513379238545894,0.0108260579859734,1,0.6666666666666666,0.7423550667944765,0.1755091751421108,0.7020367005684433,0.0,0.0,0.0,4.460856956696644
Which datasets are used?,"We develop a variety of ShapeWorldICE datasets, with a similar idea to the “skill tasks” in the bAbI framework BIBREF22. Table TABREF4 gives an overview for different ShapeWorldICE datasets we use in this paper. We consider three different types of captioning tasks, each of which focuses on a distinct aspect of reasoning abilities. Existential descriptions examine whether a certain object is present in an image. Spatial descriptions identify spatial relationships among visual objects. Quantification descriptions involve count-based and ratio-based statements, with an explicit focus on inspecting models for their counting ability. We develop two variants for each type of dataset to enable different levels of visual complexity or specific aspects of the same reasoning type. All the training and test captions sampled in this work are in English. FLOAT SELECTED: Table 1: Sample captions and images from ShapeWorldICE datasets (truthful captions in blue, false in red). Images from Existential-OneShape only contain one object, while images from Spatial-TwoShapes contain two objects. Images from the other four datasets follow the same distribution with multiple abstract objects present in a visual scene. Practical evaluation of GTD is currently only possible on synthetic data. We construct a range of datasets designed for image captioning evaluation. We call this diagnostic evaluation benchmark ShapeWorldICE (ShapeWorld for Image Captioning Evaluation). We illustrate the evaluation of specific image captioning models on ShapeWorldICE. We empirically demonstrate that the existing metrics BLEU and SPICE do not capture true caption-image agreement in all scenarios, while the GTD framework allows a fine-grained investigation of how well existing models cope with varied visual situations and linguistic constructions. We develop a variety of ShapeWorldICE datasets, with a similar idea to the “skill tasks” in the bAbI framework BIBREF22. Table TABREF4 gives an overview for different ShapeWorldICE datasets we use in this paper. We consider three different types of captioning tasks, each of which focuses on a distinct aspect of reasoning abilities. Existential descriptions examine whether a certain object is present in an image. Spatial descriptions identify spatial relationships among visual objects. Quantification descriptions involve count-based and ratio-based statements, with an explicit focus on inspecting models for their counting ability. We develop two variants for each type of dataset to enable different levels of visual complexity or specific aspects of the same reasoning type. All the training and test captions sampled in this work are in English. FLOAT SELECTED: Table 1: Sample captions and images from ShapeWorldICE datasets (truthful captions in blue, false in red). Images from Existential-OneShape only contain one object, while images from Spatial-TwoShapes contain two objects. Images from the other four datasets follow the same distribution with multiple abstract objects present in a visual scene.","Existential (OneShape, MultiShapes), Spacial (TwoShapes, Multishapes), Quantification (Count, Ratio) datasets are generated from ShapeWorldICE ShapeWorldICE datasets: OneShape, MultiShapes, TwoShapes, MultiShapes, Count, and Ratio","The correlation results for Q1-Q5 on DUC-05, DUC-06, and DUC-07 are as follows:

* Spearman's ρ: 0.75, 0.82, 0.85
* Kendall's τ: 0.73, 0.80, 0.83
* Pearson's r: 0.85, 0.90, 0.92

Note that the best-performing version of BEST-ROUGE differs across years",0.0655737659768882,0.0,0.0,1.933916701673568,13.03590592571055,12.87762105380711,0.1322115384615384,0.0101460034644889,0.1245751306414604,0.2066972535992706,0.1245751082897186,0.182434007525444,-1.3651577234268188,0.0563166104257106,0.0154166452743502,3,0.0,0.726122006747989,0.5712936138546751,0.7851744554187005,0.25,0.0,0.0,3.7261506808350013
What are previous state of the art results?,"FLOAT SELECTED: Table 1: The results of two previous models, and results of this study, in which we apply a boundary assembling method. Precision, recall, and F1 scores are shown for both named entity and nominal mention. For both tasks and their overall performance, we outperform the other two models. Our best model performance with its Precision, Recall, and F1 scores on named entity and nominal mention are shown in Table TABREF5. This best model performance is achieved with a dropout rate of 0.1, and a learning rate of 0.05. Our results are compared with state-of-the-art models BIBREF15, BIBREF19, BIBREF20 on the same Sina Weibo training and test datasets. Our model shows an absolute improvement of 2% for the overall F1 score. FLOAT SELECTED: Table 1: The results of two previous models, and results of this study, in which we apply a boundary assembling method. Precision, recall, and F1 scores are shown for both named entity and nominal mention. For both tasks and their overall performance, we outperform the other two models.","Overall F1 score:
- He and Sun (2017) 58.23
- Peng and Dredze (2017) 58.99
- Xu et al. (2018) 59.11 For Named entity the maximum precision was 66.67%, and the average 62.58%, same values for Recall was 55.97% and 50.33%, and for F1 57.14% and 55.64%. Where for Nominal Mention had maximum recall of 74.48% and average of 73.67%, Recall had values of 54.55% and 53.7%,  and F1 had values of  62.97% and 62.12%. Finally the Overall F1 score had maximum value of 59.11% and average of 58.77%","For sentence classification, the following methods were used:

1. Multi-class multi-label classification using Scikit-learn library in Python.
2. Our own implementation for the last 2 classifiers.
3. Pattern-based approach (unsupervised) for the entire dataset",0.0659340615143101,0.0,0.0,0.5762818211163491,17.05129345953898,15.57591192803919,0.0633484162895927,0.0039840637450199,0.1815377175807953,0.3386512387631344,0.1959915496408939,0.3316313326358795,-1.1110856533050537,0.0684598609805107,0.0049090436121163,1,0.0,0.7568059919918263,0.6998880029951484,0.7995200837683598,0.0,0.0,0.0,4.669444759958554
What source-target language pairs were used in this work? ,"FLOAT SELECTED: Table 2: EM/F1 score of multi-BERTs fine-tuned on different training sets and tested on different languages (En: English, Fr: French, Zh: Chinese, Jp: Japanese, Kr: Korean, xx-yy: translated from xx to yy). The text in bold means training data language is the same as testing data language. In the lower half of Table TABREF8, the results are obtained by the translated training data. First, we found that when testing on English and Chinese, translation always degrades the performance (En v.s. En-XX, Zh v.s. Zh-XX). Even though we translate the training data into the same language as testing data, using the untranslated data still yield better results. For example, when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8, while the F1 score is only 44.1 for the model training on Zh-En. This shows that translation degrades the quality of data. There are some exceptions when testing on Korean. Translating the English training data into Chinese, Japanese and Korean still improve the performance on Korean. We also found that when translated into the same language, the English training data is always better than the Chinese data (En-XX v.s. Zh-XX), with only one exception (En-Fr v.s. Zh-Fr when testing on KorQuAD). This may be because we have less Chinese training data than English. These results show that the quality and the size of dataset are much more important than whether the training and testing are in the same language or not. FLOAT SELECTED: Table 2: EM/F1 score of multi-BERTs fine-tuned on different training sets and tested on different languages (En: English, Fr: French, Zh: Chinese, Jp: Japanese, Kr: Korean, xx-yy: translated from xx to yy). The text in bold means training data language is the same as testing data language. We have training and testing sets in three different languages: English, Chinese and Korean. The English dataset is SQuAD BIBREF2. The Chinese dataset is DRCD BIBREF14, a Chinese RC dataset with 30,000+ examples in the training set and 10,000+ examples in the development set. The Korean dataset is KorQuAD BIBREF15, a Korean RC dataset with 60,000+ examples in the training set and 10,000+ examples in the development set, created in exactly the same procedure as SQuAD. We always use the development sets of SQuAD, DRCD and KorQuAD for testing since the testing sets of the corpora have not been released yet. Next, to construct a diverse cross-lingual RC dataset with compromised quality, we translated the English and Chinese datasets into more languages, with Google Translate. An obvious issue with this method is that some examples might no longer have a recoverable span. To solve the problem, we use fuzzy matching to find the most possible answer, which calculates minimal edit distance between translated answer and all possible spans. If the minimal edit distance is larger than min(10, lengths of translated answer - 1), we drop the examples during training, and treat them as noise when testing. In this way, we can recover more than 95% of examples. The following generated datasets are recovered with same setting.","En-Fr, En-Zh, En-Jp, En-Kr, Zh-En, Zh-Fr, Zh-Jp, Zh-Kr to English, Chinese or Korean  ","MEED outperforms the baselines in terms of perplexity on all three sets (Cornell, DailyDialog, and test set) with statistically significant improvements ($p$-value < 0.05)",0.0,0.0,0.0,1.2196223684528658,6.6698019787512814,5.927355118200723,0.045045045045045,0.0099009900990099,0.0068461350165307,0.0474627673489238,0.0068461294285953,0.0258835535496473,-1.1779749393463137,0.0381548628211021,0.0030904693798238,1,,0.7657226066269741,0.1839562401703297,0.7358249606813188,0.0,0.0,0.0,1.8737794729296793
How large is their MNER SnapCaptions dataset?,"The SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs where named entities in captions are manually labeled by expert human annotators (entity types: PER, LOC, ORG, MISC). These captions are collected exclusively from snaps submitted to public and crowd-sourced stories (aka Snapchat Live Stories or Our Stories). Examples of such public crowd-sourced stories are “New York Story” or “Thanksgiving Story”, which comprise snaps that are aggregated for various public events, venues, etc. All snaps were posted between year 2016 and 2017, and do not contain raw images or other associated information (only textual captions and obfuscated visual descriptor features extracted from the pre-trained InceptionNet are available). We split the dataset into train (70%), validation (15%), and test sets (15%). The captions data have average length of 30.7 characters (5.81 words) with vocabulary size 15,733, where 6,612 are considered unknown tokens from Stanford GloVE embeddings BIBREF22 . Named entities annotated in the SnapCaptions dataset include many of new and emerging entities, and they are found in various surface forms (various nicknames, typos, etc.) To the best of our knowledge, SnapCaptions is the only dataset that contains natural image-caption pairs with expert-annotated named entities. The SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs where named entities in captions are manually labeled by expert human annotators (entity types: PER, LOC, ORG, MISC). These captions are collected exclusively from snaps submitted to public and crowd-sourced stories (aka Snapchat Live Stories or Our Stories). Examples of such public crowd-sourced stories are “New York Story” or “Thanksgiving Story”, which comprise snaps that are aggregated for various public events, venues, etc. All snaps were posted between year 2016 and 2017, and do not contain raw images or other associated information (only textual captions and obfuscated visual descriptor features extracted from the pre-trained InceptionNet are available). We split the dataset into train (70%), validation (15%), and test sets (15%). The captions data have average length of 30.7 characters (5.81 words) with vocabulary size 15,733, where 6,612 are considered unknown tokens from Stanford GloVE embeddings BIBREF22 . Named entities annotated in the SnapCaptions dataset include many of new and emerging entities, and they are found in various surface forms (various nicknames, typos, etc.) To the best of our knowledge, SnapCaptions is the only dataset that contains natural image-caption pairs with expert-annotated named entities.", 10000,Torch-Struct is implemented on top of PyTorch,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0099009900990099,0.022271916270256,0.0,0.0222719181329011,,-1.4009623527526855,0.0649158731102943,0.0011469998585949,1,,0.7274561373126542,0.1812275658561888,0.7249102634247553,0.3333333333333333,0.0,0.0,0.4489546330943464
What is masked document generation?,"Based on the above observations, we propose Step (as shorthand for Sequence-to-Sequence TransformEr Pre-training), which can be pre-trained on large scale unlabeled documents. Specifically, we design three tasks for seq2seq model pre-training, namely Sentence Reordering (SR), Next Sentence Generation (NSG), and Masked Document Generation (MDG). SR learns to recover a document with randomly shuffled sentences. NSG generates the next segment of a document based on its preceding segment. MDG recovers a masked document to its original form. After pre-trianing Step using the three tasks on unlabeled documents, we fine-tune it on supervised summarization datasets. Based on the above observations, we propose Step (as shorthand for Sequence-to-Sequence TransformEr Pre-training), which can be pre-trained on large scale unlabeled documents. Specifically, we design three tasks for seq2seq model pre-training, namely Sentence Reordering (SR), Next Sentence Generation (NSG), and Masked Document Generation (MDG). SR learns to recover a document with randomly shuffled sentences. NSG generates the next segment of a document based on its preceding segment. MDG recovers a masked document to its original form. After pre-trianing Step using the three tasks on unlabeled documents, we fine-tune it on supervised summarization datasets.",A task for seq2seq model pra-training that recovers a masked document to its original form. ,"Emolex, EmoSenticNet, Dictionary of Affect in Language, Affective Norms for English Words, and Linguistic Inquiry and Word Count",0.0606060556106523,0.0,0.0,1.6808304933415033,13.150029932977343,10.463651116221612,0.0301204819277108,0.0099009900990099,0.0624050684273242,0.3158169726582722,0.062405064702034,0.213326558470726,-0.9178202152252196,0.0584510415792465,0.0027060937941458,1,,0.7329049633211727,0.1722383117433352,0.6889532469733408,0.0,0.0,0.0,3.472246643078422
What useful information does attention capture?,"Our analysis shows that attention models traditional alignment in some cases more closely while it captures information beyond alignment in others. For instance, attention agrees with traditional alignments to a high degree in the case of nouns. However, it captures other information rather than only the translational equivalent in the case of verbs. To better understand how attention accuracy affects translation quality, we analyse the relationship between attention loss and word prediction loss for individual part-of-speech classes. Figure FIGREF22 shows how attention loss differs when generating different POS tags. One can see that attention loss varies substantially across different POS tags. In particular, we focus on the cases of NOUN and VERB which are the most frequent POS tags in the dataset. As shown, the attention of NOUN is the closest to alignments on average. But the average attention loss for VERB is almost two times larger than the loss for NOUN. One can notice that less than half of the attention is paid to alignment points for most of the POS tags. To examine how the rest of attention in each case has been distributed over the source sentence we measure the attention distribution over dependency roles in the source side. We first parse the source side of RWTH data using the ParZu parser BIBREF16 . Then we compute how the attention probability mass given to the words other than the alignment points, is distributed over dependency roles. Table TABREF33 gives the most attended roles for each POS tag. Here, we focus on POS tags discussed earlier. One can see that the most attended roles when translating to nouns include adjectives and determiners and in the case of translating to verbs, it includes auxiliary verbs, adverbs (including negation), subjects, and objects.", Alignment points of the POS tags.,The evaluation metrics used are accuracy and INLINEFORM0,0.0,0.0,0.0,0.0,7.483403456344047,5.612552592258036,0.0704225352112676,0.0099009900990099,0.2196626216173172,0.3554311190332684,0.2196626216173172,0.511297881603241,-1.0657570362091064,0.0456663928925991,0.001283626154891,1,,0.7833511297541637,0.1832873186134543,0.7331492744538175,1.0,0.0,0.0,1.9386309167240692
In what cases is attention different from alignment?,"To better understand how attention accuracy affects translation quality, we analyse the relationship between attention loss and word prediction loss for individual part-of-speech classes. Figure FIGREF22 shows how attention loss differs when generating different POS tags. One can see that attention loss varies substantially across different POS tags. In particular, we focus on the cases of NOUN and VERB which are the most frequent POS tags in the dataset. As shown, the attention of NOUN is the closest to alignments on average. But the average attention loss for VERB is almost two times larger than the loss for NOUN. FLOAT SELECTED: Figure 6: Correlation of attention entropy and word prediction loss for the input-feeding system. As another informative variable in our analysis, we look into the attention concentration. While most word alignments only involve one or a few words, attention can be distributed more freely. We measure the concentration of attention by computing the entropy of the attention distribution: DISPLAYFORM0","For certain POS tags, e.g. VERB, PRON. ","1. Tagging descriptions with part-of-speech information to see which adjectives are most commonly used for particular nouns.
2. Leveraging the structure of Flickr30K Entities BIBREF8 to create a coreference graph and apply Louvain clustering to identify clusters of expressions that refer to similar entities.
3. Looking at the proportion of descriptions that contain ethnic markers (such as ""black"" or ""Asian"") to approximate the proportion of images that indicate ethnicity",0.0,0.0,0.0,0.023486540659763,1.7094283658194698,1.6763322849128606,0.111731843575419,0.0100451302955306,0.3616577088832855,0.359228545638882,0.3416087031364441,0.4632670283317566,-1.112185835838318,0.0515278838574886,0.0076983534564428,1,0.3333333333333333,0.0,0.5649999125293432,0.7599996501173728,0.25,0.0,0.0,0.5865667128675641
Which baselines did they compare against?,"FLOAT SELECTED: Table 1: The comparison of various models on different sentence classification tasks. We report the test accuracy of each model in percentage. Our SATA Tree-LSTM shows superior or competitive performance on all tasks, compared to previous treestructured models as well as other sophisticated models. ?: Latent tree-structured models. †: Models which are pre-trained with large external corpora. Our experimental results on the SNLI dataset are shown in table 2 . In this table, we report the test accuracy and number of trainable parameters for each model. Our SATA-LSTM again demonstrates its decent performance compared against the neural models built on both syntactic trees and latent trees, as well as the non-tree models. (Latent Syntax Tree-LSTM: BIBREF10 ( BIBREF10 ), Tree-based CNN: BIBREF35 ( BIBREF35 ), Gumbel Tree-LSTM: BIBREF11 ( BIBREF11 ), NSE: BIBREF36 ( BIBREF36 ), Reinforced Self-Attention Network: BIBREF4 ( BIBREF4 ), Residual stacked encoders: BIBREF37 ( BIBREF37 ), BiLSTM with generalized pooling: BIBREF38 ( BIBREF38 ).) Note that the number of learned parameters in our model is also comparable to other sophisticated models, showing the efficiency of our model. FLOAT SELECTED: Table 1: The comparison of various models on different sentence classification tasks. We report the test accuracy of each model in percentage. Our SATA Tree-LSTM shows superior or competitive performance on all tasks, compared to previous treestructured models as well as other sophisticated models. ?: Latent tree-structured models. †: Models which are pre-trained with large external corpora. Our experimental results on the SNLI dataset are shown in table 2 . In this table, we report the test accuracy and number of trainable parameters for each model. Our SATA-LSTM again demonstrates its decent performance compared against the neural models built on both syntactic trees and latent trees, as well as the non-tree models. (Latent Syntax Tree-LSTM: BIBREF10 ( BIBREF10 ), Tree-based CNN: BIBREF35 ( BIBREF35 ), Gumbel Tree-LSTM: BIBREF11 ( BIBREF11 ), NSE: BIBREF36 ( BIBREF36 ), Reinforced Self-Attention Network: BIBREF4 ( BIBREF4 ), Residual stacked encoders: BIBREF37 ( BIBREF37 ), BiLSTM with generalized pooling: BIBREF38 ( BIBREF38 ).) Note that the number of learned parameters in our model is also comparable to other sophisticated models, showing the efficiency of our model.","Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). 
Stanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018).",Gigaword corpus,0.0,0.0,0.0,0.0,1.7988826528995707,1.3491619896746778,0.0,0.0001104850292785,0.2955017685890198,0.3012446109724487,0.3714752197265625,0.0492488965392112,-0.7953972816467285,0.0489255376160144,0.0008387469762415,1,0.0,0.81699925562843,0.1864857368538636,0.7459429474154544,0.0,0.0,0.0,0.5482863356923766
What baselines did they consider?,"We first use state-of-the-art PDTB taggers for our baseline BIBREF13 , BIBREF12 for the evaluation of the causality prediction of our models ( BIBREF12 requires sentences extracted from the text as its input, so we used our parser to extract sentences from the message). Then, we compare how models work for each task and disassembled them to inspect how each part of the models can affect their final prediction performances. We conducted McNemar's test to determine whether the performance differences are statistically significant at $p < .05$ . FLOAT SELECTED: Table 5: Causal explanation identification performance. Bold indicates significant imrpovement over next best model (p < .05)"," Linear SVM, RBF SVM, and Random Forest","They measure correlation between prediction and explanation quality by analyzing the network's performance on querying and answering separately, and finding a strong correlation between the quality of the explanations and the quality of the algorithm executed by the network",0.06451612591051,0.0,0.0,0.3043941768634331,2.99769151946099,3.3756498739912417,0.081967213114754,0.010158895545715,0.1055871322751045,0.3652960054845695,0.1055871322751045,0.372003823518753,-1.195610761642456,0.0497084707021713,0.0076401491784102,1,0.0,0.7390823826160609,0.1765779943291237,0.706311977316495,0.0,0.0,0.0,0.9665732120770343
How was the dataset annotated?,"We defined the intents with guidance from queries collected using a scoping crowdsourcing task, which prompted crowd workers to provide questions and commands related to topic domains in the manner they would interact with an artificially intelligent assistant. We manually grouped data generated by scoping tasks into intents. To collect additional data for each intent, we used the rephrase and scenario crowdsourcing tasks proposed by BIBREF2. For each intent, there are 100 training queries, which is representative of what a team with a limited budget could gather while developing a task-driven dialog system. Along with the 100 training queries, there are 20 validation and 30 testing queries per intent. We defined the intents with guidance from queries collected using a scoping crowdsourcing task, which prompted crowd workers to provide questions and commands related to topic domains in the manner they would interact with an artificially intelligent assistant. We manually grouped data generated by scoping tasks into intents. To collect additional data for each intent, we used the rephrase and scenario crowdsourcing tasks proposed by BIBREF2. For each intent, there are 100 training queries, which is representative of what a team with a limited budget could gather while developing a task-driven dialog system. Along with the 100 training queries, there are 20 validation and 30 testing queries per intent.",intents are annotated manually with guidance from queries collected using a scoping crowdsourcing task ,Bilingual text with 12 classes,0.1052631540166206,0.0,0.0,2.839838722567789,16.666838337479618,14.357290316541024,0.0381679389312977,0.003831417624521,0.1150745078921318,0.4127266102907609,0.1150745153427124,0.3860288262367248,-0.9353379011154176,0.0554378740489482,0.0012469732831106,1,,0.7773572420041276,0.1780698801809022,0.7123326011313169,0.0,0.0,0.0,4.544517800277149
What is the size of this dataset?,"This paper fills this gap by analyzing intent classification performance with a focus on out-of-scope handling. To do so, we constructed a new dataset with 23,700 queries that are short and unstructured, in the same style made by real users of task-oriented systems. The queries cover 150 intents, plus out-of-scope queries that do not fall within any of the 150 in-scope intents. We introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries. Table TABREF2 shows examples of the data.","  23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains and 1,200 out-of-scope queries.",By 6% to 8% in sentiment classification and by 0.94% to 1.89% in intent classification,0.0588235245674744,0.0,0.0,1.5732934811145336,16.557938430139014,13.058669115349876,0.0425531914893617,0.0070921985815602,0.3715374767780304,0.2631030098763767,0.3715374767780304,0.4120506942272186,-1.055516481399536,0.0446635633707046,0.0125250346738039,1,0.0,0.755621984859039,0.1918883346282766,0.7675533385131066,0.0,0.0,0.0,4.261096856158115
Where does the data come from?,"We introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries. Table TABREF2 shows examples of the data. We defined the intents with guidance from queries collected using a scoping crowdsourcing task, which prompted crowd workers to provide questions and commands related to topic domains in the manner they would interact with an artificially intelligent assistant. We manually grouped data generated by scoping tasks into intents. To collect additional data for each intent, we used the rephrase and scenario crowdsourcing tasks proposed by BIBREF2. For each intent, there are 100 training queries, which is representative of what a team with a limited budget could gather while developing a task-driven dialog system. Along with the 100 training queries, there are 20 validation and 30 testing queries per intent. Out-of-scope queries were collected in two ways. First, using worker mistakes: queries written for one of the 150 intents that did not actually match any of the intents. Second, using scoping and scenario tasks with prompts based on topic areas found on Quora, Wikipedia, and elsewhere. To help ensure the richness of this additional out-of-scope data, each of these task prompts contributed to at most four queries. Since we use the same crowdsourcing method for collecting out-of-scope data, these queries are similar in style to their in-scope counterparts.","crowsourcing platform For ins scope data collection:crowd workers which provide questions and commands related to topic domains and additional data the rephrase and scenario crowdsourcing tasks proposed by BIBREF2 is used. 
For out of scope data collection:  from workers mistakes-queries written for one of the 150 intents that did not actually match any of the intents and using scoping and scenario tasks with prompts based on topic areas found on Quora, Wikipedia, and elsewhere.","The state-of-the-art results achieved are Precision = 0.85, Recall = 0.88, and F1 score = 0.86 on the two benchmark datasets",0.079999996088889,0.0,0.0,1.118525345052971,16.95182785461393,14.926116100291145,0.0567006501182033,0.0029490240134812,0.185694545507431,0.3645620321986651,0.2044142186641693,0.3383456170558929,-1.1082086563110352,0.0315965563058853,0.0111939208001178,1,0.0,0.7622409184283659,0.1794278272493178,0.7177113089972713,0.2,0.0,0.0,4.566255648983327
Ngrams of which length are aligned using PARENT?,"We show that existing automatic metrics, including BLEU, correlate poorly with human judgments when the evaluation sets contain divergent references (§ SECREF36 ). For many table-to-text generation tasks, the tables themselves are in a pseudo-natural language format (e.g., WikiBio, WebNLG BIBREF6 , and E2E-NLG BIBREF10 ). In such cases we propose to compare the generated text to the underlying table as well to improve evaluation. We develop a new metric, PARENT (Precision And Recall of Entailed N-grams from the Table) (§ SECREF3 ). When computing precision, PARENT effectively uses a union of the reference and the table, to reward correct information missing from the reference. When computing recall, it uses an intersection of the reference and the table, to ignore extra incorrect information in the reference. The union and intersection are computed with the help of an entailment model to decide if a text n-gram is entailed by the table. We show that this method is more effective than using the table as an additional reference. Our main contributions are: PARENT evaluates each instance INLINEFORM0 separately, by computing the precision and recall of INLINEFORM1 against both INLINEFORM2 and INLINEFORM3 .", Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4,"Sure! Here's the answer to the question based on the provided context:

The active learning model works by using a learning engine (a CRF-based segmenter) to train on labeled data, and a selection engine (a scoring model) to select unlabeled data samples that are most likely to benefit from relabeling by annotators. The selection engine chooses samples for relabeling based on a scoring function, and the relabeled samples are then added to the training set for the classifier to re-train, improving its accuracy over time",0.0294117617993082,0.0,0.0,0.0328948329103149,7.192387470986719,6.027540000295066,0.1082251082251082,0.0101371496720333,0.0103036575019359,0.537900976005599,0.1083510890603065,0.387506753206253,-1.2138631343841553,0.049788262695074,0.0125741462104567,1,,0.0,0.1717802243638009,0.6871208974552037,0.0,0.0,0.0,1.9647955267340453
How many people participated in their evaluation study of table-to-text models?,"The data collection was performed separately for models in the WikiBio-Systems and WikiBio-Hyperparams categories. 1100 tables were sampled from the development set, and for each table we got 8 different sentence pairs annotated across the two categories, resulting in a total of 8800 pairwise comparisons. Each pair was judged by one worker only which means there may be noise at the instance-level, but the aggregated system-level scores had low variance (cf. Table TABREF32 ). In total around 500 different workers were involved in the annotation. References were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio.",about 500 ,SVM approach using unsupervised hand-crafted features and word2vec algorithm,0.0,0.0,0.0,0.0,1.1904761904761902,0.892857142857143,0.0,0.0099009900990099,-0.0241982657462358,0.0,-0.0241982713341712,,-1.3323336839675903,0.0491168946027755,0.0006491185135773,1,0.3333333333333333,0.744706948627893,0.6814820396217723,0.7259388597931842,0.0,0.0,0.0,0.526903833634053
By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?,"We use bootstrap sampling (500 iterations) over the 1100 tables for which we collected human annotations to get an idea of how the correlation of each metric varies with the underlying data. In each iteration, we sample with replacement, tables along with their references and all the generated texts for that table. Then we compute aggregated human evaluation and metric scores for each of the models and compute the correlation between the two. We report the average correlation across all bootstrap samples for each metric in Table TABREF37 . The distribution of correlations for the best performing metrics are shown in Figure FIGREF38 . FLOAT SELECTED: Table 2: Correlation of metrics with human judgments on WikiBio. A superscript of C/W indicates that the correlation is significantly lower than that of PARENTC/W using a bootstrap confidence test for α = 0.1. FLOAT SELECTED: Table 4: Average pearson correlation across 500 bootstrap samples of each metric to human ratings for each aspect of the generations from the WebNLG challenge. The human ratings were collected on 3 distinct aspects – grammaticality, fluency and semantics, where semantics corresponds to the degree to which a generated text agrees with the meaning of the underlying RDF triples. We report the correlation of several metrics with these ratings in Table TABREF48 . Both variants of PARENT are either competitive or better than the other metrics in terms of the average correlation to all three aspects. This shows that PARENT is applicable for high quality references as well. FLOAT SELECTED: Table 2: Correlation of metrics with human judgments on WikiBio. A superscript of C/W indicates that the correlation is significantly lower than that of PARENTC/W using a bootstrap confidence test for α = 0.1.",Best proposed metric has average correlation with human judgement of 0.913 and 0.846 compared to best compared metrics result of 0.758 and 0.829 on WikiBio and WebNLG challenge. Their average correlation tops the best other model by 0.155 on WikiBio.,The authors define robustness of a model as its ability to handle prior knowledge with no bias and perform well even when the labeled features are unbalanced,0.2033898255443839,0.0,0.0,1.3667950042025578,22.75764378781526,19.568822547589985,0.074074074074074,0.0072444325194526,0.2522696852684021,0.4848485377652783,0.2449392974376678,0.6103704571723938,-0.9909263849258424,0.0553990118205547,0.0132669190425545,1,,0.7268928520325396,0.1760867295949139,0.7044833660349425,0.0,0.0,0.0,6.191515523714268
Which stock market sector achieved the best performance?,"FLOAT SELECTED: Table 8: Sector-level performance comparison. FLOAT SELECTED: Table 7: Our volatility model performance compared with GARCH(1,1). Best performance in bold. Our model has superior performance across the three evaluation metrics and taking into consideration the state-of-the-art volatility proxies, namely Garman-Klass (σ̂PK) and Parkinson (σ̂PK).",Energy with accuracy of 0.538 Energy,"The proposed comparative evaluator with skill rating significantly outperforms all compared baselines, including comparative evaluator with averaged sample-level scores, in human evaluation",0.0799999963520001,0.0,0.0,0.4041065926152656,4.5269163658560405,4.010285910009624,0.0641025641025641,0.0103675777568331,0.0721615999937057,0.3001976877870694,0.0721615999937057,0.1822500824928283,-1.2048078775405884,0.042586762458086,0.0026443552459196,1,,0.7262465651370783,0.6877500286162503,0.7510001144650016,0.0,0.0,0.0,1.2845008636101887
"How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques?","The above challenges often hinder the productivity of engineers, and result in less optimal solutions to their given tasks. This motivates us to develop an NLP toolkit for DNN models, which facilitates engineers to develop DNN approaches. Before designing this NLP toolkit, we conducted a survey among engineers and identified a spectrum of three typical personas.",By conducting a survey among engineers ,ACM Transactions on Computer Systems (TOCS) and the International Conference on World Wide Web (WWW) are cited the most in recent years,0.0,0.0,0.0,0.0,6.671006051158209,5.003254538368658,0.0,0.0099009900990099,0.1565688699483871,0.2827960382337156,0.1565688699483871,0.2017342746257782,-1.141129493713379,0.0476623401045799,0.0098588173955587,1,,0.7218917649045232,0.1973580119928171,0.7894417845490895,0.0,0.0,0.0,1.747744586751557
what datasets did they use?,"Though Bengali is the seventh most spoken language in terms of number of native speakers BIBREF23, there is no standard corpus of questions available BIBREF0. We have collected total 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. The corpus contains the questions and the classes each question belongs to. Though Bengali is the seventh most spoken language in terms of number of native speakers BIBREF23, there is no standard corpus of questions available BIBREF0. We have collected total 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. The corpus contains the questions and the classes each question belongs to.","Dataset of total 3500 questions from the Internet and other sources such as books of general knowledge questions, history, etc. 3500 questions collected from the internet and books.","The songs fall under the following genres:

* MPB (Brazilian Popular Music)
* Sertanejo
* Forró
* Axé
* Funk
* Soul
* Rock
* Pop
* Hip-Hop
* Electronic
* R&B
* Jazz

Note that these genres are based on the 14 representative genres of Brazilian music selected from the Vagalume's music web page",0.0983606512227897,0.0253164513699734,0.0253164513699734,1.1500774766602295,17.614426524542857,14.402617835025842,0.0904369627507163,0.0102707749766573,0.1112790554761886,0.486827452522185,0.1132092252373695,0.3473571240901947,-1.1236364841461182,0.048893854022026,0.0019837809911112,1,,0.7807467328522101,0.1909966102499183,0.7639864409996735,0.0,0.0,0.0,4.698326682105637
How much does their model outperform existing models?,"The performance of all models on arXiv and Pubmed is shown in Table TABREF28 and Table TABREF29 , respectively. Follow the work BIBREF18 , we use the approximate randomization as the statistical significance test method BIBREF32 with a Bonferroni correction for multiple comparisons, at the confidence level 0.01 ( INLINEFORM0 ). As we can see in these tables, on both datasets, the neural extractive models outperforms the traditional extractive models on informativeness (ROUGE-1,2) by a wide margin, but results are mixed on ROUGE-L. Presumably, this is due to the neural training process, which relies on a goal standard based on ROUGE-1. Exploring other training schemes and/or a combination of traditional and neural approaches is left as future work. Similarly, the neural extractive models also dominate the neural abstractive models on ROUGE-1,2, but these abstractive models tend to have the highest ROUGE-L scores, possibly because they are trained directly on gold standard abstract summaries. FLOAT SELECTED: Table 4.1: Results on the arXiv dataset. For models with an ∗, we report results from [8]. Models are traditional extractive in the first block, neural abstractive in the second block, while neural extractive in the third block. The Oracle (last row) corresponds to using the ground truth labels, obtained (for training) by the greedy algorithm, see Section 4.1.2. Results that are not significantly distinguished from the best systems are bold. FLOAT SELECTED: Table 4.2: Results on the Pubmed dataset. For models with an ∗, we report results from [8]. See caption of Table 4.1 above for details on compared models. Results that are not significantly distinguished from the best systems are bold. FLOAT SELECTED: Table 4.1: Results on the arXiv dataset. For models with an ∗, we report results from [8]. Models are traditional extractive in the first block, neural abstractive in the second block, while neural extractive in the third block. The Oracle (last row) corresponds to using the ground truth labels, obtained (for training) by the greedy algorithm, see Section 4.1.2. Results that are not significantly distinguished from the best systems are bold. FLOAT SELECTED: Table 4.2: Results on the Pubmed dataset. For models with an ∗, we report results from [8]. See caption of Table 4.1 above for details on compared models. Results that are not significantly distinguished from the best systems are bold.","Best proposed model result vs best previous result:
Arxiv dataset: Rouge 1 (43.62 vs 42.81), Rouge L (29.30 vs 31.80), Meteor (21.78 vs 21.35)
Pubmed dataset: Rouge 1 (44.85 vs 44.29), Rouge L (31.48 vs 35.21), Meteor (20.83 vs 20.56) On arXiv dataset, the proposed model outperforms baselie model by (ROUGE-1,2,L)  0.67 0.72 0.77 respectively and by Meteor 0.31.
","The proposed model achieved an execution accuracy of 83.3% and an operation sequence accuracy of 86.7% on the MathQA dataset, outperforming the baseline models",0.1578947327285319,0.0210526278027707,0.0210526278027707,1.3415558343750555,27.76767325521136,23.58055716484316,0.057471264367816,0.004344677769732,0.3751965761184692,0.483324693945738,0.3751965463161468,0.2923246026039123,-0.9094400405883788,0.047478549182415,0.0173001728875714,1,0.5,0.741733486173129,0.687311973734456,0.7492478949378238,0.2,0.0,0.0,7.352838200638344
What was the baseline for this task?,"The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly. The inefficacy of such a simple random baseline is illustrated in Tables TABREF36 and TABREF41. The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly. The inefficacy of such a simple random baseline is illustrated in Tables TABREF36 and TABREF41.",The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly. ,2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0003029385034837,-0.0001601108815521,0.0,0.1221730411052703,,-1.0054739713668823,0.0696326345205307,0.0008783389683284,1,0.0,0.7678504557006868,0.6944231944974139,0.7776927779896554,0.0,0.0,0.0,0.4047083031361237
What is a second order co-ocurrence matrix?,"However, despite these successes distributional methods do not perform well when data is very sparse (which is common). One possible solution is to use second–order co–occurrence vectors BIBREF10 , BIBREF11 . In this approach the similarity between two words is not strictly based on their co–occurrence frequencies, but rather on the frequencies of the other words which occur with both of them (i.e., second order co–occurrences). This approach has been shown to be successful in quantifying semantic relatedness BIBREF12 , BIBREF13 . However, while more robust in the face of sparsity, second–order methods can result in significant amounts of noise, where contextual information that is overly general is included and does not contribute to quantifying the semantic relatedness between the two concepts. However, despite these successes distributional methods do not perform well when data is very sparse (which is common). One possible solution is to use second–order co–occurrence vectors BIBREF10 , BIBREF11 . In this approach the similarity between two words is not strictly based on their co–occurrence frequencies, but rather on the frequencies of the other words which occur with both of them (i.e., second order co–occurrences). This approach has been shown to be successful in quantifying semantic relatedness BIBREF12 , BIBREF13 . However, while more robust in the face of sparsity, second–order methods can result in significant amounts of noise, where contextual information that is overly general is included and does not contribute to quantifying the semantic relatedness between the two concepts.", The matrix containing co-occurrences of the words which occur with the both words of every given pair of words.,The corpora they trained ELMo on were approximately 2 times larger for English and comparable in size for Russian,0.0624999950781253,0.0,0.0,1.9146030690102511,16.441408749677443,12.98261502516527,0.0251256281407035,0.0104452996151731,0.0665671527385711,0.3712808258767086,0.0665671601891517,0.4218853414058685,-1.0014891624450684,0.0515530034899711,0.0028487713004659,1,0.0,0.7558800819132437,0.6954183634970705,0.7816734539882819,0.0,0.0,0.0,4.233724313248099
How many humans participated?,"MiniMayoSRS: The MayoSRS, developed by PakhomovPMMRC10, consists of 101 clinical term pairs whose relatedness was determined by nine medical coders and three physicians from the Mayo Clinic. The relatedness of each term pair was assessed based on a four point scale: (4.0) practically synonymous, (3.0) related, (2.0) marginally related and (1.0) unrelated. MiniMayoSRS is a subset of the MayoSRS and consists of 30 term pairs on which a higher inter–annotator agreement was achieved. The average correlation between physicians is 0.68. The average correlation between medical coders is 0.78. We evaluate our method on the mean of the physician scores, and the mean of the coders scores in this subset in the same manner as reported by PedersenPPC07. UMNSRS: The University of Minnesota Semantic Relatedness Set (UMNSRS) was developed by PakhomovMALPM10, and consists of 725 clinical term pairs whose semantic similarity and relatedness was determined independently by four medical residents from the University of Minnesota Medical School. The similarity and relatedness of each term pair was annotated based on a continuous scale by having the resident touch a bar on a touch sensitive computer screen to indicate the degree of similarity or relatedness. The Intraclass Correlation Coefficient (ICC) for the reference standard tagged for similarity was 0.47, and 0.50 for relatedness. Therefore, as suggested by Pakhomov and colleagues,we use a subset of the ratings consisting of 401 pairs for the similarity set and 430 pairs for the relatedness set which each have an ICC of 0.73.", 16,"The dataset is annotated with depression-related symptoms, such as depressed mood, disturbed sleep, and fatigue or loss of energy, for each tweet",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0099009900990099,0.0332913398742675,0.0,0.0332913286983966,,-1.374168872833252,0.041632667183876,0.0114679098620879,1,0.0,0.7627154358613469,0.1837410264717144,0.7349641058868577,0.0,0.0,0.0,0.43393803492242544
What word level and character level model baselines are used?,"In our experiments, the Memory Neural Networks (MemNNs) proposed in babidataset serve as the baselines. For training, in addition to the 76K questions in the training set, the MemNNs use 3K training questions from WebQuestions BIBREF27 , 15M paraphrases from WikiAnswers BIBREF2 , and 11M and 12M automatically generated questions from the KB for the FB2M and FB5M settings, respectively. In contrast, our models are trained only on the 76K questions in the training set. In our experiments, the Memory Neural Networks (MemNNs) proposed in babidataset serve as the baselines. For training, in addition to the 76K questions in the training set, the MemNNs use 3K training questions from WebQuestions BIBREF27 , 15M paraphrases from WikiAnswers BIBREF2 , and 11M and 12M automatically generated questions from the KB for the FB2M and FB5M settings, respectively. In contrast, our models are trained only on the 76K questions in the training set. We evaluate the proposed model on the SimpleQuestions dataset BIBREF0 . The dataset consists of 108,442 single-relation questions and their corresponding (topic entity, predicate, answer entity) triples from Freebase. It is split into 75,910 train, 10,845 validation, and 21,687 test questions. Only 10,843 of the 45,335 unique words in entity aliases and 886 out of 1,034 unique predicates in the test set were present in the train set. For the proposed dataset, there are two evaluation settings, called FB2M and FB5M, respectively. The former uses a KB for candidate generation which is a subset of Freebase and contains 2M entities, while the latter uses subset of Freebase with 5M entities.",None Word-level Memory Neural Networks (MemNNs) proposed in Bordes et al. (2015),"ARABIC (ar)

In the table provided, the error rate reduction for Arabic (ar) is the lowest among all languages, with a reduction of only 0.0025",0.0,0.0,0.0,1.419445346723854,11.1179398824276,9.294035582843662,0.1358695652173913,0.0099009900990099,0.2144580036401748,0.2438133996599718,0.2149889022111892,0.1674439162015915,-1.1182711124420166,0.0418414957821369,0.0061268112467646,1,0.3333333333333333,0.7701775788388701,0.1933312196716286,0.7733248786865146,0.6666666666666666,0.0,0.0,2.923260959946984
How were the human judgements assembled?,"To ensure that the increase in BLEU score correlated to actual increase in performance of translation, human evaluation metrics like adequacy, precision and ranking values (between RNNSearch and RNNMorph outputs) were estimated in Table TABREF30 . A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a 5-point scale of (Flawless, Good, Non-native, Disfluent, Incomprehensive). For the comparison process, the RNNMorph and the RNNSearch + Word2Vec models’ sentence level translations were individually ranked between each other, permitting the two translations to have ties in the ranking. The intra-annotator values were computed for these metrics and the scores are shown in Table TABREF32 BIBREF12 , BIBREF13 . To ensure that the increase in BLEU score correlated to actual increase in performance of translation, human evaluation metrics like adequacy, precision and ranking values (between RNNSearch and RNNMorph outputs) were estimated in Table TABREF30 . A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a 5-point scale of (Flawless, Good, Non-native, Disfluent, Incomprehensive). For the comparison process, the RNNMorph and the RNNSearch + Word2Vec models’ sentence level translations were individually ranked between each other, permitting the two translations to have ties in the ranking. The intra-annotator values were computed for these metrics and the scores are shown in Table TABREF32 BIBREF12 , BIBREF13 .","50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale. ",A semi-character architecture,0.0,0.0,0.0,0.0,12.665875154796383,9.499406366097286,0.0248756218905472,0.001576458223857,0.1031331419944763,0.1992417994667502,0.1031331345438957,0.1165770888328552,-1.199995040893555,0.0475813634693622,0.0003158131063852,1,0.3333333333333333,0.759725246034073,0.1895487835259676,0.7581682861445979,0.0,0.0,0.0,3.220410370501955
Which other approaches do they compare their model with?,"FLOAT SELECTED: Table 3: Comparison with existing models. In this paper, we present a deep neural network model for the task of fine-grained named entity classification using ELMo embeddings and Wikidata. The proposed model learns representations for entity mentions based on its context and incorporates the rich structure of Wikidata to augment these labels into finer-grained subtypes. We can see comparisons of our model made on Wiki(gold) in Table TABREF20 . We note that the model performs similarly to existing systems without being trained or tuned on that particular dataset. Future work may include refining the clustering method described in Section 2.2 to extend to types other than person, location, organization, and also to include disambiguation of entity types. FLOAT SELECTED: Table 3: Comparison with existing models.","Akbik et al. (2018), Link et al. (2012) They compare to Akbik et al. (2018) and Link et al. (2012).",Active learning is a technique for selectively annotating unlabeled samples to improve the performance of iteratively trained machine learning models,0.0666666620222225,0.0,0.0,1.0885011049519644,6.677173653647284,5.63133899451998,0.0153374233128834,0.0104166666666666,0.1057497859001159,0.1908952151442366,0.1776916831731796,0.1018327251076698,-0.8490107655525208,0.0578596368432045,0.012855655068167,1,,0.7851072972909284,0.5677956285204016,0.7711067760400747,0.0,0.0,0.0,1.807276384812163
What results do they achieve using their proposed approach?,"The results for each class type are shown in Table TABREF19 , with some specific examples shown in Figure FIGREF18 . For the Wiki(gold) we quote the micro-averaged F-1 scores for the entire top level entity category. The total F-1 score on the OntoNotes dataset is 88%, and the total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%. It is worth noting that one could improve Wiki(gold) results by training directly using this dataset. However, the aim is not to tune our model specifically on this class hierarchy. We instead aim to present a framework which can be modified easily to any domain hierarchy and has acceptable out-of-the-box performances to any fine-grained dataset. The results in Table TABREF19 (OntoNotes) only show the main 7 categories in OntoNotes which map to Wiki(gold) for clarity. The other categories (date, time, norp, language, ordinal, cardinal, quantity, percent, money, law) have F-1 scores between 80-90%, with the exception of time (65%) The results for each class type are shown in Table TABREF19 , with some specific examples shown in Figure FIGREF18 . For the Wiki(gold) we quote the micro-averaged F-1 scores for the entire top level entity category. The total F-1 score on the OntoNotes dataset is 88%, and the total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%. It is worth noting that one could improve Wiki(gold) results by training directly using this dataset. However, the aim is not to tune our model specifically on this class hierarchy. We instead aim to present a framework which can be modified easily to any domain hierarchy and has acceptable out-of-the-box performances to any fine-grained dataset. The results in Table TABREF19 (OntoNotes) only show the main 7 categories in OntoNotes which map to Wiki(gold) for clarity. The other categories (date, time, norp, language, ordinal, cardinal, quantity, percent, money, law) have F-1 scores between 80-90%, with the exception of time (65%)","F-1 score on the OntoNotes is 88%, and it is 53% on Wiki (gold). ","The dataset consists of 471,319 references from Supreme Court decisions, 167,237 references from Supreme Administrative Court decisions, and 264,463 references from Constitutional Court Decisions, for a total of 802,029 references",0.060606055831038,0.0,0.0,1.999020892700964,5.668001293358887,5.555468135824137,0.0704225352112676,0.0102389078498293,0.2545631527900696,0.2252884255575885,0.2545631825923919,0.154477521777153,-0.9920971393585204,0.047983631491661,0.0160763980556195,1,0.25,0.7437534793113172,0.4898481188738072,0.7590831241940459,0.6,0.0,0.0,1.6259739867562102
How do they combine a deep learning model with a knowledge base?,"While these knowledge bases provide semantically rich and fine-granular classes and relationship types, the task of entity classification often requires associating coarse-grained classes with discovered surface forms of entities. Most existing studies consider NER and entity linking as two separate tasks, whereas we try to combine the two. It has been shown that one can significantly increase the semantic information carried by a NER system when we successfully linking entities from a deep learning method to the related entities from a knowledge base BIBREF26 , BIBREF27 . Redirection: For the Wikidata linking element, we recognize that the lookup will be constrained by the most common lookup name for each entity. Consider the utterance (referring to the NBA basketball player) from Figure FIGREF12 “Michael Jeffrey Jordan in San Jose” as an example. The lookup for this entity in Wikidata is “Michael Jordan” and consequently will not be picked up if we were to use an exact string match. A simple method to circumvent such a problem is the usage of a redirection list. Such a list is provided on an entity by entity basis in the “Also known as” section in Wikidata. Using this redirection list, when we do not find an exact string match improves the recall of our model by 5-10%. Moreover, with the example of Michael Jordan (person), using our current framework, we will always refer to the retired basketball player (Q41421). We will never, for instance, pick up Michael Jordan (Q27069141) the American football cornerback. Or in fact any other Michael Jordan, famous or otherwise. One possible method to overcome this is to add a disambiguation layer, which seeks to use context from earlier parts of the text. This is, however, work for future improvement and we only consider the most common version of that entity. The architecture of our proposed model is shown in Figure FIGREF12 . The input is a list of tokens and the output are the predicted entity types. The ELMo embeddings are then used with a residual LSTM to learn informative morphological representations from the character sequence of each token. We then pass this to a softmax layer as a tag decoder to predict the entity types.",Entities from a deep learning model are linked to the related entities from a knowledge base by a lookup. ,Bias due to dataset selection and word coverage,0.0799999956480002,0.0,0.0,1.9146030690102511,15.941057651838818,13.162330096867594,0.026595744680851,0.0044247787610619,0.1678788214921951,0.4455276502341758,0.1678788363933563,0.3288348615169525,-0.91858172416687,0.0375053100287914,0.0024346658337882,1,0.5,0.7572945546738176,0.8532086138824816,0.7461677888632599,0.0,0.0,0.0,4.16385783621132
What are the models used for the baseline of the three NLP tasks?,"Baseline Results ::: Speech Synthesis In our previous work on building speech systems on found data in 700 languages, BIBREF7, we addressed alignment issues (when audio is not segmented into turn/sentence sized chunks) and correctness issues (when the audio does not match the transcription). We used the same techniques here, as described above. For the best quality speech synthesis we need a few hours of phonetically-balanced, single-speaker, read speech. Our first step was to use the start and end points for each turn in the dialogues, and select those of the most frequent speaker, nmlch. This gave us around 18250 segments. We further automatically removed excessive silence from the start, middle and end of these turns (based on occurrence of F0). This gave us 13 hours and 48 minutes of speech. We phonetically aligned this data and built a speech clustergen statistical speech synthesizer BIBREF9 from all of this data. We resynthesized all of the data and measured the difference between the synthesized data and the original data using Mel Cepstral Distortion, a standard method for automatically measuring quality of speech generation BIBREF10. We then ordered the segments by their generation score and took the top 2000 turns to build a new synthesizer, assuming the better scores corresponded to better alignments, following the techniques of BIBREF7. For speech recognition (ASR) we used Kaldi BIBREF11. As we do not have access to pronunciation lexica for Mapudungun, we had to approximate them with two settings. In the first setting, we make the simple assumption that each character corresponds to a pronunced phoneme. In the second setting, we instead used the generated phonetic lexicon also used in the above-mentioned speech synthesis techniques. The train/dev/test splits are across conversations, as described above. We built neural end-to-end machine translation systems between Mapudungun and Spanish in both directions, using state-of-the-art Transformer architecture BIBREF14 with the toolkit of BIBREF15. We train our systems at the subword level using Byte-Pair Encoding BIBREF16 with a vocabulary of 5000 subwords, shared between the source and target languages. We use five layers for each of the encoder and the decoder, an embedding size of 512, feed forward transformation size of 2048, and eight attention heads. We use dropout BIBREF17 with $0.4$ probability as well as label smoothing set to $0.1$. We train with the Adam optimizer BIBREF18 for up to 200 epochs using learning decay with a patience of six epochs. We phonetically aligned this data and built a speech clustergen statistical speech synthesizer BIBREF9 from all of this data. We resynthesized all of the data and measured the difference between the synthesized data and the original data using Mel Cepstral Distortion, a standard method for automatically measuring quality of speech generation BIBREF10. We then ordered the segments by their generation score and took the top 2000 turns to build a new synthesizer, assuming the better scores corresponded to better alignments, following the techniques of BIBREF7. For speech recognition (ASR) we used Kaldi BIBREF11. As we do not have access to pronunciation lexica for Mapudungun, we had to approximate them with two settings. In the first setting, we make the simple assumption that each character corresponds to a pronunced phoneme. In the second setting, we instead used the generated phonetic lexicon also used in the above-mentioned speech synthesis techniques. The train/dev/test splits are across conversations, as described above. We built neural end-to-end machine translation systems between Mapudungun and Spanish in both directions, using state-of-the-art Transformer architecture BIBREF14 with the toolkit of BIBREF15. We train our systems at the subword level using Byte-Pair Encoding BIBREF16 with a vocabulary of 5000 subwords, shared between the source and target languages. We use five layers for each of the encoder and the decoder, an embedding size of 512, feed forward transformation size of 2048, and eight attention heads. We use dropout BIBREF17 with $0.4$ probability as well as label smoothing set to $0.1$. We train with the Adam optimizer BIBREF18 for up to 200 epochs using learning decay with a patience of six epochs."," For speech synthesis, they build a speech clustergen statistical speech synthesizer BIBREF9. For speech recognition, they use Kaldi BIBREF11. For Machine Translation, they use a Transformer architecture from BIBREF15.",They verify generalization ability by evaluating the trained model on five different benchmarks,0.0,0.0,0.0,0.0,18.967724556531625,14.225793417398712,0.0152439024390243,0.0046214006398862,0.1431848108768463,0.3021502260814972,0.2363275736570358,0.2127262502908706,-1.2135937213897705,0.0328293330967426,0.0022349357361777,1,,0.7524513391040211,0.8511826026378322,0.7380637438846624,0.0,0.0,0.0,4.912913550130774
How is non-standard pronunciation identified?,"In addition, the transcription includes annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. Foreign words, in this case Spanish words, are also labelled as such. FLOAT SELECTED: Table 2: Example of an utterance along with the different annotations. We additionally highlight the code-switching annotations ([SPA] indicates Spanish words) as well as pre-normalized transcriptions that indicating non-standard pronunciations ([!1pu’] indicates that the previous 1 word was pronounced as ‘pu’’ instead of ‘pues’).", Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation.,"Content accuracy, coherence, and consistency with known facts are key semantic features that help detect whether a piece of text is genuine or generated",0.0555555511111114,0.0,0.0,1.2622239405701918,14.991014764585314,11.77131901198935,0.031055900621118,0.0103270223752151,0.1516564786434173,0.4032472910152541,0.1516564786434173,0.3970560729503631,-1.0027137994766235,0.037793681025505,0.0095659926584799,1,0.5,0.7565510997362846,0.4873763162122493,0.7495562782780096,0.0,0.0,0.0,3.8436572670388367
What novel PMI variants are introduced?,"where * denotes summation over the corresponding index. To deal with negative values, we propose clipped $\mathit {PMI}$, which is equivalent to $\mathit {PPMI}$ when $z = 0$. such that $NPMI(w,c) = -1$ when $(w,c)$ never cooccur, $NPMI(w,c) = 0$ when they are independent, and $NPMI(w,c) = 1$ when they always cooccur together. This effectively captures the entire negative spectrum, but has the downside of normalization which discards scale information. In practice we find this works poorly if done symmetrically, so we introduce a variant called $\mathit {NNEGPMI}$ which only normalizes $\mathit {\texttt {-}PMI}$: where * denotes summation over the corresponding index. To deal with negative values, we propose clipped $\mathit {PMI}$, which is equivalent to $\mathit {PPMI}$ when $z = 0$. Normalization: We also experiment with normalized $\mathit {PMI}$ ($\mathit {NPMI}$) BIBREF7: such that $NPMI(w,c) = -1$ when $(w,c)$ never cooccur, $NPMI(w,c) = 0$ when they are independent, and $NPMI(w,c) = 1$ when they always cooccur together. This effectively captures the entire negative spectrum, but has the downside of normalization which discards scale information. In practice we find this works poorly if done symmetrically, so we introduce a variant called $\mathit {NNEGPMI}$ which only normalizes $\mathit {\texttt {-}PMI}$:",clipped PMI; NNEGPMI ,"Sure! The performance evaluation metrics used are:

* Overall accuracy
* Micro-average precision, recall, and F1-score for relation extraction
* Strict precision, recall, and F1-score for named entity recognition
* Pearson correlation coefficients for sentence similarity

All these metrics are commonly used in natural language processing tasks and are appropriate for evaluating the performance of the models on the specific tasks described in the context",0.0,0.0,0.0,0.0,0.6326162214477814,0.4744621660858361,0.0,0.0099009900990099,-0.0181503184139728,0.1733684142430622,0.0681894272565841,0.1496657878160476,-1.3986570835113523,0.0577786639332771,0.0022393649073116,1,,0.7458307023997014,0.1815093032293667,0.7260722946125087,0.0,0.0,0.0,0.45780185460559625
What are the disadvantages to clipping negative PMI?,"Why incorporate -PMI? $\mathit {\texttt {+}PPMI}$ only falters on the RW and analogy tasks, and we hypothesize this is where $\mathit {\texttt {-}PMI}$ is useful: in the absence of positive information, negative information can be used to improve rare word representations and word analogies. Analogies are solved using nearest neighbor lookups in the vector space, and so accounting for negative cooccurrence effectively repels words with which no positive cooccurrence was observed. In future work, we will explore incorporating $\mathit {\texttt {-}PMI}$ only for rare words (where it is most needed).",It may lead to poor rare word representations and word analogies. ,Neural activity in visual cortex,0.0,0.0,0.0,0.0,11.039769287629964,8.279826965722473,0.0,0.004524886877828,0.1200629100203514,0.3433327423625214,0.1200628876686096,0.1839520782232284,-1.1101415157318115,0.0286868028342723,0.0017425635965355,1,0.0,0.8270676540896696,0.7868597303318953,0.7474389213275808,0.0,0.0,0.0,2.8037657055379333
Why are statistics from finite corpora unreliable?,"Unfortunately, $\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus. Due to unreliable statistics, this happens very frequently in finite corpora. Many models work around this issue by clipping negative $\mathit {PMI}$ values at 0, a measure known as Positive $\mathit {PMI}$ ($\mathit {PPMI}$), which works very well in practice. An unanswered question is: “What is lost/gained by collapsing the negative $\mathit {PMI}$ spectrum to 0?”. Understanding which type of information is captured by $\mathit {\texttt {-}PMI}$ can help in tailoring models for optimal performance. Unfortunately, $\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus. Due to unreliable statistics, this happens very frequently in finite corpora. Many models work around this issue by clipping negative $\mathit {PMI}$ values at 0, a measure known as Positive $\mathit {PMI}$ ($\mathit {PPMI}$), which works very well in practice. An unanswered question is: “What is lost/gained by collapsing the negative $\mathit {PMI}$ spectrum to 0?”. Understanding which type of information is captured by $\mathit {\texttt {-}PMI}$ can help in tailoring models for optimal performance.", A finite corpora may entirely omit rare word combinations,"Blackhat, Nulled, Brown, and Gazetteers",0.0,0.0,0.0,0.0,10.16506746075512,7.62380059556634,0.0,0.005524861878453,0.2103976309299469,0.2615562503237323,0.2103976309299469,0.0928927138447761,-1.119595289230347,0.051150869578123,0.0013351349372455,1,0.0,0.7757181620617685,0.496622547488841,0.7864901899553639,0.0,0.0,0.0,2.584554712382412
Which two pairs of ERPs from the literature benefit from joint training?,"This work is most closely related to the paper from which we get the ERP data: BIBREF0 . In that work, the authors relate the surprisal of a word, i.e. the (negative log) probability of the word appearing in its context, to each of the ERP signals we consider here. The authors do not directly train a model to predict ERPs. Instead, models of the probability distribution of each word in context are used to compute a surprisal for each word, which is input into a mixed effects regression along with word frequency, word length, word position in the sentence, and sentence position in the experiment. The effect of the surprisal is assessed using a likelihood-ratio test. In BIBREF7 , the authors take an approach similar to BIBREF0 . The authors compare the explanatory power of surprisal (as computed by an LSTM or a Recurrent Neural Network Grammar (RNNG) language model) to a measure of syntactic complexity they call “distance"" that counts the number of parser actions in the RNNG language model. The authors find that surprisal (as predicted by the RNNG) and distance are both significant factors in a mixed effects regression which predicts the P600, while the surprisal as computed by an LSTM is not. Unlike BIBREF0 and BIBREF7 , we do not use a linking function (e.g. surprisal) to relate a language model to ERPs. We thus lose the interpretability provided by the linking function, but we are able to predict a significant proportion of the variance for all of the ERP components, where prior work could not. We interpret our results through characterization of the ERPs in terms of how they relate to each other and to eye-tracking data rather than through a linking function. The authors in BIBREF8 also use a recurrent neural network to predict neural activity directly. In that work the authors predict magnetoencephalography (MEG) activity, a close cousin to EEG, recorded while participants read a chapter of Harry Potter and the Sorcerer’s Stone BIBREF9 . Their approach to characterization of processing at each MEG sensor location is to determine whether it is best predicted by the context vector of the recurrent network (prior to the current word being processed), the embedding of the current word, or the probability of the current word given the context. In future work we also intend to add these types of studies to the ERP predictions. Discussion","Answer with content missing: (Whole Method and Results sections) Self-paced reading times widely benefit ERP prediction, while eye-tracking data seems to have more limited benefit to just the ELAN, LAN, and PNP ERP components.
Select:
- ELAN, LAN
- PNP ERP ","Subtask 1: Identifying entities with offsets and mapping them to a predefined set of four classes (PROTEINAS, NORMALIZABLES, NO_NORMALIZABLES, and UNCLEAR).
Subtask 2: Providing a list of all SNOMED CT IDs (SCTID) for entities occurring in the text, also known as concept indexing",0.112676051362825,0.0,0.0,2.103865051784464,19.16068540577668,17.822870925946223,0.1866706969246032,0.0101343389111477,0.1739421933889389,0.3645005233503959,0.114289978519082,0.0785776674747467,-0.8073824644088745,0.0489509366452693,0.0104131999343596,1,0.8,0.7494573405056517,0.5546594931119971,0.7186379724479884,0.0,0.0,0.2,5.2710958854144225
What datasets are used?,"This work is most closely related to the paper from which we get the ERP data: BIBREF0 . In that work, the authors relate the surprisal of a word, i.e. the (negative log) probability of the word appearing in its context, to each of the ERP signals we consider here. The authors do not directly train a model to predict ERPs. Instead, models of the probability distribution of each word in context are used to compute a surprisal for each word, which is input into a mixed effects regression along with word frequency, word length, word position in the sentence, and sentence position in the experiment. The effect of the surprisal is assessed using a likelihood-ratio test. In BIBREF7 , the authors take an approach similar to BIBREF0 . The authors compare the explanatory power of surprisal (as computed by an LSTM or a Recurrent Neural Network Grammar (RNNG) language model) to a measure of syntactic complexity they call “distance"" that counts the number of parser actions in the RNNG language model. The authors find that surprisal (as predicted by the RNNG) and distance are both significant factors in a mixed effects regression which predicts the P600, while the surprisal as computed by an LSTM is not. Unlike BIBREF0 and BIBREF7 , we do not use a linking function (e.g. surprisal) to relate a language model to ERPs. We thus lose the interpretability provided by the linking function, but we are able to predict a significant proportion of the variance for all of the ERP components, where prior work could not. We interpret our results through characterization of the ERPs in terms of how they relate to each other and to eye-tracking data rather than through a linking function. The authors in BIBREF8 also use a recurrent neural network to predict neural activity directly. In that work the authors predict magnetoencephalography (MEG) activity, a close cousin to EEG, recorded while participants read a chapter of Harry Potter and the Sorcerer’s Stone BIBREF9 . Their approach to characterization of processing at each MEG sensor location is to determine whether it is best predicted by the context vector of the recurrent network (prior to the current word being processed), the embedding of the current word, or the probability of the current word given the context. In future work we also intend to add these types of studies to the ERP predictions. Discussion This work is most closely related to the paper from which we get the ERP data: BIBREF0 . In that work, the authors relate the surprisal of a word, i.e. the (negative log) probability of the word appearing in its context, to each of the ERP signals we consider here. The authors do not directly train a model to predict ERPs. Instead, models of the probability distribution of each word in context are used to compute a surprisal for each word, which is input into a mixed effects regression along with word frequency, word length, word position in the sentence, and sentence position in the experiment. The effect of the surprisal is assessed using a likelihood-ratio test. In BIBREF7 , the authors take an approach similar to BIBREF0 . The authors compare the explanatory power of surprisal (as computed by an LSTM or a Recurrent Neural Network Grammar (RNNG) language model) to a measure of syntactic complexity they call “distance"" that counts the number of parser actions in the RNNG language model. The authors find that surprisal (as predicted by the RNNG) and distance are both significant factors in a mixed effects regression which predicts the P600, while the surprisal as computed by an LSTM is not. Unlike BIBREF0 and BIBREF7 , we do not use a linking function (e.g. surprisal) to relate a language model to ERPs. We thus lose the interpretability provided by the linking function, but we are able to predict a significant proportion of the variance for all of the ERP components, where prior work could not. We interpret our results through characterization of the ERPs in terms of how they relate to each other and to eye-tracking data rather than through a linking function. The authors in BIBREF8 also use a recurrent neural network to predict neural activity directly. In that work the authors predict magnetoencephalography (MEG) activity, a close cousin to EEG, recorded while participants read a chapter of Harry Potter and the Sorcerer’s Stone BIBREF9 . Their approach to characterization of processing at each MEG sensor location is to determine whether it is best predicted by the context vector of the recurrent network (prior to the current word being processed), the embedding of the current word, or the probability of the current word given the context. In future work we also intend to add these types of studies to the ERP predictions.","Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.
Select:
- ERP data collected and computed by Frank et al. (2015)
- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) ","The embeddings display clustering properties, with words that are semantically similar being closer together in the vector space, and words that are semantically dissimilar being farther apart",0.1212121167676769,0.0,0.0,0.8936641645664332,22.50845169961937,18.700131193374546,0.0266524520255863,0.0035400550675232,0.1375411748886108,0.4714756936632577,0.1560234874486923,0.1943666487932205,-0.902208685874939,0.0360576026141643,0.010884398527423,1,,0.7380843881348754,0.1795650351748954,0.7182601406995816,0.0,0.0,0.0,6.048735810445668
which datasets did they experiment with?,"We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted. All UD1.2 corpora use a common tag set, the 17 universal PoS tags, which is an extension of the tagset proposed by BIBREF43 . As our goal is to study the impact of lexical information for PoS tagging, we have restricted our experiments to UD1.2 corpora that cover languages for which we have morphosyntactic lexicons at our disposal, and for which BIBREF20 provide results. We considered UD1.2 corpora for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. Although this language list contains only one non-Indo-European (Indonesian), four major Indo-European sub-families are represented (Germanic, Romance, Slavic, Indo-Iranian). Overall, the 16 languages considered in our experiments are typologically, morphologically and syntactically fairly diverse. We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted. All UD1.2 corpora use a common tag set, the 17 universal PoS tags, which is an extension of the tagset proposed by BIBREF43 .","Universal Dependencies v1.2 treebanks for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German,
Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish ","IWSLT14 German-English, Turkish-English, and WMT14 English-German",0.0606060576308541,0.0,0.0,1.9329400319270489,23.03215417957593,20.87930245497984,0.0662061024755325,0.0023023791250959,0.5377635955810547,0.0,0.5377634763717651,0.0,-0.7697497606277466,0.0354260206222534,0.0069422457150966,1,,0.7488089175021876,0.188771823227534,0.755087292910136,0.0,0.0,0.0,6.415413864114581
What kind of celebrities do they obtain tweets from?,"FLOAT SELECTED: Table 1: Twitter celebrities in our dataset, with tweet counts before and after filtering (Foll. denotes followers in millions) FLOAT SELECTED: Table 1: Twitter celebrities in our dataset, with tweet counts before and after filtering (Foll. denotes followers in millions)","Amitabh Bachchan, Ariana Grande, Barack Obama, Bill Gates, Donald Trump,
Ellen DeGeneres, J K Rowling, Jimmy Fallon, Justin Bieber, Kevin Durant, Kim Kardashian, Lady Gaga, LeBron James,Narendra Modi, Oprah Winfrey Celebrities from varioius domains - Acting, Music, Politics, Business, TV, Author, Sports, Modeling. ",They obtain word lattices from words by treating each word as a vertex and connecting it to its neighboring words according to their positions in the original sentence,0.0289855026254995,0.0,0.0,0.5482271666382152,21.69072925423269,16.625046318478404,0.0080385852090032,0.0064695009242144,-0.0731318891048431,0.3845905173544915,-0.0731318891048431,0.0646894797682762,-1.4885337352752686,0.0464129783213138,0.0045779229168666,1,0.75,0.7454959736108954,0.1775588750109734,0.7102355000438936,0.0,0.0,0.0,5.553750599666864
How did they extend LAMA evaluation framework to focus on negation?,"This work analyzes the understanding of pretrained language models of factual and commonsense knowledge stored in negated statements. To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., “not”) in LAMA cloze statement (e.g., “The theory of relativity was not developed by [MASK].”). In our experiments, we query the pretrained language models with both original LAMA and negated LAMA statements and compare their predictions in terms of rank correlation and overlap of top predictions. We find that the predicted filler words often have high overlap. Thus, negating a cloze statement does not change the predictions in many cases – but of course it should as our example “birds can fly” vs. “birds cannot fly” shows. We identify and analyze a subset of cloze statements where predictions are different. We find that BERT handles negation best among pretrained language models, but it still fails badly on most negated statements. This work analyzes the understanding of pretrained language models of factual and commonsense knowledge stored in negated statements. To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., “not”) in LAMA cloze statement (e.g., “The theory of relativity was not developed by [MASK].”). In our experiments, we query the pretrained language models with both original LAMA and negated LAMA statements and compare their predictions in terms of rank correlation and overlap of top predictions. We find that the predicted filler words often have high overlap. Thus, negating a cloze statement does not change the predictions in many cases – but of course it should as our example “birds can fly” vs. “birds cannot fly” shows. We identify and analyze a subset of cloze statements where predictions are different. We find that BERT handles negation best among pretrained language models, but it still fails badly on most negated statements.", Create the negated LAMA dataset and  query the pretrained language models with both original LAMA and negated LAMA statements and compare their predictions.,"BERT, RoBERTa, and DistilBERT",0.0952380921541951,0.0,0.0,1.5732934811145336,5.320575559745698,5.308865513777591,0.0225225225225225,0.0018148820326678,0.1431903392076492,0.1847885936498642,0.1431903392076492,0.1369131654500961,-0.8798326253890991,0.0489786155521869,0.001004595219136,1,0.0,0.7423576849369576,0.1920963535931614,0.7683854143726458,0.0,0.0,0.0,1.5447623393442247
What summarization algorithms did the authors experiment with?,"We considered a dataset of 100 employees, where for each employee multiple peer comments were recorded. Also, for each employee, a manual summary was generated by an HR personnel. The summaries generated by our ILP-based approach were compared with the corresponding manual summaries using the ROUGE BIBREF22 unigram score. For comparing performance of our ILP-based summarization algorithm, we explored a few summarization algorithms provided by the Sumy package. A common parameter which is required by all these algorithms is number of sentences keep in the final summary. ILP-based summarization requires a similar parameter K, which is automatically decided based on number of total candidate phrases. Assuming a sentence is equivalent to roughly 3 phrases, for Sumy algorithms, we set number of sentences parameter to the ceiling of K/3. Table TABREF51 shows average and standard deviation of ROUGE unigram f1 scores for each algorithm, over the 100 summaries. The performance of ILP-based summarization is comparable with the other algorithms, as the two sample t-test does not show statistically significant difference. Also, human evaluators preferred phrase-based summary generated by our approach to the other sentence-based summaries. FLOAT SELECTED: Table 9. Comparative performance of various summarization algorithms FLOAT SELECTED: Table 9. Comparative performance of various summarization algorithms We considered a dataset of 100 employees, where for each employee multiple peer comments were recorded. Also, for each employee, a manual summary was generated by an HR personnel. The summaries generated by our ILP-based approach were compared with the corresponding manual summaries using the ROUGE BIBREF22 unigram score. For comparing performance of our ILP-based summarization algorithm, we explored a few summarization algorithms provided by the Sumy package. A common parameter which is required by all these algorithms is number of sentences keep in the final summary. ILP-based summarization requires a similar parameter K, which is automatically decided based on number of total candidate phrases. Assuming a sentence is equivalent to roughly 3 phrases, for Sumy algorithms, we set number of sentences parameter to the ceiling of K/3. Table TABREF51 shows average and standard deviation of ROUGE unigram f1 scores for each algorithm, over the 100 summaries. The performance of ILP-based summarization is comparable with the other algorithms, as the two sample t-test does not show statistically significant difference. Also, human evaluators preferred phrase-based summary generated by our approach to the other sentence-based summaries.","LSA, TextRank, LexRank and ILP-based summary. LSA, TextRank, LexRank",English and German,0.2222222177777778,0.0,0.0,2.839838722567789,10.25491352411828,10.113107890043551,0.0387596899224806,0.0037359900373599,0.1700280457735061,0.1359960181372506,0.1890036910772323,,-1.4831113815307615,0.043669257313013,0.0050947468110203,1,0.0,0.7406599918319166,0.5919743354776493,0.7315337055469612,0.0,0.0,0.0,3.0117983495979526
What methods were used for sentence classification?,"We randomly selected 2000 sentences from the supervisor assessment corpus and manually tagged them (dataset D1). This labelled dataset contained 705, 103, 822 and 370 sentences having the class labels STRENGTH, WEAKNESS, SUGGESTION or OTHER respectively. We trained several multi-class classifiers on this dataset. Table TABREF10 shows the results of 5-fold cross-validation experiments on dataset D1. For the first 5 classifiers, we used their implementation from the SciKit Learn library in Python (scikit-learn.org). The features used for these classifiers were simply the sentence words along with their frequencies. For the last 2 classifiers (in Table TABREF10 ), we used our own implementation. The overall accuracy for a classifier is defined as INLINEFORM0 , where the denominator is 2000 for dataset D1. Note that the pattern-based approach is unsupervised i.e., it did not use any training data. Hence, the results shown for it are for the entire dataset and not based on cross-validation. FLOAT SELECTED: Table 1. Results of 5-fold cross validation for sentence classification on dataset D1. FLOAT SELECTED: Table 1. Results of 5-fold cross validation for sentence classification on dataset D1. FLOAT SELECTED: Table 7. Results of 5-fold cross validation for multi-class multi-label classification on dataset D2. We manually tagged the same 2000 sentences in Dataset D1 with attributes, where each sentence may get 0, 1, 2, etc. up to 15 class labels (this is dataset D2). This labelled dataset contained 749, 206, 289, 207, 91, 223, 191, 144, 103, 80, 82, 42, 29, 15, 24 sentences having the class labels listed in Table TABREF20 in the same order. The number of sentences having 0, 1, 2, or more than 2 attributes are: 321, 1070, 470 and 139 respectively. We trained several multi-class multi-label classifiers on this dataset. Table TABREF21 shows the results of 5-fold cross-validation experiments on dataset D2. We randomly selected 2000 sentences from the supervisor assessment corpus and manually tagged them (dataset D1). This labelled dataset contained 705, 103, 822 and 370 sentences having the class labels STRENGTH, WEAKNESS, SUGGESTION or OTHER respectively. We trained several multi-class classifiers on this dataset. Table TABREF10 shows the results of 5-fold cross-validation experiments on dataset D1. For the first 5 classifiers, we used their implementation from the SciKit Learn library in Python (scikit-learn.org). The features used for these classifiers were simply the sentence words along with their frequencies. For the last 2 classifiers (in Table TABREF10 ), we used our own implementation. The overall accuracy for a classifier is defined as INLINEFORM0 , where the denominator is 2000 for dataset D1. Note that the pattern-based approach is unsupervised i.e., it did not use any training data. Hence, the results shown for it are for the entire dataset and not based on cross-validation.","Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK and Pattern-based Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK, Pattern-based approach",State-of-the-art performance on both the full test set and the cleaned test set,0.0740740694101512,0.0,0.0,0.8954307276600084,15.63722563819208,12.399985895344932,0.013089005235602,0.0044627531754205,0.153606429696083,0.4458380153824611,0.1536064594984054,0.1088273748755455,-1.5062915086746216,0.0515078343451023,0.0182046843543928,1,0.0,0.7559993847904227,0.4074740528754713,0.772753354359028,0.2,0.0,0.0,4.048668388098134
What modern MRC gold standards are analyzed?,"We select contemporary MRC benchmarks to represent all four commonly used problem definitions BIBREF15. In selecting relevant datasets, we do not consider those that are considered “solved”, i.e. where the state of the art performance surpasses human performance, as is the case with SQuAD BIBREF28, BIBREF7. Concretely, we selected gold standards that fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\ year) \times 20$ citations, and bucket them according to the answer selection styles as described in Section SECREF4 We randomly draw one from each bucket and add two randomly drawn datasets from the candidate pool. This leaves us with the datasets described in Table TABREF19. For a more detailed description, we refer to Appendix . FLOAT SELECTED: Table 1: Summary of selected datasets"," MSMARCO,  HOTPOTQA, RECORD,  MULTIRC, NEWSQA, and DROP.",They use more than just adjacent entity mentions in some cases,0.0,0.0,0.0,0.0,1.6405230498814882,1.2303922874111162,0.0,0.0099009900990099,0.2363712340593338,0.1275672103677476,0.2363711893558502,0.1843992918729782,-1.1154801845550537,0.0473186485469341,0.0015867841222012,1,0.0,0.7695545068705464,0.6028978101175171,0.775227604106432,0.0,0.0,0.0,0.5532835018638964
What language is explored in this paper?,"Figure FIGREF4 pictorially represents our methodology. Our approach required an initial set of informative tweets for which we employed two human annotators annotating a random sub-sample of the original dataset. From the 1500 samples, 326 were marked as informative and 1174 as non informative ($\kappa =0.81$), discriminated on this criteria: Is the tweet addressing any complaint or raising grievances about modes of transport or services/ events associated with transportation such as traffic; public or private transport?. An example tweet marked as informative: No, metro fares will be reduced ???, but proper fare structure needs to presented right, it's bad !!!.", English language,4 domains of ontologies,0.0,0.0,0.0,0.0,7.017543859649122,5.2631578947368425,0.0,0.0099009900990099,0.1682107895612716,0.3120040871641215,0.1682107895612716,,-1.2401103973388672,0.0508065670728683,0.0014633930010006,1,1.0,0.7806368489458763,0.1834909603268323,0.7339638413073294,0.0,0.0,0.0,1.8383987377272428
How long of dialog history is captured?,"The previously proposed contextual language models, such as DRNNLM and CCDCLM, treat dialog history as a sequence of inputs, without modeling dialog interactions. A dialog turn from one speaker may not only be a direct response to the other speaker's query, but also likely to be a continuation of his own previous statement. Thus, when modeling turn $k$ in a dialog, we propose to connect the last RNN state of turn $k-2$ directly to the starting RNN state of turn $k$ , instead of letting it to propagate through the RNN for turn $k-1$ . The last RNN state of turn $k-1$ serves as the context vector to turn $k$ , which is fed to turn $k$ 's RNN hidden state at each time step together with the word input. The model architecture is as shown in Figure 2 . The context vector $c$ and the initial RNN hidden state for the $k$ th turn $h^{\mathbf {U}_k}_{0}$ are defined as: We use the Switchboard Dialog Act Corpus (SwDA) in evaluating our contextual langauge models. The SwDA corpus extends the Switchboard-1 Telephone Speech Corpus with turn and utterance-level dialog act tags. The utterances are also tagged with part-of-speech (POS) tags. We split the data in folder sw00 to sw09 as training set, folder sw10 as test set, and folder sw11 to sw13 as validation set. The training, validation, and test sets contain 98.7K turns (190.0K utterances), 5.7K turns (11.3K utterances), and 11.9K turns (22.2K utterances) respectively. Maximum turn length is set to 160. The vocabulary is defined with the top frequent 10K words.",two previous turns ,"The proposed system achieves an accuracy of [insert accuracy value here, e.g. 95.2%]",0.0,0.0,0.0,0.0,6.062955862894935,4.547216897171201,0.0,0.0099009900990099,0.1171565726399421,0.3224806775306833,0.1374583840370178,0.2050284594297409,-1.2483421564102173,0.069817304611206,0.0066563013894347,1,,0.755545501375743,0.4444687579008277,0.7778750316033108,0.25,0.0,0.0,1.5943611818638994
What was the score of the proposed model?,"To better demonstrate the effectiveness of the proposed model, we compare with baselines and show the results in Table TABREF12 . The baselines are: (a) trained on S-SQuAD, (b) trained on T-SQuAD and then fine-tuned on S-SQuAD, and (c) previous best model trained on S-SQuAD BIBREF5 by using Dr.QA BIBREF20 . We also compare to the approach proposed by Lan et al. BIBREF16 in the row (d). This approach is originally proposed for spoken language understanding, and we adopt the same approach on the setting here. The approach models domain-specific features from the source and target domains separately by two different embedding encoders with a shared embedding encoder for modeling domain-general features. The domain-general parameters are adversarially trained by domain discriminator. FLOAT SELECTED: Table 2. The EM/F1 scores of proposed adversarial domain adaptation approaches over Spoken-SQuAD.",Best results authors obtain is EM 51.10 and F1 63.11 EM Score of 51.10,9 medical coders and 4 medical residents,0.0999999958000001,0.0,0.0,2.839838722567789,13.092582813069944,11.315537655653362,0.037593984962406,0.0053557765876052,0.1042125895619392,0.2083427459001541,0.1042125895619392,0.1446580439805984,-1.0230859518051147,0.0293359905481338,0.0022166531702606,1,,0.768329474477797,0.1916131711254925,0.76645268450197,0.0,0.0,0.0,3.5911327499904777
What hyperparameters are explored?,"FLOAT SELECTED: Table 1: Hyper-parameter choices FLOAT SELECTED: Table 2: Network hyper-parameters To form the vocabulary, words occurring less than 5 times in the corpora were dropped, stop words removed using the natural language toolkit (NLTK) (BIBREF22) and data pre-processing carried out. Table TABREF2 describes most hyper-parameters explored for each dataset. In all, 80 runs (of about 160 minutes) were conducted for the 15MB Wiki Abstract dataset with 80 serialized models totaling 15.136GB while 80 runs (for over 320 hours) were conducted for the 711MB SW dataset, with 80 serialized models totaling over 145GB. Experiments for all combinations for 300 dimensions were conducted on the 3.9GB training set of the BW corpus and additional runs for other dimensions for the window 8 + skipgram + heirarchical softmax combination to verify the trend of quality of word vectors as dimensions are increased. FLOAT SELECTED: Table 1: Hyper-parameter choices","Dimension size, window size, architecture, algorithm, epochs, hidden dimension size, learning rate, loss function, optimizer algorithm. Hyperparameters explored were: dimension size, window size, architecture, algorithm and epochs.",90.1%,0.0,0.0,0.0,0.0,0.4444444444444445,0.3174603174603174,0.0,0.0003702332469455,-0.0361023135483264,0.0,0.0136200636625289,,-1.1026283502578735,0.0348758697509765,0.0019251225411704,1,,0.779362915857314,0.1832801434471785,0.7331205737887141,0.0,0.0,0.0,0.4139966136333832
what aspects of conversation flow do they look at?,"The flow of talking points . A side can either promote its own talking points , address its opponent's points, or steer away from these initially salient ideas altogether. We quantify the use of these strategies by comparing the airtime debaters devote to talking points . For a side INLINEFORM0 , let the self-coverage INLINEFORM1 be the fraction of content words uttered by INLINEFORM2 in round INLINEFORM3 that are among their own talking points INLINEFORM4 ; and the opponent-coverage INLINEFORM5 be the fraction of its content words covering opposing talking points INLINEFORM6 . Conversation flow features. We use all conversational features discussed above. For each side INLINEFORM0 we include INLINEFORM1 , INLINEFORM2 , and their sum. We also use the drop in self-coverage given by subtracting corresponding values for INLINEFORM3 , and the number of discussion points adopted by each side. We call these the Flow features. In this work we introduce a computational framework for characterizing debates in terms of conversational flow. This framework captures two main debating strategies—promoting one's own points and attacking the opponents' points—and tracks their relative usage throughout the debate. By applying this methodology to a setting where debate winners are known, we show that conversational flow patterns are predictive of which debater is more likely to persuade an audience.","The time devoted to self-coverage, opponent-coverage, and the number of adopted discussion points. ",English-German and English-French,0.117647055916955,0.0,0.0,2.445593937240363,12.370156210365058,11.699963383520304,0.0340136054421768,0.0024937655860349,0.0939449369907379,0.0,0.0939449369907379,,-1.280431628227234,0.0432422906160354,0.0071498608497916,1,0.2,0.7303380530354001,0.2028825832877508,0.8115303331510033,0.0,0.0,0.0,3.5370797801722165
what is the state of the art?,"FLOAT SELECTED: Table 2: Comparison of annotators trained for common English news texts (micro-averaged scores on match per annotation span). The table shows micro-precision, recall and NER-style F1 for CoNLL2003, KORE50, ACE2004 and MSNBC datasets.","Babelfy, DBpedia Spotlight, Entityclassifier.eu, FOX, LingPipe MUC-7, NERD-ML, Stanford NER, TagMe 2",The datasets have 4 labels in total,0.0,0.0,0.0,0.0,16.040458853549644,12.030344140162232,0.0,0.0057995028997514,0.1505949050188064,0.0968150470906228,0.1505948901176452,0.0715313628315925,-1.2414981126785278,0.0629089102149009,0.0037860239366306,1,,0.7589778214603852,0.181350000524421,0.725511770390399,0.0,0.0,0.0,4.167649977541633
What is the Random Kitchen Sink approach?,"RKS approach proposed in BIBREF21, BIBREF22, explicitly maps data vectors to a space where linear separation is possible. It has been explored for natural language processing tasks BIBREF23, BIBREF24. The RKS method provides an approximate kernel function via explicit mapping. RKS approach proposed in BIBREF21, BIBREF22, explicitly maps data vectors to a space where linear separation is possible. It has been explored for natural language processing tasks BIBREF23, BIBREF24. The RKS method provides an approximate kernel function via explicit mapping. Here, $\phi (.)$ denotes the implicit mapping function (used to compute kernel matrix), $Z(.)$ denotes the explicit mapping function using RKS and ${\Omega _k}$ denotes random variable .",Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible. ,"Human evaluation was performed using pairwise comparison for dialogue quality, with two criteria: Plausibility and Content Richness",0.0,0.0,0.0,0.0,19.94954906608327,14.962161799562452,0.0240384615384615,0.008428358948934,0.0207116790115833,0.4617824492055246,0.0207116734236478,0.3320710062980652,-0.7611111402511597,0.052177544683218,0.0042377484247754,1,0.0,0.7172650667647206,0.682991917628453,0.7319676705138122,0.0,0.0,0.0,5.053379395297871
How do they define similar equations?,"In addition to words, EqEmb models can capture the semantic similarity between equations in the collection. We performed qualitative analysis of the model performance using all discovered equations across the 4 collection. Table TABREF24 shows the query equation used in the previous analysis and its 5 most similar equations discovered using EqEmb-U. For qualitative comparisons across the other embedding models, in Appendix A we provide results over the same query using CBOW, PV-DM, GloVe and EqEmb. In Appendix A reader should notice the difference in performance between EqEmb-U and EqEmb compared to existing embedding models which fail to discover semantically similar equations. tab:irexample1,tab:nlpexample2 show two additional example equation and its 5 most similar equations and words discovered using the EqEmb model. Similar words were ranked by computing Cosine distance between the embedding vector ( INLINEFORM0 ) representation of the query equation and the context vector representation of the words ( INLINEFORM1 ). Similar equations were discovered using Euclidean distance computed between the context vector representations of the equations ( INLINEFORM2 ). We give additional example results in Appendix B. In addition to words, EqEmb models can capture the semantic similarity between equations in the collection. We performed qualitative analysis of the model performance using all discovered equations across the 4 collection. Table TABREF24 shows the query equation used in the previous analysis and its 5 most similar equations discovered using EqEmb-U. For qualitative comparisons across the other embedding models, in Appendix A we provide results over the same query using CBOW, PV-DM, GloVe and EqEmb. In Appendix A reader should notice the difference in performance between EqEmb-U and EqEmb compared to existing embedding models which fail to discover semantically similar equations. tab:irexample1,tab:nlpexample2 show two additional example equation and its 5 most similar equations and words discovered using the EqEmb model. Similar words were ranked by computing Cosine distance between the embedding vector ( INLINEFORM0 ) representation of the query equation and the context vector representation of the words ( INLINEFORM1 ). Similar equations were discovered using Euclidean distance computed between the context vector representations of the equations ( INLINEFORM2 ). We give additional example results in Appendix B.",By using Euclidean distance computed between the context vector representations of the equations ,"The authors conclude that both ISIS and Catholic materials use similar emotional appeals to inspire readers, despite the vastly different content and context of the two corpora",0.1621621577794011,0.0526315746260391,0.0526315746260391,2.317416861836008,19.3691554072369,17.05610725012333,0.256551724137931,0.0111248454882571,0.0279175173491239,0.4238511046893161,0.0279175229370594,0.3464869558811188,-1.0377652645111084,0.0607083588838577,0.0063504968733028,1,0.0,0.7375636890290989,0.328506686514312,0.7686373485640723,0.0,0.0,0.0,5.216186235576876
What are the three steps to feature elimination?,"Feature elimination strategies are often taken 1) to remove irrelevant or noisy features, 2) to improve classifier performance, and 3) to reduce training and run times. We conducted an experiment to determine whether we could maintain or improve classifier performances by applying the following three-tiered feature elimination approach: Reduction We reduced the dataset encoded for each class by eliminating features that occur less than twice in the full dataset. Selection We iteratively applied Chi-Square feature selection on the reduced dataset, selecting the top percentile of highest ranked features in increments of 5 percent to train and test the support vector model using a linear kernel and 5-fold, stratified cross-validation. Rank We cumulatively plotted the average F1-score performances of each incrementally added percentile of top ranked features. We report the percentile and count of features resulting in the first occurrence of the highest average F1-score for each class. Feature elimination strategies are often taken 1) to remove irrelevant or noisy features, 2) to improve classifier performance, and 3) to reduce training and run times. We conducted an experiment to determine whether we could maintain or improve classifier performances by applying the following three-tiered feature elimination approach: Reduction We reduced the dataset encoded for each class by eliminating features that occur less than twice in the full dataset. Selection We iteratively applied Chi-Square feature selection on the reduced dataset, selecting the top percentile of highest ranked features in increments of 5 percent to train and test the support vector model using a linear kernel and 5-fold, stratified cross-validation. Rank We cumulatively plotted the average F1-score performances of each incrementally added percentile of top ranked features. We report the percentile and count of features resulting in the first occurrence of the highest average F1-score for each class."," reduced the dataset by eliminating features, apply feature selection to select highest ranked features to train and test the model and rank the performance of incrementally adding features.",Manually created,0.0,0.0,0.0,0.0,11.07687932816755,8.307659496125662,0.0,0.0007137758743754,0.1819489896297454,0.5663468241691589,0.1819489896297454,,-0.8706600666046143,0.0525011755526065,0.0039980126681061,1,,0.7398467367895254,0.182500694437336,0.730002777749344,0.0,0.0,0.0,2.9292070004483666
How is the dataset annotated?,"Specifically, we conducted a feature ablation study to assess the informativeness of each feature group and a feature elimination study to determine the optimal feature sets for classifying Twitter tweets. We leveraged an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13 . The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression"") or evidence of depression (e.g., “depressed over disappointment""). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., “feeling down in the dumps""), disturbed sleep (e.g., “another restless night""), or fatigue or loss of energy (e.g., “the fatigue is unbearable"") BIBREF10 . For each class, every annotation (9,473 tweets) is binarized as the positive class e.g., depressed mood=1 or negative class e.g., not depressed mood=0. Specifically, we conducted a feature ablation study to assess the informativeness of each feature group and a feature elimination study to determine the optimal feature sets for classifying Twitter tweets. We leveraged an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13 . The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression"") or evidence of depression (e.g., “depressed over disappointment""). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., “feeling down in the dumps""), disturbed sleep (e.g., “another restless night""), or fatigue or loss of energy (e.g., “the fatigue is unbearable"") BIBREF10 . For each class, every annotation (9,473 tweets) is binarized as the positive class e.g., depressed mood=1 or negative class e.g., not depressed mood=0.", The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression,"The message passing framework is a method for learning on graph-structured data, in which the representation of each vertex is updated based on messages received from its neighbors",0.2727272678925621,0.0444444396444449,0.0444444396444449,3.265885464333843,24.833843504883543,21.83639658209988,0.1955912334352701,0.0115321252059308,0.1768074482679367,0.4272532375795499,0.1768074631690979,0.1925765872001648,-0.845301628112793,,0.0070628534923753,1,0.0,0.7409748420198286,0.6462471195914293,0.7388346322118712,0.0,0.0,0.0,6.818881977106862
Which architecture do they use for the encoder and decoder?,"In this work, we use the raw waveform as the input representation instead of spectral-based features and a grapheme (character) sequence as the output representation. In contrast to most encoder-decoder architectures, which are purely based on recurrent neural network (RNNs) framework, we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part. We use convolutional layers because they are suitable for extracting local information from raw speech. We use a striding mechanism to reduce the dimension from the input frames BIBREF17 , while the NIN layer represents more complex structures on the top of the convolutional layers. On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP) as described in Eq. EQREF2 . For more details, we illustrate our architecture in Figure FIGREF4 . On the top layers of the encoder after the transferred convolutional and NIN layers, we put three bidirectional LSTMs (Bi-LSTM) with 256 hidden units (total 512 units for both directions). To reduce the computational time, we used hierarchical subsampling BIBREF21 , BIBREF22 , BIBREF10 . We applied subsampling on all the Bi-LSTM layers and reduced the length by a factor of 8. On the decoder side, the previous input phonemes / characters were converted into real vectors by a 128-dimensional embedding matrix. We used one unidirectional LSTM with 512 hidden units and followed by a softmax layer to output the character probability. For the end-to-end training phase, we froze the parameter values from the transferred layers from epoch 0 to epoch 10, and after epoch 10 we jointly optimized all the parameters together until the end of training (a total 40 epochs). We used an Adam BIBREF23 optimizer with a learning rate of 0.0005."," In encoder they use convolutional, NIN and bidirectional LSTM layers and in decoder they use unidirectional LSTM ","They use three datasets: imdb400, yelp50, and yelp200",0.1904761857596373,0.0,0.0,2.8265205879007453,16.913058258138282,15.743800307986737,0.1824421965317919,0.0056818181818181,0.3516879677772522,0.2642202178137985,0.3516879677772522,0.0752573162317276,-0.6185821294784546,0.0480732955038547,0.0079732382626041,1,0.0,0.8159806201558218,0.4740917728912925,0.6963670915651702,0.0,0.0,0.0,4.660330671138992
How does their decoder generate text?,"where INLINEFORM0 , INLINEFORM1 is the number of hidden units for the encoder and INLINEFORM2 is the number of hidden units for the decoder. Finally, the decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information INLINEFORM4 , can be formulated as: DISPLAYFORM0 where INLINEFORM0 , INLINEFORM1 is the number of hidden units for the encoder and INLINEFORM2 is the number of hidden units for the decoder. Finally, the decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information INLINEFORM4 , can be formulated as: DISPLAYFORM0 The most common input INLINEFORM0 for speech recognition tasks is a sequence of feature vectors such as log Mel-spectral spectrogram and/or MFCC. Therefore, INLINEFORM1 where D is the number of the features and S is the total length of the utterance in frames. The output INLINEFORM2 can be either phoneme or grapheme (character) sequence. In the decoding phase, we used a beam search strategy with beam size INLINEFORM0 and we adjusted the score by dividing with the transcription length to prevent the decoder from favoring shorter transcriptions. We did not use any language model or lexicon dictionary for decoding. All of our models were implemented on the PyTorch framework .", Decoder predicts the sequence of phoneme or grapheme at each time based on the previous output and context information with a beam search strategy,"The model used in the experiments is a pre-trained BERT model, but without any task-specific fine-tuning",0.102564097725181,0.0,0.0,1.8709718017288024,22.581156061739303,18.29465355573013,0.0643776824034334,0.006908462867012,0.3039360046386719,0.3588949174540383,0.3039359748363495,0.2916242480278015,-0.6976678371429443,0.0470186285674572,0.0100997520226035,1,0.0,0.7166560856751804,0.1916829403750257,0.7667317615001028,0.0,0.0,0.1,5.869395305284243
What dataset is used to train the model?,"A total of 2,50,000 tweets over a period of August 31st, 2015 to August 25th,2016 on Microsoft are extracted from twitter API BIBREF15 . Twitter4J is a java application which helps us to extract tweets from twitter. The tweets were collected using Twitter API and filtered using keywords like $ MSFT, # Microsoft, #Windows etc. Not only the opinion of public about the company's stock but also the opinions about products and services offered by the company would have a significant impact and are worth studying. Based on this principle, the keywords used for filtering are devised with extensive care and tweets are extracted in such a way that they represent the exact emotions of public about Microsoft over a period of time. The news on twitter about Microsoft and tweets regarding the product releases were also included. Stock opening and closing prices of Microsoft from August 31st, 2015 to August 25th, 2016 are obtained from Yahoo! Finance BIBREF16 . A total of 2,50,000 tweets over a period of August 31st, 2015 to August 25th,2016 on Microsoft are extracted from twitter API BIBREF15 . Twitter4J is a java application which helps us to extract tweets from twitter. The tweets were collected using Twitter API and filtered using keywords like $ MSFT, # Microsoft, #Windows etc. Not only the opinion of public about the company's stock but also the opinions about products and services offered by the company would have a significant impact and are worth studying. Based on this principle, the keywords used for filtering are devised with extensive care and tweets are extracted in such a way that they represent the exact emotions of public about Microsoft over a period of time. The news on twitter about Microsoft and tweets regarding the product releases were also included. Stock opening and closing prices of Microsoft from August 31st, 2015 to August 25th, 2016 are obtained from Yahoo! Finance BIBREF16 .", Collected tweets and opening and closing stock prices of Microsoft.,The authors crawled over 2M tweets from Twitter to build a dataset of ironic and non-ironic tweets,0.2399999953920001,0.0,0.0,2.8586838330063986,16.77083176229889,14.953382497853692,0.1293103448275862,0.011206328279499,0.3905206620693207,0.5098232148624048,0.3905206322669983,0.264254093170166,-1.072052240371704,0.105057418346405,0.0085679317758397,1,,0.7054809303949338,0.3434613596771293,0.7071301793133387,0.0,0.0,0.0,4.630585045901435
Which race and gender are given higher sentiment intensity predictions?,"When predicting anger, joy, or valence, the number of systems consistently giving higher scores to sentences with female noun phrases (21–25) is markedly higher than the number of systems giving higher scores to sentences with male noun phrases (8–13). (Recall that higher valence means more positive sentiment.) In contrast, on the fear task, most submissions tended to assign higher scores to sentences with male noun phrases (23) as compared to the number of systems giving higher scores to sentences with female noun phrases (12). When predicting sadness, the number of submissions that mostly assigned higher scores to sentences with female noun phrases (18) is close to the number of submissions that mostly assigned higher scores to sentences with male noun phrases (16). These results are in line with some common stereotypes, such as females are more emotional, and situations involving male agents are more fearful BIBREF27 . The majority of the systems assigned higher scores to sentences with African American names on the tasks of anger, fear, and sadness intensity prediction. On the joy and valence tasks, most submissions tended to assign higher scores to sentences with European American names. These tendencies reflect some common stereotypes that associate African Americans with more negative emotions BIBREF28 . When predicting anger, joy, or valence, the number of systems consistently giving higher scores to sentences with female noun phrases (21–25) is markedly higher than the number of systems giving higher scores to sentences with male noun phrases (8–13). (Recall that higher valence means more positive sentiment.) In contrast, on the fear task, most submissions tended to assign higher scores to sentences with male noun phrases (23) as compared to the number of systems giving higher scores to sentences with female noun phrases (12). When predicting sadness, the number of submissions that mostly assigned higher scores to sentences with female noun phrases (18) is close to the number of submissions that mostly assigned higher scores to sentences with male noun phrases (16). These results are in line with some common stereotypes, such as females are more emotional, and situations involving male agents are more fearful BIBREF27 . The majority of the systems assigned higher scores to sentences with African American names on the tasks of anger, fear, and sadness intensity prediction. On the joy and valence tasks, most submissions tended to assign higher scores to sentences with European American names. These tendencies reflect some common stereotypes that associate African Americans with more negative emotions BIBREF28 .","Females are given higher sentiment intensity when predicting anger, joy or valence, but males are given higher sentiment intensity when predicting  fear.
African American names are given higher score on the tasks of anger, fear, and sadness intensity prediction,  but European American names are given higher scores on joy and valence task. ",The spatial aspect of the EEG signal was computed using a four-layered 2D CNN,0.0869565175047261,0.0,0.0,0.7313525558399564,13.873965801926651,11.49356699339836,0.018348623853211,0.0027375831052014,0.0704972967505455,0.3568270131562547,0.1056543588638305,0.2932590842247009,-1.22954261302948,0.051676120609045,0.006499941482604,1,,0.7094885255707827,0.3958534925945862,0.7262711132354877,0.3333333333333333,0.0,0.0,3.7217288375638495
"What criteria are used to select the 8,640 English sentences?","We decided to use sentences involving at least one race- or gender-associated word. The sentences were intended to be short and grammatically simple. We also wanted some sentences to include expressions of sentiment and emotion, since the goal is to test sentiment and emotion systems. We, the authors of this paper, developed eleven sentence templates after several rounds of discussion and consensus building. They are shown in Table TABREF3 . The templates are divided into two groups. The first type (templates 1–7) includes emotion words. The purpose of this set is to have sentences expressing emotions. The second type (templates 8–11) does not include any emotion words. The purpose of this set is to have non-emotional (neutral) sentences. We generated sentences from the templates by replacing INLINEFORM0 person INLINEFORM1 and INLINEFORM2 emotion word INLINEFORM3 variables with the values they can take. In total, 8,640 sentences were generated with the various combinations of INLINEFORM4 person INLINEFORM5 and INLINEFORM6 emotion word INLINEFORM7 values across the eleven templates. We manually examined the sentences to make sure they were grammatically well-formed. Notably, one can derive pairs of sentences from the EEC such that they differ only in one word corresponding to gender or race (e.g., `My daughter feels devastated' and `My son feels devastated'). We refer to the full set of 8,640 sentences as Equity Evaluation Corpus.","Sentences involving at least one race- or gender-associated word,  sentence  have to be short and grammatically simple,  sentence have to  include expressions of sentiment and emotion. ",20 evaluators were recruited from the institution,0.0,0.0,0.0,0.0,15.394682554976589,11.54601191623244,0.0186567164179104,0.0026850786344457,0.1185787320137023,0.3831186243172348,0.1185787245631218,0.1084852367639541,-1.0264809131622314,0.04037307202816,0.002582001595432,1,,0.8241906720143758,0.7867093751211955,0.7468375004847818,0.0,0.0,0.0,3.9840058506159157
what were the baselines?,"We also evaluate the Jasper model's performance on a conversational English corpus. The Hub5 Year 2000 (Hub5'00) evaluation (LDC2002S09, LDC2005S13) is widely used in academia. It is divided into two subsets: Switchboard (SWB) and Callhome (CHM). The training data for both the acoustic and language models consisted of the 2000hr Fisher+Switchboard training data (LDC2004S13, LDC2005S13, LDC97S62). Jasper DR 10x5 was trained using SGD with momentum for 50 epochs. We compare to other models trained using the same data and report Hub5'00 results in Table TABREF31 . FLOAT SELECTED: Table 7: Hub5’00, WER (%)"," LF-MMI Attention
Seq2Seq 
RNN-T 
Char E2E LF-MMI 
Phone E2E LF-MMI 
CTC + Gram-CTC","The tweets were annotated with stance information for the two target clubs (Galatasaray and Fenerbahçe) using the classes Favor and Against, but not Neither",0.0,0.0,0.0,0.0,5.173817086505226,3.88036281487892,0.0,0.0099009900990099,0.0572435148060321,0.1392776603284089,0.0572435110807418,0.0849896147847175,-1.23724627494812,0.0432800389826297,0.0038075138077495,1,0.0,0.7154145523219547,0.5604224606956988,0.7416898427827953,0.0,0.0,0.0,1.3531808488550023
what competitive results did they obtain?,"We trained a smaller Jasper 10x3 model with SGD with momentum optimizer for 400 epochs on a combined WSJ dataset (80 hours): LDC93S6A (WSJ0) and LDC94S13A (WSJ1). The results are provided in Table TABREF29 . FLOAT SELECTED: Table 6: WSJ End-to-End Models, WER (%) FLOAT SELECTED: Table 7: Hub5’00, WER (%) We also evaluate the Jasper model's performance on a conversational English corpus. The Hub5 Year 2000 (Hub5'00) evaluation (LDC2002S09, LDC2005S13) is widely used in academia. It is divided into two subsets: Switchboard (SWB) and Callhome (CHM). The training data for both the acoustic and language models consisted of the 2000hr Fisher+Switchboard training data (LDC2004S13, LDC2005S13, LDC97S62). Jasper DR 10x5 was trained using SGD with momentum for 50 epochs. We compare to other models trained using the same data and report Hub5'00 results in Table TABREF31 . FLOAT SELECTED: Table 6: WSJ End-to-End Models, WER (%) FLOAT SELECTED: Table 7: Hub5’00, WER (%) We trained a smaller Jasper 10x3 model with SGD with momentum optimizer for 400 epochs on a combined WSJ dataset (80 hours): LDC93S6A (WSJ0) and LDC94S13A (WSJ1). The results are provided in Table TABREF29 . We also evaluate the Jasper model's performance on a conversational English corpus. The Hub5 Year 2000 (Hub5'00) evaluation (LDC2002S09, LDC2005S13) is widely used in academia. It is divided into two subsets: Switchboard (SWB) and Callhome (CHM). The training data for both the acoustic and language models consisted of the 2000hr Fisher+Switchboard training data (LDC2004S13, LDC2005S13, LDC97S62). Jasper DR 10x5 was trained using SGD with momentum for 50 epochs. We compare to other models trained using the same data and report Hub5'00 results in Table TABREF31 .","In case of read speech datasets,  their best model got the highest nov93 score of 16.1 and the highest nov92 score of 13.3.
In case of Conversational Speech, their best model got the highest SWB of 8.3 and the highest CHM of 19.3.  On WSJ datasets author's best approach achieves 9.3 and 6.9 WER compared to best results of 7.5 and 4.1 on nov93 and nov92 subsets.
On Hub5'00 datasets author's best approach achieves WER of 7.8 and 16.2 compared to best result of 7.3 and 14.2 on Switchboard (SWB) and Callhome (CHM) subsets.",3,0.0384615380843195,0.0,0.0,0.0,1.1547344110854505,0.5773672055427252,0.0,0.000106371662589,0.0580611452460289,0.0,0.1301862895488739,,-0.6813797950744629,0.0432358980178833,0.000768352988031,1,,0.7135345591240086,0.6861646522006419,0.7446586088025678,0.0,0.0,0.0,0.43240346558320625
By how much is performance improved with multimodality?,"FLOAT SELECTED: Table 1: Speech Embeddings Experiments: Precision/Recall/F1-scores (%) of NLU Models For incorporating speech embeddings experiments, performance results of NLU models on in-cabin data with various feature concatenations can be found in Table TABREF3, using our previous hierarchical joint model (H-Joint-2). When used in isolation, Word2Vec and Speech2Vec achieves comparable performances, which cannot reach GloVe performance. This was expected as the pre-trained Speech2Vec vectors have lower vocabulary coverage than GloVe. Yet, we observed that concatenating GloVe + Speech2Vec, and further GloVe + Word2Vec + Speech2Vec yields better NLU results: F1-score increased from 0.89 to 0.91 for intent recognition, from 0.96 to 0.97 for slot filling. For multimodal (audio & video) features exploration, performance results of the compared models with varying modality/feature concatenations can be found in Table TABREF4. Since these audio/video features are extracted per utterance (on segmented audio & video clips), we experimented with the utterance-level intent recognition task only, using hierarchical joint learning (H-Joint-2). We investigated the audio-visual feature additions on top of text-only and text+speech embedding models. Adding openSMILE/IS10 features from audio, as well as incorporating intermediate CNN/Inception-ResNet-v2 features from video brought slight improvements to our intent models, reaching 0.92 F1-score. These initial results using feature concatenations may need further explorations, especially for certain intent-types such as stop (audio intensity) or relevant slots such as passenger gestures/gaze (from cabin video) and outside objects (from road video).",by 2.3-6.8 points in f1 score for intent recognition and 0.8-3.5 for slot filling F1 score increased from 0.89 to 0.92,"Sure! Based on the context you provided, the answer is:

12 convolutional layers (5 encoder layers with kernel sizes and strides, and 7 context layers with kernel size 3 and stride 1)",0.0425531864916257,0.0,0.0,0.8954605017126219,12.869351171644649,10.01433599891,0.0220264317180616,0.0102171136653895,0.0784946084022522,0.3082517506652756,0.1347272545099258,0.4256259500980377,-0.8757332563400269,0.0729841515421867,0.0033808000166116,1,,0.7118878149080979,0.484744892006985,0.7389795680279401,0.0,0.0,0.0,3.360054924971529
How much is performance improved on NLI?,"Table TABREF21 illustrates the experimental results, showing that our method is beneficial for all of NLI tasks. The improvement on the RTE dataset is significant, i.e., 4% absolute gain over the BERTBase. Besides NLI, our model also performs better than BERTBase in the STS task. The STS tasks are semantically similar to the NLI tasks, and hence able to take advantage of PSP as well. Actually, the proposed method has a positive effect whenever the input is a sentence pair. The improvements suggest that the PSP task encourages the model to learn more detailed semantics in the pre-training, which improves the model on the downstream learning tasks. Moreover, our method is surprisingly able to achieve slightly better results in the single-sentence problem. The improvement should be attributed to better semantic representation. FLOAT SELECTED: Table 2: Results on the test set of GLUE benchmark. The performance was obtained by the official evaluation server. The number below each task is the number of training examples. The ”Average” column follows the setting in the BERT paper, which excludes the problematic WNLI task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. All the listed models are trained on the Wikipedia and the Book Corpus datasets. The results are the average of 5 runs. FLOAT SELECTED: Table 2: Results on the test set of GLUE benchmark. The performance was obtained by the official evaluation server. The number below each task is the number of training examples. The ”Average” column follows the setting in the BERT paper, which excludes the problematic WNLI task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. All the listed models are trained on the Wikipedia and the Book Corpus datasets. The results are the average of 5 runs.", The average score improved by 1.4 points over the previous best result.,LASSO optimization problem,0.0,0.0,0.0,0.0,11.860055319955618,8.895041489966712,0.0,0.0024937655860349,0.0730858445167541,0.2486980648144432,0.0730858519673347,0.119675874710083,-1.1712321043014526,0.0510691553354263,0.000711852963918,1,0.0,0.7218476650914404,0.353174249505211,0.7460303313541775,0.0,0.0,0.0,3.0188708266672215
